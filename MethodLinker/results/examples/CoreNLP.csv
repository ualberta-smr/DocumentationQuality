Example,Extracted Function,Linked Function,Source File,Matched
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}

",main,N/A,N/A,N/A
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}

",Properties,N/A,N/A,N/A
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}

",props.setProperty,N/A,N/A,N/A
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}

",StanfordCoreNLP,StanfordCoreNLP,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}

",CoreDocument,CoreDocument,CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java,False
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}

",pipeline.annotate,StanfordCoreNLP.annotate,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}

",document.tokens,"Sentence.tokens
CoreDocument.tokens
CoreEntityMention.tokens
CoreSentence.tokens","CoreNLP\src\edu\stanford\nlp\simple\Sentence.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreEntityMention.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreSentence.java",False
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}

",get,N/A,N/A,N/A
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}

",out.println,N/A,N/A,N/A
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}

",document.sentences,"Document.sentences
CoreQuote.sentences
CoreDocument.sentences","CoreNLP\src\edu\stanford\nlp\simple\Document.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreQuote.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java",False
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}

",text,N/A,N/A,N/A
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}

",sentence.posTags,"Sentence.posTags
CoreSentence.posTags","CoreNLP\src\edu\stanford\nlp\simple\Sentence.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreSentence.java",False
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}

",sentence.nerTags,"Sentence.nerTags
CoreSentence.nerTags","CoreNLP\src\edu\stanford\nlp\simple\Sentence.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreSentence.java",False
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}

",sentence.constituencyParse,CoreSentence.constituencyParse,CoreNLP\src\edu\stanford\nlp\pipeline\CoreSentence.java,False
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}

",sentence.dependencyParse,CoreSentence.dependencyParse,CoreNLP\src\edu\stanford\nlp\pipeline\CoreSentence.java,False
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}

",relations,N/A,N/A,N/A
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}

",relations.get,N/A,N/A,N/A
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}

",sentence.entityMentions,"CoreSentence.entityMentions
CoreDocument.entityMentions","CoreNLP\src\edu\stanford\nlp\pipeline\CoreSentence.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java",False
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}

",entityMentions,N/A,N/A,N/A
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}

",originalEntityMention.canonicalEntityMention,CoreEntityMention.canonicalEntityMention,CoreNLP\src\edu\stanford\nlp\pipeline\CoreEntityMention.java,False
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}

",document.corefChains,CoreDocument.corefChains,CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java,False
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}

",document.quotes,CoreDocument.quotes,CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java,False
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}

",quotes.get,N/A,N/A,N/A
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}

",quote.speaker,CoreQuote.speaker,CoreNLP\src\edu\stanford\nlp\pipeline\CoreQuote.java,False
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}

",quote.canonicalSpeaker,CoreQuote.canonicalSpeaker,CoreNLP\src\edu\stanford\nlp\pipeline\CoreQuote.java,False
"import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class BasicPipelineExample {

    public static void main(String[] args) {

        // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

        // read some text in the text variable
        String text = ""..."";

        // create an empty Annotation just with the given text
        Annotation document = new Annotation(text);

        // run all Annotators on this text
        pipeline.annotate(document);

    }

}
",main,N/A,N/A,N/A
"import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class BasicPipelineExample {

    public static void main(String[] args) {

        // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

        // read some text in the text variable
        String text = ""..."";

        // create an empty Annotation just with the given text
        Annotation document = new Annotation(text);

        // run all Annotators on this text
        pipeline.annotate(document);

    }

}
",Properties,N/A,N/A,N/A
"import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class BasicPipelineExample {

    public static void main(String[] args) {

        // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

        // read some text in the text variable
        String text = ""..."";

        // create an empty Annotation just with the given text
        Annotation document = new Annotation(text);

        // run all Annotators on this text
        pipeline.annotate(document);

    }

}
",props.setProperty,N/A,N/A,N/A
"import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class BasicPipelineExample {

    public static void main(String[] args) {

        // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

        // read some text in the text variable
        String text = ""..."";

        // create an empty Annotation just with the given text
        Annotation document = new Annotation(text);

        // run all Annotators on this text
        pipeline.annotate(document);

    }

}
",StanfordCoreNLP,StanfordCoreNLP,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class BasicPipelineExample {

    public static void main(String[] args) {

        // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

        // read some text in the text variable
        String text = ""..."";

        // create an empty Annotation just with the given text
        Annotation document = new Annotation(text);

        // run all Annotators on this text
        pipeline.annotate(document);

    }

}
",Annotation,Annotation,CoreNLP\src\edu\stanford\nlp\pipeline\Annotation.java,False
"import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class BasicPipelineExample {

    public static void main(String[] args) {

        // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

        // read some text in the text variable
        String text = ""..."";

        // create an empty Annotation just with the given text
        Annotation document = new Annotation(text);

        // run all Annotators on this text
        pipeline.annotate(document);

    }

}
",pipeline.annotate,StanfordCoreNLP.annotate,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"// build pipeline
StanfordCoreNLP pipeline = new StanfordCoreNLP(
	PropertiesUtils.asProperties(
		""annotators"", ""tokenize,ssplit,pos,lemma,parse,natlog"",
		""ssplit.isOneSentence"", ""true"",
		""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"",
		""tokenize.language"", ""en""));

// read some text in the text variable
String text = ... // Add your text here!
Annotation document = new Annotation(text);

// run all Annotators on this text
pipeline.annotate(document);
",Annotation,Annotation,CoreNLP\src\edu\stanford\nlp\pipeline\Annotation.java,False
"// build pipeline
StanfordCoreNLP pipeline = new StanfordCoreNLP(
	PropertiesUtils.asProperties(
		""annotators"", ""tokenize,ssplit,pos,lemma,parse,natlog"",
		""ssplit.isOneSentence"", ""true"",
		""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"",
		""tokenize.language"", ""en""));

// read some text in the text variable
String text = ... // Add your text here!
Annotation document = new Annotation(text);

// run all Annotators on this text
pipeline.annotate(document);
",pipeline.annotate,"Util.annotate
StanfordCoreNLP.annotate","CoreNLP\src\edu\stanford\nlp\naturalli\Util.java
CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java",False
"// these are all the sentences in this document
// a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
List<CoreMap> sentences = document.get(SentencesAnnotation.class);

for(CoreMap sentence: sentences) {
  // traversing the words in the current sentence
  // a CoreLabel is a CoreMap with additional token-specific methods
  for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
    // this is the text of the token
    String word = token.get(TextAnnotation.class);
    // this is the POS tag of the token
    String pos = token.get(PartOfSpeechAnnotation.class);
    // this is the NER label of the token
    String ne = token.get(NamedEntityTagAnnotation.class);
  }

  // this is the parse tree of the current sentence
  Tree tree = sentence.get(TreeAnnotation.class);

  // this is the Stanford dependency graph of the current sentence
  SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
}

// This is the coreference link graph
// Each chain stores a set of mentions that link to each other,
// along with a method for getting the most representative mention
// Both sentence and token offsets start at 1!
Map<Integer, CorefChain> graph = 
  document.get(CorefChainAnnotation.class);
",document.get,N/A,N/A,N/A
"// these are all the sentences in this document
// a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
List<CoreMap> sentences = document.get(SentencesAnnotation.class);

for(CoreMap sentence: sentences) {
  // traversing the words in the current sentence
  // a CoreLabel is a CoreMap with additional token-specific methods
  for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
    // this is the text of the token
    String word = token.get(TextAnnotation.class);
    // this is the POS tag of the token
    String pos = token.get(PartOfSpeechAnnotation.class);
    // this is the NER label of the token
    String ne = token.get(NamedEntityTagAnnotation.class);
  }

  // this is the parse tree of the current sentence
  Tree tree = sentence.get(TreeAnnotation.class);

  // this is the Stanford dependency graph of the current sentence
  SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
}

// This is the coreference link graph
// Each chain stores a set of mentions that link to each other,
// along with a method for getting the most representative mention
// Both sentence and token offsets start at 1!
Map<Integer, CorefChain> graph = 
  document.get(CorefChainAnnotation.class);
",for,N/A,N/A,N/A
"// these are all the sentences in this document
// a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
List<CoreMap> sentences = document.get(SentencesAnnotation.class);

for(CoreMap sentence: sentences) {
  // traversing the words in the current sentence
  // a CoreLabel is a CoreMap with additional token-specific methods
  for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
    // this is the text of the token
    String word = token.get(TextAnnotation.class);
    // this is the POS tag of the token
    String pos = token.get(PartOfSpeechAnnotation.class);
    // this is the NER label of the token
    String ne = token.get(NamedEntityTagAnnotation.class);
  }

  // this is the parse tree of the current sentence
  Tree tree = sentence.get(TreeAnnotation.class);

  // this is the Stanford dependency graph of the current sentence
  SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
}

// This is the coreference link graph
// Each chain stores a set of mentions that link to each other,
// along with a method for getting the most representative mention
// Both sentence and token offsets start at 1!
Map<Integer, CorefChain> graph = 
  document.get(CorefChainAnnotation.class);
",sentence.get,N/A,N/A,N/A
"// these are all the sentences in this document
// a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
List<CoreMap> sentences = document.get(SentencesAnnotation.class);

for(CoreMap sentence: sentences) {
  // traversing the words in the current sentence
  // a CoreLabel is a CoreMap with additional token-specific methods
  for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
    // this is the text of the token
    String word = token.get(TextAnnotation.class);
    // this is the POS tag of the token
    String pos = token.get(PartOfSpeechAnnotation.class);
    // this is the NER label of the token
    String ne = token.get(NamedEntityTagAnnotation.class);
  }

  // this is the parse tree of the current sentence
  Tree tree = sentence.get(TreeAnnotation.class);

  // this is the Stanford dependency graph of the current sentence
  SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
}

// This is the coreference link graph
// Each chain stores a set of mentions that link to each other,
// along with a method for getting the most representative mention
// Both sentence and token offsets start at 1!
Map<Integer, CorefChain> graph = 
  document.get(CorefChainAnnotation.class);
",token.get,N/A,N/A,N/A
"import edu.stanford.nlp.simple.*;

Sentence sent = new Sentence(""Lucy is in the sky with diamonds."");
List<String> nerTags = sent.nerTags();  // [PERSON, O, O, O, O, O, O, O]
String firstPOSTag = sent.posTag(0);   // NNP
...
",Sentence,Sentence,CoreNLP\src\edu\stanford\nlp\simple\Sentence.java,False
"import edu.stanford.nlp.simple.*;

Sentence sent = new Sentence(""Lucy is in the sky with diamonds."");
List<String> nerTags = sent.nerTags();  // [PERSON, O, O, O, O, O, O, O]
String firstPOSTag = sent.posTag(0);   // NNP
...
",sent.nerTags,Sentence.nerTags,CoreNLP\src\edu\stanford\nlp\simple\Sentence.java,False
"import edu.stanford.nlp.simple.*;

Sentence sent = new Sentence(""Lucy is in the sky with diamonds."");
List<String> nerTags = sent.nerTags();  // [PERSON, O, O, O, O, O, O, O]
String firstPOSTag = sent.posTag(0);   // NNP
...
",sent.posTag,Sentence.posTag,CoreNLP\src\edu\stanford\nlp\simple\Sentence.java,False
"import edu.stanford.nlp.simple.*;

public class SimpleCoreNLPDemo {
    public static void main(String[] args) { 
        // Create a document. No computation is done yet.
        Document doc = new Document(""add your text here! It can contain multiple sentences."");
        for (Sentence sent : doc.sentences()) {  // Will iterate over two sentences
            // We're only asking for words -- no need to load any models yet
            System.out.println(""The second word of the sentence '"" + sent + ""' is "" + sent.word(1));
            // When we ask for the lemma, it will load and run the part of speech tagger
            System.out.println(""The third lemma of the sentence '"" + sent + ""' is "" + sent.lemma(2));
            // When we ask for the parse, it will load and run the parser
            System.out.println(""The parse of the sentence '"" + sent + ""' is "" + sent.parse());
            // ...
        }
    }
}
",main,N/A,N/A,N/A
"import edu.stanford.nlp.simple.*;

public class SimpleCoreNLPDemo {
    public static void main(String[] args) { 
        // Create a document. No computation is done yet.
        Document doc = new Document(""add your text here! It can contain multiple sentences."");
        for (Sentence sent : doc.sentences()) {  // Will iterate over two sentences
            // We're only asking for words -- no need to load any models yet
            System.out.println(""The second word of the sentence '"" + sent + ""' is "" + sent.word(1));
            // When we ask for the lemma, it will load and run the part of speech tagger
            System.out.println(""The third lemma of the sentence '"" + sent + ""' is "" + sent.lemma(2));
            // When we ask for the parse, it will load and run the parser
            System.out.println(""The parse of the sentence '"" + sent + ""' is "" + sent.parse());
            // ...
        }
    }
}
",doc.sentences,Document.sentences,CoreNLP\src\edu\stanford\nlp\simple\Document.java,False
"import edu.stanford.nlp.simple.*;

public class SimpleCoreNLPDemo {
    public static void main(String[] args) { 
        // Create a document. No computation is done yet.
        Document doc = new Document(""add your text here! It can contain multiple sentences."");
        for (Sentence sent : doc.sentences()) {  // Will iterate over two sentences
            // We're only asking for words -- no need to load any models yet
            System.out.println(""The second word of the sentence '"" + sent + ""' is "" + sent.word(1));
            // When we ask for the lemma, it will load and run the part of speech tagger
            System.out.println(""The third lemma of the sentence '"" + sent + ""' is "" + sent.lemma(2));
            // When we ask for the parse, it will load and run the parser
            System.out.println(""The parse of the sentence '"" + sent + ""' is "" + sent.parse());
            // ...
        }
    }
}
",out.println,N/A,N/A,N/A
"import edu.stanford.nlp.simple.*;

public class SimpleCoreNLPDemo {
    public static void main(String[] args) { 
        // Create a document. No computation is done yet.
        Document doc = new Document(""add your text here! It can contain multiple sentences."");
        for (Sentence sent : doc.sentences()) {  // Will iterate over two sentences
            // We're only asking for words -- no need to load any models yet
            System.out.println(""The second word of the sentence '"" + sent + ""' is "" + sent.word(1));
            // When we ask for the lemma, it will load and run the part of speech tagger
            System.out.println(""The third lemma of the sentence '"" + sent + ""' is "" + sent.lemma(2));
            // When we ask for the parse, it will load and run the parser
            System.out.println(""The parse of the sentence '"" + sent + ""' is "" + sent.parse());
            // ...
        }
    }
}
",sent.word,Sentence.word,CoreNLP\src\edu\stanford\nlp\simple\Sentence.java,False
"import edu.stanford.nlp.simple.*;

public class SimpleCoreNLPDemo {
    public static void main(String[] args) { 
        // Create a document. No computation is done yet.
        Document doc = new Document(""add your text here! It can contain multiple sentences."");
        for (Sentence sent : doc.sentences()) {  // Will iterate over two sentences
            // We're only asking for words -- no need to load any models yet
            System.out.println(""The second word of the sentence '"" + sent + ""' is "" + sent.word(1));
            // When we ask for the lemma, it will load and run the part of speech tagger
            System.out.println(""The third lemma of the sentence '"" + sent + ""' is "" + sent.lemma(2));
            // When we ask for the parse, it will load and run the parser
            System.out.println(""The parse of the sentence '"" + sent + ""' is "" + sent.parse());
            // ...
        }
    }
}
",sent.lemma,Sentence.lemma,CoreNLP\src\edu\stanford\nlp\simple\Sentence.java,False
"import edu.stanford.nlp.simple.*;

public class SimpleCoreNLPDemo {
    public static void main(String[] args) { 
        // Create a document. No computation is done yet.
        Document doc = new Document(""add your text here! It can contain multiple sentences."");
        for (Sentence sent : doc.sentences()) {  // Will iterate over two sentences
            // We're only asking for words -- no need to load any models yet
            System.out.println(""The second word of the sentence '"" + sent + ""' is "" + sent.word(1));
            // When we ask for the lemma, it will load and run the part of speech tagger
            System.out.println(""The third lemma of the sentence '"" + sent + ""' is "" + sent.lemma(2));
            // When we ask for the parse, it will load and run the parser
            System.out.println(""The parse of the sentence '"" + sent + ""' is "" + sent.parse());
            // ...
        }
    }
}
",sent.parse,Sentence.parse,CoreNLP\src\edu\stanford\nlp\simple\Sentence.java,False
"Sentence sent = new Sentence(""your text should go here"");
sent.algorithms().headOfSpan(new Span(0, 2));  // Should return 1
",Sentence,Sentence,CoreNLP\src\edu\stanford\nlp\simple\Sentence.java,False
"Sentence sent = new Sentence(""your text should go here"");
sent.algorithms().headOfSpan(new Span(0, 2));  // Should return 1
",sent.algorithms,Sentence.algorithms,CoreNLP\src\edu\stanford\nlp\simple\Sentence.java,False
"Sentence sent = new Sentence(""your text should go here"");
sent.algorithms().headOfSpan(new Span(0, 2));  // Should return 1
",headOfSpan,N/A,N/A,N/A
"Sentence sent = new Sentence(""your text should go here"");
sent.algorithms().headOfSpan(new Span(0, 2));  // Should return 1
",Span,Span,CoreNLP\src\edu\stanford\nlp\ie\machinereading\structure\Span.java,False
"import requests
print(requests.post('http://[::]:9000/?properties={""annotators"":""tokenize,ssplit,pos"",""outputFormat"":""json""}', data = {'data':'The quick brown fox jumped over the lazy dog.'}).text)
",print,N/A,N/A,N/A
"import requests
print(requests.post('http://[::]:9000/?properties={""annotators"":""tokenize,ssplit,pos"",""outputFormat"":""json""}', data = {'data':'The quick brown fox jumped over the lazy dog.'}).text)
",requests.post,N/A,N/A,N/A
"// creates a StanfordCoreNLP object with POS tagging, lemmatization, NER, parsing, and coreference resolution
Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
StanfordCoreNLPClient pipeline = new StanfordCoreNLPClient(props, ""http://localhost"", 9000, 2);
// read some text in the text variable
String text = ... // Add your text here!
// create an empty Annotation just with the given text
Annotation document = new Annotation(text);
// run all Annotators on this text
pipeline.annotate(document);
",Properties,N/A,N/A,N/A
"// creates a StanfordCoreNLP object with POS tagging, lemmatization, NER, parsing, and coreference resolution
Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
StanfordCoreNLPClient pipeline = new StanfordCoreNLPClient(props, ""http://localhost"", 9000, 2);
// read some text in the text variable
String text = ... // Add your text here!
// create an empty Annotation just with the given text
Annotation document = new Annotation(text);
// run all Annotators on this text
pipeline.annotate(document);
",props.setProperty,N/A,N/A,N/A
"// creates a StanfordCoreNLP object with POS tagging, lemmatization, NER, parsing, and coreference resolution
Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
StanfordCoreNLPClient pipeline = new StanfordCoreNLPClient(props, ""http://localhost"", 9000, 2);
// read some text in the text variable
String text = ... // Add your text here!
// create an empty Annotation just with the given text
Annotation document = new Annotation(text);
// run all Annotators on this text
pipeline.annotate(document);
",StanfordCoreNLPClient,StanfordCoreNLPClient,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLPClient.java,False
"// creates a StanfordCoreNLP object with POS tagging, lemmatization, NER, parsing, and coreference resolution
Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
StanfordCoreNLPClient pipeline = new StanfordCoreNLPClient(props, ""http://localhost"", 9000, 2);
// read some text in the text variable
String text = ... // Add your text here!
// create an empty Annotation just with the given text
Annotation document = new Annotation(text);
// run all Annotators on this text
pipeline.annotate(document);
",Annotation,Annotation,CoreNLP\src\edu\stanford\nlp\pipeline\Annotation.java,False
"// creates a StanfordCoreNLP object with POS tagging, lemmatization, NER, parsing, and coreference resolution
Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
StanfordCoreNLPClient pipeline = new StanfordCoreNLPClient(props, ""http://localhost"", 9000, 2);
// read some text in the text variable
String text = ... // Add your text here!
// create an empty Annotation just with the given text
Annotation document = new Annotation(text);
// run all Annotators on this text
pipeline.annotate(document);
",pipeline.annotate,"StanfordCoreNLPClient.annotate
StanfordCoreNLP.annotate","CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLPClient.java
CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java",False
"String text = ""La Universidad de Stanford se encuentra en Palo Alto."";
StanfordCoreNLP pipeline = new StanfordCoreNLP(""spanish"");
CoreDocument doc = pipeline.processToCoreDocument(text);
",StanfordCoreNLP,StanfordCoreNLP,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"String text = ""La Universidad de Stanford se encuentra en Palo Alto."";
StanfordCoreNLP pipeline = new StanfordCoreNLP(""spanish"");
CoreDocument doc = pipeline.processToCoreDocument(text);
",pipeline.processToCoreDocument,StanfordCoreNLP.processToCoreDocument,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;


public class PipelineExample {

    public static String text = ""Marie was born in Paris."";

    public static void main(String[] args) {
        // set up pipeline properties
        Properties props = new Properties();
        // set the list of annotators to run
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,depparse"");
        // build pipeline
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        // create a document object
        CoreDocument document = pipeline.processToCoreDocument(text);
    }

}
",main,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;


public class PipelineExample {

    public static String text = ""Marie was born in Paris."";

    public static void main(String[] args) {
        // set up pipeline properties
        Properties props = new Properties();
        // set the list of annotators to run
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,depparse"");
        // build pipeline
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        // create a document object
        CoreDocument document = pipeline.processToCoreDocument(text);
    }

}
",Properties,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;


public class PipelineExample {

    public static String text = ""Marie was born in Paris."";

    public static void main(String[] args) {
        // set up pipeline properties
        Properties props = new Properties();
        // set the list of annotators to run
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,depparse"");
        // build pipeline
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        // create a document object
        CoreDocument document = pipeline.processToCoreDocument(text);
    }

}
",props.setProperty,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;


public class PipelineExample {

    public static String text = ""Marie was born in Paris."";

    public static void main(String[] args) {
        // set up pipeline properties
        Properties props = new Properties();
        // set the list of annotators to run
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,depparse"");
        // build pipeline
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        // create a document object
        CoreDocument document = pipeline.processToCoreDocument(text);
    }

}
",StanfordCoreNLP,StanfordCoreNLP,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;


public class PipelineExample {

    public static String text = ""Marie was born in Paris."";

    public static void main(String[] args) {
        // set up pipeline properties
        Properties props = new Properties();
        // set the list of annotators to run
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,depparse"");
        // build pipeline
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        // create a document object
        CoreDocument document = pipeline.processToCoreDocument(text);
    }

}
",pipeline.processToCoreDocument,StanfordCoreNLP.processToCoreDocument,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.util.ArraySet;

import java.util.*;


public class CustomLemmaAnnotator implements Annotator {

  HashMap<String,String> wordToLemma = new HashMap<String,String>();

  public CustomLemmaAnnotator(String name, Properties props) {
    // load the lemma file
    // format should be tsv with word and lemma
    String lemmaFile = props.getProperty(""custom.lemma.lemmaFile"");
    List<String> lemmaEntries = IOUtils.linesFromFile(lemmaFile);
    for (String lemmaEntry : lemmaEntries) {
      wordToLemma.put(lemmaEntry.split(""\\t"")[0], lemmaEntry.split(""\\t"")[1]);
    }
  }

  public void annotate(Annotation annotation) {
    for (CoreLabel token : annotation.get(CoreAnnotations.TokensAnnotation.class)) {
      String lemma = wordToLemma.getOrDefault(token.word(), token.word());
      token.set(CoreAnnotations.LemmaAnnotation.class, lemma);
    }
  }

  @Override
  public Set<Class extends CoreAnnotation>> requires() {
    return Collections.unmodifiableSet(new ArraySet<>(Arrays.asList(
        CoreAnnotations.TextAnnotation.class,
        CoreAnnotations.TokensAnnotation.class,
        CoreAnnotations.SentencesAnnotation.class,
        CoreAnnotations.PartOfSpeechAnnotation.class
    )));
  }

  @Override
  public Set<Class extends CoreAnnotation>> requirementsSatisfied() {
    return Collections.singleton(CoreAnnotations.LemmaAnnotation.class);
  }
  
}
",CustomLemmaAnnotator,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.util.ArraySet;

import java.util.*;


public class CustomLemmaAnnotator implements Annotator {

  HashMap<String,String> wordToLemma = new HashMap<String,String>();

  public CustomLemmaAnnotator(String name, Properties props) {
    // load the lemma file
    // format should be tsv with word and lemma
    String lemmaFile = props.getProperty(""custom.lemma.lemmaFile"");
    List<String> lemmaEntries = IOUtils.linesFromFile(lemmaFile);
    for (String lemmaEntry : lemmaEntries) {
      wordToLemma.put(lemmaEntry.split(""\\t"")[0], lemmaEntry.split(""\\t"")[1]);
    }
  }

  public void annotate(Annotation annotation) {
    for (CoreLabel token : annotation.get(CoreAnnotations.TokensAnnotation.class)) {
      String lemma = wordToLemma.getOrDefault(token.word(), token.word());
      token.set(CoreAnnotations.LemmaAnnotation.class, lemma);
    }
  }

  @Override
  public Set<Class extends CoreAnnotation>> requires() {
    return Collections.unmodifiableSet(new ArraySet<>(Arrays.asList(
        CoreAnnotations.TextAnnotation.class,
        CoreAnnotations.TokensAnnotation.class,
        CoreAnnotations.SentencesAnnotation.class,
        CoreAnnotations.PartOfSpeechAnnotation.class
    )));
  }

  @Override
  public Set<Class extends CoreAnnotation>> requirementsSatisfied() {
    return Collections.singleton(CoreAnnotations.LemmaAnnotation.class);
  }
  
}
",props.getProperty,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.util.ArraySet;

import java.util.*;


public class CustomLemmaAnnotator implements Annotator {

  HashMap<String,String> wordToLemma = new HashMap<String,String>();

  public CustomLemmaAnnotator(String name, Properties props) {
    // load the lemma file
    // format should be tsv with word and lemma
    String lemmaFile = props.getProperty(""custom.lemma.lemmaFile"");
    List<String> lemmaEntries = IOUtils.linesFromFile(lemmaFile);
    for (String lemmaEntry : lemmaEntries) {
      wordToLemma.put(lemmaEntry.split(""\\t"")[0], lemmaEntry.split(""\\t"")[1]);
    }
  }

  public void annotate(Annotation annotation) {
    for (CoreLabel token : annotation.get(CoreAnnotations.TokensAnnotation.class)) {
      String lemma = wordToLemma.getOrDefault(token.word(), token.word());
      token.set(CoreAnnotations.LemmaAnnotation.class, lemma);
    }
  }

  @Override
  public Set<Class extends CoreAnnotation>> requires() {
    return Collections.unmodifiableSet(new ArraySet<>(Arrays.asList(
        CoreAnnotations.TextAnnotation.class,
        CoreAnnotations.TokensAnnotation.class,
        CoreAnnotations.SentencesAnnotation.class,
        CoreAnnotations.PartOfSpeechAnnotation.class
    )));
  }

  @Override
  public Set<Class extends CoreAnnotation>> requirementsSatisfied() {
    return Collections.singleton(CoreAnnotations.LemmaAnnotation.class);
  }
  
}
",IOUtils.linesFromFile,IOUtils.linesFromFile,CoreNLP\src\edu\stanford\nlp\io\IOUtils.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.util.ArraySet;

import java.util.*;


public class CustomLemmaAnnotator implements Annotator {

  HashMap<String,String> wordToLemma = new HashMap<String,String>();

  public CustomLemmaAnnotator(String name, Properties props) {
    // load the lemma file
    // format should be tsv with word and lemma
    String lemmaFile = props.getProperty(""custom.lemma.lemmaFile"");
    List<String> lemmaEntries = IOUtils.linesFromFile(lemmaFile);
    for (String lemmaEntry : lemmaEntries) {
      wordToLemma.put(lemmaEntry.split(""\\t"")[0], lemmaEntry.split(""\\t"")[1]);
    }
  }

  public void annotate(Annotation annotation) {
    for (CoreLabel token : annotation.get(CoreAnnotations.TokensAnnotation.class)) {
      String lemma = wordToLemma.getOrDefault(token.word(), token.word());
      token.set(CoreAnnotations.LemmaAnnotation.class, lemma);
    }
  }

  @Override
  public Set<Class extends CoreAnnotation>> requires() {
    return Collections.unmodifiableSet(new ArraySet<>(Arrays.asList(
        CoreAnnotations.TextAnnotation.class,
        CoreAnnotations.TokensAnnotation.class,
        CoreAnnotations.SentencesAnnotation.class,
        CoreAnnotations.PartOfSpeechAnnotation.class
    )));
  }

  @Override
  public Set<Class extends CoreAnnotation>> requirementsSatisfied() {
    return Collections.singleton(CoreAnnotations.LemmaAnnotation.class);
  }
  
}
",wordToLemma.put,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.util.ArraySet;

import java.util.*;


public class CustomLemmaAnnotator implements Annotator {

  HashMap<String,String> wordToLemma = new HashMap<String,String>();

  public CustomLemmaAnnotator(String name, Properties props) {
    // load the lemma file
    // format should be tsv with word and lemma
    String lemmaFile = props.getProperty(""custom.lemma.lemmaFile"");
    List<String> lemmaEntries = IOUtils.linesFromFile(lemmaFile);
    for (String lemmaEntry : lemmaEntries) {
      wordToLemma.put(lemmaEntry.split(""\\t"")[0], lemmaEntry.split(""\\t"")[1]);
    }
  }

  public void annotate(Annotation annotation) {
    for (CoreLabel token : annotation.get(CoreAnnotations.TokensAnnotation.class)) {
      String lemma = wordToLemma.getOrDefault(token.word(), token.word());
      token.set(CoreAnnotations.LemmaAnnotation.class, lemma);
    }
  }

  @Override
  public Set<Class extends CoreAnnotation>> requires() {
    return Collections.unmodifiableSet(new ArraySet<>(Arrays.asList(
        CoreAnnotations.TextAnnotation.class,
        CoreAnnotations.TokensAnnotation.class,
        CoreAnnotations.SentencesAnnotation.class,
        CoreAnnotations.PartOfSpeechAnnotation.class
    )));
  }

  @Override
  public Set<Class extends CoreAnnotation>> requirementsSatisfied() {
    return Collections.singleton(CoreAnnotations.LemmaAnnotation.class);
  }
  
}
",lemmaEntry.split,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.util.ArraySet;

import java.util.*;


public class CustomLemmaAnnotator implements Annotator {

  HashMap<String,String> wordToLemma = new HashMap<String,String>();

  public CustomLemmaAnnotator(String name, Properties props) {
    // load the lemma file
    // format should be tsv with word and lemma
    String lemmaFile = props.getProperty(""custom.lemma.lemmaFile"");
    List<String> lemmaEntries = IOUtils.linesFromFile(lemmaFile);
    for (String lemmaEntry : lemmaEntries) {
      wordToLemma.put(lemmaEntry.split(""\\t"")[0], lemmaEntry.split(""\\t"")[1]);
    }
  }

  public void annotate(Annotation annotation) {
    for (CoreLabel token : annotation.get(CoreAnnotations.TokensAnnotation.class)) {
      String lemma = wordToLemma.getOrDefault(token.word(), token.word());
      token.set(CoreAnnotations.LemmaAnnotation.class, lemma);
    }
  }

  @Override
  public Set<Class extends CoreAnnotation>> requires() {
    return Collections.unmodifiableSet(new ArraySet<>(Arrays.asList(
        CoreAnnotations.TextAnnotation.class,
        CoreAnnotations.TokensAnnotation.class,
        CoreAnnotations.SentencesAnnotation.class,
        CoreAnnotations.PartOfSpeechAnnotation.class
    )));
  }

  @Override
  public Set<Class extends CoreAnnotation>> requirementsSatisfied() {
    return Collections.singleton(CoreAnnotations.LemmaAnnotation.class);
  }
  
}
",annotate,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.util.ArraySet;

import java.util.*;


public class CustomLemmaAnnotator implements Annotator {

  HashMap<String,String> wordToLemma = new HashMap<String,String>();

  public CustomLemmaAnnotator(String name, Properties props) {
    // load the lemma file
    // format should be tsv with word and lemma
    String lemmaFile = props.getProperty(""custom.lemma.lemmaFile"");
    List<String> lemmaEntries = IOUtils.linesFromFile(lemmaFile);
    for (String lemmaEntry : lemmaEntries) {
      wordToLemma.put(lemmaEntry.split(""\\t"")[0], lemmaEntry.split(""\\t"")[1]);
    }
  }

  public void annotate(Annotation annotation) {
    for (CoreLabel token : annotation.get(CoreAnnotations.TokensAnnotation.class)) {
      String lemma = wordToLemma.getOrDefault(token.word(), token.word());
      token.set(CoreAnnotations.LemmaAnnotation.class, lemma);
    }
  }

  @Override
  public Set<Class extends CoreAnnotation>> requires() {
    return Collections.unmodifiableSet(new ArraySet<>(Arrays.asList(
        CoreAnnotations.TextAnnotation.class,
        CoreAnnotations.TokensAnnotation.class,
        CoreAnnotations.SentencesAnnotation.class,
        CoreAnnotations.PartOfSpeechAnnotation.class
    )));
  }

  @Override
  public Set<Class extends CoreAnnotation>> requirementsSatisfied() {
    return Collections.singleton(CoreAnnotations.LemmaAnnotation.class);
  }
  
}
",annotation.get,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.util.ArraySet;

import java.util.*;


public class CustomLemmaAnnotator implements Annotator {

  HashMap<String,String> wordToLemma = new HashMap<String,String>();

  public CustomLemmaAnnotator(String name, Properties props) {
    // load the lemma file
    // format should be tsv with word and lemma
    String lemmaFile = props.getProperty(""custom.lemma.lemmaFile"");
    List<String> lemmaEntries = IOUtils.linesFromFile(lemmaFile);
    for (String lemmaEntry : lemmaEntries) {
      wordToLemma.put(lemmaEntry.split(""\\t"")[0], lemmaEntry.split(""\\t"")[1]);
    }
  }

  public void annotate(Annotation annotation) {
    for (CoreLabel token : annotation.get(CoreAnnotations.TokensAnnotation.class)) {
      String lemma = wordToLemma.getOrDefault(token.word(), token.word());
      token.set(CoreAnnotations.LemmaAnnotation.class, lemma);
    }
  }

  @Override
  public Set<Class extends CoreAnnotation>> requires() {
    return Collections.unmodifiableSet(new ArraySet<>(Arrays.asList(
        CoreAnnotations.TextAnnotation.class,
        CoreAnnotations.TokensAnnotation.class,
        CoreAnnotations.SentencesAnnotation.class,
        CoreAnnotations.PartOfSpeechAnnotation.class
    )));
  }

  @Override
  public Set<Class extends CoreAnnotation>> requirementsSatisfied() {
    return Collections.singleton(CoreAnnotations.LemmaAnnotation.class);
  }
  
}
",wordToLemma.getOrDefault,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.util.ArraySet;

import java.util.*;


public class CustomLemmaAnnotator implements Annotator {

  HashMap<String,String> wordToLemma = new HashMap<String,String>();

  public CustomLemmaAnnotator(String name, Properties props) {
    // load the lemma file
    // format should be tsv with word and lemma
    String lemmaFile = props.getProperty(""custom.lemma.lemmaFile"");
    List<String> lemmaEntries = IOUtils.linesFromFile(lemmaFile);
    for (String lemmaEntry : lemmaEntries) {
      wordToLemma.put(lemmaEntry.split(""\\t"")[0], lemmaEntry.split(""\\t"")[1]);
    }
  }

  public void annotate(Annotation annotation) {
    for (CoreLabel token : annotation.get(CoreAnnotations.TokensAnnotation.class)) {
      String lemma = wordToLemma.getOrDefault(token.word(), token.word());
      token.set(CoreAnnotations.LemmaAnnotation.class, lemma);
    }
  }

  @Override
  public Set<Class extends CoreAnnotation>> requires() {
    return Collections.unmodifiableSet(new ArraySet<>(Arrays.asList(
        CoreAnnotations.TextAnnotation.class,
        CoreAnnotations.TokensAnnotation.class,
        CoreAnnotations.SentencesAnnotation.class,
        CoreAnnotations.PartOfSpeechAnnotation.class
    )));
  }

  @Override
  public Set<Class extends CoreAnnotation>> requirementsSatisfied() {
    return Collections.singleton(CoreAnnotations.LemmaAnnotation.class);
  }
  
}
",token.word,"CoreLabel.word
Token.word
Sentence.word","CoreNLP\src\edu\stanford\nlp\ling\CoreLabel.java
CoreNLP\src\edu\stanford\nlp\simple\Token.java
CoreNLP\src\edu\stanford\nlp\simple\Sentence.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.util.ArraySet;

import java.util.*;


public class CustomLemmaAnnotator implements Annotator {

  HashMap<String,String> wordToLemma = new HashMap<String,String>();

  public CustomLemmaAnnotator(String name, Properties props) {
    // load the lemma file
    // format should be tsv with word and lemma
    String lemmaFile = props.getProperty(""custom.lemma.lemmaFile"");
    List<String> lemmaEntries = IOUtils.linesFromFile(lemmaFile);
    for (String lemmaEntry : lemmaEntries) {
      wordToLemma.put(lemmaEntry.split(""\\t"")[0], lemmaEntry.split(""\\t"")[1]);
    }
  }

  public void annotate(Annotation annotation) {
    for (CoreLabel token : annotation.get(CoreAnnotations.TokensAnnotation.class)) {
      String lemma = wordToLemma.getOrDefault(token.word(), token.word());
      token.set(CoreAnnotations.LemmaAnnotation.class, lemma);
    }
  }

  @Override
  public Set<Class extends CoreAnnotation>> requires() {
    return Collections.unmodifiableSet(new ArraySet<>(Arrays.asList(
        CoreAnnotations.TextAnnotation.class,
        CoreAnnotations.TokensAnnotation.class,
        CoreAnnotations.SentencesAnnotation.class,
        CoreAnnotations.PartOfSpeechAnnotation.class
    )));
  }

  @Override
  public Set<Class extends CoreAnnotation>> requirementsSatisfied() {
    return Collections.singleton(CoreAnnotations.LemmaAnnotation.class);
  }
  
}
",token.set,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.util.ArraySet;

import java.util.*;


public class CustomLemmaAnnotator implements Annotator {

  HashMap<String,String> wordToLemma = new HashMap<String,String>();

  public CustomLemmaAnnotator(String name, Properties props) {
    // load the lemma file
    // format should be tsv with word and lemma
    String lemmaFile = props.getProperty(""custom.lemma.lemmaFile"");
    List<String> lemmaEntries = IOUtils.linesFromFile(lemmaFile);
    for (String lemmaEntry : lemmaEntries) {
      wordToLemma.put(lemmaEntry.split(""\\t"")[0], lemmaEntry.split(""\\t"")[1]);
    }
  }

  public void annotate(Annotation annotation) {
    for (CoreLabel token : annotation.get(CoreAnnotations.TokensAnnotation.class)) {
      String lemma = wordToLemma.getOrDefault(token.word(), token.word());
      token.set(CoreAnnotations.LemmaAnnotation.class, lemma);
    }
  }

  @Override
  public Set<Class extends CoreAnnotation>> requires() {
    return Collections.unmodifiableSet(new ArraySet<>(Arrays.asList(
        CoreAnnotations.TextAnnotation.class,
        CoreAnnotations.TokensAnnotation.class,
        CoreAnnotations.SentencesAnnotation.class,
        CoreAnnotations.PartOfSpeechAnnotation.class
    )));
  }

  @Override
  public Set<Class extends CoreAnnotation>> requirementsSatisfied() {
    return Collections.singleton(CoreAnnotations.LemmaAnnotation.class);
  }
  
}
",requires,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.util.ArraySet;

import java.util.*;


public class CustomLemmaAnnotator implements Annotator {

  HashMap<String,String> wordToLemma = new HashMap<String,String>();

  public CustomLemmaAnnotator(String name, Properties props) {
    // load the lemma file
    // format should be tsv with word and lemma
    String lemmaFile = props.getProperty(""custom.lemma.lemmaFile"");
    List<String> lemmaEntries = IOUtils.linesFromFile(lemmaFile);
    for (String lemmaEntry : lemmaEntries) {
      wordToLemma.put(lemmaEntry.split(""\\t"")[0], lemmaEntry.split(""\\t"")[1]);
    }
  }

  public void annotate(Annotation annotation) {
    for (CoreLabel token : annotation.get(CoreAnnotations.TokensAnnotation.class)) {
      String lemma = wordToLemma.getOrDefault(token.word(), token.word());
      token.set(CoreAnnotations.LemmaAnnotation.class, lemma);
    }
  }

  @Override
  public Set<Class extends CoreAnnotation>> requires() {
    return Collections.unmodifiableSet(new ArraySet<>(Arrays.asList(
        CoreAnnotations.TextAnnotation.class,
        CoreAnnotations.TokensAnnotation.class,
        CoreAnnotations.SentencesAnnotation.class,
        CoreAnnotations.PartOfSpeechAnnotation.class
    )));
  }

  @Override
  public Set<Class extends CoreAnnotation>> requirementsSatisfied() {
    return Collections.singleton(CoreAnnotations.LemmaAnnotation.class);
  }
  
}
",Collections.unmodifiableSet,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.util.ArraySet;

import java.util.*;


public class CustomLemmaAnnotator implements Annotator {

  HashMap<String,String> wordToLemma = new HashMap<String,String>();

  public CustomLemmaAnnotator(String name, Properties props) {
    // load the lemma file
    // format should be tsv with word and lemma
    String lemmaFile = props.getProperty(""custom.lemma.lemmaFile"");
    List<String> lemmaEntries = IOUtils.linesFromFile(lemmaFile);
    for (String lemmaEntry : lemmaEntries) {
      wordToLemma.put(lemmaEntry.split(""\\t"")[0], lemmaEntry.split(""\\t"")[1]);
    }
  }

  public void annotate(Annotation annotation) {
    for (CoreLabel token : annotation.get(CoreAnnotations.TokensAnnotation.class)) {
      String lemma = wordToLemma.getOrDefault(token.word(), token.word());
      token.set(CoreAnnotations.LemmaAnnotation.class, lemma);
    }
  }

  @Override
  public Set<Class extends CoreAnnotation>> requires() {
    return Collections.unmodifiableSet(new ArraySet<>(Arrays.asList(
        CoreAnnotations.TextAnnotation.class,
        CoreAnnotations.TokensAnnotation.class,
        CoreAnnotations.SentencesAnnotation.class,
        CoreAnnotations.PartOfSpeechAnnotation.class
    )));
  }

  @Override
  public Set<Class extends CoreAnnotation>> requirementsSatisfied() {
    return Collections.singleton(CoreAnnotations.LemmaAnnotation.class);
  }
  
}
",Arrays.asList,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.util.ArraySet;

import java.util.*;


public class CustomLemmaAnnotator implements Annotator {

  HashMap<String,String> wordToLemma = new HashMap<String,String>();

  public CustomLemmaAnnotator(String name, Properties props) {
    // load the lemma file
    // format should be tsv with word and lemma
    String lemmaFile = props.getProperty(""custom.lemma.lemmaFile"");
    List<String> lemmaEntries = IOUtils.linesFromFile(lemmaFile);
    for (String lemmaEntry : lemmaEntries) {
      wordToLemma.put(lemmaEntry.split(""\\t"")[0], lemmaEntry.split(""\\t"")[1]);
    }
  }

  public void annotate(Annotation annotation) {
    for (CoreLabel token : annotation.get(CoreAnnotations.TokensAnnotation.class)) {
      String lemma = wordToLemma.getOrDefault(token.word(), token.word());
      token.set(CoreAnnotations.LemmaAnnotation.class, lemma);
    }
  }

  @Override
  public Set<Class extends CoreAnnotation>> requires() {
    return Collections.unmodifiableSet(new ArraySet<>(Arrays.asList(
        CoreAnnotations.TextAnnotation.class,
        CoreAnnotations.TokensAnnotation.class,
        CoreAnnotations.SentencesAnnotation.class,
        CoreAnnotations.PartOfSpeechAnnotation.class
    )));
  }

  @Override
  public Set<Class extends CoreAnnotation>> requirementsSatisfied() {
    return Collections.singleton(CoreAnnotations.LemmaAnnotation.class);
  }
  
}
",requirementsSatisfied,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.util.ArraySet;

import java.util.*;


public class CustomLemmaAnnotator implements Annotator {

  HashMap<String,String> wordToLemma = new HashMap<String,String>();

  public CustomLemmaAnnotator(String name, Properties props) {
    // load the lemma file
    // format should be tsv with word and lemma
    String lemmaFile = props.getProperty(""custom.lemma.lemmaFile"");
    List<String> lemmaEntries = IOUtils.linesFromFile(lemmaFile);
    for (String lemmaEntry : lemmaEntries) {
      wordToLemma.put(lemmaEntry.split(""\\t"")[0], lemmaEntry.split(""\\t"")[1]);
    }
  }

  public void annotate(Annotation annotation) {
    for (CoreLabel token : annotation.get(CoreAnnotations.TokensAnnotation.class)) {
      String lemma = wordToLemma.getOrDefault(token.word(), token.word());
      token.set(CoreAnnotations.LemmaAnnotation.class, lemma);
    }
  }

  @Override
  public Set<Class extends CoreAnnotation>> requires() {
    return Collections.unmodifiableSet(new ArraySet<>(Arrays.asList(
        CoreAnnotations.TextAnnotation.class,
        CoreAnnotations.TokensAnnotation.class,
        CoreAnnotations.SentencesAnnotation.class,
        CoreAnnotations.PartOfSpeechAnnotation.class
    )));
  }

  @Override
  public Set<Class extends CoreAnnotation>> requirementsSatisfied() {
    return Collections.singleton(CoreAnnotations.LemmaAnnotation.class);
  }
  
}
",Collections.singleton,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class PipelineExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize"");
    // example of how to customize the PTBTokenizer (these are just random example settings!!)
    props.setProperty(""tokenize.options"", ""splitHyphenated=false,americanize=false"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s\t%d\t%d"", tok.word(), tok.beginPosition(), tok.endPosition()));
    }
  }
}
",main,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class PipelineExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize"");
    // example of how to customize the PTBTokenizer (these are just random example settings!!)
    props.setProperty(""tokenize.options"", ""splitHyphenated=false,americanize=false"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s\t%d\t%d"", tok.word(), tok.beginPosition(), tok.endPosition()));
    }
  }
}
",Properties,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class PipelineExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize"");
    // example of how to customize the PTBTokenizer (these are just random example settings!!)
    props.setProperty(""tokenize.options"", ""splitHyphenated=false,americanize=false"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s\t%d\t%d"", tok.word(), tok.beginPosition(), tok.endPosition()));
    }
  }
}
",props.setProperty,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class PipelineExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize"");
    // example of how to customize the PTBTokenizer (these are just random example settings!!)
    props.setProperty(""tokenize.options"", ""splitHyphenated=false,americanize=false"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s\t%d\t%d"", tok.word(), tok.beginPosition(), tok.endPosition()));
    }
  }
}
",StanfordCoreNLP,StanfordCoreNLP,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class PipelineExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize"");
    // example of how to customize the PTBTokenizer (these are just random example settings!!)
    props.setProperty(""tokenize.options"", ""splitHyphenated=false,americanize=false"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s\t%d\t%d"", tok.word(), tok.beginPosition(), tok.endPosition()));
    }
  }
}
",CoreDocument,CoreDocument,CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class PipelineExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize"");
    // example of how to customize the PTBTokenizer (these are just random example settings!!)
    props.setProperty(""tokenize.options"", ""splitHyphenated=false,americanize=false"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s\t%d\t%d"", tok.word(), tok.beginPosition(), tok.endPosition()));
    }
  }
}
",pipeline.annotate,StanfordCoreNLP.annotate,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class PipelineExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize"");
    // example of how to customize the PTBTokenizer (these are just random example settings!!)
    props.setProperty(""tokenize.options"", ""splitHyphenated=false,americanize=false"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s\t%d\t%d"", tok.word(), tok.beginPosition(), tok.endPosition()));
    }
  }
}
",doc.tokens,CoreDocument.tokens,CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class PipelineExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize"");
    // example of how to customize the PTBTokenizer (these are just random example settings!!)
    props.setProperty(""tokenize.options"", ""splitHyphenated=false,americanize=false"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s\t%d\t%d"", tok.word(), tok.beginPosition(), tok.endPosition()));
    }
  }
}
",out.println,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class PipelineExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize"");
    // example of how to customize the PTBTokenizer (these are just random example settings!!)
    props.setProperty(""tokenize.options"", ""splitHyphenated=false,americanize=false"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s\t%d\t%d"", tok.word(), tok.beginPosition(), tok.endPosition()));
    }
  }
}
",String.format,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class PipelineExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize"");
    // example of how to customize the PTBTokenizer (these are just random example settings!!)
    props.setProperty(""tokenize.options"", ""splitHyphenated=false,americanize=false"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s\t%d\t%d"", tok.word(), tok.beginPosition(), tok.endPosition()));
    }
  }
}
",tok.word,"CoreLabel.word
Token.word","CoreNLP\src\edu\stanford\nlp\ling\CoreLabel.java
CoreNLP\src\edu\stanford\nlp\simple\Token.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class PipelineExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize"");
    // example of how to customize the PTBTokenizer (these are just random example settings!!)
    props.setProperty(""tokenize.options"", ""splitHyphenated=false,americanize=false"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s\t%d\t%d"", tok.word(), tok.beginPosition(), tok.endPosition()));
    }
  }
}
",tok.beginPosition,"Token.beginPosition
CoreLabel.beginPosition","CoreNLP\src\edu\stanford\nlp\simple\Token.java
CoreNLP\src\edu\stanford\nlp\ling\CoreLabel.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class PipelineExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize"");
    // example of how to customize the PTBTokenizer (these are just random example settings!!)
    props.setProperty(""tokenize.options"", ""splitHyphenated=false,americanize=false"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s\t%d\t%d"", tok.word(), tok.beginPosition(), tok.endPosition()));
    }
  }
}
",tok.endPosition,"CoreLabel.endPosition
Token.endPosition","CoreNLP\src\edu\stanford\nlp\ling\CoreLabel.java
CoreNLP\src\edu\stanford\nlp\simple\Token.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;

import java.util.*;
import java.util.stream.Collectors;


public class ForumPostExample {

  public static void main(String args[]) {
    // properties
    Properties props = StringUtils.argsToProperties(""-props"", args[0]);
    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // set up document
    Annotation testDocument = new Annotation(IOUtils.stringFromFile(args[1]));
    // annotate document
    pipeline.annotate(testDocument);
    // print out the forum posts
    for (CoreMap discussionForumPost : testDocument.get(CoreAnnotations.SectionsAnnotation.class)) {
      System.err.println(""---"");
      System.err.println(""author: "" + discussionForumPost.get(CoreAnnotations.AuthorAnnotation.class));
      System.err.println(""date: "" +
          discussionForumPost.get(CoreAnnotations.SectionDateAnnotation.class));
      System.err.println(""char begin: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
      System.err.println(""char end: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
      System.err.println(""author start offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetBeginAnnotation.class));
      System.err.println(""author end offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetEndAnnotation.class));
      System.err.println(""section start tag: "" +
          discussionForumPost.get(CoreAnnotations.SectionTagAnnotation.class));
      // print out the sentences
      System.err.println(""sentences: "");
      for (CoreMap sentence : discussionForumPost.get(CoreAnnotations.SentencesAnnotation.class)) {
        System.err.println(""\t***"");
        boolean sentenceQuoted = (sentence.get(CoreAnnotations.QuotedAnnotation.class) != null) &&
            sentence.get(CoreAnnotations.QuotedAnnotation.class);
        String sentenceAuthor = sentence.get(CoreAnnotations.AuthorAnnotation.class);
        String potentialQuoteText = sentenceQuoted ? ""(QUOTING: ""+sentenceAuthor+"")"" : """" ;
        System.err.println(""\t"" + potentialQuoteText + "" "" +
            sentence.get(CoreAnnotations.TokensAnnotation.class).stream().
            map(token -> token.word()).collect(Collectors.joining("" "")));
      }
    }
  }
}
",main,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;

import java.util.*;
import java.util.stream.Collectors;


public class ForumPostExample {

  public static void main(String args[]) {
    // properties
    Properties props = StringUtils.argsToProperties(""-props"", args[0]);
    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // set up document
    Annotation testDocument = new Annotation(IOUtils.stringFromFile(args[1]));
    // annotate document
    pipeline.annotate(testDocument);
    // print out the forum posts
    for (CoreMap discussionForumPost : testDocument.get(CoreAnnotations.SectionsAnnotation.class)) {
      System.err.println(""---"");
      System.err.println(""author: "" + discussionForumPost.get(CoreAnnotations.AuthorAnnotation.class));
      System.err.println(""date: "" +
          discussionForumPost.get(CoreAnnotations.SectionDateAnnotation.class));
      System.err.println(""char begin: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
      System.err.println(""char end: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
      System.err.println(""author start offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetBeginAnnotation.class));
      System.err.println(""author end offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetEndAnnotation.class));
      System.err.println(""section start tag: "" +
          discussionForumPost.get(CoreAnnotations.SectionTagAnnotation.class));
      // print out the sentences
      System.err.println(""sentences: "");
      for (CoreMap sentence : discussionForumPost.get(CoreAnnotations.SentencesAnnotation.class)) {
        System.err.println(""\t***"");
        boolean sentenceQuoted = (sentence.get(CoreAnnotations.QuotedAnnotation.class) != null) &&
            sentence.get(CoreAnnotations.QuotedAnnotation.class);
        String sentenceAuthor = sentence.get(CoreAnnotations.AuthorAnnotation.class);
        String potentialQuoteText = sentenceQuoted ? ""(QUOTING: ""+sentenceAuthor+"")"" : """" ;
        System.err.println(""\t"" + potentialQuoteText + "" "" +
            sentence.get(CoreAnnotations.TokensAnnotation.class).stream().
            map(token -> token.word()).collect(Collectors.joining("" "")));
      }
    }
  }
}
",StanfordCoreNLP,StanfordCoreNLP,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;

import java.util.*;
import java.util.stream.Collectors;


public class ForumPostExample {

  public static void main(String args[]) {
    // properties
    Properties props = StringUtils.argsToProperties(""-props"", args[0]);
    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // set up document
    Annotation testDocument = new Annotation(IOUtils.stringFromFile(args[1]));
    // annotate document
    pipeline.annotate(testDocument);
    // print out the forum posts
    for (CoreMap discussionForumPost : testDocument.get(CoreAnnotations.SectionsAnnotation.class)) {
      System.err.println(""---"");
      System.err.println(""author: "" + discussionForumPost.get(CoreAnnotations.AuthorAnnotation.class));
      System.err.println(""date: "" +
          discussionForumPost.get(CoreAnnotations.SectionDateAnnotation.class));
      System.err.println(""char begin: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
      System.err.println(""char end: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
      System.err.println(""author start offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetBeginAnnotation.class));
      System.err.println(""author end offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetEndAnnotation.class));
      System.err.println(""section start tag: "" +
          discussionForumPost.get(CoreAnnotations.SectionTagAnnotation.class));
      // print out the sentences
      System.err.println(""sentences: "");
      for (CoreMap sentence : discussionForumPost.get(CoreAnnotations.SentencesAnnotation.class)) {
        System.err.println(""\t***"");
        boolean sentenceQuoted = (sentence.get(CoreAnnotations.QuotedAnnotation.class) != null) &&
            sentence.get(CoreAnnotations.QuotedAnnotation.class);
        String sentenceAuthor = sentence.get(CoreAnnotations.AuthorAnnotation.class);
        String potentialQuoteText = sentenceQuoted ? ""(QUOTING: ""+sentenceAuthor+"")"" : """" ;
        System.err.println(""\t"" + potentialQuoteText + "" "" +
            sentence.get(CoreAnnotations.TokensAnnotation.class).stream().
            map(token -> token.word()).collect(Collectors.joining("" "")));
      }
    }
  }
}
",Annotation,Annotation,CoreNLP\src\edu\stanford\nlp\pipeline\Annotation.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;

import java.util.*;
import java.util.stream.Collectors;


public class ForumPostExample {

  public static void main(String args[]) {
    // properties
    Properties props = StringUtils.argsToProperties(""-props"", args[0]);
    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // set up document
    Annotation testDocument = new Annotation(IOUtils.stringFromFile(args[1]));
    // annotate document
    pipeline.annotate(testDocument);
    // print out the forum posts
    for (CoreMap discussionForumPost : testDocument.get(CoreAnnotations.SectionsAnnotation.class)) {
      System.err.println(""---"");
      System.err.println(""author: "" + discussionForumPost.get(CoreAnnotations.AuthorAnnotation.class));
      System.err.println(""date: "" +
          discussionForumPost.get(CoreAnnotations.SectionDateAnnotation.class));
      System.err.println(""char begin: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
      System.err.println(""char end: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
      System.err.println(""author start offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetBeginAnnotation.class));
      System.err.println(""author end offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetEndAnnotation.class));
      System.err.println(""section start tag: "" +
          discussionForumPost.get(CoreAnnotations.SectionTagAnnotation.class));
      // print out the sentences
      System.err.println(""sentences: "");
      for (CoreMap sentence : discussionForumPost.get(CoreAnnotations.SentencesAnnotation.class)) {
        System.err.println(""\t***"");
        boolean sentenceQuoted = (sentence.get(CoreAnnotations.QuotedAnnotation.class) != null) &&
            sentence.get(CoreAnnotations.QuotedAnnotation.class);
        String sentenceAuthor = sentence.get(CoreAnnotations.AuthorAnnotation.class);
        String potentialQuoteText = sentenceQuoted ? ""(QUOTING: ""+sentenceAuthor+"")"" : """" ;
        System.err.println(""\t"" + potentialQuoteText + "" "" +
            sentence.get(CoreAnnotations.TokensAnnotation.class).stream().
            map(token -> token.word()).collect(Collectors.joining("" "")));
      }
    }
  }
}
",IOUtils.stringFromFile,IOUtils.stringFromFile,CoreNLP\src\edu\stanford\nlp\io\IOUtils.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;

import java.util.*;
import java.util.stream.Collectors;


public class ForumPostExample {

  public static void main(String args[]) {
    // properties
    Properties props = StringUtils.argsToProperties(""-props"", args[0]);
    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // set up document
    Annotation testDocument = new Annotation(IOUtils.stringFromFile(args[1]));
    // annotate document
    pipeline.annotate(testDocument);
    // print out the forum posts
    for (CoreMap discussionForumPost : testDocument.get(CoreAnnotations.SectionsAnnotation.class)) {
      System.err.println(""---"");
      System.err.println(""author: "" + discussionForumPost.get(CoreAnnotations.AuthorAnnotation.class));
      System.err.println(""date: "" +
          discussionForumPost.get(CoreAnnotations.SectionDateAnnotation.class));
      System.err.println(""char begin: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
      System.err.println(""char end: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
      System.err.println(""author start offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetBeginAnnotation.class));
      System.err.println(""author end offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetEndAnnotation.class));
      System.err.println(""section start tag: "" +
          discussionForumPost.get(CoreAnnotations.SectionTagAnnotation.class));
      // print out the sentences
      System.err.println(""sentences: "");
      for (CoreMap sentence : discussionForumPost.get(CoreAnnotations.SentencesAnnotation.class)) {
        System.err.println(""\t***"");
        boolean sentenceQuoted = (sentence.get(CoreAnnotations.QuotedAnnotation.class) != null) &&
            sentence.get(CoreAnnotations.QuotedAnnotation.class);
        String sentenceAuthor = sentence.get(CoreAnnotations.AuthorAnnotation.class);
        String potentialQuoteText = sentenceQuoted ? ""(QUOTING: ""+sentenceAuthor+"")"" : """" ;
        System.err.println(""\t"" + potentialQuoteText + "" "" +
            sentence.get(CoreAnnotations.TokensAnnotation.class).stream().
            map(token -> token.word()).collect(Collectors.joining("" "")));
      }
    }
  }
}
",pipeline.annotate,"Util.annotate
StanfordCoreNLP.annotate","CoreNLP\src\edu\stanford\nlp\naturalli\Util.java
CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;

import java.util.*;
import java.util.stream.Collectors;


public class ForumPostExample {

  public static void main(String args[]) {
    // properties
    Properties props = StringUtils.argsToProperties(""-props"", args[0]);
    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // set up document
    Annotation testDocument = new Annotation(IOUtils.stringFromFile(args[1]));
    // annotate document
    pipeline.annotate(testDocument);
    // print out the forum posts
    for (CoreMap discussionForumPost : testDocument.get(CoreAnnotations.SectionsAnnotation.class)) {
      System.err.println(""---"");
      System.err.println(""author: "" + discussionForumPost.get(CoreAnnotations.AuthorAnnotation.class));
      System.err.println(""date: "" +
          discussionForumPost.get(CoreAnnotations.SectionDateAnnotation.class));
      System.err.println(""char begin: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
      System.err.println(""char end: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
      System.err.println(""author start offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetBeginAnnotation.class));
      System.err.println(""author end offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetEndAnnotation.class));
      System.err.println(""section start tag: "" +
          discussionForumPost.get(CoreAnnotations.SectionTagAnnotation.class));
      // print out the sentences
      System.err.println(""sentences: "");
      for (CoreMap sentence : discussionForumPost.get(CoreAnnotations.SentencesAnnotation.class)) {
        System.err.println(""\t***"");
        boolean sentenceQuoted = (sentence.get(CoreAnnotations.QuotedAnnotation.class) != null) &&
            sentence.get(CoreAnnotations.QuotedAnnotation.class);
        String sentenceAuthor = sentence.get(CoreAnnotations.AuthorAnnotation.class);
        String potentialQuoteText = sentenceQuoted ? ""(QUOTING: ""+sentenceAuthor+"")"" : """" ;
        System.err.println(""\t"" + potentialQuoteText + "" "" +
            sentence.get(CoreAnnotations.TokensAnnotation.class).stream().
            map(token -> token.word()).collect(Collectors.joining("" "")));
      }
    }
  }
}
",testDocument.get,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;

import java.util.*;
import java.util.stream.Collectors;


public class ForumPostExample {

  public static void main(String args[]) {
    // properties
    Properties props = StringUtils.argsToProperties(""-props"", args[0]);
    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // set up document
    Annotation testDocument = new Annotation(IOUtils.stringFromFile(args[1]));
    // annotate document
    pipeline.annotate(testDocument);
    // print out the forum posts
    for (CoreMap discussionForumPost : testDocument.get(CoreAnnotations.SectionsAnnotation.class)) {
      System.err.println(""---"");
      System.err.println(""author: "" + discussionForumPost.get(CoreAnnotations.AuthorAnnotation.class));
      System.err.println(""date: "" +
          discussionForumPost.get(CoreAnnotations.SectionDateAnnotation.class));
      System.err.println(""char begin: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
      System.err.println(""char end: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
      System.err.println(""author start offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetBeginAnnotation.class));
      System.err.println(""author end offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetEndAnnotation.class));
      System.err.println(""section start tag: "" +
          discussionForumPost.get(CoreAnnotations.SectionTagAnnotation.class));
      // print out the sentences
      System.err.println(""sentences: "");
      for (CoreMap sentence : discussionForumPost.get(CoreAnnotations.SentencesAnnotation.class)) {
        System.err.println(""\t***"");
        boolean sentenceQuoted = (sentence.get(CoreAnnotations.QuotedAnnotation.class) != null) &&
            sentence.get(CoreAnnotations.QuotedAnnotation.class);
        String sentenceAuthor = sentence.get(CoreAnnotations.AuthorAnnotation.class);
        String potentialQuoteText = sentenceQuoted ? ""(QUOTING: ""+sentenceAuthor+"")"" : """" ;
        System.err.println(""\t"" + potentialQuoteText + "" "" +
            sentence.get(CoreAnnotations.TokensAnnotation.class).stream().
            map(token -> token.word()).collect(Collectors.joining("" "")));
      }
    }
  }
}
",err.println,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;

import java.util.*;
import java.util.stream.Collectors;


public class ForumPostExample {

  public static void main(String args[]) {
    // properties
    Properties props = StringUtils.argsToProperties(""-props"", args[0]);
    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // set up document
    Annotation testDocument = new Annotation(IOUtils.stringFromFile(args[1]));
    // annotate document
    pipeline.annotate(testDocument);
    // print out the forum posts
    for (CoreMap discussionForumPost : testDocument.get(CoreAnnotations.SectionsAnnotation.class)) {
      System.err.println(""---"");
      System.err.println(""author: "" + discussionForumPost.get(CoreAnnotations.AuthorAnnotation.class));
      System.err.println(""date: "" +
          discussionForumPost.get(CoreAnnotations.SectionDateAnnotation.class));
      System.err.println(""char begin: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
      System.err.println(""char end: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
      System.err.println(""author start offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetBeginAnnotation.class));
      System.err.println(""author end offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetEndAnnotation.class));
      System.err.println(""section start tag: "" +
          discussionForumPost.get(CoreAnnotations.SectionTagAnnotation.class));
      // print out the sentences
      System.err.println(""sentences: "");
      for (CoreMap sentence : discussionForumPost.get(CoreAnnotations.SentencesAnnotation.class)) {
        System.err.println(""\t***"");
        boolean sentenceQuoted = (sentence.get(CoreAnnotations.QuotedAnnotation.class) != null) &&
            sentence.get(CoreAnnotations.QuotedAnnotation.class);
        String sentenceAuthor = sentence.get(CoreAnnotations.AuthorAnnotation.class);
        String potentialQuoteText = sentenceQuoted ? ""(QUOTING: ""+sentenceAuthor+"")"" : """" ;
        System.err.println(""\t"" + potentialQuoteText + "" "" +
            sentence.get(CoreAnnotations.TokensAnnotation.class).stream().
            map(token -> token.word()).collect(Collectors.joining("" "")));
      }
    }
  }
}
",discussionForumPost.get,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;

import java.util.*;
import java.util.stream.Collectors;


public class ForumPostExample {

  public static void main(String args[]) {
    // properties
    Properties props = StringUtils.argsToProperties(""-props"", args[0]);
    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // set up document
    Annotation testDocument = new Annotation(IOUtils.stringFromFile(args[1]));
    // annotate document
    pipeline.annotate(testDocument);
    // print out the forum posts
    for (CoreMap discussionForumPost : testDocument.get(CoreAnnotations.SectionsAnnotation.class)) {
      System.err.println(""---"");
      System.err.println(""author: "" + discussionForumPost.get(CoreAnnotations.AuthorAnnotation.class));
      System.err.println(""date: "" +
          discussionForumPost.get(CoreAnnotations.SectionDateAnnotation.class));
      System.err.println(""char begin: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
      System.err.println(""char end: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
      System.err.println(""author start offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetBeginAnnotation.class));
      System.err.println(""author end offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetEndAnnotation.class));
      System.err.println(""section start tag: "" +
          discussionForumPost.get(CoreAnnotations.SectionTagAnnotation.class));
      // print out the sentences
      System.err.println(""sentences: "");
      for (CoreMap sentence : discussionForumPost.get(CoreAnnotations.SentencesAnnotation.class)) {
        System.err.println(""\t***"");
        boolean sentenceQuoted = (sentence.get(CoreAnnotations.QuotedAnnotation.class) != null) &&
            sentence.get(CoreAnnotations.QuotedAnnotation.class);
        String sentenceAuthor = sentence.get(CoreAnnotations.AuthorAnnotation.class);
        String potentialQuoteText = sentenceQuoted ? ""(QUOTING: ""+sentenceAuthor+"")"" : """" ;
        System.err.println(""\t"" + potentialQuoteText + "" "" +
            sentence.get(CoreAnnotations.TokensAnnotation.class).stream().
            map(token -> token.word()).collect(Collectors.joining("" "")));
      }
    }
  }
}
",sentence.get,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;

import java.util.*;
import java.util.stream.Collectors;


public class ForumPostExample {

  public static void main(String args[]) {
    // properties
    Properties props = StringUtils.argsToProperties(""-props"", args[0]);
    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // set up document
    Annotation testDocument = new Annotation(IOUtils.stringFromFile(args[1]));
    // annotate document
    pipeline.annotate(testDocument);
    // print out the forum posts
    for (CoreMap discussionForumPost : testDocument.get(CoreAnnotations.SectionsAnnotation.class)) {
      System.err.println(""---"");
      System.err.println(""author: "" + discussionForumPost.get(CoreAnnotations.AuthorAnnotation.class));
      System.err.println(""date: "" +
          discussionForumPost.get(CoreAnnotations.SectionDateAnnotation.class));
      System.err.println(""char begin: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
      System.err.println(""char end: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
      System.err.println(""author start offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetBeginAnnotation.class));
      System.err.println(""author end offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetEndAnnotation.class));
      System.err.println(""section start tag: "" +
          discussionForumPost.get(CoreAnnotations.SectionTagAnnotation.class));
      // print out the sentences
      System.err.println(""sentences: "");
      for (CoreMap sentence : discussionForumPost.get(CoreAnnotations.SentencesAnnotation.class)) {
        System.err.println(""\t***"");
        boolean sentenceQuoted = (sentence.get(CoreAnnotations.QuotedAnnotation.class) != null) &&
            sentence.get(CoreAnnotations.QuotedAnnotation.class);
        String sentenceAuthor = sentence.get(CoreAnnotations.AuthorAnnotation.class);
        String potentialQuoteText = sentenceQuoted ? ""(QUOTING: ""+sentenceAuthor+"")"" : """" ;
        System.err.println(""\t"" + potentialQuoteText + "" "" +
            sentence.get(CoreAnnotations.TokensAnnotation.class).stream().
            map(token -> token.word()).collect(Collectors.joining("" "")));
      }
    }
  }
}
",stream,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;

import java.util.*;
import java.util.stream.Collectors;


public class ForumPostExample {

  public static void main(String args[]) {
    // properties
    Properties props = StringUtils.argsToProperties(""-props"", args[0]);
    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // set up document
    Annotation testDocument = new Annotation(IOUtils.stringFromFile(args[1]));
    // annotate document
    pipeline.annotate(testDocument);
    // print out the forum posts
    for (CoreMap discussionForumPost : testDocument.get(CoreAnnotations.SectionsAnnotation.class)) {
      System.err.println(""---"");
      System.err.println(""author: "" + discussionForumPost.get(CoreAnnotations.AuthorAnnotation.class));
      System.err.println(""date: "" +
          discussionForumPost.get(CoreAnnotations.SectionDateAnnotation.class));
      System.err.println(""char begin: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
      System.err.println(""char end: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
      System.err.println(""author start offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetBeginAnnotation.class));
      System.err.println(""author end offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetEndAnnotation.class));
      System.err.println(""section start tag: "" +
          discussionForumPost.get(CoreAnnotations.SectionTagAnnotation.class));
      // print out the sentences
      System.err.println(""sentences: "");
      for (CoreMap sentence : discussionForumPost.get(CoreAnnotations.SentencesAnnotation.class)) {
        System.err.println(""\t***"");
        boolean sentenceQuoted = (sentence.get(CoreAnnotations.QuotedAnnotation.class) != null) &&
            sentence.get(CoreAnnotations.QuotedAnnotation.class);
        String sentenceAuthor = sentence.get(CoreAnnotations.AuthorAnnotation.class);
        String potentialQuoteText = sentenceQuoted ? ""(QUOTING: ""+sentenceAuthor+"")"" : """" ;
        System.err.println(""\t"" + potentialQuoteText + "" "" +
            sentence.get(CoreAnnotations.TokensAnnotation.class).stream().
            map(token -> token.word()).collect(Collectors.joining("" "")));
      }
    }
  }
}
",map,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;

import java.util.*;
import java.util.stream.Collectors;


public class ForumPostExample {

  public static void main(String args[]) {
    // properties
    Properties props = StringUtils.argsToProperties(""-props"", args[0]);
    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // set up document
    Annotation testDocument = new Annotation(IOUtils.stringFromFile(args[1]));
    // annotate document
    pipeline.annotate(testDocument);
    // print out the forum posts
    for (CoreMap discussionForumPost : testDocument.get(CoreAnnotations.SectionsAnnotation.class)) {
      System.err.println(""---"");
      System.err.println(""author: "" + discussionForumPost.get(CoreAnnotations.AuthorAnnotation.class));
      System.err.println(""date: "" +
          discussionForumPost.get(CoreAnnotations.SectionDateAnnotation.class));
      System.err.println(""char begin: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
      System.err.println(""char end: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
      System.err.println(""author start offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetBeginAnnotation.class));
      System.err.println(""author end offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetEndAnnotation.class));
      System.err.println(""section start tag: "" +
          discussionForumPost.get(CoreAnnotations.SectionTagAnnotation.class));
      // print out the sentences
      System.err.println(""sentences: "");
      for (CoreMap sentence : discussionForumPost.get(CoreAnnotations.SentencesAnnotation.class)) {
        System.err.println(""\t***"");
        boolean sentenceQuoted = (sentence.get(CoreAnnotations.QuotedAnnotation.class) != null) &&
            sentence.get(CoreAnnotations.QuotedAnnotation.class);
        String sentenceAuthor = sentence.get(CoreAnnotations.AuthorAnnotation.class);
        String potentialQuoteText = sentenceQuoted ? ""(QUOTING: ""+sentenceAuthor+"")"" : """" ;
        System.err.println(""\t"" + potentialQuoteText + "" "" +
            sentence.get(CoreAnnotations.TokensAnnotation.class).stream().
            map(token -> token.word()).collect(Collectors.joining("" "")));
      }
    }
  }
}
",token.word,"Token.word
Sentence.word","CoreNLP\src\edu\stanford\nlp\simple\Token.java
CoreNLP\src\edu\stanford\nlp\simple\Sentence.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;

import java.util.*;
import java.util.stream.Collectors;


public class ForumPostExample {

  public static void main(String args[]) {
    // properties
    Properties props = StringUtils.argsToProperties(""-props"", args[0]);
    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // set up document
    Annotation testDocument = new Annotation(IOUtils.stringFromFile(args[1]));
    // annotate document
    pipeline.annotate(testDocument);
    // print out the forum posts
    for (CoreMap discussionForumPost : testDocument.get(CoreAnnotations.SectionsAnnotation.class)) {
      System.err.println(""---"");
      System.err.println(""author: "" + discussionForumPost.get(CoreAnnotations.AuthorAnnotation.class));
      System.err.println(""date: "" +
          discussionForumPost.get(CoreAnnotations.SectionDateAnnotation.class));
      System.err.println(""char begin: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
      System.err.println(""char end: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
      System.err.println(""author start offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetBeginAnnotation.class));
      System.err.println(""author end offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetEndAnnotation.class));
      System.err.println(""section start tag: "" +
          discussionForumPost.get(CoreAnnotations.SectionTagAnnotation.class));
      // print out the sentences
      System.err.println(""sentences: "");
      for (CoreMap sentence : discussionForumPost.get(CoreAnnotations.SentencesAnnotation.class)) {
        System.err.println(""\t***"");
        boolean sentenceQuoted = (sentence.get(CoreAnnotations.QuotedAnnotation.class) != null) &&
            sentence.get(CoreAnnotations.QuotedAnnotation.class);
        String sentenceAuthor = sentence.get(CoreAnnotations.AuthorAnnotation.class);
        String potentialQuoteText = sentenceQuoted ? ""(QUOTING: ""+sentenceAuthor+"")"" : """" ;
        System.err.println(""\t"" + potentialQuoteText + "" "" +
            sentence.get(CoreAnnotations.TokensAnnotation.class).stream().
            map(token -> token.word()).collect(Collectors.joining("" "")));
      }
    }
  }
}
",collect,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;

import java.util.*;
import java.util.stream.Collectors;


public class ForumPostExample {

  public static void main(String args[]) {
    // properties
    Properties props = StringUtils.argsToProperties(""-props"", args[0]);
    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // set up document
    Annotation testDocument = new Annotation(IOUtils.stringFromFile(args[1]));
    // annotate document
    pipeline.annotate(testDocument);
    // print out the forum posts
    for (CoreMap discussionForumPost : testDocument.get(CoreAnnotations.SectionsAnnotation.class)) {
      System.err.println(""---"");
      System.err.println(""author: "" + discussionForumPost.get(CoreAnnotations.AuthorAnnotation.class));
      System.err.println(""date: "" +
          discussionForumPost.get(CoreAnnotations.SectionDateAnnotation.class));
      System.err.println(""char begin: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
      System.err.println(""char end: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
      System.err.println(""author start offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetBeginAnnotation.class));
      System.err.println(""author end offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetEndAnnotation.class));
      System.err.println(""section start tag: "" +
          discussionForumPost.get(CoreAnnotations.SectionTagAnnotation.class));
      // print out the sentences
      System.err.println(""sentences: "");
      for (CoreMap sentence : discussionForumPost.get(CoreAnnotations.SentencesAnnotation.class)) {
        System.err.println(""\t***"");
        boolean sentenceQuoted = (sentence.get(CoreAnnotations.QuotedAnnotation.class) != null) &&
            sentence.get(CoreAnnotations.QuotedAnnotation.class);
        String sentenceAuthor = sentence.get(CoreAnnotations.AuthorAnnotation.class);
        String potentialQuoteText = sentenceQuoted ? ""(QUOTING: ""+sentenceAuthor+"")"" : """" ;
        System.err.println(""\t"" + potentialQuoteText + "" "" +
            sentence.get(CoreAnnotations.TokensAnnotation.class).stream().
            map(token -> token.word()).collect(Collectors.joining("" "")));
      }
    }
  }
}
",Collectors.joining,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class SentenceSplittingExample {

  public static String text = ""Hello world. Hello world again."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display sentences
    for (CoreSentence sent : doc.sentences()) {
        System.out.println(sent.text());
    }
  }
}
",main,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class SentenceSplittingExample {

  public static String text = ""Hello world. Hello world again."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display sentences
    for (CoreSentence sent : doc.sentences()) {
        System.out.println(sent.text());
    }
  }
}
",Properties,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class SentenceSplittingExample {

  public static String text = ""Hello world. Hello world again."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display sentences
    for (CoreSentence sent : doc.sentences()) {
        System.out.println(sent.text());
    }
  }
}
",props.setProperty,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class SentenceSplittingExample {

  public static String text = ""Hello world. Hello world again."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display sentences
    for (CoreSentence sent : doc.sentences()) {
        System.out.println(sent.text());
    }
  }
}
",StanfordCoreNLP,StanfordCoreNLP,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class SentenceSplittingExample {

  public static String text = ""Hello world. Hello world again."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display sentences
    for (CoreSentence sent : doc.sentences()) {
        System.out.println(sent.text());
    }
  }
}
",CoreDocument,CoreDocument,CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class SentenceSplittingExample {

  public static String text = ""Hello world. Hello world again."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display sentences
    for (CoreSentence sent : doc.sentences()) {
        System.out.println(sent.text());
    }
  }
}
",pipeline.annotate,StanfordCoreNLP.annotate,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class SentenceSplittingExample {

  public static String text = ""Hello world. Hello world again."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display sentences
    for (CoreSentence sent : doc.sentences()) {
        System.out.println(sent.text());
    }
  }
}
",doc.sentences,"Document.sentences
CoreDocument.sentences","CoreNLP\src\edu\stanford\nlp\simple\Document.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class SentenceSplittingExample {

  public static String text = ""Hello world. Hello world again."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display sentences
    for (CoreSentence sent : doc.sentences()) {
        System.out.println(sent.text());
    }
  }
}
",out.println,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class SentenceSplittingExample {

  public static String text = ""Hello world. Hello world again."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display sentences
    for (CoreSentence sent : doc.sentences()) {
        System.out.println(sent.text());
    }
  }
}
",sent.text,"Document.text
Sentence.text
CoreDocument.text
CoreSentence.text","CoreNLP\src\edu\stanford\nlp\simple\Document.java
CoreNLP\src\edu\stanford\nlp\simple\Sentence.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreSentence.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.StringUtils;

import java.util.*;

public class MWTExpansionExample {

  public static String text = ""Pude haber querido escribirlo."";

  public static void main(String[] args) {
    // set the list of annotators to run
    Properties props = StringUtils.argsToProperties(""-props"", ""spanish"");
    props.setProperty(""annotators"", ""tokenize,ssplit,mwt"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s"", tok.word()));
    }
  }
}
",main,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.StringUtils;

import java.util.*;

public class MWTExpansionExample {

  public static String text = ""Pude haber querido escribirlo."";

  public static void main(String[] args) {
    // set the list of annotators to run
    Properties props = StringUtils.argsToProperties(""-props"", ""spanish"");
    props.setProperty(""annotators"", ""tokenize,ssplit,mwt"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s"", tok.word()));
    }
  }
}
",props.setProperty,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.StringUtils;

import java.util.*;

public class MWTExpansionExample {

  public static String text = ""Pude haber querido escribirlo."";

  public static void main(String[] args) {
    // set the list of annotators to run
    Properties props = StringUtils.argsToProperties(""-props"", ""spanish"");
    props.setProperty(""annotators"", ""tokenize,ssplit,mwt"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s"", tok.word()));
    }
  }
}
",StanfordCoreNLP,StanfordCoreNLP,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.StringUtils;

import java.util.*;

public class MWTExpansionExample {

  public static String text = ""Pude haber querido escribirlo."";

  public static void main(String[] args) {
    // set the list of annotators to run
    Properties props = StringUtils.argsToProperties(""-props"", ""spanish"");
    props.setProperty(""annotators"", ""tokenize,ssplit,mwt"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s"", tok.word()));
    }
  }
}
",CoreDocument,CoreDocument,CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.StringUtils;

import java.util.*;

public class MWTExpansionExample {

  public static String text = ""Pude haber querido escribirlo."";

  public static void main(String[] args) {
    // set the list of annotators to run
    Properties props = StringUtils.argsToProperties(""-props"", ""spanish"");
    props.setProperty(""annotators"", ""tokenize,ssplit,mwt"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s"", tok.word()));
    }
  }
}
",pipeline.annotate,"Util.annotate
StanfordCoreNLP.annotate","CoreNLP\src\edu\stanford\nlp\naturalli\Util.java
CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.StringUtils;

import java.util.*;

public class MWTExpansionExample {

  public static String text = ""Pude haber querido escribirlo."";

  public static void main(String[] args) {
    // set the list of annotators to run
    Properties props = StringUtils.argsToProperties(""-props"", ""spanish"");
    props.setProperty(""annotators"", ""tokenize,ssplit,mwt"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s"", tok.word()));
    }
  }
}
",doc.tokens,CoreDocument.tokens,CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.StringUtils;

import java.util.*;

public class MWTExpansionExample {

  public static String text = ""Pude haber querido escribirlo."";

  public static void main(String[] args) {
    // set the list of annotators to run
    Properties props = StringUtils.argsToProperties(""-props"", ""spanish"");
    props.setProperty(""annotators"", ""tokenize,ssplit,mwt"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s"", tok.word()));
    }
  }
}
",out.println,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.StringUtils;

import java.util.*;

public class MWTExpansionExample {

  public static String text = ""Pude haber querido escribirlo."";

  public static void main(String[] args) {
    // set the list of annotators to run
    Properties props = StringUtils.argsToProperties(""-props"", ""spanish"");
    props.setProperty(""annotators"", ""tokenize,ssplit,mwt"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s"", tok.word()));
    }
  }
}
",String.format,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.StringUtils;

import java.util.*;

public class MWTExpansionExample {

  public static String text = ""Pude haber querido escribirlo."";

  public static void main(String[] args) {
    // set the list of annotators to run
    Properties props = StringUtils.argsToProperties(""-props"", ""spanish"");
    props.setProperty(""annotators"", ""tokenize,ssplit,mwt"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s"", tok.word()));
    }
  }
}
",tok.word,CoreLabel.word,CoreNLP\src\edu\stanford\nlp\ling\CoreLabel.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class POSTaggingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.tag()));
    }
  }
}
",main,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class POSTaggingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.tag()));
    }
  }
}
",Properties,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class POSTaggingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.tag()));
    }
  }
}
",props.setProperty,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class POSTaggingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.tag()));
    }
  }
}
",StanfordCoreNLP,StanfordCoreNLP,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class POSTaggingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.tag()));
    }
  }
}
",pipeline.processToCoreDocument,StanfordCoreNLP.processToCoreDocument,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class POSTaggingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.tag()));
    }
  }
}
",document.tokens,CoreDocument.tokens,CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class POSTaggingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.tag()));
    }
  }
}
",out.println,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class POSTaggingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.tag()));
    }
  }
}
",String.format,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class POSTaggingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.tag()));
    }
  }
}
",tok.word,CoreLabel.word,CoreNLP\src\edu\stanford\nlp\ling\CoreLabel.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class POSTaggingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.tag()));
    }
  }
}
",tok.tag,"CoreLabel.tag
Tag.tag","CoreNLP\src\edu\stanford\nlp\ling\CoreLabel.java
CoreNLP\src\edu\stanford\nlp\ling\Tag.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;


public class LemmatizingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.lemma()));
    }
  }

}
",main,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;


public class LemmatizingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.lemma()));
    }
  }

}
",Properties,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;


public class LemmatizingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.lemma()));
    }
  }

}
",props.setProperty,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;


public class LemmatizingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.lemma()));
    }
  }

}
",StanfordCoreNLP,StanfordCoreNLP,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;


public class LemmatizingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.lemma()));
    }
  }

}
",pipeline.processToCoreDocument,StanfordCoreNLP.processToCoreDocument,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;


public class LemmatizingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.lemma()));
    }
  }

}
",document.tokens,CoreDocument.tokens,CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;


public class LemmatizingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.lemma()));
    }
  }

}
",out.println,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;


public class LemmatizingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.lemma()));
    }
  }

}
",String.format,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;


public class LemmatizingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.lemma()));
    }
  }

}
",tok.word,CoreLabel.word,CoreNLP\src\edu\stanford\nlp\ling\CoreLabel.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;


public class LemmatizingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.lemma()));
    }
  }

}
",tok.lemma,CoreLabel.lemma,CoreNLP\src\edu\stanford\nlp\ling\CoreLabel.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.Properties;
import java.util.stream.Collectors;

public class NERPipelineDemo {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
    // example customizations (these are commented out but you can uncomment them to see the results

    // disable fine grained ner
    // props.setProperty(""ner.applyFineGrained"", ""false"");

    // customize fine grained ner
    // props.setProperty(""ner.fine.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.fine.regexner.ignorecase"", ""true"");

    // add additional rules, customize TokensRegexNER annotator
    // props.setProperty(""ner.additional.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.additional.regexner.ignorecase"", ""true"");

    // add 2 additional rules files ; set the first one to be case-insensitive
    // props.setProperty(""ner.additional.regexner.mapping"", ""ignorecase=true,example_one.rules;example_two.rules"");

    // set document date to be a specific date (other options are explained in the document date section)
    // props.setProperty(""ner.docdate.useFixedDate"", ""2019-01-01"");

    // only run rules based NER
    // props.setProperty(""ner.rulesOnly"", ""true"");

    // only run statistical NER
    // props.setProperty(""ner.statisticalOnly"", ""true"");

    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // make an example document
    CoreDocument doc = new CoreDocument(""Joe Smith is from Seattle."");
    // annotate the document
    pipeline.annotate(doc);
    // view results
    System.out.println(""---"");
    System.out.println(""entities found"");
    for (CoreEntityMention em : doc.entityMentions())
      System.out.println(""\tdetected entity: \t""+em.text()+""\t""+em.entityType());
    System.out.println(""---"");
    System.out.println(""tokens and ner tags"");
    String tokensAndNERTags = doc.tokens().stream().map(token -> ""(""+token.word()+"",""+token.ner()+"")"").collect(
        Collectors.joining("" ""));
    System.out.println(tokensAndNERTags);
  }

}

",main,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.Properties;
import java.util.stream.Collectors;

public class NERPipelineDemo {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
    // example customizations (these are commented out but you can uncomment them to see the results

    // disable fine grained ner
    // props.setProperty(""ner.applyFineGrained"", ""false"");

    // customize fine grained ner
    // props.setProperty(""ner.fine.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.fine.regexner.ignorecase"", ""true"");

    // add additional rules, customize TokensRegexNER annotator
    // props.setProperty(""ner.additional.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.additional.regexner.ignorecase"", ""true"");

    // add 2 additional rules files ; set the first one to be case-insensitive
    // props.setProperty(""ner.additional.regexner.mapping"", ""ignorecase=true,example_one.rules;example_two.rules"");

    // set document date to be a specific date (other options are explained in the document date section)
    // props.setProperty(""ner.docdate.useFixedDate"", ""2019-01-01"");

    // only run rules based NER
    // props.setProperty(""ner.rulesOnly"", ""true"");

    // only run statistical NER
    // props.setProperty(""ner.statisticalOnly"", ""true"");

    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // make an example document
    CoreDocument doc = new CoreDocument(""Joe Smith is from Seattle."");
    // annotate the document
    pipeline.annotate(doc);
    // view results
    System.out.println(""---"");
    System.out.println(""entities found"");
    for (CoreEntityMention em : doc.entityMentions())
      System.out.println(""\tdetected entity: \t""+em.text()+""\t""+em.entityType());
    System.out.println(""---"");
    System.out.println(""tokens and ner tags"");
    String tokensAndNERTags = doc.tokens().stream().map(token -> ""(""+token.word()+"",""+token.ner()+"")"").collect(
        Collectors.joining("" ""));
    System.out.println(tokensAndNERTags);
  }

}

",Properties,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.Properties;
import java.util.stream.Collectors;

public class NERPipelineDemo {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
    // example customizations (these are commented out but you can uncomment them to see the results

    // disable fine grained ner
    // props.setProperty(""ner.applyFineGrained"", ""false"");

    // customize fine grained ner
    // props.setProperty(""ner.fine.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.fine.regexner.ignorecase"", ""true"");

    // add additional rules, customize TokensRegexNER annotator
    // props.setProperty(""ner.additional.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.additional.regexner.ignorecase"", ""true"");

    // add 2 additional rules files ; set the first one to be case-insensitive
    // props.setProperty(""ner.additional.regexner.mapping"", ""ignorecase=true,example_one.rules;example_two.rules"");

    // set document date to be a specific date (other options are explained in the document date section)
    // props.setProperty(""ner.docdate.useFixedDate"", ""2019-01-01"");

    // only run rules based NER
    // props.setProperty(""ner.rulesOnly"", ""true"");

    // only run statistical NER
    // props.setProperty(""ner.statisticalOnly"", ""true"");

    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // make an example document
    CoreDocument doc = new CoreDocument(""Joe Smith is from Seattle."");
    // annotate the document
    pipeline.annotate(doc);
    // view results
    System.out.println(""---"");
    System.out.println(""entities found"");
    for (CoreEntityMention em : doc.entityMentions())
      System.out.println(""\tdetected entity: \t""+em.text()+""\t""+em.entityType());
    System.out.println(""---"");
    System.out.println(""tokens and ner tags"");
    String tokensAndNERTags = doc.tokens().stream().map(token -> ""(""+token.word()+"",""+token.ner()+"")"").collect(
        Collectors.joining("" ""));
    System.out.println(tokensAndNERTags);
  }

}

",props.setProperty,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.Properties;
import java.util.stream.Collectors;

public class NERPipelineDemo {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
    // example customizations (these are commented out but you can uncomment them to see the results

    // disable fine grained ner
    // props.setProperty(""ner.applyFineGrained"", ""false"");

    // customize fine grained ner
    // props.setProperty(""ner.fine.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.fine.regexner.ignorecase"", ""true"");

    // add additional rules, customize TokensRegexNER annotator
    // props.setProperty(""ner.additional.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.additional.regexner.ignorecase"", ""true"");

    // add 2 additional rules files ; set the first one to be case-insensitive
    // props.setProperty(""ner.additional.regexner.mapping"", ""ignorecase=true,example_one.rules;example_two.rules"");

    // set document date to be a specific date (other options are explained in the document date section)
    // props.setProperty(""ner.docdate.useFixedDate"", ""2019-01-01"");

    // only run rules based NER
    // props.setProperty(""ner.rulesOnly"", ""true"");

    // only run statistical NER
    // props.setProperty(""ner.statisticalOnly"", ""true"");

    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // make an example document
    CoreDocument doc = new CoreDocument(""Joe Smith is from Seattle."");
    // annotate the document
    pipeline.annotate(doc);
    // view results
    System.out.println(""---"");
    System.out.println(""entities found"");
    for (CoreEntityMention em : doc.entityMentions())
      System.out.println(""\tdetected entity: \t""+em.text()+""\t""+em.entityType());
    System.out.println(""---"");
    System.out.println(""tokens and ner tags"");
    String tokensAndNERTags = doc.tokens().stream().map(token -> ""(""+token.word()+"",""+token.ner()+"")"").collect(
        Collectors.joining("" ""));
    System.out.println(tokensAndNERTags);
  }

}

",StanfordCoreNLP,StanfordCoreNLP,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.Properties;
import java.util.stream.Collectors;

public class NERPipelineDemo {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
    // example customizations (these are commented out but you can uncomment them to see the results

    // disable fine grained ner
    // props.setProperty(""ner.applyFineGrained"", ""false"");

    // customize fine grained ner
    // props.setProperty(""ner.fine.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.fine.regexner.ignorecase"", ""true"");

    // add additional rules, customize TokensRegexNER annotator
    // props.setProperty(""ner.additional.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.additional.regexner.ignorecase"", ""true"");

    // add 2 additional rules files ; set the first one to be case-insensitive
    // props.setProperty(""ner.additional.regexner.mapping"", ""ignorecase=true,example_one.rules;example_two.rules"");

    // set document date to be a specific date (other options are explained in the document date section)
    // props.setProperty(""ner.docdate.useFixedDate"", ""2019-01-01"");

    // only run rules based NER
    // props.setProperty(""ner.rulesOnly"", ""true"");

    // only run statistical NER
    // props.setProperty(""ner.statisticalOnly"", ""true"");

    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // make an example document
    CoreDocument doc = new CoreDocument(""Joe Smith is from Seattle."");
    // annotate the document
    pipeline.annotate(doc);
    // view results
    System.out.println(""---"");
    System.out.println(""entities found"");
    for (CoreEntityMention em : doc.entityMentions())
      System.out.println(""\tdetected entity: \t""+em.text()+""\t""+em.entityType());
    System.out.println(""---"");
    System.out.println(""tokens and ner tags"");
    String tokensAndNERTags = doc.tokens().stream().map(token -> ""(""+token.word()+"",""+token.ner()+"")"").collect(
        Collectors.joining("" ""));
    System.out.println(tokensAndNERTags);
  }

}

",CoreDocument,CoreDocument,CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.Properties;
import java.util.stream.Collectors;

public class NERPipelineDemo {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
    // example customizations (these are commented out but you can uncomment them to see the results

    // disable fine grained ner
    // props.setProperty(""ner.applyFineGrained"", ""false"");

    // customize fine grained ner
    // props.setProperty(""ner.fine.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.fine.regexner.ignorecase"", ""true"");

    // add additional rules, customize TokensRegexNER annotator
    // props.setProperty(""ner.additional.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.additional.regexner.ignorecase"", ""true"");

    // add 2 additional rules files ; set the first one to be case-insensitive
    // props.setProperty(""ner.additional.regexner.mapping"", ""ignorecase=true,example_one.rules;example_two.rules"");

    // set document date to be a specific date (other options are explained in the document date section)
    // props.setProperty(""ner.docdate.useFixedDate"", ""2019-01-01"");

    // only run rules based NER
    // props.setProperty(""ner.rulesOnly"", ""true"");

    // only run statistical NER
    // props.setProperty(""ner.statisticalOnly"", ""true"");

    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // make an example document
    CoreDocument doc = new CoreDocument(""Joe Smith is from Seattle."");
    // annotate the document
    pipeline.annotate(doc);
    // view results
    System.out.println(""---"");
    System.out.println(""entities found"");
    for (CoreEntityMention em : doc.entityMentions())
      System.out.println(""\tdetected entity: \t""+em.text()+""\t""+em.entityType());
    System.out.println(""---"");
    System.out.println(""tokens and ner tags"");
    String tokensAndNERTags = doc.tokens().stream().map(token -> ""(""+token.word()+"",""+token.ner()+"")"").collect(
        Collectors.joining("" ""));
    System.out.println(tokensAndNERTags);
  }

}

",pipeline.annotate,StanfordCoreNLP.annotate,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.Properties;
import java.util.stream.Collectors;

public class NERPipelineDemo {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
    // example customizations (these are commented out but you can uncomment them to see the results

    // disable fine grained ner
    // props.setProperty(""ner.applyFineGrained"", ""false"");

    // customize fine grained ner
    // props.setProperty(""ner.fine.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.fine.regexner.ignorecase"", ""true"");

    // add additional rules, customize TokensRegexNER annotator
    // props.setProperty(""ner.additional.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.additional.regexner.ignorecase"", ""true"");

    // add 2 additional rules files ; set the first one to be case-insensitive
    // props.setProperty(""ner.additional.regexner.mapping"", ""ignorecase=true,example_one.rules;example_two.rules"");

    // set document date to be a specific date (other options are explained in the document date section)
    // props.setProperty(""ner.docdate.useFixedDate"", ""2019-01-01"");

    // only run rules based NER
    // props.setProperty(""ner.rulesOnly"", ""true"");

    // only run statistical NER
    // props.setProperty(""ner.statisticalOnly"", ""true"");

    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // make an example document
    CoreDocument doc = new CoreDocument(""Joe Smith is from Seattle."");
    // annotate the document
    pipeline.annotate(doc);
    // view results
    System.out.println(""---"");
    System.out.println(""entities found"");
    for (CoreEntityMention em : doc.entityMentions())
      System.out.println(""\tdetected entity: \t""+em.text()+""\t""+em.entityType());
    System.out.println(""---"");
    System.out.println(""tokens and ner tags"");
    String tokensAndNERTags = doc.tokens().stream().map(token -> ""(""+token.word()+"",""+token.ner()+"")"").collect(
        Collectors.joining("" ""));
    System.out.println(tokensAndNERTags);
  }

}

",out.println,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.Properties;
import java.util.stream.Collectors;

public class NERPipelineDemo {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
    // example customizations (these are commented out but you can uncomment them to see the results

    // disable fine grained ner
    // props.setProperty(""ner.applyFineGrained"", ""false"");

    // customize fine grained ner
    // props.setProperty(""ner.fine.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.fine.regexner.ignorecase"", ""true"");

    // add additional rules, customize TokensRegexNER annotator
    // props.setProperty(""ner.additional.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.additional.regexner.ignorecase"", ""true"");

    // add 2 additional rules files ; set the first one to be case-insensitive
    // props.setProperty(""ner.additional.regexner.mapping"", ""ignorecase=true,example_one.rules;example_two.rules"");

    // set document date to be a specific date (other options are explained in the document date section)
    // props.setProperty(""ner.docdate.useFixedDate"", ""2019-01-01"");

    // only run rules based NER
    // props.setProperty(""ner.rulesOnly"", ""true"");

    // only run statistical NER
    // props.setProperty(""ner.statisticalOnly"", ""true"");

    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // make an example document
    CoreDocument doc = new CoreDocument(""Joe Smith is from Seattle."");
    // annotate the document
    pipeline.annotate(doc);
    // view results
    System.out.println(""---"");
    System.out.println(""entities found"");
    for (CoreEntityMention em : doc.entityMentions())
      System.out.println(""\tdetected entity: \t""+em.text()+""\t""+em.entityType());
    System.out.println(""---"");
    System.out.println(""tokens and ner tags"");
    String tokensAndNERTags = doc.tokens().stream().map(token -> ""(""+token.word()+"",""+token.ner()+"")"").collect(
        Collectors.joining("" ""));
    System.out.println(tokensAndNERTags);
  }

}

",doc.entityMentions,CoreDocument.entityMentions,CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.Properties;
import java.util.stream.Collectors;

public class NERPipelineDemo {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
    // example customizations (these are commented out but you can uncomment them to see the results

    // disable fine grained ner
    // props.setProperty(""ner.applyFineGrained"", ""false"");

    // customize fine grained ner
    // props.setProperty(""ner.fine.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.fine.regexner.ignorecase"", ""true"");

    // add additional rules, customize TokensRegexNER annotator
    // props.setProperty(""ner.additional.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.additional.regexner.ignorecase"", ""true"");

    // add 2 additional rules files ; set the first one to be case-insensitive
    // props.setProperty(""ner.additional.regexner.mapping"", ""ignorecase=true,example_one.rules;example_two.rules"");

    // set document date to be a specific date (other options are explained in the document date section)
    // props.setProperty(""ner.docdate.useFixedDate"", ""2019-01-01"");

    // only run rules based NER
    // props.setProperty(""ner.rulesOnly"", ""true"");

    // only run statistical NER
    // props.setProperty(""ner.statisticalOnly"", ""true"");

    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // make an example document
    CoreDocument doc = new CoreDocument(""Joe Smith is from Seattle."");
    // annotate the document
    pipeline.annotate(doc);
    // view results
    System.out.println(""---"");
    System.out.println(""entities found"");
    for (CoreEntityMention em : doc.entityMentions())
      System.out.println(""\tdetected entity: \t""+em.text()+""\t""+em.entityType());
    System.out.println(""---"");
    System.out.println(""tokens and ner tags"");
    String tokensAndNERTags = doc.tokens().stream().map(token -> ""(""+token.word()+"",""+token.ner()+"")"").collect(
        Collectors.joining("" ""));
    System.out.println(tokensAndNERTags);
  }

}

",em.text,"CoreEntityMention.text
Document.text
CoreDocument.text","CoreNLP\src\edu\stanford\nlp\pipeline\CoreEntityMention.java
CoreNLP\src\edu\stanford\nlp\simple\Document.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.Properties;
import java.util.stream.Collectors;

public class NERPipelineDemo {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
    // example customizations (these are commented out but you can uncomment them to see the results

    // disable fine grained ner
    // props.setProperty(""ner.applyFineGrained"", ""false"");

    // customize fine grained ner
    // props.setProperty(""ner.fine.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.fine.regexner.ignorecase"", ""true"");

    // add additional rules, customize TokensRegexNER annotator
    // props.setProperty(""ner.additional.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.additional.regexner.ignorecase"", ""true"");

    // add 2 additional rules files ; set the first one to be case-insensitive
    // props.setProperty(""ner.additional.regexner.mapping"", ""ignorecase=true,example_one.rules;example_two.rules"");

    // set document date to be a specific date (other options are explained in the document date section)
    // props.setProperty(""ner.docdate.useFixedDate"", ""2019-01-01"");

    // only run rules based NER
    // props.setProperty(""ner.rulesOnly"", ""true"");

    // only run statistical NER
    // props.setProperty(""ner.statisticalOnly"", ""true"");

    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // make an example document
    CoreDocument doc = new CoreDocument(""Joe Smith is from Seattle."");
    // annotate the document
    pipeline.annotate(doc);
    // view results
    System.out.println(""---"");
    System.out.println(""entities found"");
    for (CoreEntityMention em : doc.entityMentions())
      System.out.println(""\tdetected entity: \t""+em.text()+""\t""+em.entityType());
    System.out.println(""---"");
    System.out.println(""tokens and ner tags"");
    String tokensAndNERTags = doc.tokens().stream().map(token -> ""(""+token.word()+"",""+token.ner()+"")"").collect(
        Collectors.joining("" ""));
    System.out.println(tokensAndNERTags);
  }

}

",em.entityType,CoreEntityMention.entityType,CoreNLP\src\edu\stanford\nlp\pipeline\CoreEntityMention.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.Properties;
import java.util.stream.Collectors;

public class NERPipelineDemo {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
    // example customizations (these are commented out but you can uncomment them to see the results

    // disable fine grained ner
    // props.setProperty(""ner.applyFineGrained"", ""false"");

    // customize fine grained ner
    // props.setProperty(""ner.fine.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.fine.regexner.ignorecase"", ""true"");

    // add additional rules, customize TokensRegexNER annotator
    // props.setProperty(""ner.additional.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.additional.regexner.ignorecase"", ""true"");

    // add 2 additional rules files ; set the first one to be case-insensitive
    // props.setProperty(""ner.additional.regexner.mapping"", ""ignorecase=true,example_one.rules;example_two.rules"");

    // set document date to be a specific date (other options are explained in the document date section)
    // props.setProperty(""ner.docdate.useFixedDate"", ""2019-01-01"");

    // only run rules based NER
    // props.setProperty(""ner.rulesOnly"", ""true"");

    // only run statistical NER
    // props.setProperty(""ner.statisticalOnly"", ""true"");

    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // make an example document
    CoreDocument doc = new CoreDocument(""Joe Smith is from Seattle."");
    // annotate the document
    pipeline.annotate(doc);
    // view results
    System.out.println(""---"");
    System.out.println(""entities found"");
    for (CoreEntityMention em : doc.entityMentions())
      System.out.println(""\tdetected entity: \t""+em.text()+""\t""+em.entityType());
    System.out.println(""---"");
    System.out.println(""tokens and ner tags"");
    String tokensAndNERTags = doc.tokens().stream().map(token -> ""(""+token.word()+"",""+token.ner()+"")"").collect(
        Collectors.joining("" ""));
    System.out.println(tokensAndNERTags);
  }

}

",doc.tokens,"CoreDocument.tokens
CoreEntityMention.tokens","CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreEntityMention.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.Properties;
import java.util.stream.Collectors;

public class NERPipelineDemo {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
    // example customizations (these are commented out but you can uncomment them to see the results

    // disable fine grained ner
    // props.setProperty(""ner.applyFineGrained"", ""false"");

    // customize fine grained ner
    // props.setProperty(""ner.fine.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.fine.regexner.ignorecase"", ""true"");

    // add additional rules, customize TokensRegexNER annotator
    // props.setProperty(""ner.additional.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.additional.regexner.ignorecase"", ""true"");

    // add 2 additional rules files ; set the first one to be case-insensitive
    // props.setProperty(""ner.additional.regexner.mapping"", ""ignorecase=true,example_one.rules;example_two.rules"");

    // set document date to be a specific date (other options are explained in the document date section)
    // props.setProperty(""ner.docdate.useFixedDate"", ""2019-01-01"");

    // only run rules based NER
    // props.setProperty(""ner.rulesOnly"", ""true"");

    // only run statistical NER
    // props.setProperty(""ner.statisticalOnly"", ""true"");

    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // make an example document
    CoreDocument doc = new CoreDocument(""Joe Smith is from Seattle."");
    // annotate the document
    pipeline.annotate(doc);
    // view results
    System.out.println(""---"");
    System.out.println(""entities found"");
    for (CoreEntityMention em : doc.entityMentions())
      System.out.println(""\tdetected entity: \t""+em.text()+""\t""+em.entityType());
    System.out.println(""---"");
    System.out.println(""tokens and ner tags"");
    String tokensAndNERTags = doc.tokens().stream().map(token -> ""(""+token.word()+"",""+token.ner()+"")"").collect(
        Collectors.joining("" ""));
    System.out.println(tokensAndNERTags);
  }

}

",stream,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.Properties;
import java.util.stream.Collectors;

public class NERPipelineDemo {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
    // example customizations (these are commented out but you can uncomment them to see the results

    // disable fine grained ner
    // props.setProperty(""ner.applyFineGrained"", ""false"");

    // customize fine grained ner
    // props.setProperty(""ner.fine.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.fine.regexner.ignorecase"", ""true"");

    // add additional rules, customize TokensRegexNER annotator
    // props.setProperty(""ner.additional.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.additional.regexner.ignorecase"", ""true"");

    // add 2 additional rules files ; set the first one to be case-insensitive
    // props.setProperty(""ner.additional.regexner.mapping"", ""ignorecase=true,example_one.rules;example_two.rules"");

    // set document date to be a specific date (other options are explained in the document date section)
    // props.setProperty(""ner.docdate.useFixedDate"", ""2019-01-01"");

    // only run rules based NER
    // props.setProperty(""ner.rulesOnly"", ""true"");

    // only run statistical NER
    // props.setProperty(""ner.statisticalOnly"", ""true"");

    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // make an example document
    CoreDocument doc = new CoreDocument(""Joe Smith is from Seattle."");
    // annotate the document
    pipeline.annotate(doc);
    // view results
    System.out.println(""---"");
    System.out.println(""entities found"");
    for (CoreEntityMention em : doc.entityMentions())
      System.out.println(""\tdetected entity: \t""+em.text()+""\t""+em.entityType());
    System.out.println(""---"");
    System.out.println(""tokens and ner tags"");
    String tokensAndNERTags = doc.tokens().stream().map(token -> ""(""+token.word()+"",""+token.ner()+"")"").collect(
        Collectors.joining("" ""));
    System.out.println(tokensAndNERTags);
  }

}

",map,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.Properties;
import java.util.stream.Collectors;

public class NERPipelineDemo {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
    // example customizations (these are commented out but you can uncomment them to see the results

    // disable fine grained ner
    // props.setProperty(""ner.applyFineGrained"", ""false"");

    // customize fine grained ner
    // props.setProperty(""ner.fine.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.fine.regexner.ignorecase"", ""true"");

    // add additional rules, customize TokensRegexNER annotator
    // props.setProperty(""ner.additional.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.additional.regexner.ignorecase"", ""true"");

    // add 2 additional rules files ; set the first one to be case-insensitive
    // props.setProperty(""ner.additional.regexner.mapping"", ""ignorecase=true,example_one.rules;example_two.rules"");

    // set document date to be a specific date (other options are explained in the document date section)
    // props.setProperty(""ner.docdate.useFixedDate"", ""2019-01-01"");

    // only run rules based NER
    // props.setProperty(""ner.rulesOnly"", ""true"");

    // only run statistical NER
    // props.setProperty(""ner.statisticalOnly"", ""true"");

    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // make an example document
    CoreDocument doc = new CoreDocument(""Joe Smith is from Seattle."");
    // annotate the document
    pipeline.annotate(doc);
    // view results
    System.out.println(""---"");
    System.out.println(""entities found"");
    for (CoreEntityMention em : doc.entityMentions())
      System.out.println(""\tdetected entity: \t""+em.text()+""\t""+em.entityType());
    System.out.println(""---"");
    System.out.println(""tokens and ner tags"");
    String tokensAndNERTags = doc.tokens().stream().map(token -> ""(""+token.word()+"",""+token.ner()+"")"").collect(
        Collectors.joining("" ""));
    System.out.println(tokensAndNERTags);
  }

}

",token.word,Token.word,CoreNLP\src\edu\stanford\nlp\simple\Token.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.Properties;
import java.util.stream.Collectors;

public class NERPipelineDemo {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
    // example customizations (these are commented out but you can uncomment them to see the results

    // disable fine grained ner
    // props.setProperty(""ner.applyFineGrained"", ""false"");

    // customize fine grained ner
    // props.setProperty(""ner.fine.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.fine.regexner.ignorecase"", ""true"");

    // add additional rules, customize TokensRegexNER annotator
    // props.setProperty(""ner.additional.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.additional.regexner.ignorecase"", ""true"");

    // add 2 additional rules files ; set the first one to be case-insensitive
    // props.setProperty(""ner.additional.regexner.mapping"", ""ignorecase=true,example_one.rules;example_two.rules"");

    // set document date to be a specific date (other options are explained in the document date section)
    // props.setProperty(""ner.docdate.useFixedDate"", ""2019-01-01"");

    // only run rules based NER
    // props.setProperty(""ner.rulesOnly"", ""true"");

    // only run statistical NER
    // props.setProperty(""ner.statisticalOnly"", ""true"");

    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // make an example document
    CoreDocument doc = new CoreDocument(""Joe Smith is from Seattle."");
    // annotate the document
    pipeline.annotate(doc);
    // view results
    System.out.println(""---"");
    System.out.println(""entities found"");
    for (CoreEntityMention em : doc.entityMentions())
      System.out.println(""\tdetected entity: \t""+em.text()+""\t""+em.entityType());
    System.out.println(""---"");
    System.out.println(""tokens and ner tags"");
    String tokensAndNERTags = doc.tokens().stream().map(token -> ""(""+token.word()+"",""+token.ner()+"")"").collect(
        Collectors.joining("" ""));
    System.out.println(tokensAndNERTags);
  }

}

",token.ner,Token.ner,CoreNLP\src\edu\stanford\nlp\simple\Token.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.Properties;
import java.util.stream.Collectors;

public class NERPipelineDemo {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
    // example customizations (these are commented out but you can uncomment them to see the results

    // disable fine grained ner
    // props.setProperty(""ner.applyFineGrained"", ""false"");

    // customize fine grained ner
    // props.setProperty(""ner.fine.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.fine.regexner.ignorecase"", ""true"");

    // add additional rules, customize TokensRegexNER annotator
    // props.setProperty(""ner.additional.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.additional.regexner.ignorecase"", ""true"");

    // add 2 additional rules files ; set the first one to be case-insensitive
    // props.setProperty(""ner.additional.regexner.mapping"", ""ignorecase=true,example_one.rules;example_two.rules"");

    // set document date to be a specific date (other options are explained in the document date section)
    // props.setProperty(""ner.docdate.useFixedDate"", ""2019-01-01"");

    // only run rules based NER
    // props.setProperty(""ner.rulesOnly"", ""true"");

    // only run statistical NER
    // props.setProperty(""ner.statisticalOnly"", ""true"");

    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // make an example document
    CoreDocument doc = new CoreDocument(""Joe Smith is from Seattle."");
    // annotate the document
    pipeline.annotate(doc);
    // view results
    System.out.println(""---"");
    System.out.println(""entities found"");
    for (CoreEntityMention em : doc.entityMentions())
      System.out.println(""\tdetected entity: \t""+em.text()+""\t""+em.entityType());
    System.out.println(""---"");
    System.out.println(""tokens and ner tags"");
    String tokensAndNERTags = doc.tokens().stream().map(token -> ""(""+token.word()+"",""+token.ner()+"")"").collect(
        Collectors.joining("" ""));
    System.out.println(tokensAndNERTags);
  }

}

",collect,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.Properties;
import java.util.stream.Collectors;

public class NERPipelineDemo {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
    // example customizations (these are commented out but you can uncomment them to see the results

    // disable fine grained ner
    // props.setProperty(""ner.applyFineGrained"", ""false"");

    // customize fine grained ner
    // props.setProperty(""ner.fine.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.fine.regexner.ignorecase"", ""true"");

    // add additional rules, customize TokensRegexNER annotator
    // props.setProperty(""ner.additional.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.additional.regexner.ignorecase"", ""true"");

    // add 2 additional rules files ; set the first one to be case-insensitive
    // props.setProperty(""ner.additional.regexner.mapping"", ""ignorecase=true,example_one.rules;example_two.rules"");

    // set document date to be a specific date (other options are explained in the document date section)
    // props.setProperty(""ner.docdate.useFixedDate"", ""2019-01-01"");

    // only run rules based NER
    // props.setProperty(""ner.rulesOnly"", ""true"");

    // only run statistical NER
    // props.setProperty(""ner.statisticalOnly"", ""true"");

    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // make an example document
    CoreDocument doc = new CoreDocument(""Joe Smith is from Seattle."");
    // annotate the document
    pipeline.annotate(doc);
    // view results
    System.out.println(""---"");
    System.out.println(""entities found"");
    for (CoreEntityMention em : doc.entityMentions())
      System.out.println(""\tdetected entity: \t""+em.text()+""\t""+em.entityType());
    System.out.println(""---"");
    System.out.println(""tokens and ner tags"");
    String tokensAndNERTags = doc.tokens().stream().map(token -> ""(""+token.word()+"",""+token.ner()+"")"").collect(
        Collectors.joining("" ""));
    System.out.println(tokensAndNERTags);
  }

}

",Collectors.joining,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class NERConfidenceExample {

    public static void main(String[] args) {
        String exampleText = ""Joe Smith lives in California."";
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        CoreDocument document = new CoreDocument(exampleText);
        pipeline.annotate(document);
        // get confidences for entities
        for (CoreEntityMention em : document.entityMentions()) {
            System.out.println(em.text() + ""\t"" + em.entityTypeConfidences());
        }
        // get confidences for tokens
        for (CoreLabel token : document.tokens()) {
            System.out.println(token.word() + ""\t"" + token.get(CoreAnnotations.NamedEntityTagProbsAnnotation.class));
        }
    }
}
",main,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class NERConfidenceExample {

    public static void main(String[] args) {
        String exampleText = ""Joe Smith lives in California."";
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        CoreDocument document = new CoreDocument(exampleText);
        pipeline.annotate(document);
        // get confidences for entities
        for (CoreEntityMention em : document.entityMentions()) {
            System.out.println(em.text() + ""\t"" + em.entityTypeConfidences());
        }
        // get confidences for tokens
        for (CoreLabel token : document.tokens()) {
            System.out.println(token.word() + ""\t"" + token.get(CoreAnnotations.NamedEntityTagProbsAnnotation.class));
        }
    }
}
",Properties,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class NERConfidenceExample {

    public static void main(String[] args) {
        String exampleText = ""Joe Smith lives in California."";
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        CoreDocument document = new CoreDocument(exampleText);
        pipeline.annotate(document);
        // get confidences for entities
        for (CoreEntityMention em : document.entityMentions()) {
            System.out.println(em.text() + ""\t"" + em.entityTypeConfidences());
        }
        // get confidences for tokens
        for (CoreLabel token : document.tokens()) {
            System.out.println(token.word() + ""\t"" + token.get(CoreAnnotations.NamedEntityTagProbsAnnotation.class));
        }
    }
}
",props.setProperty,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class NERConfidenceExample {

    public static void main(String[] args) {
        String exampleText = ""Joe Smith lives in California."";
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        CoreDocument document = new CoreDocument(exampleText);
        pipeline.annotate(document);
        // get confidences for entities
        for (CoreEntityMention em : document.entityMentions()) {
            System.out.println(em.text() + ""\t"" + em.entityTypeConfidences());
        }
        // get confidences for tokens
        for (CoreLabel token : document.tokens()) {
            System.out.println(token.word() + ""\t"" + token.get(CoreAnnotations.NamedEntityTagProbsAnnotation.class));
        }
    }
}
",StanfordCoreNLP,StanfordCoreNLP,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class NERConfidenceExample {

    public static void main(String[] args) {
        String exampleText = ""Joe Smith lives in California."";
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        CoreDocument document = new CoreDocument(exampleText);
        pipeline.annotate(document);
        // get confidences for entities
        for (CoreEntityMention em : document.entityMentions()) {
            System.out.println(em.text() + ""\t"" + em.entityTypeConfidences());
        }
        // get confidences for tokens
        for (CoreLabel token : document.tokens()) {
            System.out.println(token.word() + ""\t"" + token.get(CoreAnnotations.NamedEntityTagProbsAnnotation.class));
        }
    }
}
",CoreDocument,CoreDocument,CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class NERConfidenceExample {

    public static void main(String[] args) {
        String exampleText = ""Joe Smith lives in California."";
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        CoreDocument document = new CoreDocument(exampleText);
        pipeline.annotate(document);
        // get confidences for entities
        for (CoreEntityMention em : document.entityMentions()) {
            System.out.println(em.text() + ""\t"" + em.entityTypeConfidences());
        }
        // get confidences for tokens
        for (CoreLabel token : document.tokens()) {
            System.out.println(token.word() + ""\t"" + token.get(CoreAnnotations.NamedEntityTagProbsAnnotation.class));
        }
    }
}
",pipeline.annotate,StanfordCoreNLP.annotate,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class NERConfidenceExample {

    public static void main(String[] args) {
        String exampleText = ""Joe Smith lives in California."";
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        CoreDocument document = new CoreDocument(exampleText);
        pipeline.annotate(document);
        // get confidences for entities
        for (CoreEntityMention em : document.entityMentions()) {
            System.out.println(em.text() + ""\t"" + em.entityTypeConfidences());
        }
        // get confidences for tokens
        for (CoreLabel token : document.tokens()) {
            System.out.println(token.word() + ""\t"" + token.get(CoreAnnotations.NamedEntityTagProbsAnnotation.class));
        }
    }
}
",document.entityMentions,CoreDocument.entityMentions,CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class NERConfidenceExample {

    public static void main(String[] args) {
        String exampleText = ""Joe Smith lives in California."";
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        CoreDocument document = new CoreDocument(exampleText);
        pipeline.annotate(document);
        // get confidences for entities
        for (CoreEntityMention em : document.entityMentions()) {
            System.out.println(em.text() + ""\t"" + em.entityTypeConfidences());
        }
        // get confidences for tokens
        for (CoreLabel token : document.tokens()) {
            System.out.println(token.word() + ""\t"" + token.get(CoreAnnotations.NamedEntityTagProbsAnnotation.class));
        }
    }
}
",out.println,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class NERConfidenceExample {

    public static void main(String[] args) {
        String exampleText = ""Joe Smith lives in California."";
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        CoreDocument document = new CoreDocument(exampleText);
        pipeline.annotate(document);
        // get confidences for entities
        for (CoreEntityMention em : document.entityMentions()) {
            System.out.println(em.text() + ""\t"" + em.entityTypeConfidences());
        }
        // get confidences for tokens
        for (CoreLabel token : document.tokens()) {
            System.out.println(token.word() + ""\t"" + token.get(CoreAnnotations.NamedEntityTagProbsAnnotation.class));
        }
    }
}
",em.text,"CoreEntityMention.text
Document.text
CoreDocument.text","CoreNLP\src\edu\stanford\nlp\pipeline\CoreEntityMention.java
CoreNLP\src\edu\stanford\nlp\simple\Document.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class NERConfidenceExample {

    public static void main(String[] args) {
        String exampleText = ""Joe Smith lives in California."";
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        CoreDocument document = new CoreDocument(exampleText);
        pipeline.annotate(document);
        // get confidences for entities
        for (CoreEntityMention em : document.entityMentions()) {
            System.out.println(em.text() + ""\t"" + em.entityTypeConfidences());
        }
        // get confidences for tokens
        for (CoreLabel token : document.tokens()) {
            System.out.println(token.word() + ""\t"" + token.get(CoreAnnotations.NamedEntityTagProbsAnnotation.class));
        }
    }
}
",em.entityTypeConfidences,CoreEntityMention.entityTypeConfidences,CoreNLP\src\edu\stanford\nlp\pipeline\CoreEntityMention.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class NERConfidenceExample {

    public static void main(String[] args) {
        String exampleText = ""Joe Smith lives in California."";
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        CoreDocument document = new CoreDocument(exampleText);
        pipeline.annotate(document);
        // get confidences for entities
        for (CoreEntityMention em : document.entityMentions()) {
            System.out.println(em.text() + ""\t"" + em.entityTypeConfidences());
        }
        // get confidences for tokens
        for (CoreLabel token : document.tokens()) {
            System.out.println(token.word() + ""\t"" + token.get(CoreAnnotations.NamedEntityTagProbsAnnotation.class));
        }
    }
}
",document.tokens,"CoreDocument.tokens
CoreEntityMention.tokens","CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreEntityMention.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class NERConfidenceExample {

    public static void main(String[] args) {
        String exampleText = ""Joe Smith lives in California."";
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        CoreDocument document = new CoreDocument(exampleText);
        pipeline.annotate(document);
        // get confidences for entities
        for (CoreEntityMention em : document.entityMentions()) {
            System.out.println(em.text() + ""\t"" + em.entityTypeConfidences());
        }
        // get confidences for tokens
        for (CoreLabel token : document.tokens()) {
            System.out.println(token.word() + ""\t"" + token.get(CoreAnnotations.NamedEntityTagProbsAnnotation.class));
        }
    }
}
",token.word,CoreLabel.word,CoreNLP\src\edu\stanford\nlp\ling\CoreLabel.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class NERConfidenceExample {

    public static void main(String[] args) {
        String exampleText = ""Joe Smith lives in California."";
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        CoreDocument document = new CoreDocument(exampleText);
        pipeline.annotate(document);
        // get confidences for entities
        for (CoreEntityMention em : document.entityMentions()) {
            System.out.println(em.text() + ""\t"" + em.entityTypeConfidences());
        }
        // get confidences for tokens
        for (CoreLabel token : document.tokens()) {
            System.out.println(token.word() + ""\t"" + token.get(CoreAnnotations.NamedEntityTagProbsAnnotation.class));
        }
    }
}
",token.get,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}
",main,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}
",Properties,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}
",props.setProperty,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}
",StanfordCoreNLP,StanfordCoreNLP,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}
",Annotation,Annotation,CoreNLP\src\edu\stanford\nlp\pipeline\Annotation.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}
",pipeline.annotate,StanfordCoreNLP.annotate,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}
",annotation.get,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}
",get,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}
",out.println,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}
",tree.constituents,Tree.constituents,CoreNLP\src\edu\stanford\nlp\trees\Tree.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}
",LabeledScoredConstituentFactory,LabeledScoredConstituentFactory,CoreNLP\src\edu\stanford\nlp\trees\LabeledScoredConstituentFactory.java,True
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}
",constituent.label,"Constituent.label
Tree.label","CoreNLP\src\edu\stanford\nlp\trees\Constituent.java
CoreNLP\src\edu\stanford\nlp\trees\Tree.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}
",toString,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}
",equals,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}
",err.println,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}
",constituent.toString,"Sentence.toString
Tree.toString
Constituent.toString
Annotation.toString","CoreNLP\src\edu\stanford\nlp\simple\Sentence.java
CoreNLP\src\edu\stanford\nlp\trees\Tree.java
CoreNLP\src\edu\stanford\nlp\trees\Constituent.java
CoreNLP\src\edu\stanford\nlp\pipeline\Annotation.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}
",tree.getLeaves,Tree.getLeaves,CoreNLP\src\edu\stanford\nlp\trees\Tree.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}
",subList,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}
",constituent.start,Constituent.start,CoreNLP\src\edu\stanford\nlp\trees\Constituent.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}
",constituent.end,Constituent.end,CoreNLP\src\edu\stanford\nlp\trees\Constituent.java,False
"import java.util.Properties;

import edu.stanford.nlp.coref.CorefCoreAnnotations;
import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.coref.data.Mention;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;

public class CorefExample {
  public static void main(String[] args) throws Exception {
    Annotation document = new Annotation(""Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008."");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,coref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);
    System.out.println(""---"");
    System.out.println(""coref chains"");
    for (CorefChain cc : document.get(CorefCoreAnnotations.CorefChainAnnotation.class).values()) {
      System.out.println(""\t"" + cc);
    }
    for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""mentions"");
      for (Mention m : sentence.get(CorefCoreAnnotations.CorefMentionsAnnotation.class)) {
        System.out.println(""\t"" + m);
       }
    }
  }
}
",main,N/A,N/A,N/A
"import java.util.Properties;

import edu.stanford.nlp.coref.CorefCoreAnnotations;
import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.coref.data.Mention;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;

public class CorefExample {
  public static void main(String[] args) throws Exception {
    Annotation document = new Annotation(""Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008."");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,coref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);
    System.out.println(""---"");
    System.out.println(""coref chains"");
    for (CorefChain cc : document.get(CorefCoreAnnotations.CorefChainAnnotation.class).values()) {
      System.out.println(""\t"" + cc);
    }
    for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""mentions"");
      for (Mention m : sentence.get(CorefCoreAnnotations.CorefMentionsAnnotation.class)) {
        System.out.println(""\t"" + m);
       }
    }
  }
}
",Annotation,Annotation,CoreNLP\src\edu\stanford\nlp\pipeline\Annotation.java,False
"import java.util.Properties;

import edu.stanford.nlp.coref.CorefCoreAnnotations;
import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.coref.data.Mention;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;

public class CorefExample {
  public static void main(String[] args) throws Exception {
    Annotation document = new Annotation(""Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008."");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,coref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);
    System.out.println(""---"");
    System.out.println(""coref chains"");
    for (CorefChain cc : document.get(CorefCoreAnnotations.CorefChainAnnotation.class).values()) {
      System.out.println(""\t"" + cc);
    }
    for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""mentions"");
      for (Mention m : sentence.get(CorefCoreAnnotations.CorefMentionsAnnotation.class)) {
        System.out.println(""\t"" + m);
       }
    }
  }
}
",Properties,N/A,N/A,N/A
"import java.util.Properties;

import edu.stanford.nlp.coref.CorefCoreAnnotations;
import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.coref.data.Mention;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;

public class CorefExample {
  public static void main(String[] args) throws Exception {
    Annotation document = new Annotation(""Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008."");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,coref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);
    System.out.println(""---"");
    System.out.println(""coref chains"");
    for (CorefChain cc : document.get(CorefCoreAnnotations.CorefChainAnnotation.class).values()) {
      System.out.println(""\t"" + cc);
    }
    for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""mentions"");
      for (Mention m : sentence.get(CorefCoreAnnotations.CorefMentionsAnnotation.class)) {
        System.out.println(""\t"" + m);
       }
    }
  }
}
",props.setProperty,N/A,N/A,N/A
"import java.util.Properties;

import edu.stanford.nlp.coref.CorefCoreAnnotations;
import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.coref.data.Mention;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;

public class CorefExample {
  public static void main(String[] args) throws Exception {
    Annotation document = new Annotation(""Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008."");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,coref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);
    System.out.println(""---"");
    System.out.println(""coref chains"");
    for (CorefChain cc : document.get(CorefCoreAnnotations.CorefChainAnnotation.class).values()) {
      System.out.println(""\t"" + cc);
    }
    for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""mentions"");
      for (Mention m : sentence.get(CorefCoreAnnotations.CorefMentionsAnnotation.class)) {
        System.out.println(""\t"" + m);
       }
    }
  }
}
",StanfordCoreNLP,StanfordCoreNLP,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"import java.util.Properties;

import edu.stanford.nlp.coref.CorefCoreAnnotations;
import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.coref.data.Mention;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;

public class CorefExample {
  public static void main(String[] args) throws Exception {
    Annotation document = new Annotation(""Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008."");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,coref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);
    System.out.println(""---"");
    System.out.println(""coref chains"");
    for (CorefChain cc : document.get(CorefCoreAnnotations.CorefChainAnnotation.class).values()) {
      System.out.println(""\t"" + cc);
    }
    for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""mentions"");
      for (Mention m : sentence.get(CorefCoreAnnotations.CorefMentionsAnnotation.class)) {
        System.out.println(""\t"" + m);
       }
    }
  }
}
",pipeline.annotate,StanfordCoreNLP.annotate,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"import java.util.Properties;

import edu.stanford.nlp.coref.CorefCoreAnnotations;
import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.coref.data.Mention;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;

public class CorefExample {
  public static void main(String[] args) throws Exception {
    Annotation document = new Annotation(""Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008."");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,coref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);
    System.out.println(""---"");
    System.out.println(""coref chains"");
    for (CorefChain cc : document.get(CorefCoreAnnotations.CorefChainAnnotation.class).values()) {
      System.out.println(""\t"" + cc);
    }
    for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""mentions"");
      for (Mention m : sentence.get(CorefCoreAnnotations.CorefMentionsAnnotation.class)) {
        System.out.println(""\t"" + m);
       }
    }
  }
}
",out.println,N/A,N/A,N/A
"import java.util.Properties;

import edu.stanford.nlp.coref.CorefCoreAnnotations;
import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.coref.data.Mention;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;

public class CorefExample {
  public static void main(String[] args) throws Exception {
    Annotation document = new Annotation(""Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008."");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,coref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);
    System.out.println(""---"");
    System.out.println(""coref chains"");
    for (CorefChain cc : document.get(CorefCoreAnnotations.CorefChainAnnotation.class).values()) {
      System.out.println(""\t"" + cc);
    }
    for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""mentions"");
      for (Mention m : sentence.get(CorefCoreAnnotations.CorefMentionsAnnotation.class)) {
        System.out.println(""\t"" + m);
       }
    }
  }
}
",document.get,N/A,N/A,N/A
"import java.util.Properties;

import edu.stanford.nlp.coref.CorefCoreAnnotations;
import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.coref.data.Mention;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;

public class CorefExample {
  public static void main(String[] args) throws Exception {
    Annotation document = new Annotation(""Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008."");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,coref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);
    System.out.println(""---"");
    System.out.println(""coref chains"");
    for (CorefChain cc : document.get(CorefCoreAnnotations.CorefChainAnnotation.class).values()) {
      System.out.println(""\t"" + cc);
    }
    for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""mentions"");
      for (Mention m : sentence.get(CorefCoreAnnotations.CorefMentionsAnnotation.class)) {
        System.out.println(""\t"" + m);
       }
    }
  }
}
",values,N/A,N/A,N/A
"import java.util.Properties;

import edu.stanford.nlp.coref.CorefCoreAnnotations;
import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.coref.data.Mention;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;

public class CorefExample {
  public static void main(String[] args) throws Exception {
    Annotation document = new Annotation(""Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008."");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,coref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);
    System.out.println(""---"");
    System.out.println(""coref chains"");
    for (CorefChain cc : document.get(CorefCoreAnnotations.CorefChainAnnotation.class).values()) {
      System.out.println(""\t"" + cc);
    }
    for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""mentions"");
      for (Mention m : sentence.get(CorefCoreAnnotations.CorefMentionsAnnotation.class)) {
        System.out.println(""\t"" + m);
       }
    }
  }
}
",sentence.get,N/A,N/A,N/A
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.naturalli.NaturalLogicAnnotations;
import edu.stanford.nlp.util.CoreMap;

import java.util.Collection;
import java.util.Properties;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create the Stanford CoreNLP pipeline
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,depparse,natlog,openie"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // Annotate an example document.
    Annotation doc = new Annotation(""Obama was born in Hawaii. He is our president."");
    pipeline.annotate(doc);

    // Loop over sentences in the document
    for (CoreMap sentence : doc.get(CoreAnnotations.SentencesAnnotation.class)) {
      // Get the OpenIE triples for the sentence
      Collection<RelationTriple> triples =
	          sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class);
      // Print the triples
      for (RelationTriple triple : triples) {
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}
",main,N/A,N/A,N/A
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.naturalli.NaturalLogicAnnotations;
import edu.stanford.nlp.util.CoreMap;

import java.util.Collection;
import java.util.Properties;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create the Stanford CoreNLP pipeline
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,depparse,natlog,openie"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // Annotate an example document.
    Annotation doc = new Annotation(""Obama was born in Hawaii. He is our president."");
    pipeline.annotate(doc);

    // Loop over sentences in the document
    for (CoreMap sentence : doc.get(CoreAnnotations.SentencesAnnotation.class)) {
      // Get the OpenIE triples for the sentence
      Collection<RelationTriple> triples =
	          sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class);
      // Print the triples
      for (RelationTriple triple : triples) {
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}
",Properties,N/A,N/A,N/A
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.naturalli.NaturalLogicAnnotations;
import edu.stanford.nlp.util.CoreMap;

import java.util.Collection;
import java.util.Properties;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create the Stanford CoreNLP pipeline
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,depparse,natlog,openie"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // Annotate an example document.
    Annotation doc = new Annotation(""Obama was born in Hawaii. He is our president."");
    pipeline.annotate(doc);

    // Loop over sentences in the document
    for (CoreMap sentence : doc.get(CoreAnnotations.SentencesAnnotation.class)) {
      // Get the OpenIE triples for the sentence
      Collection<RelationTriple> triples =
	          sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class);
      // Print the triples
      for (RelationTriple triple : triples) {
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}
",props.setProperty,N/A,N/A,N/A
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.naturalli.NaturalLogicAnnotations;
import edu.stanford.nlp.util.CoreMap;

import java.util.Collection;
import java.util.Properties;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create the Stanford CoreNLP pipeline
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,depparse,natlog,openie"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // Annotate an example document.
    Annotation doc = new Annotation(""Obama was born in Hawaii. He is our president."");
    pipeline.annotate(doc);

    // Loop over sentences in the document
    for (CoreMap sentence : doc.get(CoreAnnotations.SentencesAnnotation.class)) {
      // Get the OpenIE triples for the sentence
      Collection<RelationTriple> triples =
	          sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class);
      // Print the triples
      for (RelationTriple triple : triples) {
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}
",StanfordCoreNLP,StanfordCoreNLP,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.naturalli.NaturalLogicAnnotations;
import edu.stanford.nlp.util.CoreMap;

import java.util.Collection;
import java.util.Properties;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create the Stanford CoreNLP pipeline
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,depparse,natlog,openie"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // Annotate an example document.
    Annotation doc = new Annotation(""Obama was born in Hawaii. He is our president."");
    pipeline.annotate(doc);

    // Loop over sentences in the document
    for (CoreMap sentence : doc.get(CoreAnnotations.SentencesAnnotation.class)) {
      // Get the OpenIE triples for the sentence
      Collection<RelationTriple> triples =
	          sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class);
      // Print the triples
      for (RelationTriple triple : triples) {
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}
",Annotation,Annotation,CoreNLP\src\edu\stanford\nlp\pipeline\Annotation.java,False
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.naturalli.NaturalLogicAnnotations;
import edu.stanford.nlp.util.CoreMap;

import java.util.Collection;
import java.util.Properties;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create the Stanford CoreNLP pipeline
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,depparse,natlog,openie"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // Annotate an example document.
    Annotation doc = new Annotation(""Obama was born in Hawaii. He is our president."");
    pipeline.annotate(doc);

    // Loop over sentences in the document
    for (CoreMap sentence : doc.get(CoreAnnotations.SentencesAnnotation.class)) {
      // Get the OpenIE triples for the sentence
      Collection<RelationTriple> triples =
	          sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class);
      // Print the triples
      for (RelationTriple triple : triples) {
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}
",pipeline.annotate,StanfordCoreNLP.annotate,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.naturalli.NaturalLogicAnnotations;
import edu.stanford.nlp.util.CoreMap;

import java.util.Collection;
import java.util.Properties;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create the Stanford CoreNLP pipeline
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,depparse,natlog,openie"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // Annotate an example document.
    Annotation doc = new Annotation(""Obama was born in Hawaii. He is our president."");
    pipeline.annotate(doc);

    // Loop over sentences in the document
    for (CoreMap sentence : doc.get(CoreAnnotations.SentencesAnnotation.class)) {
      // Get the OpenIE triples for the sentence
      Collection<RelationTriple> triples =
	          sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class);
      // Print the triples
      for (RelationTriple triple : triples) {
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}
",doc.get,N/A,N/A,N/A
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.naturalli.NaturalLogicAnnotations;
import edu.stanford.nlp.util.CoreMap;

import java.util.Collection;
import java.util.Properties;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create the Stanford CoreNLP pipeline
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,depparse,natlog,openie"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // Annotate an example document.
    Annotation doc = new Annotation(""Obama was born in Hawaii. He is our president."");
    pipeline.annotate(doc);

    // Loop over sentences in the document
    for (CoreMap sentence : doc.get(CoreAnnotations.SentencesAnnotation.class)) {
      // Get the OpenIE triples for the sentence
      Collection<RelationTriple> triples =
	          sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class);
      // Print the triples
      for (RelationTriple triple : triples) {
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}
",sentence.get,N/A,N/A,N/A
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.naturalli.NaturalLogicAnnotations;
import edu.stanford.nlp.util.CoreMap;

import java.util.Collection;
import java.util.Properties;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create the Stanford CoreNLP pipeline
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,depparse,natlog,openie"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // Annotate an example document.
    Annotation doc = new Annotation(""Obama was born in Hawaii. He is our president."");
    pipeline.annotate(doc);

    // Loop over sentences in the document
    for (CoreMap sentence : doc.get(CoreAnnotations.SentencesAnnotation.class)) {
      // Get the OpenIE triples for the sentence
      Collection<RelationTriple> triples =
	          sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class);
      // Print the triples
      for (RelationTriple triple : triples) {
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}
",out.println,N/A,N/A,N/A
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.naturalli.NaturalLogicAnnotations;
import edu.stanford.nlp.util.CoreMap;

import java.util.Collection;
import java.util.Properties;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create the Stanford CoreNLP pipeline
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,depparse,natlog,openie"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // Annotate an example document.
    Annotation doc = new Annotation(""Obama was born in Hawaii. He is our president."");
    pipeline.annotate(doc);

    // Loop over sentences in the document
    for (CoreMap sentence : doc.get(CoreAnnotations.SentencesAnnotation.class)) {
      // Get the OpenIE triples for the sentence
      Collection<RelationTriple> triples =
	          sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class);
      // Print the triples
      for (RelationTriple triple : triples) {
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}
",triple.subjectLemmaGloss,RelationTriple.subjectLemmaGloss,CoreNLP\src\edu\stanford\nlp\ie\util\RelationTriple.java,False
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.naturalli.NaturalLogicAnnotations;
import edu.stanford.nlp.util.CoreMap;

import java.util.Collection;
import java.util.Properties;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create the Stanford CoreNLP pipeline
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,depparse,natlog,openie"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // Annotate an example document.
    Annotation doc = new Annotation(""Obama was born in Hawaii. He is our president."");
    pipeline.annotate(doc);

    // Loop over sentences in the document
    for (CoreMap sentence : doc.get(CoreAnnotations.SentencesAnnotation.class)) {
      // Get the OpenIE triples for the sentence
      Collection<RelationTriple> triples =
	          sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class);
      // Print the triples
      for (RelationTriple triple : triples) {
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}
",triple.relationLemmaGloss,RelationTriple.relationLemmaGloss,CoreNLP\src\edu\stanford\nlp\ie\util\RelationTriple.java,False
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.naturalli.NaturalLogicAnnotations;
import edu.stanford.nlp.util.CoreMap;

import java.util.Collection;
import java.util.Properties;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create the Stanford CoreNLP pipeline
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,depparse,natlog,openie"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // Annotate an example document.
    Annotation doc = new Annotation(""Obama was born in Hawaii. He is our president."");
    pipeline.annotate(doc);

    // Loop over sentences in the document
    for (CoreMap sentence : doc.get(CoreAnnotations.SentencesAnnotation.class)) {
      // Get the OpenIE triples for the sentence
      Collection<RelationTriple> triples =
	          sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class);
      // Print the triples
      for (RelationTriple triple : triples) {
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}
",triple.objectLemmaGloss,RelationTriple.objectLemmaGloss,CoreNLP\src\edu\stanford\nlp\ie\util\RelationTriple.java,False
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.simple.*;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create a CoreNLP document
    Document doc = new Document(""Obama was born in Hawaii. He is our president."");

    // Iterate over the sentences in the document
    for (Sentence sent : doc.sentences()) {
      // Iterate over the triples in the sentence
      for (RelationTriple triple : sent.openieTriples()) {
        // Print the triple
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}
",main,N/A,N/A,N/A
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.simple.*;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create a CoreNLP document
    Document doc = new Document(""Obama was born in Hawaii. He is our president."");

    // Iterate over the sentences in the document
    for (Sentence sent : doc.sentences()) {
      // Iterate over the triples in the sentence
      for (RelationTriple triple : sent.openieTriples()) {
        // Print the triple
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}
",Document,Document,CoreNLP\src\edu\stanford\nlp\coref\data\Document.java,False
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.simple.*;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create a CoreNLP document
    Document doc = new Document(""Obama was born in Hawaii. He is our president."");

    // Iterate over the sentences in the document
    for (Sentence sent : doc.sentences()) {
      // Iterate over the triples in the sentence
      for (RelationTriple triple : sent.openieTriples()) {
        // Print the triple
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}
",doc.sentences,Document.sentences,CoreNLP\src\edu\stanford\nlp\simple\Document.java,False
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.simple.*;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create a CoreNLP document
    Document doc = new Document(""Obama was born in Hawaii. He is our president."");

    // Iterate over the sentences in the document
    for (Sentence sent : doc.sentences()) {
      // Iterate over the triples in the sentence
      for (RelationTriple triple : sent.openieTriples()) {
        // Print the triple
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}
",sent.openieTriples,Sentence.openieTriples,CoreNLP\src\edu\stanford\nlp\simple\Sentence.java,False
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.simple.*;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create a CoreNLP document
    Document doc = new Document(""Obama was born in Hawaii. He is our president."");

    // Iterate over the sentences in the document
    for (Sentence sent : doc.sentences()) {
      // Iterate over the triples in the sentence
      for (RelationTriple triple : sent.openieTriples()) {
        // Print the triple
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}
",out.println,N/A,N/A,N/A
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.simple.*;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create a CoreNLP document
    Document doc = new Document(""Obama was born in Hawaii. He is our president."");

    // Iterate over the sentences in the document
    for (Sentence sent : doc.sentences()) {
      // Iterate over the triples in the sentence
      for (RelationTriple triple : sent.openieTriples()) {
        // Print the triple
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}
",triple.subjectLemmaGloss,RelationTriple.subjectLemmaGloss,CoreNLP\src\edu\stanford\nlp\ie\util\RelationTriple.java,False
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.simple.*;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create a CoreNLP document
    Document doc = new Document(""Obama was born in Hawaii. He is our president."");

    // Iterate over the sentences in the document
    for (Sentence sent : doc.sentences()) {
      // Iterate over the triples in the sentence
      for (RelationTriple triple : sent.openieTriples()) {
        // Print the triple
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}
",triple.relationLemmaGloss,RelationTriple.relationLemmaGloss,CoreNLP\src\edu\stanford\nlp\ie\util\RelationTriple.java,False
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.simple.*;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create a CoreNLP document
    Document doc = new Document(""Obama was born in Hawaii. He is our president."");

    // Iterate over the sentences in the document
    for (Sentence sent : doc.sentences()) {
      // Iterate over the triples in the sentence
      for (RelationTriple triple : sent.openieTriples()) {
        // Print the triple
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}
",triple.objectLemmaGloss,RelationTriple.objectLemmaGloss,CoreNLP\src\edu\stanford\nlp\ie\util\RelationTriple.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.time.*;

import java.util.Properties;

public class SUTimeBasicExample {

    public static String[] examples = {
            ""The concert will be on February 4th at the ampitheater."",
            ""The meeting will be held at 4:00pm in the library"",
            ""The conflict has lasted for over 15 years and shows no signs of abating.""
    };


    public static void main(String[] args) {
        // set up pipeline properties
        Properties props = new Properties();
        // general properties
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        props.setProperty(""ner.docdate.usePresent"", ""true"");
        props.setProperty(""sutime.includeRange"", ""true"");
        props.setProperty(""sutime.markTimeRanges"", ""true"");
        // build pipeline
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        for (String example : examples) {
            CoreDocument document = new CoreDocument(example);
            pipeline.annotate(document);
            for (CoreEntityMention cem : document.entityMentions()) {
                System.out.println(""temporal expression: ""+cem.text());
                System.out.println(""temporal value: "" +
                                   cem.coreMap().get(TimeAnnotations.TimexAnnotation.class));
            }
        }
    }
}
",main,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.time.*;

import java.util.Properties;

public class SUTimeBasicExample {

    public static String[] examples = {
            ""The concert will be on February 4th at the ampitheater."",
            ""The meeting will be held at 4:00pm in the library"",
            ""The conflict has lasted for over 15 years and shows no signs of abating.""
    };


    public static void main(String[] args) {
        // set up pipeline properties
        Properties props = new Properties();
        // general properties
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        props.setProperty(""ner.docdate.usePresent"", ""true"");
        props.setProperty(""sutime.includeRange"", ""true"");
        props.setProperty(""sutime.markTimeRanges"", ""true"");
        // build pipeline
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        for (String example : examples) {
            CoreDocument document = new CoreDocument(example);
            pipeline.annotate(document);
            for (CoreEntityMention cem : document.entityMentions()) {
                System.out.println(""temporal expression: ""+cem.text());
                System.out.println(""temporal value: "" +
                                   cem.coreMap().get(TimeAnnotations.TimexAnnotation.class));
            }
        }
    }
}
",Properties,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.time.*;

import java.util.Properties;

public class SUTimeBasicExample {

    public static String[] examples = {
            ""The concert will be on February 4th at the ampitheater."",
            ""The meeting will be held at 4:00pm in the library"",
            ""The conflict has lasted for over 15 years and shows no signs of abating.""
    };


    public static void main(String[] args) {
        // set up pipeline properties
        Properties props = new Properties();
        // general properties
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        props.setProperty(""ner.docdate.usePresent"", ""true"");
        props.setProperty(""sutime.includeRange"", ""true"");
        props.setProperty(""sutime.markTimeRanges"", ""true"");
        // build pipeline
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        for (String example : examples) {
            CoreDocument document = new CoreDocument(example);
            pipeline.annotate(document);
            for (CoreEntityMention cem : document.entityMentions()) {
                System.out.println(""temporal expression: ""+cem.text());
                System.out.println(""temporal value: "" +
                                   cem.coreMap().get(TimeAnnotations.TimexAnnotation.class));
            }
        }
    }
}
",props.setProperty,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.time.*;

import java.util.Properties;

public class SUTimeBasicExample {

    public static String[] examples = {
            ""The concert will be on February 4th at the ampitheater."",
            ""The meeting will be held at 4:00pm in the library"",
            ""The conflict has lasted for over 15 years and shows no signs of abating.""
    };


    public static void main(String[] args) {
        // set up pipeline properties
        Properties props = new Properties();
        // general properties
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        props.setProperty(""ner.docdate.usePresent"", ""true"");
        props.setProperty(""sutime.includeRange"", ""true"");
        props.setProperty(""sutime.markTimeRanges"", ""true"");
        // build pipeline
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        for (String example : examples) {
            CoreDocument document = new CoreDocument(example);
            pipeline.annotate(document);
            for (CoreEntityMention cem : document.entityMentions()) {
                System.out.println(""temporal expression: ""+cem.text());
                System.out.println(""temporal value: "" +
                                   cem.coreMap().get(TimeAnnotations.TimexAnnotation.class));
            }
        }
    }
}
",StanfordCoreNLP,StanfordCoreNLP,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.time.*;

import java.util.Properties;

public class SUTimeBasicExample {

    public static String[] examples = {
            ""The concert will be on February 4th at the ampitheater."",
            ""The meeting will be held at 4:00pm in the library"",
            ""The conflict has lasted for over 15 years and shows no signs of abating.""
    };


    public static void main(String[] args) {
        // set up pipeline properties
        Properties props = new Properties();
        // general properties
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        props.setProperty(""ner.docdate.usePresent"", ""true"");
        props.setProperty(""sutime.includeRange"", ""true"");
        props.setProperty(""sutime.markTimeRanges"", ""true"");
        // build pipeline
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        for (String example : examples) {
            CoreDocument document = new CoreDocument(example);
            pipeline.annotate(document);
            for (CoreEntityMention cem : document.entityMentions()) {
                System.out.println(""temporal expression: ""+cem.text());
                System.out.println(""temporal value: "" +
                                   cem.coreMap().get(TimeAnnotations.TimexAnnotation.class));
            }
        }
    }
}
",CoreDocument,CoreDocument,CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.time.*;

import java.util.Properties;

public class SUTimeBasicExample {

    public static String[] examples = {
            ""The concert will be on February 4th at the ampitheater."",
            ""The meeting will be held at 4:00pm in the library"",
            ""The conflict has lasted for over 15 years and shows no signs of abating.""
    };


    public static void main(String[] args) {
        // set up pipeline properties
        Properties props = new Properties();
        // general properties
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        props.setProperty(""ner.docdate.usePresent"", ""true"");
        props.setProperty(""sutime.includeRange"", ""true"");
        props.setProperty(""sutime.markTimeRanges"", ""true"");
        // build pipeline
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        for (String example : examples) {
            CoreDocument document = new CoreDocument(example);
            pipeline.annotate(document);
            for (CoreEntityMention cem : document.entityMentions()) {
                System.out.println(""temporal expression: ""+cem.text());
                System.out.println(""temporal value: "" +
                                   cem.coreMap().get(TimeAnnotations.TimexAnnotation.class));
            }
        }
    }
}
",pipeline.annotate,StanfordCoreNLP.annotate,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.time.*;

import java.util.Properties;

public class SUTimeBasicExample {

    public static String[] examples = {
            ""The concert will be on February 4th at the ampitheater."",
            ""The meeting will be held at 4:00pm in the library"",
            ""The conflict has lasted for over 15 years and shows no signs of abating.""
    };


    public static void main(String[] args) {
        // set up pipeline properties
        Properties props = new Properties();
        // general properties
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        props.setProperty(""ner.docdate.usePresent"", ""true"");
        props.setProperty(""sutime.includeRange"", ""true"");
        props.setProperty(""sutime.markTimeRanges"", ""true"");
        // build pipeline
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        for (String example : examples) {
            CoreDocument document = new CoreDocument(example);
            pipeline.annotate(document);
            for (CoreEntityMention cem : document.entityMentions()) {
                System.out.println(""temporal expression: ""+cem.text());
                System.out.println(""temporal value: "" +
                                   cem.coreMap().get(TimeAnnotations.TimexAnnotation.class));
            }
        }
    }
}
",document.entityMentions,CoreDocument.entityMentions,CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.time.*;

import java.util.Properties;

public class SUTimeBasicExample {

    public static String[] examples = {
            ""The concert will be on February 4th at the ampitheater."",
            ""The meeting will be held at 4:00pm in the library"",
            ""The conflict has lasted for over 15 years and shows no signs of abating.""
    };


    public static void main(String[] args) {
        // set up pipeline properties
        Properties props = new Properties();
        // general properties
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        props.setProperty(""ner.docdate.usePresent"", ""true"");
        props.setProperty(""sutime.includeRange"", ""true"");
        props.setProperty(""sutime.markTimeRanges"", ""true"");
        // build pipeline
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        for (String example : examples) {
            CoreDocument document = new CoreDocument(example);
            pipeline.annotate(document);
            for (CoreEntityMention cem : document.entityMentions()) {
                System.out.println(""temporal expression: ""+cem.text());
                System.out.println(""temporal value: "" +
                                   cem.coreMap().get(TimeAnnotations.TimexAnnotation.class));
            }
        }
    }
}
",out.println,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.time.*;

import java.util.Properties;

public class SUTimeBasicExample {

    public static String[] examples = {
            ""The concert will be on February 4th at the ampitheater."",
            ""The meeting will be held at 4:00pm in the library"",
            ""The conflict has lasted for over 15 years and shows no signs of abating.""
    };


    public static void main(String[] args) {
        // set up pipeline properties
        Properties props = new Properties();
        // general properties
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        props.setProperty(""ner.docdate.usePresent"", ""true"");
        props.setProperty(""sutime.includeRange"", ""true"");
        props.setProperty(""sutime.markTimeRanges"", ""true"");
        // build pipeline
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        for (String example : examples) {
            CoreDocument document = new CoreDocument(example);
            pipeline.annotate(document);
            for (CoreEntityMention cem : document.entityMentions()) {
                System.out.println(""temporal expression: ""+cem.text());
                System.out.println(""temporal value: "" +
                                   cem.coreMap().get(TimeAnnotations.TimexAnnotation.class));
            }
        }
    }
}
",cem.text,"CoreEntityMention.text
Document.text
CoreDocument.text
Timex.text","CoreNLP\src\edu\stanford\nlp\pipeline\CoreEntityMention.java
CoreNLP\src\edu\stanford\nlp\simple\Document.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java
CoreNLP\src\edu\stanford\nlp\time\Timex.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.time.*;

import java.util.Properties;

public class SUTimeBasicExample {

    public static String[] examples = {
            ""The concert will be on February 4th at the ampitheater."",
            ""The meeting will be held at 4:00pm in the library"",
            ""The conflict has lasted for over 15 years and shows no signs of abating.""
    };


    public static void main(String[] args) {
        // set up pipeline properties
        Properties props = new Properties();
        // general properties
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        props.setProperty(""ner.docdate.usePresent"", ""true"");
        props.setProperty(""sutime.includeRange"", ""true"");
        props.setProperty(""sutime.markTimeRanges"", ""true"");
        // build pipeline
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        for (String example : examples) {
            CoreDocument document = new CoreDocument(example);
            pipeline.annotate(document);
            for (CoreEntityMention cem : document.entityMentions()) {
                System.out.println(""temporal expression: ""+cem.text());
                System.out.println(""temporal value: "" +
                                   cem.coreMap().get(TimeAnnotations.TimexAnnotation.class));
            }
        }
    }
}
",cem.coreMap,CoreEntityMention.coreMap,CoreNLP\src\edu\stanford\nlp\pipeline\CoreEntityMention.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.time.*;

import java.util.Properties;

public class SUTimeBasicExample {

    public static String[] examples = {
            ""The concert will be on February 4th at the ampitheater."",
            ""The meeting will be held at 4:00pm in the library"",
            ""The conflict has lasted for over 15 years and shows no signs of abating.""
    };


    public static void main(String[] args) {
        // set up pipeline properties
        Properties props = new Properties();
        // general properties
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        props.setProperty(""ner.docdate.usePresent"", ""true"");
        props.setProperty(""sutime.includeRange"", ""true"");
        props.setProperty(""sutime.markTimeRanges"", ""true"");
        // build pipeline
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        for (String example : examples) {
            CoreDocument document = new CoreDocument(example);
            pipeline.annotate(document);
            for (CoreEntityMention cem : document.entityMentions()) {
                System.out.println(""temporal expression: ""+cem.text());
                System.out.println(""temporal value: "" +
                                   cem.coreMap().get(TimeAnnotations.TimexAnnotation.class));
            }
        }
    }
}
",get,N/A,N/A,N/A
"  // Financial Quarters
  FYQ1 = {
      type: QUARTER_OF_YEAR,
      label: ""FYQ1"",
      value: TimeWithRange(TimeRange(IsoDate(ANY,10,1), IsoDate(ANY,12,31), QUARTER))
  }
  FYQ2 = {
      type: QUARTER_OF_YEAR,
      label: ""FYQ2"",
      value: TimeWithRange(TimeRange(IsoDate(ANY,1,1), IsoDate(ANY,3,31), QUARTER))
  }
  FYQ3 = {
      type: QUARTER_OF_YEAR,
      label: ""FYQ3"",
      value: TimeWithRange(TimeRange(IsoDate(ANY,4,1), IsoDate(ANY,6,30), QUARTER))
  }
  FYQ4 = {
      type: QUARTER_OF_YEAR,
      label: ""FYQ4"",
      value: TimeWithRange(TimeRange(IsoDate(ANY,7,1), IsoDate(ANY,9,30), QUARTER))
  }
",TimeWithRange,N/A,N/A,N/A
"  // Financial Quarters
  FYQ1 = {
      type: QUARTER_OF_YEAR,
      label: ""FYQ1"",
      value: TimeWithRange(TimeRange(IsoDate(ANY,10,1), IsoDate(ANY,12,31), QUARTER))
  }
  FYQ2 = {
      type: QUARTER_OF_YEAR,
      label: ""FYQ2"",
      value: TimeWithRange(TimeRange(IsoDate(ANY,1,1), IsoDate(ANY,3,31), QUARTER))
  }
  FYQ3 = {
      type: QUARTER_OF_YEAR,
      label: ""FYQ3"",
      value: TimeWithRange(TimeRange(IsoDate(ANY,4,1), IsoDate(ANY,6,30), QUARTER))
  }
  FYQ4 = {
      type: QUARTER_OF_YEAR,
      label: ""FYQ4"",
      value: TimeWithRange(TimeRange(IsoDate(ANY,7,1), IsoDate(ANY,9,30), QUARTER))
  }
",TimeRange,N/A,N/A,N/A
"  // Financial Quarters
  FYQ1 = {
      type: QUARTER_OF_YEAR,
      label: ""FYQ1"",
      value: TimeWithRange(TimeRange(IsoDate(ANY,10,1), IsoDate(ANY,12,31), QUARTER))
  }
  FYQ2 = {
      type: QUARTER_OF_YEAR,
      label: ""FYQ2"",
      value: TimeWithRange(TimeRange(IsoDate(ANY,1,1), IsoDate(ANY,3,31), QUARTER))
  }
  FYQ3 = {
      type: QUARTER_OF_YEAR,
      label: ""FYQ3"",
      value: TimeWithRange(TimeRange(IsoDate(ANY,4,1), IsoDate(ANY,6,30), QUARTER))
  }
  FYQ4 = {
      type: QUARTER_OF_YEAR,
      label: ""FYQ4"",
      value: TimeWithRange(TimeRange(IsoDate(ANY,7,1), IsoDate(ANY,9,30), QUARTER))
  }
",IsoDate,N/A,N/A,N/A
"  # Finanical Quarters
  FISCAL_YEAR_QUARTER_MAP = {
    ""Q1"": FYQ1,
    ""Q2"": FYQ2,
    ""Q3"": FYQ3,
    ""Q4"": FYQ4
  }
  FISCAL_YEAR_QUARTER_YEAR_OFFSETS_MAP = {
    ""Q1"": 1,
    ""Q2"": 0,
    ""Q3"": 0,
    ""Q4"": 0
  }
  $FiscalYearQuarterTerm = CreateRegex(Keys(FISCAL_YEAR_QUARTER_MAP))
",CreateRegex,N/A,N/A,N/A
"  # Finanical Quarters
  FISCAL_YEAR_QUARTER_MAP = {
    ""Q1"": FYQ1,
    ""Q2"": FYQ2,
    ""Q3"": FYQ3,
    ""Q4"": FYQ4
  }
  FISCAL_YEAR_QUARTER_YEAR_OFFSETS_MAP = {
    ""Q1"": 1,
    ""Q2"": 0,
    ""Q3"": 0,
    ""Q4"": 0
  }
  $FiscalYearQuarterTerm = CreateRegex(Keys(FISCAL_YEAR_QUARTER_MAP))
",Keys,N/A,N/A,N/A
"  {
    matchWithResults: TRUE,
    pattern: ((/$FiscalYearQuarterTerm/) (FY)? (/(FY)?([0-9]{4})/)),
    result:  TemporalCompose(INTERSECT, IsoDate(Subtract({type: ""NUMBER"", value: $$3.matchResults[0].word.group(2)}, FISCAL_YEAR_QUARTER_YEAR_OFFSETS_MAP[$1[0].word]), ANY, ANY), FISCAL_YEAR_QUARTER_MAP[$1[0].word])
  }

  {
    pattern: ((/$FiscalYearQuarterTerm/)),
    result: FISCAL_YEAR_QUARTER_MAP[$1[0].word]
  }
",TemporalCompose,N/A,N/A,N/A
"  {
    matchWithResults: TRUE,
    pattern: ((/$FiscalYearQuarterTerm/) (FY)? (/(FY)?([0-9]{4})/)),
    result:  TemporalCompose(INTERSECT, IsoDate(Subtract({type: ""NUMBER"", value: $$3.matchResults[0].word.group(2)}, FISCAL_YEAR_QUARTER_YEAR_OFFSETS_MAP[$1[0].word]), ANY, ANY), FISCAL_YEAR_QUARTER_MAP[$1[0].word])
  }

  {
    pattern: ((/$FiscalYearQuarterTerm/)),
    result: FISCAL_YEAR_QUARTER_MAP[$1[0].word]
  }
",IsoDate,N/A,N/A,N/A
"  {
    matchWithResults: TRUE,
    pattern: ((/$FiscalYearQuarterTerm/) (FY)? (/(FY)?([0-9]{4})/)),
    result:  TemporalCompose(INTERSECT, IsoDate(Subtract({type: ""NUMBER"", value: $$3.matchResults[0].word.group(2)}, FISCAL_YEAR_QUARTER_YEAR_OFFSETS_MAP[$1[0].word]), ANY, ANY), FISCAL_YEAR_QUARTER_MAP[$1[0].word])
  }

  {
    pattern: ((/$FiscalYearQuarterTerm/)),
    result: FISCAL_YEAR_QUARTER_MAP[$1[0].word]
  }
",Subtract,N/A,N/A,N/A
"  {
    matchWithResults: TRUE,
    pattern: ((/$FiscalYearQuarterTerm/) (FY)? (/(FY)?([0-9]{4})/)),
    result:  TemporalCompose(INTERSECT, IsoDate(Subtract({type: ""NUMBER"", value: $$3.matchResults[0].word.group(2)}, FISCAL_YEAR_QUARTER_YEAR_OFFSETS_MAP[$1[0].word]), ANY, ANY), FISCAL_YEAR_QUARTER_MAP[$1[0].word])
  }

  {
    pattern: ((/$FiscalYearQuarterTerm/)),
    result: FISCAL_YEAR_QUARTER_MAP[$1[0].word]
  }
",word.group,N/A,N/A,N/A
"TemporalCompose(INTERSECT, IsoDate(Subtract({type: ""NUMBER"", value: $$3.matchResults[0].word.group(2)}, FISCAL_YEAR_QUARTER_YEAR_OFFSETS_MAP[$1[0].word]), ANY, ANY), FISCAL_YEAR_QUARTER_MAP[$1[0].word])
",TemporalCompose,N/A,N/A,N/A
"TemporalCompose(INTERSECT, IsoDate(Subtract({type: ""NUMBER"", value: $$3.matchResults[0].word.group(2)}, FISCAL_YEAR_QUARTER_YEAR_OFFSETS_MAP[$1[0].word]), ANY, ANY), FISCAL_YEAR_QUARTER_MAP[$1[0].word])
",IsoDate,N/A,N/A,N/A
"TemporalCompose(INTERSECT, IsoDate(Subtract({type: ""NUMBER"", value: $$3.matchResults[0].word.group(2)}, FISCAL_YEAR_QUARTER_YEAR_OFFSETS_MAP[$1[0].word]), ANY, ANY), FISCAL_YEAR_QUARTER_MAP[$1[0].word])
",Subtract,N/A,N/A,N/A
"TemporalCompose(INTERSECT, IsoDate(Subtract({type: ""NUMBER"", value: $$3.matchResults[0].word.group(2)}, FISCAL_YEAR_QUARTER_YEAR_OFFSETS_MAP[$1[0].word]), ANY, ANY), FISCAL_YEAR_QUARTER_MAP[$1[0].word])
",word.group,N/A,N/A,N/A
"  // Dates are rough with respect to northern hemisphere (actual
  // solstice/equinox days depend on the year)
  SPRING_EQUINOX = {
    type: DAY_OF_YEAR,
	value: InexactTime( TimeRange( IsoDate(ANY, 3, 20), IsoDate(ANY, 3, 21) ) )
  }
  SUMMER_SOLSTICE = {
    type: DAY_OF_YEAR,
	value: InexactTime( TimeRange( IsoDate(ANY, 6, 20), IsoDate(ANY, 6, 21) ) )
  }
  FALL_EQUINOX = {
    type: DAY_OF_YEAR,
	value: InexactTime( TimeRange( IsoDate(ANY, 9, 22), IsoDate(ANY, 9, 23) ) )
  }
  WINTER_SOLSTICE = {
    type: DAY_OF_YEAR,
	value: InexactTime( TimeRange( IsoDate(ANY, 12, 21), IsoDate(ANY, 12, 22) ) )
  }

  ...
  
  // Dates for seasons are rough with respect to northern hemisphere
  SPRING = {
      type: SEASON_OF_YEAR,
      label: ""SP"",
      value: InexactTime( SPRING_EQUINOX, QUARTER, TimeRange( MARCH, JUNE, QUARTER ) ) }
  SUMMER = {
      type: SEASON_OF_YEAR,
      label: ""SU"",
      value: InexactTime( SUMMER_SOLSTICE, QUARTER, TimeRange( JUNE, SEPTEMBER, QUARTER ) )
  }
  FALL = {
      type: SEASON_OF_YEAR,
      label: ""FA"",
      value: InexactTime( FALL_EQUINOX, QUARTER, TimeRange( SEPTEMBER, DECEMBER, QUARTER ) )
  }
  WINTER = {
      type: SEASON_OF_YEAR,
      label: ""WI"",
      value: InexactTime( WINTER_SOLSTICE, QUARTER, TimeRange( DECEMBER, MARCH, QUARTER ) )
  }
",InexactTime,N/A,N/A,N/A
"  // Dates are rough with respect to northern hemisphere (actual
  // solstice/equinox days depend on the year)
  SPRING_EQUINOX = {
    type: DAY_OF_YEAR,
	value: InexactTime( TimeRange( IsoDate(ANY, 3, 20), IsoDate(ANY, 3, 21) ) )
  }
  SUMMER_SOLSTICE = {
    type: DAY_OF_YEAR,
	value: InexactTime( TimeRange( IsoDate(ANY, 6, 20), IsoDate(ANY, 6, 21) ) )
  }
  FALL_EQUINOX = {
    type: DAY_OF_YEAR,
	value: InexactTime( TimeRange( IsoDate(ANY, 9, 22), IsoDate(ANY, 9, 23) ) )
  }
  WINTER_SOLSTICE = {
    type: DAY_OF_YEAR,
	value: InexactTime( TimeRange( IsoDate(ANY, 12, 21), IsoDate(ANY, 12, 22) ) )
  }

  ...
  
  // Dates for seasons are rough with respect to northern hemisphere
  SPRING = {
      type: SEASON_OF_YEAR,
      label: ""SP"",
      value: InexactTime( SPRING_EQUINOX, QUARTER, TimeRange( MARCH, JUNE, QUARTER ) ) }
  SUMMER = {
      type: SEASON_OF_YEAR,
      label: ""SU"",
      value: InexactTime( SUMMER_SOLSTICE, QUARTER, TimeRange( JUNE, SEPTEMBER, QUARTER ) )
  }
  FALL = {
      type: SEASON_OF_YEAR,
      label: ""FA"",
      value: InexactTime( FALL_EQUINOX, QUARTER, TimeRange( SEPTEMBER, DECEMBER, QUARTER ) )
  }
  WINTER = {
      type: SEASON_OF_YEAR,
      label: ""WI"",
      value: InexactTime( WINTER_SOLSTICE, QUARTER, TimeRange( DECEMBER, MARCH, QUARTER ) )
  }
",TimeRange,N/A,N/A,N/A
"  // Dates are rough with respect to northern hemisphere (actual
  // solstice/equinox days depend on the year)
  SPRING_EQUINOX = {
    type: DAY_OF_YEAR,
	value: InexactTime( TimeRange( IsoDate(ANY, 3, 20), IsoDate(ANY, 3, 21) ) )
  }
  SUMMER_SOLSTICE = {
    type: DAY_OF_YEAR,
	value: InexactTime( TimeRange( IsoDate(ANY, 6, 20), IsoDate(ANY, 6, 21) ) )
  }
  FALL_EQUINOX = {
    type: DAY_OF_YEAR,
	value: InexactTime( TimeRange( IsoDate(ANY, 9, 22), IsoDate(ANY, 9, 23) ) )
  }
  WINTER_SOLSTICE = {
    type: DAY_OF_YEAR,
	value: InexactTime( TimeRange( IsoDate(ANY, 12, 21), IsoDate(ANY, 12, 22) ) )
  }

  ...
  
  // Dates for seasons are rough with respect to northern hemisphere
  SPRING = {
      type: SEASON_OF_YEAR,
      label: ""SP"",
      value: InexactTime( SPRING_EQUINOX, QUARTER, TimeRange( MARCH, JUNE, QUARTER ) ) }
  SUMMER = {
      type: SEASON_OF_YEAR,
      label: ""SU"",
      value: InexactTime( SUMMER_SOLSTICE, QUARTER, TimeRange( JUNE, SEPTEMBER, QUARTER ) )
  }
  FALL = {
      type: SEASON_OF_YEAR,
      label: ""FA"",
      value: InexactTime( FALL_EQUINOX, QUARTER, TimeRange( SEPTEMBER, DECEMBER, QUARTER ) )
  }
  WINTER = {
      type: SEASON_OF_YEAR,
      label: ""WI"",
      value: InexactTime( WINTER_SOLSTICE, QUARTER, TimeRange( DECEMBER, MARCH, QUARTER ) )
  }
",IsoDate,N/A,N/A,N/A
"// Dates are rough with respect to northern hemisphere (actual
// solstice/equinox days depend on the year)
SPRING_EQUINOX = {
   type: DAY_OF_YEAR,
   value: InexactTime( TimeRange( IsoDate(ANY, 9, 22), IsoDate(ANY, 9, 23) ) )
}
SUMMER_SOLSTICE = {
   type: DAY_OF_YEAR,
   value: InexactTime( TimeRange( IsoDate(ANY, 12, 21), IsoDate(ANY, 12, 22) ) )
}
FALL_EQUINOX = {
   type: DAY_OF_YEAR,
   value: InexactTime( TimeRange( IsoDate(ANY, 3, 20), IsoDate(ANY, 3, 21) ) )
}
WINTER_SOLSTICE = {
   type: DAY_OF_YEAR,
   value: InexactTime( TimeRange( IsoDate(ANY, 6, 20), IsoDate(ANY, 6, 21) ) )
}

// Dates for seasons are rough with respect to northern hemisphere
SPRING = {
    type: SEASON_OF_YEAR,
    label: ""SP"",
    value: InexactTime( SPRING_EQUINOX, QUARTER, TimeRange( SEPTEMBER, DECEMBER, QUARTER ) ) }
SUMMER = {
    type: SEASON_OF_YEAR,
    label: ""SU"",
    value: InexactTime( SUMMER_SOLSTICE, QUARTER, TimeRange( DECEMBER, MARCH, QUARTER ) )
}
FALL = {
    type: SEASON_OF_YEAR,
    label: ""FA"",
    value: InexactTime( FALL_EQUINOX, QUARTER, TimeRange( MARCH, JUNE, QUARTER ) )
}
WINTER = {
    type: SEASON_OF_YEAR,
    label: ""WI"",
    value: InexactTime( WINTER_SOLSTICE, QUARTER, TimeRange( JUNE, SEPTEMBER, QUARTER ) )
}
",InexactTime,N/A,N/A,N/A
"// Dates are rough with respect to northern hemisphere (actual
// solstice/equinox days depend on the year)
SPRING_EQUINOX = {
   type: DAY_OF_YEAR,
   value: InexactTime( TimeRange( IsoDate(ANY, 9, 22), IsoDate(ANY, 9, 23) ) )
}
SUMMER_SOLSTICE = {
   type: DAY_OF_YEAR,
   value: InexactTime( TimeRange( IsoDate(ANY, 12, 21), IsoDate(ANY, 12, 22) ) )
}
FALL_EQUINOX = {
   type: DAY_OF_YEAR,
   value: InexactTime( TimeRange( IsoDate(ANY, 3, 20), IsoDate(ANY, 3, 21) ) )
}
WINTER_SOLSTICE = {
   type: DAY_OF_YEAR,
   value: InexactTime( TimeRange( IsoDate(ANY, 6, 20), IsoDate(ANY, 6, 21) ) )
}

// Dates for seasons are rough with respect to northern hemisphere
SPRING = {
    type: SEASON_OF_YEAR,
    label: ""SP"",
    value: InexactTime( SPRING_EQUINOX, QUARTER, TimeRange( SEPTEMBER, DECEMBER, QUARTER ) ) }
SUMMER = {
    type: SEASON_OF_YEAR,
    label: ""SU"",
    value: InexactTime( SUMMER_SOLSTICE, QUARTER, TimeRange( DECEMBER, MARCH, QUARTER ) )
}
FALL = {
    type: SEASON_OF_YEAR,
    label: ""FA"",
    value: InexactTime( FALL_EQUINOX, QUARTER, TimeRange( MARCH, JUNE, QUARTER ) )
}
WINTER = {
    type: SEASON_OF_YEAR,
    label: ""WI"",
    value: InexactTime( WINTER_SOLSTICE, QUARTER, TimeRange( JUNE, SEPTEMBER, QUARTER ) )
}
",TimeRange,N/A,N/A,N/A
"// Dates are rough with respect to northern hemisphere (actual
// solstice/equinox days depend on the year)
SPRING_EQUINOX = {
   type: DAY_OF_YEAR,
   value: InexactTime( TimeRange( IsoDate(ANY, 9, 22), IsoDate(ANY, 9, 23) ) )
}
SUMMER_SOLSTICE = {
   type: DAY_OF_YEAR,
   value: InexactTime( TimeRange( IsoDate(ANY, 12, 21), IsoDate(ANY, 12, 22) ) )
}
FALL_EQUINOX = {
   type: DAY_OF_YEAR,
   value: InexactTime( TimeRange( IsoDate(ANY, 3, 20), IsoDate(ANY, 3, 21) ) )
}
WINTER_SOLSTICE = {
   type: DAY_OF_YEAR,
   value: InexactTime( TimeRange( IsoDate(ANY, 6, 20), IsoDate(ANY, 6, 21) ) )
}

// Dates for seasons are rough with respect to northern hemisphere
SPRING = {
    type: SEASON_OF_YEAR,
    label: ""SP"",
    value: InexactTime( SPRING_EQUINOX, QUARTER, TimeRange( SEPTEMBER, DECEMBER, QUARTER ) ) }
SUMMER = {
    type: SEASON_OF_YEAR,
    label: ""SU"",
    value: InexactTime( SUMMER_SOLSTICE, QUARTER, TimeRange( DECEMBER, MARCH, QUARTER ) )
}
FALL = {
    type: SEASON_OF_YEAR,
    label: ""FA"",
    value: InexactTime( FALL_EQUINOX, QUARTER, TimeRange( MARCH, JUNE, QUARTER ) )
}
WINTER = {
    type: SEASON_OF_YEAR,
    label: ""WI"",
    value: InexactTime( WINTER_SOLSTICE, QUARTER, TimeRange( JUNE, SEPTEMBER, QUARTER ) )
}
",IsoDate,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}
",getType,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}
",uncheckedCast,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}
",main,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}
",StringUtils.argsToProperties,StringUtils.argsToProperties,CoreNLP\src\edu\stanford\nlp\util\StringUtils.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}
",IOUtils.stringFromFile,IOUtils.stringFromFile,CoreNLP\src\edu\stanford\nlp\io\IOUtils.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}
",props.getProperty,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}
",Properties,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}
",pipelineProps.setProperty,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}
",StanfordCoreNLP,StanfordCoreNLP,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}
",Annotation,Annotation,CoreNLP\src\edu\stanford\nlp\pipeline\Annotation.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}
",pipeline.annotate,"Util.annotate
StanfordCoreNLP.annotate","CoreNLP\src\edu\stanford\nlp\naturalli\Util.java
CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}
",split,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}
",TokenSequencePattern.getNewEnv,TokenSequencePattern.getNewEnv,CoreNLP\src\edu\stanford\nlp\ling\tokensregex\TokenSequencePattern.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}
",env.setDefaultStringMatchFlags,Env.setDefaultStringMatchFlags,CoreNLP\src\edu\stanford\nlp\ling\tokensregex\Env.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}
",env.setDefaultStringPatternFlags,Env.setDefaultStringPatternFlags,CoreNLP\src\edu\stanford\nlp\ling\tokensregex\Env.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}
",CoreMapExpressionExtractor.createExtractorFromFiles,CoreMapExpressionExtractor.createExtractorFromFiles,CoreNLP\src\edu\stanford\nlp\ling\tokensregex\CoreMapExpressionExtractor.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}
",exampleSentencesAnnotation.get,Env.get,CoreNLP\src\edu\stanford\nlp\ling\tokensregex\Env.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}
",out.println,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}
",sentence.get,Env.get,CoreNLP\src\edu\stanford\nlp\ling\tokensregex\Env.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}
",extractor.extractExpressions,CoreMapExpressionExtractor.extractExpressions,CoreNLP\src\edu\stanford\nlp\ling\tokensregex\CoreMapExpressionExtractor.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}
",token.word,"CoreLabel.word
Token.word
Sentence.word","CoreNLP\src\edu\stanford\nlp\ling\CoreLabel.java
CoreNLP\src\edu\stanford\nlp\simple\Token.java
CoreNLP\src\edu\stanford\nlp\simple\Sentence.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}
",token.tag,"Token.tag
CoreLabel.tag","CoreNLP\src\edu\stanford\nlp\simple\Token.java
CoreNLP\src\edu\stanford\nlp\ling\CoreLabel.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}
",token.ner,"Token.ner
CoreLabel.ner","CoreNLP\src\edu\stanford\nlp\simple\Token.java
CoreNLP\src\edu\stanford\nlp\ling\CoreLabel.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}
",me.getText,MatchedExpression.getText,CoreNLP\src\edu\stanford\nlp\ling\tokensregex\MatchedExpression.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}
",me.getValue,"Match.getValue
MatchedExpression.getValue
Token.getValue
CoreMapExpressionExtractor.getValue","CoreNLP\src\edu\stanford\nlp\ling\tokensregex\matcher\Match.java
CoreNLP\src\edu\stanford\nlp\ling\tokensregex\MatchedExpression.java
CoreNLP\src\edu\stanford\nlp\ling\tokensregex\parser\Token.java
CoreNLP\src\edu\stanford\nlp\ling\tokensregex\CoreMapExpressionExtractor.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}
",me.getCharOffsets,MatchedExpression.getCharOffsets,CoreNLP\src\edu\stanford\nlp\ling\tokensregex\MatchedExpression.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}
",me.getAnnotation,MatchedExpression.getAnnotation,CoreNLP\src\edu\stanford\nlp\ling\tokensregex\MatchedExpression.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}
",get,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;

public class TokensRegexAnnotatorDemo {

  // key for matched expressions
  public static class MyMatchedExpressionAnnotation implements CoreAnnotation<List<CoreMap>> {
    @Override
    public Class<List<CoreMap>> getType() {
      return ErasureUtils.<Class<List<CoreMap>>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) throws ClassNotFoundException {
    // set properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,tokensregex"");
    props.setProperty(""tokensregex.rules"", ""basic_ner.rules"");
    props.setProperty(""tokensregex.matchedExpressionsAnnotationKey"",
        ""edu.stanford.nlp.examples.TokensRegexAnnotatorDemo$MyMatchedExpressionAnnotation"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // annotate
    Annotation ann = new Annotation(""There will be a big announcement by Apple Inc today at 5:00pm.  "" +
        ""She has worked at Miller Corp. for 5 years."");
    pipeline.annotate(ann);
    // show results
    System.out.println(""---"");
    System.out.println(""tokens\n"");
    for (CoreMap sentence : ann.get(CoreAnnotations.SentencesAnnotation.class)) {
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.ner());
      }
      System.out.println("""");
    }
    System.out.println(""---"");
    System.out.println(""matched expressions\n"");
    for (CoreMap me : ann.get(MyMatchedExpressionAnnotation.class)) {
      System.out.println(me);
    }
  }
}
",getType,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;

public class TokensRegexAnnotatorDemo {

  // key for matched expressions
  public static class MyMatchedExpressionAnnotation implements CoreAnnotation<List<CoreMap>> {
    @Override
    public Class<List<CoreMap>> getType() {
      return ErasureUtils.<Class<List<CoreMap>>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) throws ClassNotFoundException {
    // set properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,tokensregex"");
    props.setProperty(""tokensregex.rules"", ""basic_ner.rules"");
    props.setProperty(""tokensregex.matchedExpressionsAnnotationKey"",
        ""edu.stanford.nlp.examples.TokensRegexAnnotatorDemo$MyMatchedExpressionAnnotation"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // annotate
    Annotation ann = new Annotation(""There will be a big announcement by Apple Inc today at 5:00pm.  "" +
        ""She has worked at Miller Corp. for 5 years."");
    pipeline.annotate(ann);
    // show results
    System.out.println(""---"");
    System.out.println(""tokens\n"");
    for (CoreMap sentence : ann.get(CoreAnnotations.SentencesAnnotation.class)) {
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.ner());
      }
      System.out.println("""");
    }
    System.out.println(""---"");
    System.out.println(""matched expressions\n"");
    for (CoreMap me : ann.get(MyMatchedExpressionAnnotation.class)) {
      System.out.println(me);
    }
  }
}
",uncheckedCast,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;

public class TokensRegexAnnotatorDemo {

  // key for matched expressions
  public static class MyMatchedExpressionAnnotation implements CoreAnnotation<List<CoreMap>> {
    @Override
    public Class<List<CoreMap>> getType() {
      return ErasureUtils.<Class<List<CoreMap>>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) throws ClassNotFoundException {
    // set properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,tokensregex"");
    props.setProperty(""tokensregex.rules"", ""basic_ner.rules"");
    props.setProperty(""tokensregex.matchedExpressionsAnnotationKey"",
        ""edu.stanford.nlp.examples.TokensRegexAnnotatorDemo$MyMatchedExpressionAnnotation"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // annotate
    Annotation ann = new Annotation(""There will be a big announcement by Apple Inc today at 5:00pm.  "" +
        ""She has worked at Miller Corp. for 5 years."");
    pipeline.annotate(ann);
    // show results
    System.out.println(""---"");
    System.out.println(""tokens\n"");
    for (CoreMap sentence : ann.get(CoreAnnotations.SentencesAnnotation.class)) {
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.ner());
      }
      System.out.println("""");
    }
    System.out.println(""---"");
    System.out.println(""matched expressions\n"");
    for (CoreMap me : ann.get(MyMatchedExpressionAnnotation.class)) {
      System.out.println(me);
    }
  }
}
",main,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;

public class TokensRegexAnnotatorDemo {

  // key for matched expressions
  public static class MyMatchedExpressionAnnotation implements CoreAnnotation<List<CoreMap>> {
    @Override
    public Class<List<CoreMap>> getType() {
      return ErasureUtils.<Class<List<CoreMap>>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) throws ClassNotFoundException {
    // set properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,tokensregex"");
    props.setProperty(""tokensregex.rules"", ""basic_ner.rules"");
    props.setProperty(""tokensregex.matchedExpressionsAnnotationKey"",
        ""edu.stanford.nlp.examples.TokensRegexAnnotatorDemo$MyMatchedExpressionAnnotation"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // annotate
    Annotation ann = new Annotation(""There will be a big announcement by Apple Inc today at 5:00pm.  "" +
        ""She has worked at Miller Corp. for 5 years."");
    pipeline.annotate(ann);
    // show results
    System.out.println(""---"");
    System.out.println(""tokens\n"");
    for (CoreMap sentence : ann.get(CoreAnnotations.SentencesAnnotation.class)) {
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.ner());
      }
      System.out.println("""");
    }
    System.out.println(""---"");
    System.out.println(""matched expressions\n"");
    for (CoreMap me : ann.get(MyMatchedExpressionAnnotation.class)) {
      System.out.println(me);
    }
  }
}
",Properties,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;

public class TokensRegexAnnotatorDemo {

  // key for matched expressions
  public static class MyMatchedExpressionAnnotation implements CoreAnnotation<List<CoreMap>> {
    @Override
    public Class<List<CoreMap>> getType() {
      return ErasureUtils.<Class<List<CoreMap>>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) throws ClassNotFoundException {
    // set properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,tokensregex"");
    props.setProperty(""tokensregex.rules"", ""basic_ner.rules"");
    props.setProperty(""tokensregex.matchedExpressionsAnnotationKey"",
        ""edu.stanford.nlp.examples.TokensRegexAnnotatorDemo$MyMatchedExpressionAnnotation"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // annotate
    Annotation ann = new Annotation(""There will be a big announcement by Apple Inc today at 5:00pm.  "" +
        ""She has worked at Miller Corp. for 5 years."");
    pipeline.annotate(ann);
    // show results
    System.out.println(""---"");
    System.out.println(""tokens\n"");
    for (CoreMap sentence : ann.get(CoreAnnotations.SentencesAnnotation.class)) {
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.ner());
      }
      System.out.println("""");
    }
    System.out.println(""---"");
    System.out.println(""matched expressions\n"");
    for (CoreMap me : ann.get(MyMatchedExpressionAnnotation.class)) {
      System.out.println(me);
    }
  }
}
",props.setProperty,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;

public class TokensRegexAnnotatorDemo {

  // key for matched expressions
  public static class MyMatchedExpressionAnnotation implements CoreAnnotation<List<CoreMap>> {
    @Override
    public Class<List<CoreMap>> getType() {
      return ErasureUtils.<Class<List<CoreMap>>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) throws ClassNotFoundException {
    // set properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,tokensregex"");
    props.setProperty(""tokensregex.rules"", ""basic_ner.rules"");
    props.setProperty(""tokensregex.matchedExpressionsAnnotationKey"",
        ""edu.stanford.nlp.examples.TokensRegexAnnotatorDemo$MyMatchedExpressionAnnotation"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // annotate
    Annotation ann = new Annotation(""There will be a big announcement by Apple Inc today at 5:00pm.  "" +
        ""She has worked at Miller Corp. for 5 years."");
    pipeline.annotate(ann);
    // show results
    System.out.println(""---"");
    System.out.println(""tokens\n"");
    for (CoreMap sentence : ann.get(CoreAnnotations.SentencesAnnotation.class)) {
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.ner());
      }
      System.out.println("""");
    }
    System.out.println(""---"");
    System.out.println(""matched expressions\n"");
    for (CoreMap me : ann.get(MyMatchedExpressionAnnotation.class)) {
      System.out.println(me);
    }
  }
}
",StanfordCoreNLP,StanfordCoreNLP,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;

public class TokensRegexAnnotatorDemo {

  // key for matched expressions
  public static class MyMatchedExpressionAnnotation implements CoreAnnotation<List<CoreMap>> {
    @Override
    public Class<List<CoreMap>> getType() {
      return ErasureUtils.<Class<List<CoreMap>>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) throws ClassNotFoundException {
    // set properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,tokensregex"");
    props.setProperty(""tokensregex.rules"", ""basic_ner.rules"");
    props.setProperty(""tokensregex.matchedExpressionsAnnotationKey"",
        ""edu.stanford.nlp.examples.TokensRegexAnnotatorDemo$MyMatchedExpressionAnnotation"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // annotate
    Annotation ann = new Annotation(""There will be a big announcement by Apple Inc today at 5:00pm.  "" +
        ""She has worked at Miller Corp. for 5 years."");
    pipeline.annotate(ann);
    // show results
    System.out.println(""---"");
    System.out.println(""tokens\n"");
    for (CoreMap sentence : ann.get(CoreAnnotations.SentencesAnnotation.class)) {
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.ner());
      }
      System.out.println("""");
    }
    System.out.println(""---"");
    System.out.println(""matched expressions\n"");
    for (CoreMap me : ann.get(MyMatchedExpressionAnnotation.class)) {
      System.out.println(me);
    }
  }
}
",pipeline.annotate,"Util.annotate
TokensRegexAnnotator.annotate
StanfordCoreNLP.annotate","CoreNLP\src\edu\stanford\nlp\naturalli\Util.java
CoreNLP\src\edu\stanford\nlp\pipeline\TokensRegexAnnotator.java
CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;

public class TokensRegexAnnotatorDemo {

  // key for matched expressions
  public static class MyMatchedExpressionAnnotation implements CoreAnnotation<List<CoreMap>> {
    @Override
    public Class<List<CoreMap>> getType() {
      return ErasureUtils.<Class<List<CoreMap>>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) throws ClassNotFoundException {
    // set properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,tokensregex"");
    props.setProperty(""tokensregex.rules"", ""basic_ner.rules"");
    props.setProperty(""tokensregex.matchedExpressionsAnnotationKey"",
        ""edu.stanford.nlp.examples.TokensRegexAnnotatorDemo$MyMatchedExpressionAnnotation"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // annotate
    Annotation ann = new Annotation(""There will be a big announcement by Apple Inc today at 5:00pm.  "" +
        ""She has worked at Miller Corp. for 5 years."");
    pipeline.annotate(ann);
    // show results
    System.out.println(""---"");
    System.out.println(""tokens\n"");
    for (CoreMap sentence : ann.get(CoreAnnotations.SentencesAnnotation.class)) {
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.ner());
      }
      System.out.println("""");
    }
    System.out.println(""---"");
    System.out.println(""matched expressions\n"");
    for (CoreMap me : ann.get(MyMatchedExpressionAnnotation.class)) {
      System.out.println(me);
    }
  }
}
",out.println,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;

public class TokensRegexAnnotatorDemo {

  // key for matched expressions
  public static class MyMatchedExpressionAnnotation implements CoreAnnotation<List<CoreMap>> {
    @Override
    public Class<List<CoreMap>> getType() {
      return ErasureUtils.<Class<List<CoreMap>>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) throws ClassNotFoundException {
    // set properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,tokensregex"");
    props.setProperty(""tokensregex.rules"", ""basic_ner.rules"");
    props.setProperty(""tokensregex.matchedExpressionsAnnotationKey"",
        ""edu.stanford.nlp.examples.TokensRegexAnnotatorDemo$MyMatchedExpressionAnnotation"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // annotate
    Annotation ann = new Annotation(""There will be a big announcement by Apple Inc today at 5:00pm.  "" +
        ""She has worked at Miller Corp. for 5 years."");
    pipeline.annotate(ann);
    // show results
    System.out.println(""---"");
    System.out.println(""tokens\n"");
    for (CoreMap sentence : ann.get(CoreAnnotations.SentencesAnnotation.class)) {
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.ner());
      }
      System.out.println("""");
    }
    System.out.println(""---"");
    System.out.println(""matched expressions\n"");
    for (CoreMap me : ann.get(MyMatchedExpressionAnnotation.class)) {
      System.out.println(me);
    }
  }
}
",ann.get,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;

public class TokensRegexAnnotatorDemo {

  // key for matched expressions
  public static class MyMatchedExpressionAnnotation implements CoreAnnotation<List<CoreMap>> {
    @Override
    public Class<List<CoreMap>> getType() {
      return ErasureUtils.<Class<List<CoreMap>>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) throws ClassNotFoundException {
    // set properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,tokensregex"");
    props.setProperty(""tokensregex.rules"", ""basic_ner.rules"");
    props.setProperty(""tokensregex.matchedExpressionsAnnotationKey"",
        ""edu.stanford.nlp.examples.TokensRegexAnnotatorDemo$MyMatchedExpressionAnnotation"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // annotate
    Annotation ann = new Annotation(""There will be a big announcement by Apple Inc today at 5:00pm.  "" +
        ""She has worked at Miller Corp. for 5 years."");
    pipeline.annotate(ann);
    // show results
    System.out.println(""---"");
    System.out.println(""tokens\n"");
    for (CoreMap sentence : ann.get(CoreAnnotations.SentencesAnnotation.class)) {
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.ner());
      }
      System.out.println("""");
    }
    System.out.println(""---"");
    System.out.println(""matched expressions\n"");
    for (CoreMap me : ann.get(MyMatchedExpressionAnnotation.class)) {
      System.out.println(me);
    }
  }
}
",sentence.get,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;

public class TokensRegexAnnotatorDemo {

  // key for matched expressions
  public static class MyMatchedExpressionAnnotation implements CoreAnnotation<List<CoreMap>> {
    @Override
    public Class<List<CoreMap>> getType() {
      return ErasureUtils.<Class<List<CoreMap>>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) throws ClassNotFoundException {
    // set properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,tokensregex"");
    props.setProperty(""tokensregex.rules"", ""basic_ner.rules"");
    props.setProperty(""tokensregex.matchedExpressionsAnnotationKey"",
        ""edu.stanford.nlp.examples.TokensRegexAnnotatorDemo$MyMatchedExpressionAnnotation"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // annotate
    Annotation ann = new Annotation(""There will be a big announcement by Apple Inc today at 5:00pm.  "" +
        ""She has worked at Miller Corp. for 5 years."");
    pipeline.annotate(ann);
    // show results
    System.out.println(""---"");
    System.out.println(""tokens\n"");
    for (CoreMap sentence : ann.get(CoreAnnotations.SentencesAnnotation.class)) {
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.ner());
      }
      System.out.println("""");
    }
    System.out.println(""---"");
    System.out.println(""matched expressions\n"");
    for (CoreMap me : ann.get(MyMatchedExpressionAnnotation.class)) {
      System.out.println(me);
    }
  }
}
",token.word,"CoreLabel.word
Token.word
Sentence.word","CoreNLP\src\edu\stanford\nlp\ling\CoreLabel.java
CoreNLP\src\edu\stanford\nlp\simple\Token.java
CoreNLP\src\edu\stanford\nlp\simple\Sentence.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;

public class TokensRegexAnnotatorDemo {

  // key for matched expressions
  public static class MyMatchedExpressionAnnotation implements CoreAnnotation<List<CoreMap>> {
    @Override
    public Class<List<CoreMap>> getType() {
      return ErasureUtils.<Class<List<CoreMap>>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) throws ClassNotFoundException {
    // set properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,tokensregex"");
    props.setProperty(""tokensregex.rules"", ""basic_ner.rules"");
    props.setProperty(""tokensregex.matchedExpressionsAnnotationKey"",
        ""edu.stanford.nlp.examples.TokensRegexAnnotatorDemo$MyMatchedExpressionAnnotation"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // annotate
    Annotation ann = new Annotation(""There will be a big announcement by Apple Inc today at 5:00pm.  "" +
        ""She has worked at Miller Corp. for 5 years."");
    pipeline.annotate(ann);
    // show results
    System.out.println(""---"");
    System.out.println(""tokens\n"");
    for (CoreMap sentence : ann.get(CoreAnnotations.SentencesAnnotation.class)) {
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.ner());
      }
      System.out.println("""");
    }
    System.out.println(""---"");
    System.out.println(""matched expressions\n"");
    for (CoreMap me : ann.get(MyMatchedExpressionAnnotation.class)) {
      System.out.println(me);
    }
  }
}
",token.ner,"Token.ner
CoreLabel.ner","CoreNLP\src\edu\stanford\nlp\simple\Token.java
CoreNLP\src\edu\stanford\nlp\ling\CoreLabel.java",False
"{ ruleType: ""tokens"", pattern: ([{word:""I""}] [{word:/like|love/} & {tag:""VBP""}] ([{word:""pizza""}])), action: Annotate($1, ner, ""FOOD""), result: ""PIZZA"" }

",Annotate,N/A,N/A,N/A
"# make all patterns case-sensitive
ENV.defaultStringMatchFlags = 0
ENV.defaultStringPatternFlags = 0

# these Java classes will be used by the rules
ner = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$NamedEntityTagAnnotation"" }
tokens = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$TokensAnnotation"" }

# define some regexes over tokens
$COMPANY_BEGINNING = ""/[A-Z][A-Za-z]+/""
$COMPANY_ENDING = ""/(Corp|Inc)\.?/""

# rule for recognizing company names
{ ruleType: ""tokens"", pattern: ([{word:$COMPANY_BEGINNING} & {tag:""NNP""}]+ [{word:$COMPANY_ENDING}]), action: Annotate($0, ner, ""COMPANY""), result: ""COMPANY_RESULT"" }
",Annotate,N/A,N/A,N/A
"---
sentence number: 0
sentence text: She has worked at Miller Corp. for 5 years.
She		PRP	null
has		VBZ	null
worked		VBN	null
at		IN	null
Miller		NNP	COMPANY
Corp.		NNP	COMPANY
for		IN	null
5		CD	null
years		NNS	null
.		.	null

matched expression: Miller Corp.
matched expression value: STRING(COMPANY_RESULT)
matched expression char offsets: (18,30)
matched expression tokens:[Miller-5, Corp.-6]
---
sentence number: 1
sentence text: There will be a big announcement by Apple Inc today at 5:00pm.
There		EX	null
will		MD	null
be		VB	null
a		DT	null
big		JJ	null
announcement		NN	null
by		IN	null
Apple		NNP	COMPANY
Inc		NNP	COMPANY
today		NN	null
at		IN	null
5:00		CD	null
pm		NN	null
.		.	null

matched expression: Apple Inc
matched expression value: STRING(COMPANY_RESULT)
matched expression char offsets: (80,89)
matched expression tokens:[Apple-8, Inc-9]
---
sentence number: 2
sentence text: He works for apple inc in cupertino.
He		PRP	null
works		VBZ	null
for		IN	null
apple		NN	null
inc		NN	null
in		IN	null
cupertino		NN	null
.		.	null
",STRING,N/A,N/A,N/A
"# uncomment to make all patterns case-insensitive in the rules file
# ENV.defaultStringMatchFlags = 66
# ENV.defaultStringPatternFlags = 66

# these Java classes will be used by the rules
ner = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$NamedEntityTagAnnotation"" }
tokens = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$TokensAnnotation"" }

# variables for complex regexes
$JOB_TITLE_BASES = ""/president|secretary|general/""
$JOB_TITLE_MODIFIERS = ""/vice|assistant|deputy/""

# first phase identifies components of job titles
# a TokensRegex pipeline can run various stages
# to specify a particular stage, set ENV.defaults[""stage""] to the stage number
ENV.defaults[""stage""] = 1

# tokens match phase
{ ruleType: ""tokens"", pattern: ([{word:$JOB_TITLE_MODIFIERS}]+), action: Annotate($0, ner, ""JOB_TITLE_MODIFIER"") }
{ ruleType: ""tokens"", pattern: ([{word:$JOB_TITLE_BASES}]), action: Annotate($0, ner, ""JOB_TITLE_BASE"") }

# second phase identifies complete job titles from components found in first phase
ENV.defaults[""stage""] = 2
{ ruleType: ""tokens"", pattern: ([{ner: ""JOB_TITLE_MODIFIER""}]+ [{ner: ""JOB_TITLE_BASE""}]), 
  action: Annotate($0, ner, ""COMPLETE_JOB_TITLE""), result: ""FOUND_COMPLETE_JOB_TITLE""}

# third phase is a filter phase, and it removes matched expressions that the filter matches
ENV.defaults[""stage""] = 3
# clean up component named entity tags from stage 1
{ ruleType: ""tokens"", pattern: ([{ner:""JOB_TITLE_MODIFIER""} | {ner:""JOB_TITLE_BASE""}]+), action: Annotate($0, ner, ""O"") }
# filter out the matched expression ""deputy vice president""
{ ruleType: ""filter"", pattern: ([{word:""deputy""}] [{word:""vice""}] [{word:""president""}]) }
",Annotate,N/A,N/A,N/A
"---
sentence number: 0
sentence text: He is the vice president.
He		PRP	O
is		VBZ	O
the		DT	O
vice		NN	COMPLETE_JOB_TITLE
president		NN	COMPLETE_JOB_TITLE
.		.	O

matched expression: vice president
matched expression value: STRING(FOUND_COMPLETE_JOB_TITLE)
matched expression char offsets: (10,24)
matched expression tokens:[vice-4, president-5]
---
sentence number: 1
sentence text: He is the assistant vice president.
He		PRP	O
is		VBZ	O
the		DT	O
assistant		JJ	COMPLETE_JOB_TITLE
vice		NN	COMPLETE_JOB_TITLE
president		NN	COMPLETE_JOB_TITLE
.		.	O

matched expression: assistant vice president
matched expression value: STRING(FOUND_COMPLETE_JOB_TITLE)
matched expression char offsets: (36,60)
matched expression tokens:[assistant-4, vice-5, president-6]
---
sentence number: 2
sentence text: He is the deputy vice president.
He		PRP	O
is		VBZ	O
the		DT	O
deputy		NN	COMPLETE_JOB_TITLE
vice		NN	COMPLETE_JOB_TITLE
president		NN	COMPLETE_JOB_TITLE
.		.	O
---
sentence number: 3
sentence text: He is the president.
He		PRP	O
is		VBZ	O
the		DT	O
president		NN	O
.		.	O
---
sentence number: 4
sentence text: He is the President.
He		PRP	O
is		VBZ	O
the		DT	O
President		NNP	O
.		.	O
",STRING,N/A,N/A,N/A
"---
sentence number: 0
sentence text: John said, ""I thought the pizza was great!""
John		NNP	PERSON
said		VBD	O
,		,	O
``		``	O
I		PRP	O
thought		VBD	O
the		DT	O
pizza		NN	O
was		VBD	O
great		JJ	O
!		.	O
''		''	O

matched expression: ""I thought the pizza was great!""
matched expression value: STRING(QUOTE)
matched expression char offsets: (11,43)
matched expression tokens:[``-4, I-5, thought-6, the-7, pizza-8, was-9, great-10, !-11, ''-12]
",STRING,N/A,N/A,N/A
"# rules for finding employment relations
{ ruleType: ""tokens"", pattern: (([{ner:""PERSON""}]+) /works/ /for/ ([{ner:""ORGANIZATION""}]+)), 
  result: Concat(""("", $$1.text, "","", ""works_for"", "","", $$2.text, "")"") } 

{ ruleType: ""tokens"", pattern: (([{ner:""PERSON""}]+) /is/ /employed/ /at|by/ ([{ner:""ORGANIZATION""}]+)), 
  result: Concat(""("", $$1.text, "","", ""works_for"", "","", $$2.text, "")"") } 
",Concat,N/A,N/A,N/A
"---
sentence number: 0
sentence text: Joe Smith works for Google.
Joe		NNP	PERSON
Smith		NNP	PERSON
works		VBZ	O
for		IN	O
Google		NNP	ORGANIZATION
.		.	O

matched expression: Joe Smith works for Google
matched expression value: STRING((Joe Smith,works_for,Google))
matched expression char offsets: (0,26)
matched expression tokens:[Joe-1, Smith-2, works-3, for-4, Google-5]
---
sentence number: 1
sentence text: Jane Smith is employed by Apple.
Jane		NNP	PERSON
Smith		NNP	PERSON
is		VBZ	O
employed		VBN	O
by		IN	O
Apple		NNP	ORGANIZATION
.		.	O

matched expression: Jane Smith is employed by Apple
matched expression value: STRING((Jane Smith,works_for,Apple))
matched expression char offsets: (28,59)
matched expression tokens:[Jane-1, Smith-2, is-3, employed-4, by-5, Apple-6]
",STRING,N/A,N/A,N/A
"orig = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$OriginalTextAnnotation"" }
numtokens = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$NumerizedTokensAnnotation"" }
numcomptype = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$NumericCompositeTypeAnnotation"" }
numcompvalue = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$NumericCompositeValueAnnotation"" }

mytokens = { type: ""CLASS"", value: ""edu.stanford.nlp.examples.TokensRegexDemo$MyTokensAnnotation"" }
type = { type: ""CLASS"", value: ""edu.stanford.nlp.examples.TokensRegexDemo$MyTypeAnnotation"" }
value = { type: ""CLASS"", value: ""edu.stanford.nlp.examples.TokensRegexDemo$MyValueAnnotation"" }

ENV.defaultResultAnnotationKey = ( type, value )
ENV.defaultNestedResultsAnnotationKey = mytokens
ENV.defaults[""stage.limitIters""] = 0

// Numbers
{ ruleType: ""tokens"", pattern: ( [ numcomptype:""NUMBER"" ] ), result: ( ""EXPR"", $0[0].numcompvalue ) }

// Operators
{ pattern: ( ""+"" ),            result: ( ""OP"", ""Add"" ),      priority: 1}
{ pattern: ( /plus/ ),         result: ( ""OP"", ""Add"" ),      priority: 1}
{ pattern: ( ""-"" ),            result: ( ""OP"", ""Subtract"" ), priority: 1}
{ pattern: ( /minus/ ),        result: ( ""OP"", ""Subtract"" ), priority: 1}
{ pattern: ( ""*"" ),            result: ( ""OP"", ""Multiply"" ), priority: 2}
{ pattern: ( /times/ ),        result: ( ""OP"", ""Multiply"" ), priority: 2}
{ pattern: ( ""/"" ),            result: ( ""OP"", ""Divide"" ),   priority: 2}
{ pattern: ( /divided/ /by/ ), result: ( ""OP"", ""Divide"" ),   priority: 2}
{ pattern: ( ""^"" ),            result: ( ""OP"", ""Pow"" ),      priority: 3}

$OP = ( [ type:""OP"" ] )
$EXPR = ( [ type:""EXPR"" ] )

{ ruleType: ""composite"", pattern: ( ($EXPR) ($OP) ($EXPR) ), result: (""EXPR"", Call($2[0].value, $1[0].value, $3[0].value)) }

{ ruleType: ""composite"", pattern: ( [orig:""(""] ($EXPR) [orig:"")""] ), result: (""EXPR"", $1[0].value) }
",Call,N/A,N/A,N/A
"---
sentence number: 0
sentence text: (5 + 5) + 5
-LRB-		-LRB-	O
5		CD	NUMBER
+		CC	O
5		CD	NUMBER
-RRB-		-RRB-	O
+		CC	O
5		CD	NUMBER

matched expression: -LRB-5 + 5-RRB- + 5
matched expression value: LIST([STRING(EXPR), NUMBER(15)])
matched expression char offsets: (0,11)
matched expression tokens:[-LRB--1, 5-2, +-3, 5-4, -RRB--5, +-6, 5-7]
",LIST,N/A,N/A,N/A
"---
sentence number: 0
sentence text: (5 + 5) + 5
-LRB-		-LRB-	O
5		CD	NUMBER
+		CC	O
5		CD	NUMBER
-RRB-		-RRB-	O
+		CC	O
5		CD	NUMBER

matched expression: -LRB-5 + 5-RRB- + 5
matched expression value: LIST([STRING(EXPR), NUMBER(15)])
matched expression char offsets: (0,11)
matched expression tokens:[-LRB--1, 5-2, +-3, 5-4, -RRB--5, +-6, 5-7]
",STRING,N/A,N/A,N/A
"---
sentence number: 0
sentence text: (5 + 5) + 5
-LRB-		-LRB-	O
5		CD	NUMBER
+		CC	O
5		CD	NUMBER
-RRB-		-RRB-	O
+		CC	O
5		CD	NUMBER

matched expression: -LRB-5 + 5-RRB- + 5
matched expression value: LIST([STRING(EXPR), NUMBER(15)])
matched expression char offsets: (0,11)
matched expression tokens:[-LRB--1, 5-2, +-3, 5-4, -RRB--5, +-6, 5-7]
",NUMBER,N/A,N/A,N/A
"Exception in thread ""main"" java.lang.NoSuchMethodError: edu.stanford.nlp.tagger.maxent.TaggerConfig.getTaggerDataInputStream(Ljava/lang/String;)Ljava/io/DataInputStream;
",TaggerConfig.getTaggerDataInputStream,N/A,N/A,N/A
"Caused by: java.lang.NoSuchMethodError: edu.stanford.nlp.util.Generics.newHashMap()Ljava/util/Map;
    at edu.stanford.nlp.pipeline.AnnotatorPool.(AnnotatorPool.java:27)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.getDefaultAnnotatorPool(StanfordCoreNLP.java:305)
",Generics.newHashMap,Generics.newHashMap,CoreNLP\src\edu\stanford\nlp\util\Generics.java,False
"Caused by: java.lang.NoSuchMethodError: edu.stanford.nlp.util.Generics.newHashMap()Ljava/util/Map;
    at edu.stanford.nlp.pipeline.AnnotatorPool.(AnnotatorPool.java:27)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.getDefaultAnnotatorPool(StanfordCoreNLP.java:305)
",StanfordCoreNLP.getDefaultAnnotatorPool,StanfordCoreNLP.getDefaultAnnotatorPool,CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"  public AnnotationPipeline buildPipeline() {
    AnnotationPipeline pl = new AnnotationPipeline();
    pl.addAnnotator(new TokenizerAnnotator(false));
    pl.addAnnotator(new WordsToSentencesAnnotator(false));
    pl.addAnnotator(new POSTaggerAnnotator(false));
    pl.addAnnotator(new MorphaAnnotator(false));
    pl.addAnnotator(new TimeAnnotator(""sutime"", props));
    pl.addAnnotator(new PhraseAnnotator(phrasesFile, false));
    return pl;
  }
",buildPipeline,N/A,N/A,N/A
"  public AnnotationPipeline buildPipeline() {
    AnnotationPipeline pl = new AnnotationPipeline();
    pl.addAnnotator(new TokenizerAnnotator(false));
    pl.addAnnotator(new WordsToSentencesAnnotator(false));
    pl.addAnnotator(new POSTaggerAnnotator(false));
    pl.addAnnotator(new MorphaAnnotator(false));
    pl.addAnnotator(new TimeAnnotator(""sutime"", props));
    pl.addAnnotator(new PhraseAnnotator(phrasesFile, false));
    return pl;
  }
",AnnotationPipeline,AnnotationPipeline,CoreNLP\src\edu\stanford\nlp\pipeline\AnnotationPipeline.java,False
"  public AnnotationPipeline buildPipeline() {
    AnnotationPipeline pl = new AnnotationPipeline();
    pl.addAnnotator(new TokenizerAnnotator(false));
    pl.addAnnotator(new WordsToSentencesAnnotator(false));
    pl.addAnnotator(new POSTaggerAnnotator(false));
    pl.addAnnotator(new MorphaAnnotator(false));
    pl.addAnnotator(new TimeAnnotator(""sutime"", props));
    pl.addAnnotator(new PhraseAnnotator(phrasesFile, false));
    return pl;
  }
",pl.addAnnotator,AnnotationPipeline.addAnnotator,CoreNLP\src\edu\stanford\nlp\pipeline\AnnotationPipeline.java,False
"  public AnnotationPipeline buildPipeline() {
    AnnotationPipeline pl = new AnnotationPipeline();
    pl.addAnnotator(new TokenizerAnnotator(false));
    pl.addAnnotator(new WordsToSentencesAnnotator(false));
    pl.addAnnotator(new POSTaggerAnnotator(false));
    pl.addAnnotator(new MorphaAnnotator(false));
    pl.addAnnotator(new TimeAnnotator(""sutime"", props));
    pl.addAnnotator(new PhraseAnnotator(phrasesFile, false));
    return pl;
  }
",TokenizerAnnotator,TokenizerAnnotator,CoreNLP\src\edu\stanford\nlp\pipeline\TokenizerAnnotator.java,False
"  public AnnotationPipeline buildPipeline() {
    AnnotationPipeline pl = new AnnotationPipeline();
    pl.addAnnotator(new TokenizerAnnotator(false));
    pl.addAnnotator(new WordsToSentencesAnnotator(false));
    pl.addAnnotator(new POSTaggerAnnotator(false));
    pl.addAnnotator(new MorphaAnnotator(false));
    pl.addAnnotator(new TimeAnnotator(""sutime"", props));
    pl.addAnnotator(new PhraseAnnotator(phrasesFile, false));
    return pl;
  }
",WordsToSentencesAnnotator,WordsToSentencesAnnotator,CoreNLP\src\edu\stanford\nlp\pipeline\WordsToSentencesAnnotator.java,False
"  public AnnotationPipeline buildPipeline() {
    AnnotationPipeline pl = new AnnotationPipeline();
    pl.addAnnotator(new TokenizerAnnotator(false));
    pl.addAnnotator(new WordsToSentencesAnnotator(false));
    pl.addAnnotator(new POSTaggerAnnotator(false));
    pl.addAnnotator(new MorphaAnnotator(false));
    pl.addAnnotator(new TimeAnnotator(""sutime"", props));
    pl.addAnnotator(new PhraseAnnotator(phrasesFile, false));
    return pl;
  }
",POSTaggerAnnotator,POSTaggerAnnotator,CoreNLP\src\edu\stanford\nlp\pipeline\POSTaggerAnnotator.java,False
"  public AnnotationPipeline buildPipeline() {
    AnnotationPipeline pl = new AnnotationPipeline();
    pl.addAnnotator(new TokenizerAnnotator(false));
    pl.addAnnotator(new WordsToSentencesAnnotator(false));
    pl.addAnnotator(new POSTaggerAnnotator(false));
    pl.addAnnotator(new MorphaAnnotator(false));
    pl.addAnnotator(new TimeAnnotator(""sutime"", props));
    pl.addAnnotator(new PhraseAnnotator(phrasesFile, false));
    return pl;
  }
",MorphaAnnotator,MorphaAnnotator,CoreNLP\src\edu\stanford\nlp\pipeline\MorphaAnnotator.java,False
"  public AnnotationPipeline buildPipeline() {
    AnnotationPipeline pl = new AnnotationPipeline();
    pl.addAnnotator(new TokenizerAnnotator(false));
    pl.addAnnotator(new WordsToSentencesAnnotator(false));
    pl.addAnnotator(new POSTaggerAnnotator(false));
    pl.addAnnotator(new MorphaAnnotator(false));
    pl.addAnnotator(new TimeAnnotator(""sutime"", props));
    pl.addAnnotator(new PhraseAnnotator(phrasesFile, false));
    return pl;
  }
",TimeAnnotator,TimeAnnotator,CoreNLP\src\edu\stanford\nlp\time\TimeAnnotator.java,False
"  public AnnotationPipeline buildPipeline() {
    AnnotationPipeline pl = new AnnotationPipeline();
    pl.addAnnotator(new TokenizerAnnotator(false));
    pl.addAnnotator(new WordsToSentencesAnnotator(false));
    pl.addAnnotator(new POSTaggerAnnotator(false));
    pl.addAnnotator(new MorphaAnnotator(false));
    pl.addAnnotator(new TimeAnnotator(""sutime"", props));
    pl.addAnnotator(new PhraseAnnotator(phrasesFile, false));
    return pl;
  }
",PhraseAnnotator,N/A,N/A,N/A
"AnnotationPipeline pipeline = buildPipeline();
Annotation annotation = new Annotation(""It's like a topography that is made from cartography of me."");
pipeline.annotate(annotation);
",buildPipeline,N/A,N/A,N/A
"AnnotationPipeline pipeline = buildPipeline();
Annotation annotation = new Annotation(""It's like a topography that is made from cartography of me."");
pipeline.annotate(annotation);
",Annotation,Annotation,CoreNLP\src\edu\stanford\nlp\pipeline\Annotation.java,False
"AnnotationPipeline pipeline = buildPipeline();
Annotation annotation = new Annotation(""It's like a topography that is made from cartography of me."");
pipeline.annotate(annotation);
",pipeline.annotate,AnnotationPipeline.annotate,CoreNLP\src\edu\stanford\nlp\pipeline\AnnotationPipeline.java,False
"public void annotate(Annotation annotation);
public Set<Requirement> requirementsSatisfied();
public Set<Requirement> requires();
",annotate,N/A,N/A,N/A
"public void annotate(Annotation annotation);
public Set<Requirement> requirementsSatisfied();
public Set<Requirement> requires();
",requirementsSatisfied,N/A,N/A,N/A
"public void annotate(Annotation annotation);
public Set<Requirement> requirementsSatisfied();
public Set<Requirement> requires();
",requires,N/A,N/A,N/A
