Example,Extracted Function,Linked Function,Source File,Linked
load(),load,N/A,N/A,N/A
retrieve(),retrieve,N/A,N/A,N/A
write(),write,N/A,N/A,N/A
writestr(),writestr,N/A,N/A,N/A
seek(),seek,N/A,N/A,N/A
tell(),tell,N/A,N/A,N/A
seek(),seek,N/A,N/A,N/A
tell(),tell,N/A,N/A,N/A
seek(),seek,N/A,N/A,N/A
tell(),tell,N/A,N/A,N/A
readline(),readline,N/A,N/A,N/A
read(),read,N/A,N/A,N/A
readline(),readline,N/A,N/A,N/A
tell(),tell,N/A,N/A,N/A
find(),find,N/A,N/A,N/A
load(),load,N/A,N/A,N/A
download(),download,N/A,N/A,N/A
Downloader.default_download_dir(),Downloader.default_download_dir,Downloader.default_download_dir,nltk\nltk\downloader.py,True
download(),download,N/A,N/A,N/A
default_download_dir(),default_download_dir,N/A,N/A,N/A
download(),download,N/A,N/A,N/A
_package_to_columns(),_package_to_columns,N/A,N/A,N/A
unify(),unify,N/A,N/A,N/A
walk(),walk,N/A,N/A,N/A
cyclic(),cyclic,N/A,N/A,N/A
freeze(),freeze,N/A,N/A,N/A
equal_values(),equal_values,N/A,N/A,N/A
freeze(),freeze,N/A,N/A,N/A
copy(),copy,N/A,N/A,N/A
nltk.featstruct.rename_variables(),featstruct.rename_variables,N/A,N/A,N/A
nltk.featstruct.retract_bindings(),featstruct.retract_bindings,N/A,N/A,N/A
nltk.featstruct.substitute_bindings(),featstruct.substitute_bindings,N/A,N/A,N/A
nltk.featstruct.find_variables(),featstruct.find_variables,N/A,N/A,N/A
FreqDist.N(),FreqDist.N,FreqDist.N,nltk\nltk\probability.py,True
FreqDist.B(),FreqDist.B,FreqDist.B,nltk\nltk\probability.py,True
bins-self.B(),self.B,N/A,N/A,N/A
self.B(),self.B,N/A,N/A,N/A
Counter.setdefault(),Counter.setdefault,N/A,N/A,N/A
Counter.update(),Counter.update,N/A,N/A,N/A
self.prob(samp),self.prob,N/A,N/A,N/A
log(p),log,N/A,N/A,N/A
log(2**(logx)+2**(logy)),log,N/A,N/A,N/A
findall(),findall,N/A,N/A,N/A
fields(),fields,N/A,N/A,N/A
decode(),decode,N/A,N/A,N/A
StandardFormat.fields(),StandardFormat.fields,StandardFormat.fields,nltk\nltk\toolbox.py,True
encode(),encode,N/A,N/A,N/A
parents(),parents,N/A,N/A,N/A
parent_indices(),parent_indices,N/A,N/A,N/A
left_siblings(),left_siblings,N/A,N/A,N/A
right_siblings(),right_siblings,N/A,N/A,N/A
parents(),parents,N/A,N/A,N/A
parent_indices(),parent_indices,N/A,N/A,N/A
ptree.parent()[ptree.parent_index()] is ptree,ptree.parent,N/A,N/A,N/A
ptree.parent()[ptree.parent_index()] is ptree,ptree.parent_index,N/A,N/A,N/A
ptree.parent_index(),ptree.parent_index,N/A,N/A,N/A
ptree.parent.index(ptree),parent.index,N/A,N/A,N/A
index(),index,N/A,N/A,N/A
ptree.parent(),ptree.parent,N/A,N/A,N/A
log(p),log,N/A,N/A,N/A
"Tree(label, children)",Tree,__init__,nltk\nltk\tree\tree.py,True
Tree.fromstring(s),Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,True
tp=self.leaf_treeposition(i),self.leaf_treeposition,N/A,N/A,N/A
self[tp]==self.leaves()[i],self.leaves,N/A,N/A,N/A
self.leaves()[start:end],self.leaves,N/A,N/A,N/A
">>> download('treebank') 
[nltk_data] Downloading package 'treebank'...
[nltk_data]   Unzipping corpora/treebank.zip.
",download,N/A,N/A,N/A
">>> download('all-corpora') 
[nltk_data] Downloading package 'abc'...
[nltk_data]   Unzipping corpora/abc.zip.
[nltk_data] Downloading package 'alpino'...
[nltk_data]   Unzipping corpora/alpino.zip.
  ...
[nltk_data] Downloading package 'words'...
[nltk_data]   Unzipping corpora/words.zip.
",download,N/A,N/A,N/A
">>> from nltk.featstruct import unify
>>> unify(dict(x=1, y=dict()), dict(a='a', y=dict(b='b')))  
{'y': {'b': 'b'}, 'x': 1, 'a': 'a'}
",unify,N/A,N/A,N/A
">>> from nltk.featstruct import unify
>>> unify(dict(x=1, y=dict()), dict(a='a', y=dict(b='b')))  
{'y': {'b': 'b'}, 'x': 1, 'a': 'a'}
",dict,N/A,N/A,N/A
"def handler(s, position, reentrances, match): ...
",handler,N/A,N/A,N/A
">>> from nltk.featstruct import FeatStruct
>>> FeatStruct('[a=?x]').unify(FeatStruct('[b=?x]'))
[a=?x, b=?x2]
",FeatStruct,__init__,nltk\nltk\featstruct.py,True
">>> from nltk.featstruct import FeatStruct
>>> FeatStruct('[a=?x]').unify(FeatStruct('[b=?x]'))
[a=?x, b=?x2]
",unify,N/A,N/A,N/A
">>> from nltk.probability import ConditionalFreqDist
>>> from nltk.tokenize import word_tokenize
>>> sent = ""the the the dog dog some other words that we do not care about""
>>> cfdist = ConditionalFreqDist()
>>> for word in word_tokenize(sent):
...     condition = len(word)
...     cfdist[condition][word] += 1
",ConditionalFreqDist,__init__,nltk\nltk\probability.py,True
">>> from nltk.probability import ConditionalFreqDist
>>> from nltk.tokenize import word_tokenize
>>> sent = ""the the the dog dog some other words that we do not care about""
>>> cfdist = ConditionalFreqDist()
>>> for word in word_tokenize(sent):
...     condition = len(word)
...     cfdist[condition][word] += 1
",word_tokenize,N/A,N/A,N/A
">>> from nltk.probability import ConditionalFreqDist
>>> from nltk.tokenize import word_tokenize
>>> sent = ""the the the dog dog some other words that we do not care about""
>>> cfdist = ConditionalFreqDist()
>>> for word in word_tokenize(sent):
...     condition = len(word)
...     cfdist[condition][word] += 1
",len,N/A,N/A,N/A
">>> cfdist = ConditionalFreqDist((len(word), word) for word in word_tokenize(sent))
",ConditionalFreqDist,__init__,nltk\nltk\probability.py,True
">>> cfdist = ConditionalFreqDist((len(word), word) for word in word_tokenize(sent))
",len,N/A,N/A,N/A
">>> cfdist = ConditionalFreqDist((len(word), word) for word in word_tokenize(sent))
",word_tokenize,N/A,N/A,N/A
">>> cfdist[3]
FreqDist({'the': 3, 'dog': 2, 'not': 1})
>>> cfdist[3].freq('the')
0.5
>>> cfdist[3]['dog']
2
",FreqDist,__init__,nltk\nltk\probability.py,True
">>> cfdist[3]
FreqDist({'the': 3, 'dog': 2, 'not': 1})
>>> cfdist[3].freq('the')
0.5
>>> cfdist[3]['dog']
2
",freq,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.probability import ConditionalProbDist, ELEProbDist
>>> cfdist = ConditionalFreqDist(brown.tagged_words()[:5000])
>>> cpdist = ConditionalProbDist(cfdist, ELEProbDist, 10)
>>> cpdist['passed'].max()
'VBD'
>>> cpdist['passed'].prob('VBD')
0.423...
",ConditionalFreqDist,__init__,nltk\nltk\probability.py,True
">>> from nltk.corpus import brown
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.probability import ConditionalProbDist, ELEProbDist
>>> cfdist = ConditionalFreqDist(brown.tagged_words()[:5000])
>>> cpdist = ConditionalProbDist(cfdist, ELEProbDist, 10)
>>> cpdist['passed'].max()
'VBD'
>>> cpdist['passed'].prob('VBD')
0.423...
",brown.tagged_words,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.probability import ConditionalProbDist, ELEProbDist
>>> cfdist = ConditionalFreqDist(brown.tagged_words()[:5000])
>>> cpdist = ConditionalProbDist(cfdist, ELEProbDist, 10)
>>> cpdist['passed'].max()
'VBD'
>>> cpdist['passed'].prob('VBD')
0.423...
",ConditionalProbDist,__init__,nltk\nltk\probability.py,True
">>> from nltk.corpus import brown
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.probability import ConditionalProbDist, ELEProbDist
>>> cfdist = ConditionalFreqDist(brown.tagged_words()[:5000])
>>> cpdist = ConditionalProbDist(cfdist, ELEProbDist, 10)
>>> cpdist['passed'].max()
'VBD'
>>> cpdist['passed'].prob('VBD')
0.423...
",max,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.probability import ConditionalProbDist, ELEProbDist
>>> cfdist = ConditionalFreqDist(brown.tagged_words()[:5000])
>>> cpdist = ConditionalProbDist(cfdist, ELEProbDist, 10)
>>> cpdist['passed'].max()
'VBD'
>>> cpdist['passed'].prob('VBD')
0.423...
",prob,N/A,N/A,N/A
">>> from nltk.tokenize import word_tokenize
>>> from nltk.probability import FreqDist
>>> sent = 'This is an example sentence'
>>> fdist = FreqDist()
>>> for word in word_tokenize(sent):
...    fdist[word.lower()] += 1
",FreqDist,__init__,nltk\nltk\probability.py,True
">>> from nltk.tokenize import word_tokenize
>>> from nltk.probability import FreqDist
>>> sent = 'This is an example sentence'
>>> fdist = FreqDist()
>>> for word in word_tokenize(sent):
...    fdist[word.lower()] += 1
",word_tokenize,N/A,N/A,N/A
">>> from nltk.tokenize import word_tokenize
>>> from nltk.probability import FreqDist
>>> sent = 'This is an example sentence'
>>> fdist = FreqDist()
>>> for word in word_tokenize(sent):
...    fdist[word.lower()] += 1
",word.lower,N/A,N/A,N/A
">>> fdist = FreqDist(word.lower() for word in word_tokenize(sent))
",FreqDist,__init__,nltk\nltk\probability.py,True
">>> fdist = FreqDist(word.lower() for word in word_tokenize(sent))
",word.lower,N/A,N/A,N/A
">>> fdist = FreqDist(word.lower() for word in word_tokenize(sent))
",word_tokenize,N/A,N/A,N/A
">>> from nltk.probability import ProbabilisticMixIn
>>> class A:
...     def __init__(self, x, y): self.data = (x,y)
...
>>> class ProbabilisticA(A, ProbabilisticMixIn):
...     def __init__(self, x, y, **prob_kwarg):
...         A.__init__(self, x, y)
...         ProbabilisticMixIn.__init__(self, **prob_kwarg)
",__init__,N/A,N/A,N/A
">>> from nltk.probability import ProbabilisticMixIn
>>> class A:
...     def __init__(self, x, y): self.data = (x,y)
...
>>> class ProbabilisticA(A, ProbabilisticMixIn):
...     def __init__(self, x, y, **prob_kwarg):
...         A.__init__(self, x, y)
...         ProbabilisticMixIn.__init__(self, **prob_kwarg)
",ProbabilisticA,N/A,N/A,N/A
">>> from nltk.probability import ProbabilisticMixIn
>>> class A:
...     def __init__(self, x, y): self.data = (x,y)
...
>>> class ProbabilisticA(A, ProbabilisticMixIn):
...     def __init__(self, x, y, **prob_kwarg):
...         A.__init__(self, x, y)
...         ProbabilisticMixIn.__init__(self, **prob_kwarg)
",A.__init__,N/A,N/A,N/A
">>> import nltk.corpus
>>> from nltk.text import Text
>>> moby = Text(nltk.corpus.gutenberg.words('melville-moby_dick.txt'))
",Text,__init__,nltk\nltk\text.py,True
">>> import nltk.corpus
>>> from nltk.text import Text
>>> moby = Text(nltk.corpus.gutenberg.words('melville-moby_dick.txt'))
",gutenberg.words,N/A,N/A,N/A
">>> from nltk.book import text4
>>> text4.collocation_list()[:2]
[('United', 'States'), ('fellow', 'citizens')]
",text4.collocation_list,N/A,N/A,N/A
">>> from nltk.book import text4
>>> text4.collocations() 
United States; fellow citizens; four years; ...
",text4.collocations,N/A,N/A,N/A
">>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",print,N/A,N/A,N/A
">>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",text5.findall,N/A,N/A,N/A
">>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",text1.findall,N/A,N/A,N/A
">>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",text9.findall,N/A,N/A,N/A
">>> import nltk.corpus
>>> from nltk.text import TextCollection
>>> print('hack'); from nltk.book import text1, text2, text3
hack...
>>> gutenberg = TextCollection(nltk.corpus.gutenberg)
>>> mytexts = TextCollection([text1, text2, text3])
",print,N/A,N/A,N/A
">>> import nltk.corpus
>>> from nltk.text import TextCollection
>>> print('hack'); from nltk.book import text1, text2, text3
hack...
>>> gutenberg = TextCollection(nltk.corpus.gutenberg)
>>> mytexts = TextCollection([text1, text2, text3])
",TextCollection,__init__,nltk\nltk\text.py,True
">>> from nltk.text import TokenSearcher
>>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",print,N/A,N/A,N/A
">>> from nltk.text import TokenSearcher
>>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",text5.findall,N/A,N/A,N/A
">>> from nltk.text import TokenSearcher
>>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",text1.findall,N/A,N/A,N/A
">>> from nltk.text import TokenSearcher
>>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",text9.findall,N/A,N/A,N/A
"for parent_index in ptree.parent_indices(parent):
    parent[parent_index] is ptree
",ptree.parent_indices,N/A,N/A,N/A
"for treepos in ptree.treepositions(root):
    root[treepos] is ptree
",ptree.treepositions,N/A,N/A,N/A
">>> from nltk.probability import ProbabilisticMixIn
>>> class A:
...     def __init__(self, x, y): self.data = (x,y)
...
>>> class ProbabilisticA(A, ProbabilisticMixIn):
...     def __init__(self, x, y, **prob_kwarg):
...         A.__init__(self, x, y)
...         ProbabilisticMixIn.__init__(self, **prob_kwarg)
",__init__,N/A,N/A,N/A
">>> from nltk.probability import ProbabilisticMixIn
>>> class A:
...     def __init__(self, x, y): self.data = (x,y)
...
>>> class ProbabilisticA(A, ProbabilisticMixIn):
...     def __init__(self, x, y, **prob_kwarg):
...         A.__init__(self, x, y)
...         ProbabilisticMixIn.__init__(self, **prob_kwarg)
",ProbabilisticA,N/A,N/A,N/A
">>> from nltk.probability import ProbabilisticMixIn
>>> class A:
...     def __init__(self, x, y): self.data = (x,y)
...
>>> class ProbabilisticA(A, ProbabilisticMixIn):
...     def __init__(self, x, y, **prob_kwarg):
...         A.__init__(self, x, y)
...         ProbabilisticMixIn.__init__(self, **prob_kwarg)
",A.__init__,N/A,N/A,N/A
">>> from nltk.tree import Tree
>>> print(Tree(1, [2, Tree(3, [4]), 5]))
(1 2 (3 4) 5)
>>> vp = Tree('VP', [Tree('V', ['saw']),
...                  Tree('NP', ['him'])])
>>> s = Tree('S', [Tree('NP', ['I']), vp])
>>> print(s)
(S (NP I) (VP (V saw) (NP him)))
>>> print(s[1])
(VP (V saw) (NP him))
>>> print(s[1,1])
(NP him)
>>> t = Tree.fromstring(""(S (NP I) (VP (V saw) (NP him)))"")
>>> s == t
True
>>> t[1][1].set_label('X')
>>> t[1][1].label()
'X'
>>> print(t)
(S (NP I) (VP (V saw) (X him)))
>>> t[0], t[1,1] = t[1,1], t[0]
>>> print(t)
(S (X him) (VP (V saw) (NP I)))
",print,N/A,N/A,N/A
">>> from nltk.tree import Tree
>>> print(Tree(1, [2, Tree(3, [4]), 5]))
(1 2 (3 4) 5)
>>> vp = Tree('VP', [Tree('V', ['saw']),
...                  Tree('NP', ['him'])])
>>> s = Tree('S', [Tree('NP', ['I']), vp])
>>> print(s)
(S (NP I) (VP (V saw) (NP him)))
>>> print(s[1])
(VP (V saw) (NP him))
>>> print(s[1,1])
(NP him)
>>> t = Tree.fromstring(""(S (NP I) (VP (V saw) (NP him)))"")
>>> s == t
True
>>> t[1][1].set_label('X')
>>> t[1][1].label()
'X'
>>> print(t)
(S (NP I) (VP (V saw) (X him)))
>>> t[0], t[1,1] = t[1,1], t[0]
>>> print(t)
(S (X him) (VP (V saw) (NP I)))
",Tree,__init__,nltk\nltk\tree\tree.py,True
">>> from nltk.tree import Tree
>>> print(Tree(1, [2, Tree(3, [4]), 5]))
(1 2 (3 4) 5)
>>> vp = Tree('VP', [Tree('V', ['saw']),
...                  Tree('NP', ['him'])])
>>> s = Tree('S', [Tree('NP', ['I']), vp])
>>> print(s)
(S (NP I) (VP (V saw) (NP him)))
>>> print(s[1])
(VP (V saw) (NP him))
>>> print(s[1,1])
(NP him)
>>> t = Tree.fromstring(""(S (NP I) (VP (V saw) (NP him)))"")
>>> s == t
True
>>> t[1][1].set_label('X')
>>> t[1][1].label()
'X'
>>> print(t)
(S (NP I) (VP (V saw) (X him)))
>>> t[0], t[1,1] = t[1,1], t[0]
>>> print(t)
(S (X him) (VP (V saw) (NP I)))
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> from nltk.tree import Tree
>>> print(Tree(1, [2, Tree(3, [4]), 5]))
(1 2 (3 4) 5)
>>> vp = Tree('VP', [Tree('V', ['saw']),
...                  Tree('NP', ['him'])])
>>> s = Tree('S', [Tree('NP', ['I']), vp])
>>> print(s)
(S (NP I) (VP (V saw) (NP him)))
>>> print(s[1])
(VP (V saw) (NP him))
>>> print(s[1,1])
(NP him)
>>> t = Tree.fromstring(""(S (NP I) (VP (V saw) (NP him)))"")
>>> s == t
True
>>> t[1][1].set_label('X')
>>> t[1][1].label()
'X'
>>> print(t)
(S (NP I) (VP (V saw) (X him)))
>>> t[0], t[1,1] = t[1,1], t[0]
>>> print(t)
(S (X him) (VP (V saw) (NP I)))
",set_label,N/A,N/A,N/A
">>> from nltk.tree import Tree
>>> print(Tree(1, [2, Tree(3, [4]), 5]))
(1 2 (3 4) 5)
>>> vp = Tree('VP', [Tree('V', ['saw']),
...                  Tree('NP', ['him'])])
>>> s = Tree('S', [Tree('NP', ['I']), vp])
>>> print(s)
(S (NP I) (VP (V saw) (NP him)))
>>> print(s[1])
(VP (V saw) (NP him))
>>> print(s[1,1])
(NP him)
>>> t = Tree.fromstring(""(S (NP I) (VP (V saw) (NP him)))"")
>>> s == t
True
>>> t[1][1].set_label('X')
>>> t[1][1].label()
'X'
>>> print(t)
(S (NP I) (VP (V saw) (X him)))
>>> t[0], t[1,1] = t[1,1], t[0]
>>> print(t)
(S (X him) (VP (V saw) (NP I)))
",label,N/A,N/A,N/A
">>> len(t)
2
",len,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> print(t.flatten())
(S the dog chased the cat)
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> print(t.flatten())
(S the dog chased the cat)
",print,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> print(t.flatten())
(S the dog chased the cat)
",t.flatten,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.height()
5
>>> print(t[0,0])
(D the)
>>> t[0,0].height()
2
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.height()
5
>>> print(t[0,0])
(D the)
>>> t[0,0].height()
2
",t.height,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.height()
5
>>> print(t[0,0])
(D the)
>>> t[0,0].height()
2
",print,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.height()
5
>>> print(t[0,0])
(D the)
>>> t[0,0].height()
2
",height,N/A,N/A,N/A
">>> t = Tree.fromstring('(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))')
>>> t.label()
'S'
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring('(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))')
>>> t.label()
'S'
",t.label,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.leaves()
['the', 'dog', 'chased', 'the', 'cat']
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.leaves()
['the', 'dog', 'chased', 'the', 'cat']
",t.leaves,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.pos()
[('the', 'D'), ('dog', 'N'), ('chased', 'V'), ('the', 'D'), ('cat', 'N')]
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.pos()
[('the', 'D'), ('dog', 'N'), ('chased', 'V'), ('the', 'D'), ('cat', 'N')]
",t.pos,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.productions()
[S -> NP VP, NP -> D N, D -> 'the', N -> 'dog', VP -> V NP, V -> 'chased',
NP -> D N, D -> 'the', N -> 'cat']
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.productions()
[S -> NP VP, NP -> D N, D -> 'the', N -> 'dog', VP -> V NP, V -> 'chased',
NP -> D N, D -> 'the', N -> 'cat']
",t.productions,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.set_label(""T"")
>>> print(t)
(T (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.set_label(""T"")
>>> print(t)
(T (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))
",t.set_label,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.set_label(""T"")
>>> print(t)
(T (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))
",print,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> for s in t.subtrees(lambda t: t.height() == 2):
...     print(s)
(D the)
(N dog)
(V chased)
(D the)
(N cat)
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> for s in t.subtrees(lambda t: t.height() == 2):
...     print(s)
(D the)
(N dog)
(V chased)
(D the)
(N cat)
",t.subtrees,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> for s in t.subtrees(lambda t: t.height() == 2):
...     print(s)
(D the)
(N dog)
(V chased)
(D the)
(N cat)
",t.height,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> for s in t.subtrees(lambda t: t.height() == 2):
...     print(s)
(D the)
(N dog)
(V chased)
(D the)
(N cat)
",print,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.treepositions() 
[(), (0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0), (1,), (1, 0), (1, 0, 0), ...]
>>> for pos in t.treepositions('leaves'):
...     t[pos] = t[pos][::-1].upper()
>>> print(t)
(S (NP (D EHT) (N GOD)) (VP (V DESAHC) (NP (D EHT) (N TAC))))
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.treepositions() 
[(), (0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0), (1,), (1, 0), (1, 0, 0), ...]
>>> for pos in t.treepositions('leaves'):
...     t[pos] = t[pos][::-1].upper()
>>> print(t)
(S (NP (D EHT) (N GOD)) (VP (V DESAHC) (NP (D EHT) (N TAC))))
",t.treepositions,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.treepositions() 
[(), (0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0), (1,), (1, 0), (1, 0, 0), ...]
>>> for pos in t.treepositions('leaves'):
...     t[pos] = t[pos][::-1].upper()
>>> print(t)
(S (NP (D EHT) (N GOD)) (VP (V DESAHC) (NP (D EHT) (N TAC))))
",upper,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.treepositions() 
[(), (0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0), (1,), (1, 0), (1, 0, 0), ...]
>>> for pos in t.treepositions('leaves'):
...     t[pos] = t[pos][::-1].upper()
>>> print(t)
(S (NP (D EHT) (N GOD)) (VP (V DESAHC) (NP (D EHT) (N TAC))))
",print,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import acyclic_branches_depth_first as tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(tree(wn.synset('certified.a.01'), lambda s:s.also_sees(), cut_mark='...', depth=4))
[Synset('certified.a.01'),
 [Synset('authorized.a.01'),
  [Synset('lawful.a.01'),
   [Synset('legal.a.01'),
    ""Cycle(Synset('lawful.a.01'),0,...)"",
    [Synset('legitimate.a.01'), '...']],
   [Synset('straight.a.06'),
    [Synset('honest.a.01'), '...'],
    ""Cycle(Synset('lawful.a.01'),0,...)""]],
  [Synset('legitimate.a.01'),
   ""Cycle(Synset('authorized.a.01'),1,...)"",
   [Synset('legal.a.01'),
    [Synset('lawful.a.01'), '...'],
    ""Cycle(Synset('legitimate.a.01'),0,...)""],
   [Synset('valid.a.01'),
    ""Cycle(Synset('legitimate.a.01'),0,...)"",
    [Synset('reasonable.a.01'), '...']]],
  [Synset('official.a.01'), ""Cycle(Synset('authorized.a.01'),1,...)""]],
 [Synset('documented.a.01')]]
",pprint,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import acyclic_branches_depth_first as tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(tree(wn.synset('certified.a.01'), lambda s:s.also_sees(), cut_mark='...', depth=4))
[Synset('certified.a.01'),
 [Synset('authorized.a.01'),
  [Synset('lawful.a.01'),
   [Synset('legal.a.01'),
    ""Cycle(Synset('lawful.a.01'),0,...)"",
    [Synset('legitimate.a.01'), '...']],
   [Synset('straight.a.06'),
    [Synset('honest.a.01'), '...'],
    ""Cycle(Synset('lawful.a.01'),0,...)""]],
  [Synset('legitimate.a.01'),
   ""Cycle(Synset('authorized.a.01'),1,...)"",
   [Synset('legal.a.01'),
    [Synset('lawful.a.01'), '...'],
    ""Cycle(Synset('legitimate.a.01'),0,...)""],
   [Synset('valid.a.01'),
    ""Cycle(Synset('legitimate.a.01'),0,...)"",
    [Synset('reasonable.a.01'), '...']]],
  [Synset('official.a.01'), ""Cycle(Synset('authorized.a.01'),1,...)""]],
 [Synset('documented.a.01')]]
",tree,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import acyclic_branches_depth_first as tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(tree(wn.synset('certified.a.01'), lambda s:s.also_sees(), cut_mark='...', depth=4))
[Synset('certified.a.01'),
 [Synset('authorized.a.01'),
  [Synset('lawful.a.01'),
   [Synset('legal.a.01'),
    ""Cycle(Synset('lawful.a.01'),0,...)"",
    [Synset('legitimate.a.01'), '...']],
   [Synset('straight.a.06'),
    [Synset('honest.a.01'), '...'],
    ""Cycle(Synset('lawful.a.01'),0,...)""]],
  [Synset('legitimate.a.01'),
   ""Cycle(Synset('authorized.a.01'),1,...)"",
   [Synset('legal.a.01'),
    [Synset('lawful.a.01'), '...'],
    ""Cycle(Synset('legitimate.a.01'),0,...)""],
   [Synset('valid.a.01'),
    ""Cycle(Synset('legitimate.a.01'),0,...)"",
    [Synset('reasonable.a.01'), '...']]],
  [Synset('official.a.01'), ""Cycle(Synset('authorized.a.01'),1,...)""]],
 [Synset('documented.a.01')]]
",wn.synset,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import acyclic_branches_depth_first as tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(tree(wn.synset('certified.a.01'), lambda s:s.also_sees(), cut_mark='...', depth=4))
[Synset('certified.a.01'),
 [Synset('authorized.a.01'),
  [Synset('lawful.a.01'),
   [Synset('legal.a.01'),
    ""Cycle(Synset('lawful.a.01'),0,...)"",
    [Synset('legitimate.a.01'), '...']],
   [Synset('straight.a.06'),
    [Synset('honest.a.01'), '...'],
    ""Cycle(Synset('lawful.a.01'),0,...)""]],
  [Synset('legitimate.a.01'),
   ""Cycle(Synset('authorized.a.01'),1,...)"",
   [Synset('legal.a.01'),
    [Synset('lawful.a.01'), '...'],
    ""Cycle(Synset('legitimate.a.01'),0,...)""],
   [Synset('valid.a.01'),
    ""Cycle(Synset('legitimate.a.01'),0,...)"",
    [Synset('reasonable.a.01'), '...']]],
  [Synset('official.a.01'), ""Cycle(Synset('authorized.a.01'),1,...)""]],
 [Synset('documented.a.01')]]
",s.also_sees,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import acyclic_branches_depth_first as tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(tree(wn.synset('certified.a.01'), lambda s:s.also_sees(), cut_mark='...', depth=4))
[Synset('certified.a.01'),
 [Synset('authorized.a.01'),
  [Synset('lawful.a.01'),
   [Synset('legal.a.01'),
    ""Cycle(Synset('lawful.a.01'),0,...)"",
    [Synset('legitimate.a.01'), '...']],
   [Synset('straight.a.06'),
    [Synset('honest.a.01'), '...'],
    ""Cycle(Synset('lawful.a.01'),0,...)""]],
  [Synset('legitimate.a.01'),
   ""Cycle(Synset('authorized.a.01'),1,...)"",
   [Synset('legal.a.01'),
    [Synset('lawful.a.01'), '...'],
    ""Cycle(Synset('legitimate.a.01'),0,...)""],
   [Synset('valid.a.01'),
    ""Cycle(Synset('legitimate.a.01'),0,...)"",
    [Synset('reasonable.a.01'), '...']]],
  [Synset('official.a.01'), ""Cycle(Synset('authorized.a.01'),1,...)""]],
 [Synset('documented.a.01')]]
",Synset,__init__,nltk\nltk\corpus\reader\wordnet.py,True
">>> import nltk
>>> from nltk.util import acyclic_branches_depth_first as tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(tree(wn.synset('certified.a.01'), lambda s:s.also_sees(), cut_mark='...', depth=4))
[Synset('certified.a.01'),
 [Synset('authorized.a.01'),
  [Synset('lawful.a.01'),
   [Synset('legal.a.01'),
    ""Cycle(Synset('lawful.a.01'),0,...)"",
    [Synset('legitimate.a.01'), '...']],
   [Synset('straight.a.06'),
    [Synset('honest.a.01'), '...'],
    ""Cycle(Synset('lawful.a.01'),0,...)""]],
  [Synset('legitimate.a.01'),
   ""Cycle(Synset('authorized.a.01'),1,...)"",
   [Synset('legal.a.01'),
    [Synset('lawful.a.01'), '...'],
    ""Cycle(Synset('legitimate.a.01'),0,...)""],
   [Synset('valid.a.01'),
    ""Cycle(Synset('legitimate.a.01'),0,...)"",
    [Synset('reasonable.a.01'), '...']]],
  [Synset('official.a.01'), ""Cycle(Synset('authorized.a.01'),1,...)""]],
 [Synset('documented.a.01')]]
",Cycle,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import acyclic_depth_first as acyclic_tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(acyclic_tree(wn.synset('dog.n.01'), lambda s:s.hypernyms(),cut_mark='...'))
[Synset('dog.n.01'),
 [Synset('canine.n.02'),
  [Synset('carnivore.n.01'),
   [Synset('placental.n.01'),
    [Synset('mammal.n.01'),
     [Synset('vertebrate.n.01'),
      [Synset('chordate.n.01'),
       [Synset('animal.n.01'),
        [Synset('organism.n.01'),
         [Synset('living_thing.n.01'),
          [Synset('whole.n.02'),
           [Synset('object.n.01'),
            [Synset('physical_entity.n.01'),
             [Synset('entity.n.01')]]]]]]]]]]]]],
 [Synset('domestic_animal.n.01'), ""Cycle(Synset('animal.n.01'),-3,...)""]]
",pprint,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import acyclic_depth_first as acyclic_tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(acyclic_tree(wn.synset('dog.n.01'), lambda s:s.hypernyms(),cut_mark='...'))
[Synset('dog.n.01'),
 [Synset('canine.n.02'),
  [Synset('carnivore.n.01'),
   [Synset('placental.n.01'),
    [Synset('mammal.n.01'),
     [Synset('vertebrate.n.01'),
      [Synset('chordate.n.01'),
       [Synset('animal.n.01'),
        [Synset('organism.n.01'),
         [Synset('living_thing.n.01'),
          [Synset('whole.n.02'),
           [Synset('object.n.01'),
            [Synset('physical_entity.n.01'),
             [Synset('entity.n.01')]]]]]]]]]]]]],
 [Synset('domestic_animal.n.01'), ""Cycle(Synset('animal.n.01'),-3,...)""]]
",acyclic_tree,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import acyclic_depth_first as acyclic_tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(acyclic_tree(wn.synset('dog.n.01'), lambda s:s.hypernyms(),cut_mark='...'))
[Synset('dog.n.01'),
 [Synset('canine.n.02'),
  [Synset('carnivore.n.01'),
   [Synset('placental.n.01'),
    [Synset('mammal.n.01'),
     [Synset('vertebrate.n.01'),
      [Synset('chordate.n.01'),
       [Synset('animal.n.01'),
        [Synset('organism.n.01'),
         [Synset('living_thing.n.01'),
          [Synset('whole.n.02'),
           [Synset('object.n.01'),
            [Synset('physical_entity.n.01'),
             [Synset('entity.n.01')]]]]]]]]]]]]],
 [Synset('domestic_animal.n.01'), ""Cycle(Synset('animal.n.01'),-3,...)""]]
",wn.synset,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import acyclic_depth_first as acyclic_tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(acyclic_tree(wn.synset('dog.n.01'), lambda s:s.hypernyms(),cut_mark='...'))
[Synset('dog.n.01'),
 [Synset('canine.n.02'),
  [Synset('carnivore.n.01'),
   [Synset('placental.n.01'),
    [Synset('mammal.n.01'),
     [Synset('vertebrate.n.01'),
      [Synset('chordate.n.01'),
       [Synset('animal.n.01'),
        [Synset('organism.n.01'),
         [Synset('living_thing.n.01'),
          [Synset('whole.n.02'),
           [Synset('object.n.01'),
            [Synset('physical_entity.n.01'),
             [Synset('entity.n.01')]]]]]]]]]]]]],
 [Synset('domestic_animal.n.01'), ""Cycle(Synset('animal.n.01'),-3,...)""]]
",s.hypernyms,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import acyclic_depth_first as acyclic_tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(acyclic_tree(wn.synset('dog.n.01'), lambda s:s.hypernyms(),cut_mark='...'))
[Synset('dog.n.01'),
 [Synset('canine.n.02'),
  [Synset('carnivore.n.01'),
   [Synset('placental.n.01'),
    [Synset('mammal.n.01'),
     [Synset('vertebrate.n.01'),
      [Synset('chordate.n.01'),
       [Synset('animal.n.01'),
        [Synset('organism.n.01'),
         [Synset('living_thing.n.01'),
          [Synset('whole.n.02'),
           [Synset('object.n.01'),
            [Synset('physical_entity.n.01'),
             [Synset('entity.n.01')]]]]]]]]]]]]],
 [Synset('domestic_animal.n.01'), ""Cycle(Synset('animal.n.01'),-3,...)""]]
",Synset,__init__,nltk\nltk\corpus\reader\wordnet.py,True
">>> import nltk
>>> from nltk.util import acyclic_depth_first as acyclic_tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(acyclic_tree(wn.synset('dog.n.01'), lambda s:s.hypernyms(),cut_mark='...'))
[Synset('dog.n.01'),
 [Synset('canine.n.02'),
  [Synset('carnivore.n.01'),
   [Synset('placental.n.01'),
    [Synset('mammal.n.01'),
     [Synset('vertebrate.n.01'),
      [Synset('chordate.n.01'),
       [Synset('animal.n.01'),
        [Synset('organism.n.01'),
         [Synset('living_thing.n.01'),
          [Synset('whole.n.02'),
           [Synset('object.n.01'),
            [Synset('physical_entity.n.01'),
             [Synset('entity.n.01')]]]]]]]]]]]]],
 [Synset('domestic_animal.n.01'), ""Cycle(Synset('animal.n.01'),-3,...)""]]
",Cycle,N/A,N/A,N/A
">>> from nltk.util import bigrams
>>> list(bigrams([1,2,3,4,5]))
[(1, 2), (2, 3), (3, 4), (4, 5)]
",list,N/A,N/A,N/A
">>> from nltk.util import bigrams
>>> list(bigrams([1,2,3,4,5]))
[(1, 2), (2, 3), (3, 4), (4, 5)]
",bigrams,N/A,N/A,N/A
">>> choose(4, 2)
6
>>> choose(6, 2)
15
",choose,N/A,N/A,N/A
">>> sent = 'a b c'.split()
",split,N/A,N/A,N/A
">>> list(everygrams(sent))
[('a',), ('a', 'b'), ('a', 'b', 'c'), ('b',), ('b', 'c'), ('c',)]
",list,N/A,N/A,N/A
">>> list(everygrams(sent))
[('a',), ('a', 'b'), ('a', 'b', 'c'), ('b',), ('b', 'c'), ('c',)]
",everygrams,N/A,N/A,N/A
">>> sorted(everygrams(sent), key=len)
[('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c'), ('a', 'b', 'c')]
",sorted,N/A,N/A,N/A
">>> sorted(everygrams(sent), key=len)
[('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c'), ('a', 'b', 'c')]
",everygrams,N/A,N/A,N/A
">>> list(everygrams(sent, max_len=2))
[('a',), ('a', 'b'), ('b',), ('b', 'c'), ('c',)]
",list,N/A,N/A,N/A
">>> list(everygrams(sent, max_len=2))
[('a',), ('a', 'b'), ('b',), ('b', 'c'), ('c',)]
",everygrams,N/A,N/A,N/A
">>> from nltk.util import flatten
>>> flatten(1, 2, ['b', 'a' , ['c', 'd']], 3)
[1, 2, 'b', 'a', 'c', 'd', 3]
",flatten,N/A,N/A,N/A
"locale.setlocale(locale.LC_ALL, '')
",locale.setlocale,N/A,N/A,N/A
">>> from nltk.util import ngrams
>>> list(ngrams([1,2,3,4,5], 3))
[(1, 2, 3), (2, 3, 4), (3, 4, 5)]
",list,N/A,N/A,N/A
">>> from nltk.util import ngrams
>>> list(ngrams([1,2,3,4,5], 3))
[(1, 2, 3), (2, 3, 4), (3, 4, 5)]
",ngrams,N/A,N/A,N/A
">>> list(ngrams([1,2,3,4,5], 2, pad_right=True))
[(1, 2), (2, 3), (3, 4), (4, 5), (5, None)]
>>> list(ngrams([1,2,3,4,5], 2, pad_right=True, right_pad_symbol=''))
[(1, 2), (2, 3), (3, 4), (4, 5), (5, '')]
>>> list(ngrams([1,2,3,4,5], 2, pad_left=True, left_pad_symbol=''))
[('', 1), (1, 2), (2, 3), (3, 4), (4, 5)]
>>> list(ngrams([1,2,3,4,5], 2, pad_left=True, pad_right=True, left_pad_symbol='', right_pad_symbol=''))
[('', 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, '')]
",list,N/A,N/A,N/A
">>> list(ngrams([1,2,3,4,5], 2, pad_right=True))
[(1, 2), (2, 3), (3, 4), (4, 5), (5, None)]
>>> list(ngrams([1,2,3,4,5], 2, pad_right=True, right_pad_symbol=''))
[(1, 2), (2, 3), (3, 4), (4, 5), (5, '')]
>>> list(ngrams([1,2,3,4,5], 2, pad_left=True, left_pad_symbol=''))
[('', 1), (1, 2), (2, 3), (3, 4), (4, 5)]
>>> list(ngrams([1,2,3,4,5], 2, pad_left=True, pad_right=True, left_pad_symbol='', right_pad_symbol=''))
[('', 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, '')]
",ngrams,N/A,N/A,N/A
">>> list(pad_sequence([1,2,3,4,5], 2, pad_left=True, pad_right=True, left_pad_symbol='', right_pad_symbol=''))
['', 1, 2, 3, 4, 5, '']
>>> list(pad_sequence([1,2,3,4,5], 2, pad_left=True, left_pad_symbol=''))
['', 1, 2, 3, 4, 5]
>>> list(pad_sequence([1,2,3,4,5], 2, pad_right=True, right_pad_symbol=''))
[1, 2, 3, 4, 5, '']
",list,N/A,N/A,N/A
">>> list(pad_sequence([1,2,3,4,5], 2, pad_left=True, pad_right=True, left_pad_symbol='', right_pad_symbol=''))
['', 1, 2, 3, 4, 5, '']
>>> list(pad_sequence([1,2,3,4,5], 2, pad_left=True, left_pad_symbol=''))
['', 1, 2, 3, 4, 5]
>>> list(pad_sequence([1,2,3,4,5], 2, pad_right=True, right_pad_symbol=''))
[1, 2, 3, 4, 5, '']
",pad_sequence,N/A,N/A,N/A
">>> sent = ""Insurgents killed in ongoing fighting"".split()
>>> list(skipgrams(sent, 2, 2))
[('Insurgents', 'killed'), ('Insurgents', 'in'), ('Insurgents', 'ongoing'), ('killed', 'in'), ('killed', 'ongoing'), ('killed', 'fighting'), ('in', 'ongoing'), ('in', 'fighting'), ('ongoing', 'fighting')]
>>> list(skipgrams(sent, 3, 2))
[('Insurgents', 'killed', 'in'), ('Insurgents', 'killed', 'ongoing'), ('Insurgents', 'killed', 'fighting'), ('Insurgents', 'in', 'ongoing'), ('Insurgents', 'in', 'fighting'), ('Insurgents', 'ongoing', 'fighting'), ('killed', 'in', 'ongoing'), ('killed', 'in', 'fighting'), ('killed', 'ongoing', 'fighting'), ('in', 'ongoing', 'fighting')]
",split,N/A,N/A,N/A
">>> sent = ""Insurgents killed in ongoing fighting"".split()
>>> list(skipgrams(sent, 2, 2))
[('Insurgents', 'killed'), ('Insurgents', 'in'), ('Insurgents', 'ongoing'), ('killed', 'in'), ('killed', 'ongoing'), ('killed', 'fighting'), ('in', 'ongoing'), ('in', 'fighting'), ('ongoing', 'fighting')]
>>> list(skipgrams(sent, 3, 2))
[('Insurgents', 'killed', 'in'), ('Insurgents', 'killed', 'ongoing'), ('Insurgents', 'killed', 'fighting'), ('Insurgents', 'in', 'ongoing'), ('Insurgents', 'in', 'fighting'), ('Insurgents', 'ongoing', 'fighting'), ('killed', 'in', 'ongoing'), ('killed', 'in', 'fighting'), ('killed', 'ongoing', 'fighting'), ('in', 'ongoing', 'fighting')]
",list,N/A,N/A,N/A
">>> sent = ""Insurgents killed in ongoing fighting"".split()
>>> list(skipgrams(sent, 2, 2))
[('Insurgents', 'killed'), ('Insurgents', 'in'), ('Insurgents', 'ongoing'), ('killed', 'in'), ('killed', 'ongoing'), ('killed', 'fighting'), ('in', 'ongoing'), ('in', 'fighting'), ('ongoing', 'fighting')]
>>> list(skipgrams(sent, 3, 2))
[('Insurgents', 'killed', 'in'), ('Insurgents', 'killed', 'ongoing'), ('Insurgents', 'killed', 'fighting'), ('Insurgents', 'in', 'ongoing'), ('Insurgents', 'in', 'fighting'), ('Insurgents', 'ongoing', 'fighting'), ('killed', 'in', 'ongoing'), ('killed', 'in', 'fighting'), ('killed', 'ongoing', 'fighting'), ('in', 'ongoing', 'fighting')]
",skipgrams,N/A,N/A,N/A
">>> from nltk.util import trigrams
>>> list(trigrams([1,2,3,4,5]))
[(1, 2, 3), (2, 3, 4), (3, 4, 5)]
",list,N/A,N/A,N/A
">>> from nltk.util import trigrams
>>> list(trigrams([1,2,3,4,5]))
[(1, 2, 3), (2, 3, 4), (3, 4, 5)]
",trigrams,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import unweighted_minimum_spanning_tree as mst
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(mst(wn.synset('bound.a.01'), lambda s:s.also_sees()))
[Synset('bound.a.01'),
 [Synset('unfree.a.02'),
  [Synset('confined.a.02')],
  [Synset('dependent.a.01')],
  [Synset('restricted.a.01'), [Synset('classified.a.02')]]]]
",pprint,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import unweighted_minimum_spanning_tree as mst
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(mst(wn.synset('bound.a.01'), lambda s:s.also_sees()))
[Synset('bound.a.01'),
 [Synset('unfree.a.02'),
  [Synset('confined.a.02')],
  [Synset('dependent.a.01')],
  [Synset('restricted.a.01'), [Synset('classified.a.02')]]]]
",mst,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import unweighted_minimum_spanning_tree as mst
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(mst(wn.synset('bound.a.01'), lambda s:s.also_sees()))
[Synset('bound.a.01'),
 [Synset('unfree.a.02'),
  [Synset('confined.a.02')],
  [Synset('dependent.a.01')],
  [Synset('restricted.a.01'), [Synset('classified.a.02')]]]]
",wn.synset,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import unweighted_minimum_spanning_tree as mst
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(mst(wn.synset('bound.a.01'), lambda s:s.also_sees()))
[Synset('bound.a.01'),
 [Synset('unfree.a.02'),
  [Synset('confined.a.02')],
  [Synset('dependent.a.01')],
  [Synset('restricted.a.01'), [Synset('classified.a.02')]]]]
",s.also_sees,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import unweighted_minimum_spanning_tree as mst
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(mst(wn.synset('bound.a.01'), lambda s:s.also_sees()))
[Synset('bound.a.01'),
 [Synset('unfree.a.02'),
  [Synset('confined.a.02')],
  [Synset('dependent.a.01')],
  [Synset('restricted.a.01'), [Synset('classified.a.02')]]]]
",Synset,__init__,nltk\nltk\corpus\reader\wordnet.py,True
">>> lesk(['I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.'], 'bank', 'n')
Synset('savings_bank.n.02')
",lesk,N/A,N/A,N/A
">>> lesk(['I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.'], 'bank', 'n')
Synset('savings_bank.n.02')
",Synset,__init__,nltk\nltk\corpus\reader\wordnet.py,True
load(),load,N/A,N/A,N/A
retrieve(),retrieve,N/A,N/A,N/A
find(),find,N/A,N/A,N/A
load(),load,N/A,N/A,N/A
write(),write,N/A,N/A,N/A
writestr(),writestr,N/A,N/A,N/A
seek(),seek,N/A,N/A,N/A
tell(),tell,N/A,N/A,N/A
seek(),seek,N/A,N/A,N/A
tell(),tell,N/A,N/A,N/A
seek(),seek,N/A,N/A,N/A
tell(),tell,N/A,N/A,N/A
readline(),readline,N/A,N/A,N/A
read(),read,N/A,N/A,N/A
readline(),readline,N/A,N/A,N/A
tell(),tell,N/A,N/A,N/A
download(),download,N/A,N/A,N/A
Downloader.default_download_dir(),Downloader.default_download_dir,Downloader.default_download_dir,nltk\nltk\downloader.py,True
download(),download,N/A,N/A,N/A
default_download_dir(),default_download_dir,N/A,N/A,N/A
download(),download,N/A,N/A,N/A
_package_to_columns(),_package_to_columns,N/A,N/A,N/A
unify(),unify,N/A,N/A,N/A
walk(),walk,N/A,N/A,N/A
cyclic(),cyclic,N/A,N/A,N/A
freeze(),freeze,N/A,N/A,N/A
equal_values(),equal_values,N/A,N/A,N/A
freeze(),freeze,N/A,N/A,N/A
copy(),copy,N/A,N/A,N/A
nltk.featstruct.rename_variables(),featstruct.rename_variables,N/A,N/A,N/A
nltk.featstruct.retract_bindings(),featstruct.retract_bindings,N/A,N/A,N/A
nltk.featstruct.substitute_bindings(),featstruct.substitute_bindings,N/A,N/A,N/A
nltk.featstruct.find_variables(),featstruct.find_variables,N/A,N/A,N/A
FreqDist.N(),FreqDist.N,FreqDist.N,nltk\nltk\probability.py,True
FreqDist.B(),FreqDist.B,FreqDist.B,nltk\nltk\probability.py,True
bins-self.B(),self.B,N/A,N/A,N/A
self.B(),self.B,N/A,N/A,N/A
Counter.setdefault(),Counter.setdefault,N/A,N/A,N/A
Counter.update(),Counter.update,N/A,N/A,N/A
self.prob(samp),self.prob,N/A,N/A,N/A
log(p),log,N/A,N/A,N/A
log(2**(logx)+2**(logy)),log,N/A,N/A,N/A
findall(),findall,N/A,N/A,N/A
fields(),fields,N/A,N/A,N/A
decode(),decode,N/A,N/A,N/A
StandardFormat.fields(),StandardFormat.fields,StandardFormat.fields,nltk\nltk\toolbox.py,True
encode(),encode,N/A,N/A,N/A
log(p),log,N/A,N/A,N/A
"Tree(label, children)",Tree,__init__,nltk\nltk\tree\tree.py,True
Tree.fromstring(s),Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,True
tp=self.leaf_treeposition(i),self.leaf_treeposition,N/A,N/A,N/A
self[tp]==self.leaves()[i],self.leaves,N/A,N/A,N/A
self.leaves()[start:end],self.leaves,N/A,N/A,N/A
ptree.parent()[ptree.parent_index()] is ptree,ptree.parent,N/A,N/A,N/A
ptree.parent()[ptree.parent_index()] is ptree,ptree.parent_index,N/A,N/A,N/A
ptree.parent_index(),ptree.parent_index,N/A,N/A,N/A
ptree.parent.index(ptree),parent.index,N/A,N/A,N/A
index(),index,N/A,N/A,N/A
ptree.parent(),ptree.parent,N/A,N/A,N/A
parents(),parents,N/A,N/A,N/A
parent_indices(),parent_indices,N/A,N/A,N/A
left_siblings(),left_siblings,N/A,N/A,N/A
right_siblings(),right_siblings,N/A,N/A,N/A
parents(),parents,N/A,N/A,N/A
parent_indices(),parent_indices,N/A,N/A,N/A
">>> download('treebank') 
[nltk_data] Downloading package 'treebank'...
[nltk_data]   Unzipping corpora/treebank.zip.
",download,N/A,N/A,N/A
">>> download('all-corpora') 
[nltk_data] Downloading package 'abc'...
[nltk_data]   Unzipping corpora/abc.zip.
[nltk_data] Downloading package 'alpino'...
[nltk_data]   Unzipping corpora/alpino.zip.
  ...
[nltk_data] Downloading package 'words'...
[nltk_data]   Unzipping corpora/words.zip.
",download,N/A,N/A,N/A
">>> from nltk.featstruct import unify
>>> unify(dict(x=1, y=dict()), dict(a='a', y=dict(b='b')))  
{'y': {'b': 'b'}, 'x': 1, 'a': 'a'}
",unify,N/A,N/A,N/A
">>> from nltk.featstruct import unify
>>> unify(dict(x=1, y=dict()), dict(a='a', y=dict(b='b')))  
{'y': {'b': 'b'}, 'x': 1, 'a': 'a'}
",dict,N/A,N/A,N/A
">>> from nltk.featstruct import FeatStruct
>>> FeatStruct('[a=?x]').unify(FeatStruct('[b=?x]'))
[a=?x, b=?x2]
",FeatStruct,__init__,nltk\nltk\featstruct.py,True
">>> from nltk.featstruct import FeatStruct
>>> FeatStruct('[a=?x]').unify(FeatStruct('[b=?x]'))
[a=?x, b=?x2]
",unify,N/A,N/A,N/A
"def handler(s, position, reentrances, match): ...
",handler,N/A,N/A,N/A
">>> from nltk.probability import ConditionalFreqDist
>>> from nltk.tokenize import word_tokenize
>>> sent = ""the the the dog dog some other words that we do not care about""
>>> cfdist = ConditionalFreqDist()
>>> for word in word_tokenize(sent):
...     condition = len(word)
...     cfdist[condition][word] += 1
",ConditionalFreqDist,__init__,nltk\nltk\probability.py,True
">>> from nltk.probability import ConditionalFreqDist
>>> from nltk.tokenize import word_tokenize
>>> sent = ""the the the dog dog some other words that we do not care about""
>>> cfdist = ConditionalFreqDist()
>>> for word in word_tokenize(sent):
...     condition = len(word)
...     cfdist[condition][word] += 1
",word_tokenize,N/A,N/A,N/A
">>> from nltk.probability import ConditionalFreqDist
>>> from nltk.tokenize import word_tokenize
>>> sent = ""the the the dog dog some other words that we do not care about""
>>> cfdist = ConditionalFreqDist()
>>> for word in word_tokenize(sent):
...     condition = len(word)
...     cfdist[condition][word] += 1
",len,N/A,N/A,N/A
">>> cfdist = ConditionalFreqDist((len(word), word) for word in word_tokenize(sent))
",ConditionalFreqDist,__init__,nltk\nltk\probability.py,True
">>> cfdist = ConditionalFreqDist((len(word), word) for word in word_tokenize(sent))
",len,N/A,N/A,N/A
">>> cfdist = ConditionalFreqDist((len(word), word) for word in word_tokenize(sent))
",word_tokenize,N/A,N/A,N/A
">>> cfdist[3]
FreqDist({'the': 3, 'dog': 2, 'not': 1})
>>> cfdist[3].freq('the')
0.5
>>> cfdist[3]['dog']
2
",FreqDist,__init__,nltk\nltk\probability.py,True
">>> cfdist[3]
FreqDist({'the': 3, 'dog': 2, 'not': 1})
>>> cfdist[3].freq('the')
0.5
>>> cfdist[3]['dog']
2
",freq,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.probability import ConditionalProbDist, ELEProbDist
>>> cfdist = ConditionalFreqDist(brown.tagged_words()[:5000])
>>> cpdist = ConditionalProbDist(cfdist, ELEProbDist, 10)
>>> cpdist['passed'].max()
'VBD'
>>> cpdist['passed'].prob('VBD')
0.423...
",ConditionalFreqDist,__init__,nltk\nltk\probability.py,True
">>> from nltk.corpus import brown
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.probability import ConditionalProbDist, ELEProbDist
>>> cfdist = ConditionalFreqDist(brown.tagged_words()[:5000])
>>> cpdist = ConditionalProbDist(cfdist, ELEProbDist, 10)
>>> cpdist['passed'].max()
'VBD'
>>> cpdist['passed'].prob('VBD')
0.423...
",brown.tagged_words,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.probability import ConditionalProbDist, ELEProbDist
>>> cfdist = ConditionalFreqDist(brown.tagged_words()[:5000])
>>> cpdist = ConditionalProbDist(cfdist, ELEProbDist, 10)
>>> cpdist['passed'].max()
'VBD'
>>> cpdist['passed'].prob('VBD')
0.423...
",ConditionalProbDist,__init__,nltk\nltk\probability.py,True
">>> from nltk.corpus import brown
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.probability import ConditionalProbDist, ELEProbDist
>>> cfdist = ConditionalFreqDist(brown.tagged_words()[:5000])
>>> cpdist = ConditionalProbDist(cfdist, ELEProbDist, 10)
>>> cpdist['passed'].max()
'VBD'
>>> cpdist['passed'].prob('VBD')
0.423...
",max,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.probability import ConditionalProbDist, ELEProbDist
>>> cfdist = ConditionalFreqDist(brown.tagged_words()[:5000])
>>> cpdist = ConditionalProbDist(cfdist, ELEProbDist, 10)
>>> cpdist['passed'].max()
'VBD'
>>> cpdist['passed'].prob('VBD')
0.423...
",prob,N/A,N/A,N/A
">>> from nltk.tokenize import word_tokenize
>>> from nltk.probability import FreqDist
>>> sent = 'This is an example sentence'
>>> fdist = FreqDist()
>>> for word in word_tokenize(sent):
...    fdist[word.lower()] += 1
",FreqDist,__init__,nltk\nltk\probability.py,True
">>> from nltk.tokenize import word_tokenize
>>> from nltk.probability import FreqDist
>>> sent = 'This is an example sentence'
>>> fdist = FreqDist()
>>> for word in word_tokenize(sent):
...    fdist[word.lower()] += 1
",word_tokenize,N/A,N/A,N/A
">>> from nltk.tokenize import word_tokenize
>>> from nltk.probability import FreqDist
>>> sent = 'This is an example sentence'
>>> fdist = FreqDist()
>>> for word in word_tokenize(sent):
...    fdist[word.lower()] += 1
",word.lower,N/A,N/A,N/A
">>> fdist = FreqDist(word.lower() for word in word_tokenize(sent))
",FreqDist,__init__,nltk\nltk\probability.py,True
">>> fdist = FreqDist(word.lower() for word in word_tokenize(sent))
",word.lower,N/A,N/A,N/A
">>> fdist = FreqDist(word.lower() for word in word_tokenize(sent))
",word_tokenize,N/A,N/A,N/A
">>> from nltk.probability import ProbabilisticMixIn
>>> class A:
...     def __init__(self, x, y): self.data = (x,y)
...
>>> class ProbabilisticA(A, ProbabilisticMixIn):
...     def __init__(self, x, y, **prob_kwarg):
...         A.__init__(self, x, y)
...         ProbabilisticMixIn.__init__(self, **prob_kwarg)
",__init__,N/A,N/A,N/A
">>> from nltk.probability import ProbabilisticMixIn
>>> class A:
...     def __init__(self, x, y): self.data = (x,y)
...
>>> class ProbabilisticA(A, ProbabilisticMixIn):
...     def __init__(self, x, y, **prob_kwarg):
...         A.__init__(self, x, y)
...         ProbabilisticMixIn.__init__(self, **prob_kwarg)
",ProbabilisticA,N/A,N/A,N/A
">>> from nltk.probability import ProbabilisticMixIn
>>> class A:
...     def __init__(self, x, y): self.data = (x,y)
...
>>> class ProbabilisticA(A, ProbabilisticMixIn):
...     def __init__(self, x, y, **prob_kwarg):
...         A.__init__(self, x, y)
...         ProbabilisticMixIn.__init__(self, **prob_kwarg)
",A.__init__,N/A,N/A,N/A
">>> from nltk.text import TokenSearcher
>>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",print,N/A,N/A,N/A
">>> from nltk.text import TokenSearcher
>>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",text5.findall,N/A,N/A,N/A
">>> from nltk.text import TokenSearcher
>>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",text1.findall,N/A,N/A,N/A
">>> from nltk.text import TokenSearcher
>>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",text9.findall,N/A,N/A,N/A
">>> import nltk.corpus
>>> from nltk.text import Text
>>> moby = Text(nltk.corpus.gutenberg.words('melville-moby_dick.txt'))
",Text,__init__,nltk\nltk\text.py,True
">>> import nltk.corpus
>>> from nltk.text import Text
>>> moby = Text(nltk.corpus.gutenberg.words('melville-moby_dick.txt'))
",gutenberg.words,N/A,N/A,N/A
">>> from nltk.book import text4
>>> text4.collocation_list()[:2]
[('United', 'States'), ('fellow', 'citizens')]
",text4.collocation_list,N/A,N/A,N/A
">>> from nltk.book import text4
>>> text4.collocations() 
United States; fellow citizens; four years; ...
",text4.collocations,N/A,N/A,N/A
">>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",print,N/A,N/A,N/A
">>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",text5.findall,N/A,N/A,N/A
">>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",text1.findall,N/A,N/A,N/A
">>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",text9.findall,N/A,N/A,N/A
">>> import nltk.corpus
>>> from nltk.text import TextCollection
>>> print('hack'); from nltk.book import text1, text2, text3
hack...
>>> gutenberg = TextCollection(nltk.corpus.gutenberg)
>>> mytexts = TextCollection([text1, text2, text3])
",print,N/A,N/A,N/A
">>> import nltk.corpus
>>> from nltk.text import TextCollection
>>> print('hack'); from nltk.book import text1, text2, text3
hack...
>>> gutenberg = TextCollection(nltk.corpus.gutenberg)
>>> mytexts = TextCollection([text1, text2, text3])
",TextCollection,__init__,nltk\nltk\text.py,True
">>> from nltk.probability import ProbabilisticMixIn
>>> class A:
...     def __init__(self, x, y): self.data = (x,y)
...
>>> class ProbabilisticA(A, ProbabilisticMixIn):
...     def __init__(self, x, y, **prob_kwarg):
...         A.__init__(self, x, y)
...         ProbabilisticMixIn.__init__(self, **prob_kwarg)
",__init__,N/A,N/A,N/A
">>> from nltk.probability import ProbabilisticMixIn
>>> class A:
...     def __init__(self, x, y): self.data = (x,y)
...
>>> class ProbabilisticA(A, ProbabilisticMixIn):
...     def __init__(self, x, y, **prob_kwarg):
...         A.__init__(self, x, y)
...         ProbabilisticMixIn.__init__(self, **prob_kwarg)
",ProbabilisticA,N/A,N/A,N/A
">>> from nltk.probability import ProbabilisticMixIn
>>> class A:
...     def __init__(self, x, y): self.data = (x,y)
...
>>> class ProbabilisticA(A, ProbabilisticMixIn):
...     def __init__(self, x, y, **prob_kwarg):
...         A.__init__(self, x, y)
...         ProbabilisticMixIn.__init__(self, **prob_kwarg)
",A.__init__,N/A,N/A,N/A
">>> from nltk.tree import Tree
>>> print(Tree(1, [2, Tree(3, [4]), 5]))
(1 2 (3 4) 5)
>>> vp = Tree('VP', [Tree('V', ['saw']),
...                  Tree('NP', ['him'])])
>>> s = Tree('S', [Tree('NP', ['I']), vp])
>>> print(s)
(S (NP I) (VP (V saw) (NP him)))
>>> print(s[1])
(VP (V saw) (NP him))
>>> print(s[1,1])
(NP him)
>>> t = Tree.fromstring(""(S (NP I) (VP (V saw) (NP him)))"")
>>> s == t
True
>>> t[1][1].set_label('X')
>>> t[1][1].label()
'X'
>>> print(t)
(S (NP I) (VP (V saw) (X him)))
>>> t[0], t[1,1] = t[1,1], t[0]
>>> print(t)
(S (X him) (VP (V saw) (NP I)))
",print,N/A,N/A,N/A
">>> from nltk.tree import Tree
>>> print(Tree(1, [2, Tree(3, [4]), 5]))
(1 2 (3 4) 5)
>>> vp = Tree('VP', [Tree('V', ['saw']),
...                  Tree('NP', ['him'])])
>>> s = Tree('S', [Tree('NP', ['I']), vp])
>>> print(s)
(S (NP I) (VP (V saw) (NP him)))
>>> print(s[1])
(VP (V saw) (NP him))
>>> print(s[1,1])
(NP him)
>>> t = Tree.fromstring(""(S (NP I) (VP (V saw) (NP him)))"")
>>> s == t
True
>>> t[1][1].set_label('X')
>>> t[1][1].label()
'X'
>>> print(t)
(S (NP I) (VP (V saw) (X him)))
>>> t[0], t[1,1] = t[1,1], t[0]
>>> print(t)
(S (X him) (VP (V saw) (NP I)))
",Tree,__init__,nltk\nltk\tree\tree.py,True
">>> from nltk.tree import Tree
>>> print(Tree(1, [2, Tree(3, [4]), 5]))
(1 2 (3 4) 5)
>>> vp = Tree('VP', [Tree('V', ['saw']),
...                  Tree('NP', ['him'])])
>>> s = Tree('S', [Tree('NP', ['I']), vp])
>>> print(s)
(S (NP I) (VP (V saw) (NP him)))
>>> print(s[1])
(VP (V saw) (NP him))
>>> print(s[1,1])
(NP him)
>>> t = Tree.fromstring(""(S (NP I) (VP (V saw) (NP him)))"")
>>> s == t
True
>>> t[1][1].set_label('X')
>>> t[1][1].label()
'X'
>>> print(t)
(S (NP I) (VP (V saw) (X him)))
>>> t[0], t[1,1] = t[1,1], t[0]
>>> print(t)
(S (X him) (VP (V saw) (NP I)))
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> from nltk.tree import Tree
>>> print(Tree(1, [2, Tree(3, [4]), 5]))
(1 2 (3 4) 5)
>>> vp = Tree('VP', [Tree('V', ['saw']),
...                  Tree('NP', ['him'])])
>>> s = Tree('S', [Tree('NP', ['I']), vp])
>>> print(s)
(S (NP I) (VP (V saw) (NP him)))
>>> print(s[1])
(VP (V saw) (NP him))
>>> print(s[1,1])
(NP him)
>>> t = Tree.fromstring(""(S (NP I) (VP (V saw) (NP him)))"")
>>> s == t
True
>>> t[1][1].set_label('X')
>>> t[1][1].label()
'X'
>>> print(t)
(S (NP I) (VP (V saw) (X him)))
>>> t[0], t[1,1] = t[1,1], t[0]
>>> print(t)
(S (X him) (VP (V saw) (NP I)))
",set_label,N/A,N/A,N/A
">>> from nltk.tree import Tree
>>> print(Tree(1, [2, Tree(3, [4]), 5]))
(1 2 (3 4) 5)
>>> vp = Tree('VP', [Tree('V', ['saw']),
...                  Tree('NP', ['him'])])
>>> s = Tree('S', [Tree('NP', ['I']), vp])
>>> print(s)
(S (NP I) (VP (V saw) (NP him)))
>>> print(s[1])
(VP (V saw) (NP him))
>>> print(s[1,1])
(NP him)
>>> t = Tree.fromstring(""(S (NP I) (VP (V saw) (NP him)))"")
>>> s == t
True
>>> t[1][1].set_label('X')
>>> t[1][1].label()
'X'
>>> print(t)
(S (NP I) (VP (V saw) (X him)))
>>> t[0], t[1,1] = t[1,1], t[0]
>>> print(t)
(S (X him) (VP (V saw) (NP I)))
",label,N/A,N/A,N/A
">>> len(t)
2
",len,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> print(t.flatten())
(S the dog chased the cat)
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> print(t.flatten())
(S the dog chased the cat)
",print,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> print(t.flatten())
(S the dog chased the cat)
",t.flatten,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.height()
5
>>> print(t[0,0])
(D the)
>>> t[0,0].height()
2
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.height()
5
>>> print(t[0,0])
(D the)
>>> t[0,0].height()
2
",t.height,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.height()
5
>>> print(t[0,0])
(D the)
>>> t[0,0].height()
2
",print,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.height()
5
>>> print(t[0,0])
(D the)
>>> t[0,0].height()
2
",height,N/A,N/A,N/A
">>> t = Tree.fromstring('(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))')
>>> t.label()
'S'
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring('(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))')
>>> t.label()
'S'
",t.label,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.leaves()
['the', 'dog', 'chased', 'the', 'cat']
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.leaves()
['the', 'dog', 'chased', 'the', 'cat']
",t.leaves,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.pos()
[('the', 'D'), ('dog', 'N'), ('chased', 'V'), ('the', 'D'), ('cat', 'N')]
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.pos()
[('the', 'D'), ('dog', 'N'), ('chased', 'V'), ('the', 'D'), ('cat', 'N')]
",t.pos,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.productions()
[S -> NP VP, NP -> D N, D -> 'the', N -> 'dog', VP -> V NP, V -> 'chased',
NP -> D N, D -> 'the', N -> 'cat']
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.productions()
[S -> NP VP, NP -> D N, D -> 'the', N -> 'dog', VP -> V NP, V -> 'chased',
NP -> D N, D -> 'the', N -> 'cat']
",t.productions,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.set_label(""T"")
>>> print(t)
(T (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.set_label(""T"")
>>> print(t)
(T (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))
",t.set_label,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.set_label(""T"")
>>> print(t)
(T (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))
",print,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> for s in t.subtrees(lambda t: t.height() == 2):
...     print(s)
(D the)
(N dog)
(V chased)
(D the)
(N cat)
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> for s in t.subtrees(lambda t: t.height() == 2):
...     print(s)
(D the)
(N dog)
(V chased)
(D the)
(N cat)
",t.subtrees,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> for s in t.subtrees(lambda t: t.height() == 2):
...     print(s)
(D the)
(N dog)
(V chased)
(D the)
(N cat)
",t.height,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> for s in t.subtrees(lambda t: t.height() == 2):
...     print(s)
(D the)
(N dog)
(V chased)
(D the)
(N cat)
",print,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.treepositions() 
[(), (0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0), (1,), (1, 0), (1, 0, 0), ...]
>>> for pos in t.treepositions('leaves'):
...     t[pos] = t[pos][::-1].upper()
>>> print(t)
(S (NP (D EHT) (N GOD)) (VP (V DESAHC) (NP (D EHT) (N TAC))))
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.treepositions() 
[(), (0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0), (1,), (1, 0), (1, 0, 0), ...]
>>> for pos in t.treepositions('leaves'):
...     t[pos] = t[pos][::-1].upper()
>>> print(t)
(S (NP (D EHT) (N GOD)) (VP (V DESAHC) (NP (D EHT) (N TAC))))
",t.treepositions,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.treepositions() 
[(), (0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0), (1,), (1, 0), (1, 0, 0), ...]
>>> for pos in t.treepositions('leaves'):
...     t[pos] = t[pos][::-1].upper()
>>> print(t)
(S (NP (D EHT) (N GOD)) (VP (V DESAHC) (NP (D EHT) (N TAC))))
",upper,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.treepositions() 
[(), (0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0), (1,), (1, 0), (1, 0, 0), ...]
>>> for pos in t.treepositions('leaves'):
...     t[pos] = t[pos][::-1].upper()
>>> print(t)
(S (NP (D EHT) (N GOD)) (VP (V DESAHC) (NP (D EHT) (N TAC))))
",print,N/A,N/A,N/A
"for parent_index in ptree.parent_indices(parent):
    parent[parent_index] is ptree
",ptree.parent_indices,N/A,N/A,N/A
"for treepos in ptree.treepositions(root):
    root[treepos] is ptree
",ptree.treepositions,N/A,N/A,N/A
">>> from nltk.util import bigrams
>>> list(bigrams([1,2,3,4,5]))
[(1, 2), (2, 3), (3, 4), (4, 5)]
",list,N/A,N/A,N/A
">>> from nltk.util import bigrams
>>> list(bigrams([1,2,3,4,5]))
[(1, 2), (2, 3), (3, 4), (4, 5)]
",bigrams,N/A,N/A,N/A
">>> choose(4, 2)
6
>>> choose(6, 2)
15
",choose,N/A,N/A,N/A
">>> sent = 'a b c'.split()
>>> list(everygrams(sent))
[('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c'), ('a', 'b', 'c')]
>>> list(everygrams(sent, max_len=2))
[('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c')]
",split,N/A,N/A,N/A
">>> sent = 'a b c'.split()
>>> list(everygrams(sent))
[('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c'), ('a', 'b', 'c')]
>>> list(everygrams(sent, max_len=2))
[('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c')]
",list,N/A,N/A,N/A
">>> sent = 'a b c'.split()
>>> list(everygrams(sent))
[('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c'), ('a', 'b', 'c')]
>>> list(everygrams(sent, max_len=2))
[('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c')]
",everygrams,N/A,N/A,N/A
">>> from nltk.util import flatten
>>> flatten(1, 2, ['b', 'a' , ['c', 'd']], 3)
[1, 2, 'b', 'a', 'c', 'd', 3]
",flatten,N/A,N/A,N/A
"locale.setlocale(locale.LC_ALL, '')
",locale.setlocale,N/A,N/A,N/A
">>> from nltk.util import ngrams
>>> list(ngrams([1,2,3,4,5], 3))
[(1, 2, 3), (2, 3, 4), (3, 4, 5)]
",list,N/A,N/A,N/A
">>> from nltk.util import ngrams
>>> list(ngrams([1,2,3,4,5], 3))
[(1, 2, 3), (2, 3, 4), (3, 4, 5)]
",ngrams,N/A,N/A,N/A
">>> list(ngrams([1,2,3,4,5], 2, pad_right=True))
[(1, 2), (2, 3), (3, 4), (4, 5), (5, None)]
>>> list(ngrams([1,2,3,4,5], 2, pad_right=True, right_pad_symbol=''))
[(1, 2), (2, 3), (3, 4), (4, 5), (5, '')]
>>> list(ngrams([1,2,3,4,5], 2, pad_left=True, left_pad_symbol=''))
[('', 1), (1, 2), (2, 3), (3, 4), (4, 5)]
>>> list(ngrams([1,2,3,4,5], 2, pad_left=True, pad_right=True, left_pad_symbol='', right_pad_symbol=''))
[('', 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, '')]
",list,N/A,N/A,N/A
">>> list(ngrams([1,2,3,4,5], 2, pad_right=True))
[(1, 2), (2, 3), (3, 4), (4, 5), (5, None)]
>>> list(ngrams([1,2,3,4,5], 2, pad_right=True, right_pad_symbol=''))
[(1, 2), (2, 3), (3, 4), (4, 5), (5, '')]
>>> list(ngrams([1,2,3,4,5], 2, pad_left=True, left_pad_symbol=''))
[('', 1), (1, 2), (2, 3), (3, 4), (4, 5)]
>>> list(ngrams([1,2,3,4,5], 2, pad_left=True, pad_right=True, left_pad_symbol='', right_pad_symbol=''))
[('', 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, '')]
",ngrams,N/A,N/A,N/A
">>> list(pad_sequence([1,2,3,4,5], 2, pad_left=True, pad_right=True, left_pad_symbol='', right_pad_symbol=''))
['', 1, 2, 3, 4, 5, '']
>>> list(pad_sequence([1,2,3,4,5], 2, pad_left=True, left_pad_symbol=''))
['', 1, 2, 3, 4, 5]
>>> list(pad_sequence([1,2,3,4,5], 2, pad_right=True, right_pad_symbol=''))
[1, 2, 3, 4, 5, '']
",list,N/A,N/A,N/A
">>> list(pad_sequence([1,2,3,4,5], 2, pad_left=True, pad_right=True, left_pad_symbol='', right_pad_symbol=''))
['', 1, 2, 3, 4, 5, '']
>>> list(pad_sequence([1,2,3,4,5], 2, pad_left=True, left_pad_symbol=''))
['', 1, 2, 3, 4, 5]
>>> list(pad_sequence([1,2,3,4,5], 2, pad_right=True, right_pad_symbol=''))
[1, 2, 3, 4, 5, '']
",pad_sequence,N/A,N/A,N/A
">>> sent = ""Insurgents killed in ongoing fighting"".split()
>>> list(skipgrams(sent, 2, 2))
[('Insurgents', 'killed'), ('Insurgents', 'in'), ('Insurgents', 'ongoing'), ('killed', 'in'), ('killed', 'ongoing'), ('killed', 'fighting'), ('in', 'ongoing'), ('in', 'fighting'), ('ongoing', 'fighting')]
>>> list(skipgrams(sent, 3, 2))
[('Insurgents', 'killed', 'in'), ('Insurgents', 'killed', 'ongoing'), ('Insurgents', 'killed', 'fighting'), ('Insurgents', 'in', 'ongoing'), ('Insurgents', 'in', 'fighting'), ('Insurgents', 'ongoing', 'fighting'), ('killed', 'in', 'ongoing'), ('killed', 'in', 'fighting'), ('killed', 'ongoing', 'fighting'), ('in', 'ongoing', 'fighting')]
",split,N/A,N/A,N/A
">>> sent = ""Insurgents killed in ongoing fighting"".split()
>>> list(skipgrams(sent, 2, 2))
[('Insurgents', 'killed'), ('Insurgents', 'in'), ('Insurgents', 'ongoing'), ('killed', 'in'), ('killed', 'ongoing'), ('killed', 'fighting'), ('in', 'ongoing'), ('in', 'fighting'), ('ongoing', 'fighting')]
>>> list(skipgrams(sent, 3, 2))
[('Insurgents', 'killed', 'in'), ('Insurgents', 'killed', 'ongoing'), ('Insurgents', 'killed', 'fighting'), ('Insurgents', 'in', 'ongoing'), ('Insurgents', 'in', 'fighting'), ('Insurgents', 'ongoing', 'fighting'), ('killed', 'in', 'ongoing'), ('killed', 'in', 'fighting'), ('killed', 'ongoing', 'fighting'), ('in', 'ongoing', 'fighting')]
",list,N/A,N/A,N/A
">>> sent = ""Insurgents killed in ongoing fighting"".split()
>>> list(skipgrams(sent, 2, 2))
[('Insurgents', 'killed'), ('Insurgents', 'in'), ('Insurgents', 'ongoing'), ('killed', 'in'), ('killed', 'ongoing'), ('killed', 'fighting'), ('in', 'ongoing'), ('in', 'fighting'), ('ongoing', 'fighting')]
>>> list(skipgrams(sent, 3, 2))
[('Insurgents', 'killed', 'in'), ('Insurgents', 'killed', 'ongoing'), ('Insurgents', 'killed', 'fighting'), ('Insurgents', 'in', 'ongoing'), ('Insurgents', 'in', 'fighting'), ('Insurgents', 'ongoing', 'fighting'), ('killed', 'in', 'ongoing'), ('killed', 'in', 'fighting'), ('killed', 'ongoing', 'fighting'), ('in', 'ongoing', 'fighting')]
",skipgrams,N/A,N/A,N/A
">>> from nltk.util import trigrams
>>> list(trigrams([1,2,3,4,5]))
[(1, 2, 3), (2, 3, 4), (3, 4, 5)]
",list,N/A,N/A,N/A
">>> from nltk.util import trigrams
>>> list(trigrams([1,2,3,4,5]))
[(1, 2, 3), (2, 3, 4), (3, 4, 5)]
",trigrams,N/A,N/A,N/A
">>> lesk(['I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.'], 'bank', 'n')
Synset('savings_bank.n.02')
",lesk,N/A,N/A,N/A
">>> lesk(['I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.'], 'bank', 'n')
Synset('savings_bank.n.02')
",Synset,__init__,nltk\nltk\corpus\reader\wordnet.py,True
load(),load,N/A,N/A,N/A
retrieve(),retrieve,N/A,N/A,N/A
write(),write,N/A,N/A,N/A
writestr(),writestr,N/A,N/A,N/A
seek(),seek,N/A,N/A,N/A
tell(),tell,N/A,N/A,N/A
seek(),seek,N/A,N/A,N/A
tell(),tell,N/A,N/A,N/A
seek(),seek,N/A,N/A,N/A
tell(),tell,N/A,N/A,N/A
readline(),readline,N/A,N/A,N/A
read(),read,N/A,N/A,N/A
readline(),readline,N/A,N/A,N/A
tell(),tell,N/A,N/A,N/A
find(),find,N/A,N/A,N/A
load(),load,N/A,N/A,N/A
download(),download,N/A,N/A,N/A
Downloader.default_download_dir(),Downloader.default_download_dir,Downloader.default_download_dir,nltk\nltk\downloader.py,True
download(),download,N/A,N/A,N/A
default_download_dir(),default_download_dir,N/A,N/A,N/A
download(),download,N/A,N/A,N/A
_package_to_columns(),_package_to_columns,N/A,N/A,N/A
unify(),unify,N/A,N/A,N/A
walk(),walk,N/A,N/A,N/A
cyclic(),cyclic,N/A,N/A,N/A
freeze(),freeze,N/A,N/A,N/A
equal_values(),equal_values,N/A,N/A,N/A
freeze(),freeze,N/A,N/A,N/A
copy(),copy,N/A,N/A,N/A
nltk.featstruct.rename_variables(),featstruct.rename_variables,N/A,N/A,N/A
nltk.featstruct.retract_bindings(),featstruct.retract_bindings,N/A,N/A,N/A
nltk.featstruct.substitute_bindings(),featstruct.substitute_bindings,N/A,N/A,N/A
nltk.featstruct.find_variables(),featstruct.find_variables,N/A,N/A,N/A
FreqDist.N(),FreqDist.N,FreqDist.N,nltk\nltk\probability.py,True
FreqDist.B(),FreqDist.B,FreqDist.B,nltk\nltk\probability.py,True
bins-self.B(),self.B,N/A,N/A,N/A
self.B(),self.B,N/A,N/A,N/A
Counter.setdefault(),Counter.setdefault,N/A,N/A,N/A
Counter.update(),Counter.update,N/A,N/A,N/A
self.prob(samp),self.prob,N/A,N/A,N/A
log(p),log,N/A,N/A,N/A
log(2**(logx)+2**(logy)),log,N/A,N/A,N/A
findall(),findall,N/A,N/A,N/A
fields(),fields,N/A,N/A,N/A
decode(),decode,N/A,N/A,N/A
StandardFormat.fields(),StandardFormat.fields,StandardFormat.fields,nltk\nltk\toolbox.py,True
encode(),encode,N/A,N/A,N/A
parents(),parents,N/A,N/A,N/A
parent_indices(),parent_indices,N/A,N/A,N/A
left_siblings(),left_siblings,N/A,N/A,N/A
right_siblings(),right_siblings,N/A,N/A,N/A
parents(),parents,N/A,N/A,N/A
parent_indices(),parent_indices,N/A,N/A,N/A
ptree.parent()[ptree.parent_index()] is ptree,ptree.parent,N/A,N/A,N/A
ptree.parent()[ptree.parent_index()] is ptree,ptree.parent_index,N/A,N/A,N/A
ptree.parent_index(),ptree.parent_index,N/A,N/A,N/A
ptree.parent.index(ptree),parent.index,N/A,N/A,N/A
index(),index,N/A,N/A,N/A
ptree.parent(),ptree.parent,N/A,N/A,N/A
log(p),log,N/A,N/A,N/A
"Tree(label, children)",Tree,__init__,nltk\nltk\tree\tree.py,True
Tree.fromstring(s),Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,True
tp=self.leaf_treeposition(i),self.leaf_treeposition,N/A,N/A,N/A
self[tp]==self.leaves()[i],self.leaves,N/A,N/A,N/A
self.leaves()[start:end],self.leaves,N/A,N/A,N/A
">>> download('treebank') 
[nltk_data] Downloading package 'treebank'...
[nltk_data]   Unzipping corpora/treebank.zip.
",download,N/A,N/A,N/A
">>> download('all-corpora') 
[nltk_data] Downloading package 'abc'...
[nltk_data]   Unzipping corpora/abc.zip.
[nltk_data] Downloading package 'alpino'...
[nltk_data]   Unzipping corpora/alpino.zip.
  ...
[nltk_data] Downloading package 'words'...
[nltk_data]   Unzipping corpora/words.zip.
",download,N/A,N/A,N/A
">>> from nltk.featstruct import unify
>>> unify(dict(x=1, y=dict()), dict(a='a', y=dict(b='b')))  
{'y': {'b': 'b'}, 'x': 1, 'a': 'a'}
",unify,N/A,N/A,N/A
">>> from nltk.featstruct import unify
>>> unify(dict(x=1, y=dict()), dict(a='a', y=dict(b='b')))  
{'y': {'b': 'b'}, 'x': 1, 'a': 'a'}
",dict,N/A,N/A,N/A
"def handler(s, position, reentrances, match): ...
",handler,N/A,N/A,N/A
">>> from nltk.featstruct import FeatStruct
>>> FeatStruct('[a=?x]').unify(FeatStruct('[b=?x]'))
[a=?x, b=?x2]
",FeatStruct,__init__,nltk\nltk\featstruct.py,True
">>> from nltk.featstruct import FeatStruct
>>> FeatStruct('[a=?x]').unify(FeatStruct('[b=?x]'))
[a=?x, b=?x2]
",unify,N/A,N/A,N/A
">>> from nltk.probability import ConditionalFreqDist
>>> from nltk.tokenize import word_tokenize
>>> sent = ""the the the dog dog some other words that we do not care about""
>>> cfdist = ConditionalFreqDist()
>>> for word in word_tokenize(sent):
...     condition = len(word)
...     cfdist[condition][word] += 1
",ConditionalFreqDist,__init__,nltk\nltk\probability.py,True
">>> from nltk.probability import ConditionalFreqDist
>>> from nltk.tokenize import word_tokenize
>>> sent = ""the the the dog dog some other words that we do not care about""
>>> cfdist = ConditionalFreqDist()
>>> for word in word_tokenize(sent):
...     condition = len(word)
...     cfdist[condition][word] += 1
",word_tokenize,N/A,N/A,N/A
">>> from nltk.probability import ConditionalFreqDist
>>> from nltk.tokenize import word_tokenize
>>> sent = ""the the the dog dog some other words that we do not care about""
>>> cfdist = ConditionalFreqDist()
>>> for word in word_tokenize(sent):
...     condition = len(word)
...     cfdist[condition][word] += 1
",len,N/A,N/A,N/A
">>> cfdist = ConditionalFreqDist((len(word), word) for word in word_tokenize(sent))
",ConditionalFreqDist,__init__,nltk\nltk\probability.py,True
">>> cfdist = ConditionalFreqDist((len(word), word) for word in word_tokenize(sent))
",len,N/A,N/A,N/A
">>> cfdist = ConditionalFreqDist((len(word), word) for word in word_tokenize(sent))
",word_tokenize,N/A,N/A,N/A
">>> cfdist[3]
FreqDist({'the': 3, 'dog': 2, 'not': 1})
>>> cfdist[3].freq('the')
0.5
>>> cfdist[3]['dog']
2
",FreqDist,__init__,nltk\nltk\probability.py,True
">>> cfdist[3]
FreqDist({'the': 3, 'dog': 2, 'not': 1})
>>> cfdist[3].freq('the')
0.5
>>> cfdist[3]['dog']
2
",freq,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.probability import ConditionalProbDist, ELEProbDist
>>> cfdist = ConditionalFreqDist(brown.tagged_words()[:5000])
>>> cpdist = ConditionalProbDist(cfdist, ELEProbDist, 10)
>>> cpdist['passed'].max()
'VBD'
>>> cpdist['passed'].prob('VBD')
0.423...
",ConditionalFreqDist,__init__,nltk\nltk\probability.py,True
">>> from nltk.corpus import brown
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.probability import ConditionalProbDist, ELEProbDist
>>> cfdist = ConditionalFreqDist(brown.tagged_words()[:5000])
>>> cpdist = ConditionalProbDist(cfdist, ELEProbDist, 10)
>>> cpdist['passed'].max()
'VBD'
>>> cpdist['passed'].prob('VBD')
0.423...
",brown.tagged_words,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.probability import ConditionalProbDist, ELEProbDist
>>> cfdist = ConditionalFreqDist(brown.tagged_words()[:5000])
>>> cpdist = ConditionalProbDist(cfdist, ELEProbDist, 10)
>>> cpdist['passed'].max()
'VBD'
>>> cpdist['passed'].prob('VBD')
0.423...
",ConditionalProbDist,__init__,nltk\nltk\probability.py,True
">>> from nltk.corpus import brown
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.probability import ConditionalProbDist, ELEProbDist
>>> cfdist = ConditionalFreqDist(brown.tagged_words()[:5000])
>>> cpdist = ConditionalProbDist(cfdist, ELEProbDist, 10)
>>> cpdist['passed'].max()
'VBD'
>>> cpdist['passed'].prob('VBD')
0.423...
",max,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.probability import ConditionalProbDist, ELEProbDist
>>> cfdist = ConditionalFreqDist(brown.tagged_words()[:5000])
>>> cpdist = ConditionalProbDist(cfdist, ELEProbDist, 10)
>>> cpdist['passed'].max()
'VBD'
>>> cpdist['passed'].prob('VBD')
0.423...
",prob,N/A,N/A,N/A
">>> from nltk.tokenize import word_tokenize
>>> from nltk.probability import FreqDist
>>> sent = 'This is an example sentence'
>>> fdist = FreqDist()
>>> for word in word_tokenize(sent):
...    fdist[word.lower()] += 1
",FreqDist,__init__,nltk\nltk\probability.py,True
">>> from nltk.tokenize import word_tokenize
>>> from nltk.probability import FreqDist
>>> sent = 'This is an example sentence'
>>> fdist = FreqDist()
>>> for word in word_tokenize(sent):
...    fdist[word.lower()] += 1
",word_tokenize,N/A,N/A,N/A
">>> from nltk.tokenize import word_tokenize
>>> from nltk.probability import FreqDist
>>> sent = 'This is an example sentence'
>>> fdist = FreqDist()
>>> for word in word_tokenize(sent):
...    fdist[word.lower()] += 1
",word.lower,N/A,N/A,N/A
">>> fdist = FreqDist(word.lower() for word in word_tokenize(sent))
",FreqDist,__init__,nltk\nltk\probability.py,True
">>> fdist = FreqDist(word.lower() for word in word_tokenize(sent))
",word.lower,N/A,N/A,N/A
">>> fdist = FreqDist(word.lower() for word in word_tokenize(sent))
",word_tokenize,N/A,N/A,N/A
">>> from nltk.probability import ProbabilisticMixIn
>>> class A:
...     def __init__(self, x, y): self.data = (x,y)
...
>>> class ProbabilisticA(A, ProbabilisticMixIn):
...     def __init__(self, x, y, **prob_kwarg):
...         A.__init__(self, x, y)
...         ProbabilisticMixIn.__init__(self, **prob_kwarg)
",__init__,N/A,N/A,N/A
">>> from nltk.probability import ProbabilisticMixIn
>>> class A:
...     def __init__(self, x, y): self.data = (x,y)
...
>>> class ProbabilisticA(A, ProbabilisticMixIn):
...     def __init__(self, x, y, **prob_kwarg):
...         A.__init__(self, x, y)
...         ProbabilisticMixIn.__init__(self, **prob_kwarg)
",ProbabilisticA,N/A,N/A,N/A
">>> from nltk.probability import ProbabilisticMixIn
>>> class A:
...     def __init__(self, x, y): self.data = (x,y)
...
>>> class ProbabilisticA(A, ProbabilisticMixIn):
...     def __init__(self, x, y, **prob_kwarg):
...         A.__init__(self, x, y)
...         ProbabilisticMixIn.__init__(self, **prob_kwarg)
",A.__init__,N/A,N/A,N/A
">>> import nltk.corpus
>>> from nltk.text import Text
>>> moby = Text(nltk.corpus.gutenberg.words('melville-moby_dick.txt'))
",Text,__init__,nltk\nltk\text.py,True
">>> import nltk.corpus
>>> from nltk.text import Text
>>> moby = Text(nltk.corpus.gutenberg.words('melville-moby_dick.txt'))
",gutenberg.words,N/A,N/A,N/A
">>> from nltk.book import text4
>>> text4.collocation_list()[:2]
[('United', 'States'), ('fellow', 'citizens')]
",text4.collocation_list,N/A,N/A,N/A
">>> from nltk.book import text4
>>> text4.collocations() 
United States; fellow citizens; four years; ...
",text4.collocations,N/A,N/A,N/A
">>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",print,N/A,N/A,N/A
">>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",text5.findall,N/A,N/A,N/A
">>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",text1.findall,N/A,N/A,N/A
">>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",text9.findall,N/A,N/A,N/A
">>> import nltk.corpus
>>> from nltk.text import TextCollection
>>> print('hack'); from nltk.book import text1, text2, text3
hack...
>>> gutenberg = TextCollection(nltk.corpus.gutenberg)
>>> mytexts = TextCollection([text1, text2, text3])
",print,N/A,N/A,N/A
">>> import nltk.corpus
>>> from nltk.text import TextCollection
>>> print('hack'); from nltk.book import text1, text2, text3
hack...
>>> gutenberg = TextCollection(nltk.corpus.gutenberg)
>>> mytexts = TextCollection([text1, text2, text3])
",TextCollection,__init__,nltk\nltk\text.py,True
">>> from nltk.text import TokenSearcher
>>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",print,N/A,N/A,N/A
">>> from nltk.text import TokenSearcher
>>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",text5.findall,N/A,N/A,N/A
">>> from nltk.text import TokenSearcher
>>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",text1.findall,N/A,N/A,N/A
">>> from nltk.text import TokenSearcher
>>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",text9.findall,N/A,N/A,N/A
"for parent_index in ptree.parent_indices(parent):
    parent[parent_index] is ptree
",ptree.parent_indices,N/A,N/A,N/A
"for treepos in ptree.treepositions(root):
    root[treepos] is ptree
",ptree.treepositions,N/A,N/A,N/A
">>> from nltk.probability import ProbabilisticMixIn
>>> class A:
...     def __init__(self, x, y): self.data = (x,y)
...
>>> class ProbabilisticA(A, ProbabilisticMixIn):
...     def __init__(self, x, y, **prob_kwarg):
...         A.__init__(self, x, y)
...         ProbabilisticMixIn.__init__(self, **prob_kwarg)
",__init__,N/A,N/A,N/A
">>> from nltk.probability import ProbabilisticMixIn
>>> class A:
...     def __init__(self, x, y): self.data = (x,y)
...
>>> class ProbabilisticA(A, ProbabilisticMixIn):
...     def __init__(self, x, y, **prob_kwarg):
...         A.__init__(self, x, y)
...         ProbabilisticMixIn.__init__(self, **prob_kwarg)
",ProbabilisticA,N/A,N/A,N/A
">>> from nltk.probability import ProbabilisticMixIn
>>> class A:
...     def __init__(self, x, y): self.data = (x,y)
...
>>> class ProbabilisticA(A, ProbabilisticMixIn):
...     def __init__(self, x, y, **prob_kwarg):
...         A.__init__(self, x, y)
...         ProbabilisticMixIn.__init__(self, **prob_kwarg)
",A.__init__,N/A,N/A,N/A
">>> from nltk.tree import Tree
>>> print(Tree(1, [2, Tree(3, [4]), 5]))
(1 2 (3 4) 5)
>>> vp = Tree('VP', [Tree('V', ['saw']),
...                  Tree('NP', ['him'])])
>>> s = Tree('S', [Tree('NP', ['I']), vp])
>>> print(s)
(S (NP I) (VP (V saw) (NP him)))
>>> print(s[1])
(VP (V saw) (NP him))
>>> print(s[1,1])
(NP him)
>>> t = Tree.fromstring(""(S (NP I) (VP (V saw) (NP him)))"")
>>> s == t
True
>>> t[1][1].set_label('X')
>>> t[1][1].label()
'X'
>>> print(t)
(S (NP I) (VP (V saw) (X him)))
>>> t[0], t[1,1] = t[1,1], t[0]
>>> print(t)
(S (X him) (VP (V saw) (NP I)))
",print,N/A,N/A,N/A
">>> from nltk.tree import Tree
>>> print(Tree(1, [2, Tree(3, [4]), 5]))
(1 2 (3 4) 5)
>>> vp = Tree('VP', [Tree('V', ['saw']),
...                  Tree('NP', ['him'])])
>>> s = Tree('S', [Tree('NP', ['I']), vp])
>>> print(s)
(S (NP I) (VP (V saw) (NP him)))
>>> print(s[1])
(VP (V saw) (NP him))
>>> print(s[1,1])
(NP him)
>>> t = Tree.fromstring(""(S (NP I) (VP (V saw) (NP him)))"")
>>> s == t
True
>>> t[1][1].set_label('X')
>>> t[1][1].label()
'X'
>>> print(t)
(S (NP I) (VP (V saw) (X him)))
>>> t[0], t[1,1] = t[1,1], t[0]
>>> print(t)
(S (X him) (VP (V saw) (NP I)))
",Tree,__init__,nltk\nltk\tree\tree.py,True
">>> from nltk.tree import Tree
>>> print(Tree(1, [2, Tree(3, [4]), 5]))
(1 2 (3 4) 5)
>>> vp = Tree('VP', [Tree('V', ['saw']),
...                  Tree('NP', ['him'])])
>>> s = Tree('S', [Tree('NP', ['I']), vp])
>>> print(s)
(S (NP I) (VP (V saw) (NP him)))
>>> print(s[1])
(VP (V saw) (NP him))
>>> print(s[1,1])
(NP him)
>>> t = Tree.fromstring(""(S (NP I) (VP (V saw) (NP him)))"")
>>> s == t
True
>>> t[1][1].set_label('X')
>>> t[1][1].label()
'X'
>>> print(t)
(S (NP I) (VP (V saw) (X him)))
>>> t[0], t[1,1] = t[1,1], t[0]
>>> print(t)
(S (X him) (VP (V saw) (NP I)))
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> from nltk.tree import Tree
>>> print(Tree(1, [2, Tree(3, [4]), 5]))
(1 2 (3 4) 5)
>>> vp = Tree('VP', [Tree('V', ['saw']),
...                  Tree('NP', ['him'])])
>>> s = Tree('S', [Tree('NP', ['I']), vp])
>>> print(s)
(S (NP I) (VP (V saw) (NP him)))
>>> print(s[1])
(VP (V saw) (NP him))
>>> print(s[1,1])
(NP him)
>>> t = Tree.fromstring(""(S (NP I) (VP (V saw) (NP him)))"")
>>> s == t
True
>>> t[1][1].set_label('X')
>>> t[1][1].label()
'X'
>>> print(t)
(S (NP I) (VP (V saw) (X him)))
>>> t[0], t[1,1] = t[1,1], t[0]
>>> print(t)
(S (X him) (VP (V saw) (NP I)))
",set_label,N/A,N/A,N/A
">>> from nltk.tree import Tree
>>> print(Tree(1, [2, Tree(3, [4]), 5]))
(1 2 (3 4) 5)
>>> vp = Tree('VP', [Tree('V', ['saw']),
...                  Tree('NP', ['him'])])
>>> s = Tree('S', [Tree('NP', ['I']), vp])
>>> print(s)
(S (NP I) (VP (V saw) (NP him)))
>>> print(s[1])
(VP (V saw) (NP him))
>>> print(s[1,1])
(NP him)
>>> t = Tree.fromstring(""(S (NP I) (VP (V saw) (NP him)))"")
>>> s == t
True
>>> t[1][1].set_label('X')
>>> t[1][1].label()
'X'
>>> print(t)
(S (NP I) (VP (V saw) (X him)))
>>> t[0], t[1,1] = t[1,1], t[0]
>>> print(t)
(S (X him) (VP (V saw) (NP I)))
",label,N/A,N/A,N/A
">>> len(t)
2
",len,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> print(t.flatten())
(S the dog chased the cat)
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> print(t.flatten())
(S the dog chased the cat)
",print,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> print(t.flatten())
(S the dog chased the cat)
",t.flatten,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.height()
5
>>> print(t[0,0])
(D the)
>>> t[0,0].height()
2
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.height()
5
>>> print(t[0,0])
(D the)
>>> t[0,0].height()
2
",t.height,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.height()
5
>>> print(t[0,0])
(D the)
>>> t[0,0].height()
2
",print,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.height()
5
>>> print(t[0,0])
(D the)
>>> t[0,0].height()
2
",height,N/A,N/A,N/A
">>> t = Tree.fromstring('(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))')
>>> t.label()
'S'
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring('(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))')
>>> t.label()
'S'
",t.label,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.leaves()
['the', 'dog', 'chased', 'the', 'cat']
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.leaves()
['the', 'dog', 'chased', 'the', 'cat']
",t.leaves,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.pos()
[('the', 'D'), ('dog', 'N'), ('chased', 'V'), ('the', 'D'), ('cat', 'N')]
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.pos()
[('the', 'D'), ('dog', 'N'), ('chased', 'V'), ('the', 'D'), ('cat', 'N')]
",t.pos,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.productions()
[S -> NP VP, NP -> D N, D -> 'the', N -> 'dog', VP -> V NP, V -> 'chased',
NP -> D N, D -> 'the', N -> 'cat']
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.productions()
[S -> NP VP, NP -> D N, D -> 'the', N -> 'dog', VP -> V NP, V -> 'chased',
NP -> D N, D -> 'the', N -> 'cat']
",t.productions,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.set_label(""T"")
>>> print(t)
(T (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.set_label(""T"")
>>> print(t)
(T (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))
",t.set_label,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.set_label(""T"")
>>> print(t)
(T (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))
",print,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> for s in t.subtrees(lambda t: t.height() == 2):
...     print(s)
(D the)
(N dog)
(V chased)
(D the)
(N cat)
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> for s in t.subtrees(lambda t: t.height() == 2):
...     print(s)
(D the)
(N dog)
(V chased)
(D the)
(N cat)
",t.subtrees,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> for s in t.subtrees(lambda t: t.height() == 2):
...     print(s)
(D the)
(N dog)
(V chased)
(D the)
(N cat)
",t.height,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> for s in t.subtrees(lambda t: t.height() == 2):
...     print(s)
(D the)
(N dog)
(V chased)
(D the)
(N cat)
",print,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.treepositions() 
[(), (0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0), (1,), (1, 0), (1, 0, 0), ...]
>>> for pos in t.treepositions('leaves'):
...     t[pos] = t[pos][::-1].upper()
>>> print(t)
(S (NP (D EHT) (N GOD)) (VP (V DESAHC) (NP (D EHT) (N TAC))))
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.treepositions() 
[(), (0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0), (1,), (1, 0), (1, 0, 0), ...]
>>> for pos in t.treepositions('leaves'):
...     t[pos] = t[pos][::-1].upper()
>>> print(t)
(S (NP (D EHT) (N GOD)) (VP (V DESAHC) (NP (D EHT) (N TAC))))
",t.treepositions,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.treepositions() 
[(), (0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0), (1,), (1, 0), (1, 0, 0), ...]
>>> for pos in t.treepositions('leaves'):
...     t[pos] = t[pos][::-1].upper()
>>> print(t)
(S (NP (D EHT) (N GOD)) (VP (V DESAHC) (NP (D EHT) (N TAC))))
",upper,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.treepositions() 
[(), (0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0), (1,), (1, 0), (1, 0, 0), ...]
>>> for pos in t.treepositions('leaves'):
...     t[pos] = t[pos][::-1].upper()
>>> print(t)
(S (NP (D EHT) (N GOD)) (VP (V DESAHC) (NP (D EHT) (N TAC))))
",print,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import acyclic_branches_depth_first as tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(tree(wn.synset('certified.a.01'), lambda s:s.also_sees(), cut_mark='...', depth=4))
[Synset('certified.a.01'),
 [Synset('authorized.a.01'),
  [Synset('lawful.a.01'),
   [Synset('legal.a.01'),
    ""Cycle(Synset('lawful.a.01'),0,...)"",
    [Synset('legitimate.a.01'), '...']],
   [Synset('straight.a.06'),
    [Synset('honest.a.01'), '...'],
    ""Cycle(Synset('lawful.a.01'),0,...)""]],
  [Synset('legitimate.a.01'),
   ""Cycle(Synset('authorized.a.01'),1,...)"",
   [Synset('legal.a.01'),
    [Synset('lawful.a.01'), '...'],
    ""Cycle(Synset('legitimate.a.01'),0,...)""],
   [Synset('valid.a.01'),
    ""Cycle(Synset('legitimate.a.01'),0,...)"",
    [Synset('reasonable.a.01'), '...']]],
  [Synset('official.a.01'), ""Cycle(Synset('authorized.a.01'),1,...)""]],
 [Synset('documented.a.01')]]
",pprint,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import acyclic_branches_depth_first as tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(tree(wn.synset('certified.a.01'), lambda s:s.also_sees(), cut_mark='...', depth=4))
[Synset('certified.a.01'),
 [Synset('authorized.a.01'),
  [Synset('lawful.a.01'),
   [Synset('legal.a.01'),
    ""Cycle(Synset('lawful.a.01'),0,...)"",
    [Synset('legitimate.a.01'), '...']],
   [Synset('straight.a.06'),
    [Synset('honest.a.01'), '...'],
    ""Cycle(Synset('lawful.a.01'),0,...)""]],
  [Synset('legitimate.a.01'),
   ""Cycle(Synset('authorized.a.01'),1,...)"",
   [Synset('legal.a.01'),
    [Synset('lawful.a.01'), '...'],
    ""Cycle(Synset('legitimate.a.01'),0,...)""],
   [Synset('valid.a.01'),
    ""Cycle(Synset('legitimate.a.01'),0,...)"",
    [Synset('reasonable.a.01'), '...']]],
  [Synset('official.a.01'), ""Cycle(Synset('authorized.a.01'),1,...)""]],
 [Synset('documented.a.01')]]
",tree,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import acyclic_branches_depth_first as tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(tree(wn.synset('certified.a.01'), lambda s:s.also_sees(), cut_mark='...', depth=4))
[Synset('certified.a.01'),
 [Synset('authorized.a.01'),
  [Synset('lawful.a.01'),
   [Synset('legal.a.01'),
    ""Cycle(Synset('lawful.a.01'),0,...)"",
    [Synset('legitimate.a.01'), '...']],
   [Synset('straight.a.06'),
    [Synset('honest.a.01'), '...'],
    ""Cycle(Synset('lawful.a.01'),0,...)""]],
  [Synset('legitimate.a.01'),
   ""Cycle(Synset('authorized.a.01'),1,...)"",
   [Synset('legal.a.01'),
    [Synset('lawful.a.01'), '...'],
    ""Cycle(Synset('legitimate.a.01'),0,...)""],
   [Synset('valid.a.01'),
    ""Cycle(Synset('legitimate.a.01'),0,...)"",
    [Synset('reasonable.a.01'), '...']]],
  [Synset('official.a.01'), ""Cycle(Synset('authorized.a.01'),1,...)""]],
 [Synset('documented.a.01')]]
",wn.synset,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import acyclic_branches_depth_first as tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(tree(wn.synset('certified.a.01'), lambda s:s.also_sees(), cut_mark='...', depth=4))
[Synset('certified.a.01'),
 [Synset('authorized.a.01'),
  [Synset('lawful.a.01'),
   [Synset('legal.a.01'),
    ""Cycle(Synset('lawful.a.01'),0,...)"",
    [Synset('legitimate.a.01'), '...']],
   [Synset('straight.a.06'),
    [Synset('honest.a.01'), '...'],
    ""Cycle(Synset('lawful.a.01'),0,...)""]],
  [Synset('legitimate.a.01'),
   ""Cycle(Synset('authorized.a.01'),1,...)"",
   [Synset('legal.a.01'),
    [Synset('lawful.a.01'), '...'],
    ""Cycle(Synset('legitimate.a.01'),0,...)""],
   [Synset('valid.a.01'),
    ""Cycle(Synset('legitimate.a.01'),0,...)"",
    [Synset('reasonable.a.01'), '...']]],
  [Synset('official.a.01'), ""Cycle(Synset('authorized.a.01'),1,...)""]],
 [Synset('documented.a.01')]]
",s.also_sees,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import acyclic_branches_depth_first as tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(tree(wn.synset('certified.a.01'), lambda s:s.also_sees(), cut_mark='...', depth=4))
[Synset('certified.a.01'),
 [Synset('authorized.a.01'),
  [Synset('lawful.a.01'),
   [Synset('legal.a.01'),
    ""Cycle(Synset('lawful.a.01'),0,...)"",
    [Synset('legitimate.a.01'), '...']],
   [Synset('straight.a.06'),
    [Synset('honest.a.01'), '...'],
    ""Cycle(Synset('lawful.a.01'),0,...)""]],
  [Synset('legitimate.a.01'),
   ""Cycle(Synset('authorized.a.01'),1,...)"",
   [Synset('legal.a.01'),
    [Synset('lawful.a.01'), '...'],
    ""Cycle(Synset('legitimate.a.01'),0,...)""],
   [Synset('valid.a.01'),
    ""Cycle(Synset('legitimate.a.01'),0,...)"",
    [Synset('reasonable.a.01'), '...']]],
  [Synset('official.a.01'), ""Cycle(Synset('authorized.a.01'),1,...)""]],
 [Synset('documented.a.01')]]
",Synset,__init__,nltk\nltk\corpus\reader\wordnet.py,True
">>> import nltk
>>> from nltk.util import acyclic_branches_depth_first as tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(tree(wn.synset('certified.a.01'), lambda s:s.also_sees(), cut_mark='...', depth=4))
[Synset('certified.a.01'),
 [Synset('authorized.a.01'),
  [Synset('lawful.a.01'),
   [Synset('legal.a.01'),
    ""Cycle(Synset('lawful.a.01'),0,...)"",
    [Synset('legitimate.a.01'), '...']],
   [Synset('straight.a.06'),
    [Synset('honest.a.01'), '...'],
    ""Cycle(Synset('lawful.a.01'),0,...)""]],
  [Synset('legitimate.a.01'),
   ""Cycle(Synset('authorized.a.01'),1,...)"",
   [Synset('legal.a.01'),
    [Synset('lawful.a.01'), '...'],
    ""Cycle(Synset('legitimate.a.01'),0,...)""],
   [Synset('valid.a.01'),
    ""Cycle(Synset('legitimate.a.01'),0,...)"",
    [Synset('reasonable.a.01'), '...']]],
  [Synset('official.a.01'), ""Cycle(Synset('authorized.a.01'),1,...)""]],
 [Synset('documented.a.01')]]
",Cycle,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import acyclic_depth_first as acyclic_tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(acyclic_tree(wn.synset('dog.n.01'), lambda s:s.hypernyms(),cut_mark='...'))
[Synset('dog.n.01'),
 [Synset('canine.n.02'),
  [Synset('carnivore.n.01'),
   [Synset('placental.n.01'),
    [Synset('mammal.n.01'),
     [Synset('vertebrate.n.01'),
      [Synset('chordate.n.01'),
       [Synset('animal.n.01'),
        [Synset('organism.n.01'),
         [Synset('living_thing.n.01'),
          [Synset('whole.n.02'),
           [Synset('object.n.01'),
            [Synset('physical_entity.n.01'),
             [Synset('entity.n.01')]]]]]]]]]]]]],
 [Synset('domestic_animal.n.01'), ""Cycle(Synset('animal.n.01'),-3,...)""]]
",pprint,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import acyclic_depth_first as acyclic_tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(acyclic_tree(wn.synset('dog.n.01'), lambda s:s.hypernyms(),cut_mark='...'))
[Synset('dog.n.01'),
 [Synset('canine.n.02'),
  [Synset('carnivore.n.01'),
   [Synset('placental.n.01'),
    [Synset('mammal.n.01'),
     [Synset('vertebrate.n.01'),
      [Synset('chordate.n.01'),
       [Synset('animal.n.01'),
        [Synset('organism.n.01'),
         [Synset('living_thing.n.01'),
          [Synset('whole.n.02'),
           [Synset('object.n.01'),
            [Synset('physical_entity.n.01'),
             [Synset('entity.n.01')]]]]]]]]]]]]],
 [Synset('domestic_animal.n.01'), ""Cycle(Synset('animal.n.01'),-3,...)""]]
",acyclic_tree,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import acyclic_depth_first as acyclic_tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(acyclic_tree(wn.synset('dog.n.01'), lambda s:s.hypernyms(),cut_mark='...'))
[Synset('dog.n.01'),
 [Synset('canine.n.02'),
  [Synset('carnivore.n.01'),
   [Synset('placental.n.01'),
    [Synset('mammal.n.01'),
     [Synset('vertebrate.n.01'),
      [Synset('chordate.n.01'),
       [Synset('animal.n.01'),
        [Synset('organism.n.01'),
         [Synset('living_thing.n.01'),
          [Synset('whole.n.02'),
           [Synset('object.n.01'),
            [Synset('physical_entity.n.01'),
             [Synset('entity.n.01')]]]]]]]]]]]]],
 [Synset('domestic_animal.n.01'), ""Cycle(Synset('animal.n.01'),-3,...)""]]
",wn.synset,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import acyclic_depth_first as acyclic_tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(acyclic_tree(wn.synset('dog.n.01'), lambda s:s.hypernyms(),cut_mark='...'))
[Synset('dog.n.01'),
 [Synset('canine.n.02'),
  [Synset('carnivore.n.01'),
   [Synset('placental.n.01'),
    [Synset('mammal.n.01'),
     [Synset('vertebrate.n.01'),
      [Synset('chordate.n.01'),
       [Synset('animal.n.01'),
        [Synset('organism.n.01'),
         [Synset('living_thing.n.01'),
          [Synset('whole.n.02'),
           [Synset('object.n.01'),
            [Synset('physical_entity.n.01'),
             [Synset('entity.n.01')]]]]]]]]]]]]],
 [Synset('domestic_animal.n.01'), ""Cycle(Synset('animal.n.01'),-3,...)""]]
",s.hypernyms,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import acyclic_depth_first as acyclic_tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(acyclic_tree(wn.synset('dog.n.01'), lambda s:s.hypernyms(),cut_mark='...'))
[Synset('dog.n.01'),
 [Synset('canine.n.02'),
  [Synset('carnivore.n.01'),
   [Synset('placental.n.01'),
    [Synset('mammal.n.01'),
     [Synset('vertebrate.n.01'),
      [Synset('chordate.n.01'),
       [Synset('animal.n.01'),
        [Synset('organism.n.01'),
         [Synset('living_thing.n.01'),
          [Synset('whole.n.02'),
           [Synset('object.n.01'),
            [Synset('physical_entity.n.01'),
             [Synset('entity.n.01')]]]]]]]]]]]]],
 [Synset('domestic_animal.n.01'), ""Cycle(Synset('animal.n.01'),-3,...)""]]
",Synset,__init__,nltk\nltk\corpus\reader\wordnet.py,True
">>> import nltk
>>> from nltk.util import acyclic_depth_first as acyclic_tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(acyclic_tree(wn.synset('dog.n.01'), lambda s:s.hypernyms(),cut_mark='...'))
[Synset('dog.n.01'),
 [Synset('canine.n.02'),
  [Synset('carnivore.n.01'),
   [Synset('placental.n.01'),
    [Synset('mammal.n.01'),
     [Synset('vertebrate.n.01'),
      [Synset('chordate.n.01'),
       [Synset('animal.n.01'),
        [Synset('organism.n.01'),
         [Synset('living_thing.n.01'),
          [Synset('whole.n.02'),
           [Synset('object.n.01'),
            [Synset('physical_entity.n.01'),
             [Synset('entity.n.01')]]]]]]]]]]]]],
 [Synset('domestic_animal.n.01'), ""Cycle(Synset('animal.n.01'),-3,...)""]]
",Cycle,N/A,N/A,N/A
">>> from nltk.util import bigrams
>>> list(bigrams([1,2,3,4,5]))
[(1, 2), (2, 3), (3, 4), (4, 5)]
",list,N/A,N/A,N/A
">>> from nltk.util import bigrams
>>> list(bigrams([1,2,3,4,5]))
[(1, 2), (2, 3), (3, 4), (4, 5)]
",bigrams,N/A,N/A,N/A
">>> choose(4, 2)
6
>>> choose(6, 2)
15
",choose,N/A,N/A,N/A
">>> sent = 'a b c'.split()
",split,N/A,N/A,N/A
">>> list(everygrams(sent))
[('a',), ('a', 'b'), ('a', 'b', 'c'), ('b',), ('b', 'c'), ('c',)]
",list,N/A,N/A,N/A
">>> list(everygrams(sent))
[('a',), ('a', 'b'), ('a', 'b', 'c'), ('b',), ('b', 'c'), ('c',)]
",everygrams,N/A,N/A,N/A
">>> sorted(everygrams(sent), key=len)
[('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c'), ('a', 'b', 'c')]
",sorted,N/A,N/A,N/A
">>> sorted(everygrams(sent), key=len)
[('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c'), ('a', 'b', 'c')]
",everygrams,N/A,N/A,N/A
">>> list(everygrams(sent, max_len=2))
[('a',), ('a', 'b'), ('b',), ('b', 'c'), ('c',)]
",list,N/A,N/A,N/A
">>> list(everygrams(sent, max_len=2))
[('a',), ('a', 'b'), ('b',), ('b', 'c'), ('c',)]
",everygrams,N/A,N/A,N/A
">>> from nltk.util import flatten
>>> flatten(1, 2, ['b', 'a' , ['c', 'd']], 3)
[1, 2, 'b', 'a', 'c', 'd', 3]
",flatten,N/A,N/A,N/A
"locale.setlocale(locale.LC_ALL, '')
",locale.setlocale,N/A,N/A,N/A
">>> from nltk.util import ngrams
>>> list(ngrams([1,2,3,4,5], 3))
[(1, 2, 3), (2, 3, 4), (3, 4, 5)]
",list,N/A,N/A,N/A
">>> from nltk.util import ngrams
>>> list(ngrams([1,2,3,4,5], 3))
[(1, 2, 3), (2, 3, 4), (3, 4, 5)]
",ngrams,N/A,N/A,N/A
">>> list(ngrams([1,2,3,4,5], 2, pad_right=True))
[(1, 2), (2, 3), (3, 4), (4, 5), (5, None)]
>>> list(ngrams([1,2,3,4,5], 2, pad_right=True, right_pad_symbol=''))
[(1, 2), (2, 3), (3, 4), (4, 5), (5, '')]
>>> list(ngrams([1,2,3,4,5], 2, pad_left=True, left_pad_symbol=''))
[('', 1), (1, 2), (2, 3), (3, 4), (4, 5)]
>>> list(ngrams([1,2,3,4,5], 2, pad_left=True, pad_right=True, left_pad_symbol='', right_pad_symbol=''))
[('', 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, '')]
",list,N/A,N/A,N/A
">>> list(ngrams([1,2,3,4,5], 2, pad_right=True))
[(1, 2), (2, 3), (3, 4), (4, 5), (5, None)]
>>> list(ngrams([1,2,3,4,5], 2, pad_right=True, right_pad_symbol=''))
[(1, 2), (2, 3), (3, 4), (4, 5), (5, '')]
>>> list(ngrams([1,2,3,4,5], 2, pad_left=True, left_pad_symbol=''))
[('', 1), (1, 2), (2, 3), (3, 4), (4, 5)]
>>> list(ngrams([1,2,3,4,5], 2, pad_left=True, pad_right=True, left_pad_symbol='', right_pad_symbol=''))
[('', 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, '')]
",ngrams,N/A,N/A,N/A
">>> list(pad_sequence([1,2,3,4,5], 2, pad_left=True, pad_right=True, left_pad_symbol='', right_pad_symbol=''))
['', 1, 2, 3, 4, 5, '']
>>> list(pad_sequence([1,2,3,4,5], 2, pad_left=True, left_pad_symbol=''))
['', 1, 2, 3, 4, 5]
>>> list(pad_sequence([1,2,3,4,5], 2, pad_right=True, right_pad_symbol=''))
[1, 2, 3, 4, 5, '']
",list,N/A,N/A,N/A
">>> list(pad_sequence([1,2,3,4,5], 2, pad_left=True, pad_right=True, left_pad_symbol='', right_pad_symbol=''))
['', 1, 2, 3, 4, 5, '']
>>> list(pad_sequence([1,2,3,4,5], 2, pad_left=True, left_pad_symbol=''))
['', 1, 2, 3, 4, 5]
>>> list(pad_sequence([1,2,3,4,5], 2, pad_right=True, right_pad_symbol=''))
[1, 2, 3, 4, 5, '']
",pad_sequence,N/A,N/A,N/A
">>> sent = ""Insurgents killed in ongoing fighting"".split()
>>> list(skipgrams(sent, 2, 2))
[('Insurgents', 'killed'), ('Insurgents', 'in'), ('Insurgents', 'ongoing'), ('killed', 'in'), ('killed', 'ongoing'), ('killed', 'fighting'), ('in', 'ongoing'), ('in', 'fighting'), ('ongoing', 'fighting')]
>>> list(skipgrams(sent, 3, 2))
[('Insurgents', 'killed', 'in'), ('Insurgents', 'killed', 'ongoing'), ('Insurgents', 'killed', 'fighting'), ('Insurgents', 'in', 'ongoing'), ('Insurgents', 'in', 'fighting'), ('Insurgents', 'ongoing', 'fighting'), ('killed', 'in', 'ongoing'), ('killed', 'in', 'fighting'), ('killed', 'ongoing', 'fighting'), ('in', 'ongoing', 'fighting')]
",split,N/A,N/A,N/A
">>> sent = ""Insurgents killed in ongoing fighting"".split()
>>> list(skipgrams(sent, 2, 2))
[('Insurgents', 'killed'), ('Insurgents', 'in'), ('Insurgents', 'ongoing'), ('killed', 'in'), ('killed', 'ongoing'), ('killed', 'fighting'), ('in', 'ongoing'), ('in', 'fighting'), ('ongoing', 'fighting')]
>>> list(skipgrams(sent, 3, 2))
[('Insurgents', 'killed', 'in'), ('Insurgents', 'killed', 'ongoing'), ('Insurgents', 'killed', 'fighting'), ('Insurgents', 'in', 'ongoing'), ('Insurgents', 'in', 'fighting'), ('Insurgents', 'ongoing', 'fighting'), ('killed', 'in', 'ongoing'), ('killed', 'in', 'fighting'), ('killed', 'ongoing', 'fighting'), ('in', 'ongoing', 'fighting')]
",list,N/A,N/A,N/A
">>> sent = ""Insurgents killed in ongoing fighting"".split()
>>> list(skipgrams(sent, 2, 2))
[('Insurgents', 'killed'), ('Insurgents', 'in'), ('Insurgents', 'ongoing'), ('killed', 'in'), ('killed', 'ongoing'), ('killed', 'fighting'), ('in', 'ongoing'), ('in', 'fighting'), ('ongoing', 'fighting')]
>>> list(skipgrams(sent, 3, 2))
[('Insurgents', 'killed', 'in'), ('Insurgents', 'killed', 'ongoing'), ('Insurgents', 'killed', 'fighting'), ('Insurgents', 'in', 'ongoing'), ('Insurgents', 'in', 'fighting'), ('Insurgents', 'ongoing', 'fighting'), ('killed', 'in', 'ongoing'), ('killed', 'in', 'fighting'), ('killed', 'ongoing', 'fighting'), ('in', 'ongoing', 'fighting')]
",skipgrams,N/A,N/A,N/A
">>> from nltk.util import trigrams
>>> list(trigrams([1,2,3,4,5]))
[(1, 2, 3), (2, 3, 4), (3, 4, 5)]
",list,N/A,N/A,N/A
">>> from nltk.util import trigrams
>>> list(trigrams([1,2,3,4,5]))
[(1, 2, 3), (2, 3, 4), (3, 4, 5)]
",trigrams,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import unweighted_minimum_spanning_tree as mst
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(mst(wn.synset('bound.a.01'), lambda s:s.also_sees()))
[Synset('bound.a.01'),
 [Synset('unfree.a.02'),
  [Synset('confined.a.02')],
  [Synset('dependent.a.01')],
  [Synset('restricted.a.01'), [Synset('classified.a.02')]]]]
",pprint,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import unweighted_minimum_spanning_tree as mst
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(mst(wn.synset('bound.a.01'), lambda s:s.also_sees()))
[Synset('bound.a.01'),
 [Synset('unfree.a.02'),
  [Synset('confined.a.02')],
  [Synset('dependent.a.01')],
  [Synset('restricted.a.01'), [Synset('classified.a.02')]]]]
",mst,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import unweighted_minimum_spanning_tree as mst
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(mst(wn.synset('bound.a.01'), lambda s:s.also_sees()))
[Synset('bound.a.01'),
 [Synset('unfree.a.02'),
  [Synset('confined.a.02')],
  [Synset('dependent.a.01')],
  [Synset('restricted.a.01'), [Synset('classified.a.02')]]]]
",wn.synset,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import unweighted_minimum_spanning_tree as mst
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(mst(wn.synset('bound.a.01'), lambda s:s.also_sees()))
[Synset('bound.a.01'),
 [Synset('unfree.a.02'),
  [Synset('confined.a.02')],
  [Synset('dependent.a.01')],
  [Synset('restricted.a.01'), [Synset('classified.a.02')]]]]
",s.also_sees,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import unweighted_minimum_spanning_tree as mst
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(mst(wn.synset('bound.a.01'), lambda s:s.also_sees()))
[Synset('bound.a.01'),
 [Synset('unfree.a.02'),
  [Synset('confined.a.02')],
  [Synset('dependent.a.01')],
  [Synset('restricted.a.01'), [Synset('classified.a.02')]]]]
",Synset,__init__,nltk\nltk\corpus\reader\wordnet.py,True
">>> lesk(['I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.'], 'bank', 'n')
Synset('savings_bank.n.02')
",lesk,N/A,N/A,N/A
">>> lesk(['I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.'], 'bank', 'n')
Synset('savings_bank.n.02')
",Synset,__init__,nltk\nltk\corpus\reader\wordnet.py,True
load(),load,N/A,N/A,N/A
retrieve(),retrieve,N/A,N/A,N/A
find(),find,N/A,N/A,N/A
load(),load,N/A,N/A,N/A
write(),write,N/A,N/A,N/A
writestr(),writestr,N/A,N/A,N/A
seek(),seek,N/A,N/A,N/A
tell(),tell,N/A,N/A,N/A
seek(),seek,N/A,N/A,N/A
tell(),tell,N/A,N/A,N/A
seek(),seek,N/A,N/A,N/A
tell(),tell,N/A,N/A,N/A
readline(),readline,N/A,N/A,N/A
read(),read,N/A,N/A,N/A
readline(),readline,N/A,N/A,N/A
tell(),tell,N/A,N/A,N/A
download(),download,N/A,N/A,N/A
Downloader.default_download_dir(),Downloader.default_download_dir,Downloader.default_download_dir,nltk\nltk\downloader.py,True
download(),download,N/A,N/A,N/A
default_download_dir(),default_download_dir,N/A,N/A,N/A
download(),download,N/A,N/A,N/A
_package_to_columns(),_package_to_columns,N/A,N/A,N/A
unify(),unify,N/A,N/A,N/A
walk(),walk,N/A,N/A,N/A
cyclic(),cyclic,N/A,N/A,N/A
freeze(),freeze,N/A,N/A,N/A
equal_values(),equal_values,N/A,N/A,N/A
freeze(),freeze,N/A,N/A,N/A
copy(),copy,N/A,N/A,N/A
nltk.featstruct.rename_variables(),featstruct.rename_variables,N/A,N/A,N/A
nltk.featstruct.retract_bindings(),featstruct.retract_bindings,N/A,N/A,N/A
nltk.featstruct.substitute_bindings(),featstruct.substitute_bindings,N/A,N/A,N/A
nltk.featstruct.find_variables(),featstruct.find_variables,N/A,N/A,N/A
FreqDist.N(),FreqDist.N,FreqDist.N,nltk\nltk\probability.py,True
FreqDist.B(),FreqDist.B,FreqDist.B,nltk\nltk\probability.py,True
bins-self.B(),self.B,N/A,N/A,N/A
self.B(),self.B,N/A,N/A,N/A
Counter.setdefault(),Counter.setdefault,N/A,N/A,N/A
Counter.update(),Counter.update,N/A,N/A,N/A
self.prob(samp),self.prob,N/A,N/A,N/A
log(p),log,N/A,N/A,N/A
log(2**(logx)+2**(logy)),log,N/A,N/A,N/A
findall(),findall,N/A,N/A,N/A
fields(),fields,N/A,N/A,N/A
decode(),decode,N/A,N/A,N/A
StandardFormat.fields(),StandardFormat.fields,StandardFormat.fields,nltk\nltk\toolbox.py,True
encode(),encode,N/A,N/A,N/A
log(p),log,N/A,N/A,N/A
"Tree(label, children)",Tree,__init__,nltk\nltk\tree\tree.py,True
Tree.fromstring(s),Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,True
tp=self.leaf_treeposition(i),self.leaf_treeposition,N/A,N/A,N/A
self[tp]==self.leaves()[i],self.leaves,N/A,N/A,N/A
self.leaves()[start:end],self.leaves,N/A,N/A,N/A
ptree.parent()[ptree.parent_index()] is ptree,ptree.parent,N/A,N/A,N/A
ptree.parent()[ptree.parent_index()] is ptree,ptree.parent_index,N/A,N/A,N/A
ptree.parent_index(),ptree.parent_index,N/A,N/A,N/A
ptree.parent.index(ptree),parent.index,N/A,N/A,N/A
index(),index,N/A,N/A,N/A
ptree.parent(),ptree.parent,N/A,N/A,N/A
parents(),parents,N/A,N/A,N/A
parent_indices(),parent_indices,N/A,N/A,N/A
left_siblings(),left_siblings,N/A,N/A,N/A
right_siblings(),right_siblings,N/A,N/A,N/A
parents(),parents,N/A,N/A,N/A
parent_indices(),parent_indices,N/A,N/A,N/A
">>> download('treebank') 
[nltk_data] Downloading package 'treebank'...
[nltk_data]   Unzipping corpora/treebank.zip.
",download,N/A,N/A,N/A
">>> download('all-corpora') 
[nltk_data] Downloading package 'abc'...
[nltk_data]   Unzipping corpora/abc.zip.
[nltk_data] Downloading package 'alpino'...
[nltk_data]   Unzipping corpora/alpino.zip.
  ...
[nltk_data] Downloading package 'words'...
[nltk_data]   Unzipping corpora/words.zip.
",download,N/A,N/A,N/A
">>> from nltk.featstruct import unify
>>> unify(dict(x=1, y=dict()), dict(a='a', y=dict(b='b')))  
{'y': {'b': 'b'}, 'x': 1, 'a': 'a'}
",unify,N/A,N/A,N/A
">>> from nltk.featstruct import unify
>>> unify(dict(x=1, y=dict()), dict(a='a', y=dict(b='b')))  
{'y': {'b': 'b'}, 'x': 1, 'a': 'a'}
",dict,N/A,N/A,N/A
">>> from nltk.featstruct import FeatStruct
>>> FeatStruct('[a=?x]').unify(FeatStruct('[b=?x]'))
[a=?x, b=?x2]
",FeatStruct,__init__,nltk\nltk\featstruct.py,True
">>> from nltk.featstruct import FeatStruct
>>> FeatStruct('[a=?x]').unify(FeatStruct('[b=?x]'))
[a=?x, b=?x2]
",unify,N/A,N/A,N/A
"def handler(s, position, reentrances, match): ...
",handler,N/A,N/A,N/A
">>> from nltk.probability import ConditionalFreqDist
>>> from nltk.tokenize import word_tokenize
>>> sent = ""the the the dog dog some other words that we do not care about""
>>> cfdist = ConditionalFreqDist()
>>> for word in word_tokenize(sent):
...     condition = len(word)
...     cfdist[condition][word] += 1
",ConditionalFreqDist,__init__,nltk\nltk\probability.py,True
">>> from nltk.probability import ConditionalFreqDist
>>> from nltk.tokenize import word_tokenize
>>> sent = ""the the the dog dog some other words that we do not care about""
>>> cfdist = ConditionalFreqDist()
>>> for word in word_tokenize(sent):
...     condition = len(word)
...     cfdist[condition][word] += 1
",word_tokenize,N/A,N/A,N/A
">>> from nltk.probability import ConditionalFreqDist
>>> from nltk.tokenize import word_tokenize
>>> sent = ""the the the dog dog some other words that we do not care about""
>>> cfdist = ConditionalFreqDist()
>>> for word in word_tokenize(sent):
...     condition = len(word)
...     cfdist[condition][word] += 1
",len,N/A,N/A,N/A
">>> cfdist = ConditionalFreqDist((len(word), word) for word in word_tokenize(sent))
",ConditionalFreqDist,__init__,nltk\nltk\probability.py,True
">>> cfdist = ConditionalFreqDist((len(word), word) for word in word_tokenize(sent))
",len,N/A,N/A,N/A
">>> cfdist = ConditionalFreqDist((len(word), word) for word in word_tokenize(sent))
",word_tokenize,N/A,N/A,N/A
">>> cfdist[3]
FreqDist({'the': 3, 'dog': 2, 'not': 1})
>>> cfdist[3].freq('the')
0.5
>>> cfdist[3]['dog']
2
",FreqDist,__init__,nltk\nltk\probability.py,True
">>> cfdist[3]
FreqDist({'the': 3, 'dog': 2, 'not': 1})
>>> cfdist[3].freq('the')
0.5
>>> cfdist[3]['dog']
2
",freq,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.probability import ConditionalProbDist, ELEProbDist
>>> cfdist = ConditionalFreqDist(brown.tagged_words()[:5000])
>>> cpdist = ConditionalProbDist(cfdist, ELEProbDist, 10)
>>> cpdist['passed'].max()
'VBD'
>>> cpdist['passed'].prob('VBD')
0.423...
",ConditionalFreqDist,__init__,nltk\nltk\probability.py,True
">>> from nltk.corpus import brown
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.probability import ConditionalProbDist, ELEProbDist
>>> cfdist = ConditionalFreqDist(brown.tagged_words()[:5000])
>>> cpdist = ConditionalProbDist(cfdist, ELEProbDist, 10)
>>> cpdist['passed'].max()
'VBD'
>>> cpdist['passed'].prob('VBD')
0.423...
",brown.tagged_words,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.probability import ConditionalProbDist, ELEProbDist
>>> cfdist = ConditionalFreqDist(brown.tagged_words()[:5000])
>>> cpdist = ConditionalProbDist(cfdist, ELEProbDist, 10)
>>> cpdist['passed'].max()
'VBD'
>>> cpdist['passed'].prob('VBD')
0.423...
",ConditionalProbDist,__init__,nltk\nltk\probability.py,True
">>> from nltk.corpus import brown
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.probability import ConditionalProbDist, ELEProbDist
>>> cfdist = ConditionalFreqDist(brown.tagged_words()[:5000])
>>> cpdist = ConditionalProbDist(cfdist, ELEProbDist, 10)
>>> cpdist['passed'].max()
'VBD'
>>> cpdist['passed'].prob('VBD')
0.423...
",max,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.probability import ConditionalProbDist, ELEProbDist
>>> cfdist = ConditionalFreqDist(brown.tagged_words()[:5000])
>>> cpdist = ConditionalProbDist(cfdist, ELEProbDist, 10)
>>> cpdist['passed'].max()
'VBD'
>>> cpdist['passed'].prob('VBD')
0.423...
",prob,N/A,N/A,N/A
">>> from nltk.tokenize import word_tokenize
>>> from nltk.probability import FreqDist
>>> sent = 'This is an example sentence'
>>> fdist = FreqDist()
>>> for word in word_tokenize(sent):
...    fdist[word.lower()] += 1
",FreqDist,__init__,nltk\nltk\probability.py,True
">>> from nltk.tokenize import word_tokenize
>>> from nltk.probability import FreqDist
>>> sent = 'This is an example sentence'
>>> fdist = FreqDist()
>>> for word in word_tokenize(sent):
...    fdist[word.lower()] += 1
",word_tokenize,N/A,N/A,N/A
">>> from nltk.tokenize import word_tokenize
>>> from nltk.probability import FreqDist
>>> sent = 'This is an example sentence'
>>> fdist = FreqDist()
>>> for word in word_tokenize(sent):
...    fdist[word.lower()] += 1
",word.lower,N/A,N/A,N/A
">>> fdist = FreqDist(word.lower() for word in word_tokenize(sent))
",FreqDist,__init__,nltk\nltk\probability.py,True
">>> fdist = FreqDist(word.lower() for word in word_tokenize(sent))
",word.lower,N/A,N/A,N/A
">>> fdist = FreqDist(word.lower() for word in word_tokenize(sent))
",word_tokenize,N/A,N/A,N/A
">>> from nltk.probability import ProbabilisticMixIn
>>> class A:
...     def __init__(self, x, y): self.data = (x,y)
...
>>> class ProbabilisticA(A, ProbabilisticMixIn):
...     def __init__(self, x, y, **prob_kwarg):
...         A.__init__(self, x, y)
...         ProbabilisticMixIn.__init__(self, **prob_kwarg)
",__init__,N/A,N/A,N/A
">>> from nltk.probability import ProbabilisticMixIn
>>> class A:
...     def __init__(self, x, y): self.data = (x,y)
...
>>> class ProbabilisticA(A, ProbabilisticMixIn):
...     def __init__(self, x, y, **prob_kwarg):
...         A.__init__(self, x, y)
...         ProbabilisticMixIn.__init__(self, **prob_kwarg)
",ProbabilisticA,N/A,N/A,N/A
">>> from nltk.probability import ProbabilisticMixIn
>>> class A:
...     def __init__(self, x, y): self.data = (x,y)
...
>>> class ProbabilisticA(A, ProbabilisticMixIn):
...     def __init__(self, x, y, **prob_kwarg):
...         A.__init__(self, x, y)
...         ProbabilisticMixIn.__init__(self, **prob_kwarg)
",A.__init__,N/A,N/A,N/A
">>> from nltk.text import TokenSearcher
>>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",print,N/A,N/A,N/A
">>> from nltk.text import TokenSearcher
>>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",text5.findall,N/A,N/A,N/A
">>> from nltk.text import TokenSearcher
>>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",text1.findall,N/A,N/A,N/A
">>> from nltk.text import TokenSearcher
>>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",text9.findall,N/A,N/A,N/A
">>> import nltk.corpus
>>> from nltk.text import Text
>>> moby = Text(nltk.corpus.gutenberg.words('melville-moby_dick.txt'))
",Text,__init__,nltk\nltk\text.py,True
">>> import nltk.corpus
>>> from nltk.text import Text
>>> moby = Text(nltk.corpus.gutenberg.words('melville-moby_dick.txt'))
",gutenberg.words,N/A,N/A,N/A
">>> from nltk.book import text4
>>> text4.collocation_list()[:2]
[('United', 'States'), ('fellow', 'citizens')]
",text4.collocation_list,N/A,N/A,N/A
">>> from nltk.book import text4
>>> text4.collocations() 
United States; fellow citizens; four years; ...
",text4.collocations,N/A,N/A,N/A
">>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",print,N/A,N/A,N/A
">>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",text5.findall,N/A,N/A,N/A
">>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",text1.findall,N/A,N/A,N/A
">>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the
",text9.findall,N/A,N/A,N/A
">>> import nltk.corpus
>>> from nltk.text import TextCollection
>>> print('hack'); from nltk.book import text1, text2, text3
hack...
>>> gutenberg = TextCollection(nltk.corpus.gutenberg)
>>> mytexts = TextCollection([text1, text2, text3])
",print,N/A,N/A,N/A
">>> import nltk.corpus
>>> from nltk.text import TextCollection
>>> print('hack'); from nltk.book import text1, text2, text3
hack...
>>> gutenberg = TextCollection(nltk.corpus.gutenberg)
>>> mytexts = TextCollection([text1, text2, text3])
",TextCollection,__init__,nltk\nltk\text.py,True
">>> from nltk.probability import ProbabilisticMixIn
>>> class A:
...     def __init__(self, x, y): self.data = (x,y)
...
>>> class ProbabilisticA(A, ProbabilisticMixIn):
...     def __init__(self, x, y, **prob_kwarg):
...         A.__init__(self, x, y)
...         ProbabilisticMixIn.__init__(self, **prob_kwarg)
",__init__,N/A,N/A,N/A
">>> from nltk.probability import ProbabilisticMixIn
>>> class A:
...     def __init__(self, x, y): self.data = (x,y)
...
>>> class ProbabilisticA(A, ProbabilisticMixIn):
...     def __init__(self, x, y, **prob_kwarg):
...         A.__init__(self, x, y)
...         ProbabilisticMixIn.__init__(self, **prob_kwarg)
",ProbabilisticA,N/A,N/A,N/A
">>> from nltk.probability import ProbabilisticMixIn
>>> class A:
...     def __init__(self, x, y): self.data = (x,y)
...
>>> class ProbabilisticA(A, ProbabilisticMixIn):
...     def __init__(self, x, y, **prob_kwarg):
...         A.__init__(self, x, y)
...         ProbabilisticMixIn.__init__(self, **prob_kwarg)
",A.__init__,N/A,N/A,N/A
">>> from nltk.tree import Tree
>>> print(Tree(1, [2, Tree(3, [4]), 5]))
(1 2 (3 4) 5)
>>> vp = Tree('VP', [Tree('V', ['saw']),
...                  Tree('NP', ['him'])])
>>> s = Tree('S', [Tree('NP', ['I']), vp])
>>> print(s)
(S (NP I) (VP (V saw) (NP him)))
>>> print(s[1])
(VP (V saw) (NP him))
>>> print(s[1,1])
(NP him)
>>> t = Tree.fromstring(""(S (NP I) (VP (V saw) (NP him)))"")
>>> s == t
True
>>> t[1][1].set_label('X')
>>> t[1][1].label()
'X'
>>> print(t)
(S (NP I) (VP (V saw) (X him)))
>>> t[0], t[1,1] = t[1,1], t[0]
>>> print(t)
(S (X him) (VP (V saw) (NP I)))
",print,N/A,N/A,N/A
">>> from nltk.tree import Tree
>>> print(Tree(1, [2, Tree(3, [4]), 5]))
(1 2 (3 4) 5)
>>> vp = Tree('VP', [Tree('V', ['saw']),
...                  Tree('NP', ['him'])])
>>> s = Tree('S', [Tree('NP', ['I']), vp])
>>> print(s)
(S (NP I) (VP (V saw) (NP him)))
>>> print(s[1])
(VP (V saw) (NP him))
>>> print(s[1,1])
(NP him)
>>> t = Tree.fromstring(""(S (NP I) (VP (V saw) (NP him)))"")
>>> s == t
True
>>> t[1][1].set_label('X')
>>> t[1][1].label()
'X'
>>> print(t)
(S (NP I) (VP (V saw) (X him)))
>>> t[0], t[1,1] = t[1,1], t[0]
>>> print(t)
(S (X him) (VP (V saw) (NP I)))
",Tree,__init__,nltk\nltk\tree\tree.py,True
">>> from nltk.tree import Tree
>>> print(Tree(1, [2, Tree(3, [4]), 5]))
(1 2 (3 4) 5)
>>> vp = Tree('VP', [Tree('V', ['saw']),
...                  Tree('NP', ['him'])])
>>> s = Tree('S', [Tree('NP', ['I']), vp])
>>> print(s)
(S (NP I) (VP (V saw) (NP him)))
>>> print(s[1])
(VP (V saw) (NP him))
>>> print(s[1,1])
(NP him)
>>> t = Tree.fromstring(""(S (NP I) (VP (V saw) (NP him)))"")
>>> s == t
True
>>> t[1][1].set_label('X')
>>> t[1][1].label()
'X'
>>> print(t)
(S (NP I) (VP (V saw) (X him)))
>>> t[0], t[1,1] = t[1,1], t[0]
>>> print(t)
(S (X him) (VP (V saw) (NP I)))
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> from nltk.tree import Tree
>>> print(Tree(1, [2, Tree(3, [4]), 5]))
(1 2 (3 4) 5)
>>> vp = Tree('VP', [Tree('V', ['saw']),
...                  Tree('NP', ['him'])])
>>> s = Tree('S', [Tree('NP', ['I']), vp])
>>> print(s)
(S (NP I) (VP (V saw) (NP him)))
>>> print(s[1])
(VP (V saw) (NP him))
>>> print(s[1,1])
(NP him)
>>> t = Tree.fromstring(""(S (NP I) (VP (V saw) (NP him)))"")
>>> s == t
True
>>> t[1][1].set_label('X')
>>> t[1][1].label()
'X'
>>> print(t)
(S (NP I) (VP (V saw) (X him)))
>>> t[0], t[1,1] = t[1,1], t[0]
>>> print(t)
(S (X him) (VP (V saw) (NP I)))
",set_label,N/A,N/A,N/A
">>> from nltk.tree import Tree
>>> print(Tree(1, [2, Tree(3, [4]), 5]))
(1 2 (3 4) 5)
>>> vp = Tree('VP', [Tree('V', ['saw']),
...                  Tree('NP', ['him'])])
>>> s = Tree('S', [Tree('NP', ['I']), vp])
>>> print(s)
(S (NP I) (VP (V saw) (NP him)))
>>> print(s[1])
(VP (V saw) (NP him))
>>> print(s[1,1])
(NP him)
>>> t = Tree.fromstring(""(S (NP I) (VP (V saw) (NP him)))"")
>>> s == t
True
>>> t[1][1].set_label('X')
>>> t[1][1].label()
'X'
>>> print(t)
(S (NP I) (VP (V saw) (X him)))
>>> t[0], t[1,1] = t[1,1], t[0]
>>> print(t)
(S (X him) (VP (V saw) (NP I)))
",label,N/A,N/A,N/A
">>> len(t)
2
",len,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> print(t.flatten())
(S the dog chased the cat)
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> print(t.flatten())
(S the dog chased the cat)
",print,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> print(t.flatten())
(S the dog chased the cat)
",t.flatten,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.height()
5
>>> print(t[0,0])
(D the)
>>> t[0,0].height()
2
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.height()
5
>>> print(t[0,0])
(D the)
>>> t[0,0].height()
2
",t.height,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.height()
5
>>> print(t[0,0])
(D the)
>>> t[0,0].height()
2
",print,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.height()
5
>>> print(t[0,0])
(D the)
>>> t[0,0].height()
2
",height,N/A,N/A,N/A
">>> t = Tree.fromstring('(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))')
>>> t.label()
'S'
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring('(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))')
>>> t.label()
'S'
",t.label,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.leaves()
['the', 'dog', 'chased', 'the', 'cat']
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.leaves()
['the', 'dog', 'chased', 'the', 'cat']
",t.leaves,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.pos()
[('the', 'D'), ('dog', 'N'), ('chased', 'V'), ('the', 'D'), ('cat', 'N')]
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.pos()
[('the', 'D'), ('dog', 'N'), ('chased', 'V'), ('the', 'D'), ('cat', 'N')]
",t.pos,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.productions()
[S -> NP VP, NP -> D N, D -> 'the', N -> 'dog', VP -> V NP, V -> 'chased',
NP -> D N, D -> 'the', N -> 'cat']
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.productions()
[S -> NP VP, NP -> D N, D -> 'the', N -> 'dog', VP -> V NP, V -> 'chased',
NP -> D N, D -> 'the', N -> 'cat']
",t.productions,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.set_label(""T"")
>>> print(t)
(T (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.set_label(""T"")
>>> print(t)
(T (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))
",t.set_label,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.set_label(""T"")
>>> print(t)
(T (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))
",print,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> for s in t.subtrees(lambda t: t.height() == 2):
...     print(s)
(D the)
(N dog)
(V chased)
(D the)
(N cat)
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> for s in t.subtrees(lambda t: t.height() == 2):
...     print(s)
(D the)
(N dog)
(V chased)
(D the)
(N cat)
",t.subtrees,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> for s in t.subtrees(lambda t: t.height() == 2):
...     print(s)
(D the)
(N dog)
(V chased)
(D the)
(N cat)
",t.height,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> for s in t.subtrees(lambda t: t.height() == 2):
...     print(s)
(D the)
(N dog)
(V chased)
(D the)
(N cat)
",print,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.treepositions() 
[(), (0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0), (1,), (1, 0), (1, 0, 0), ...]
>>> for pos in t.treepositions('leaves'):
...     t[pos] = t[pos][::-1].upper()
>>> print(t)
(S (NP (D EHT) (N GOD)) (VP (V DESAHC) (NP (D EHT) (N TAC))))
",Tree.fromstring,Tree.fromstring,nltk\nltk\tree\tree.py,False
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.treepositions() 
[(), (0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0), (1,), (1, 0), (1, 0, 0), ...]
>>> for pos in t.treepositions('leaves'):
...     t[pos] = t[pos][::-1].upper()
>>> print(t)
(S (NP (D EHT) (N GOD)) (VP (V DESAHC) (NP (D EHT) (N TAC))))
",t.treepositions,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.treepositions() 
[(), (0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0), (1,), (1, 0), (1, 0, 0), ...]
>>> for pos in t.treepositions('leaves'):
...     t[pos] = t[pos][::-1].upper()
>>> print(t)
(S (NP (D EHT) (N GOD)) (VP (V DESAHC) (NP (D EHT) (N TAC))))
",upper,N/A,N/A,N/A
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.treepositions() 
[(), (0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0), (1,), (1, 0), (1, 0, 0), ...]
>>> for pos in t.treepositions('leaves'):
...     t[pos] = t[pos][::-1].upper()
>>> print(t)
(S (NP (D EHT) (N GOD)) (VP (V DESAHC) (NP (D EHT) (N TAC))))
",print,N/A,N/A,N/A
"for parent_index in ptree.parent_indices(parent):
    parent[parent_index] is ptree
",ptree.parent_indices,N/A,N/A,N/A
"for treepos in ptree.treepositions(root):
    root[treepos] is ptree
",ptree.treepositions,N/A,N/A,N/A
">>> from nltk.util import bigrams
>>> list(bigrams([1,2,3,4,5]))
[(1, 2), (2, 3), (3, 4), (4, 5)]
",list,N/A,N/A,N/A
">>> from nltk.util import bigrams
>>> list(bigrams([1,2,3,4,5]))
[(1, 2), (2, 3), (3, 4), (4, 5)]
",bigrams,N/A,N/A,N/A
">>> choose(4, 2)
6
>>> choose(6, 2)
15
",choose,N/A,N/A,N/A
">>> sent = 'a b c'.split()
>>> list(everygrams(sent))
[('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c'), ('a', 'b', 'c')]
>>> list(everygrams(sent, max_len=2))
[('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c')]
",split,N/A,N/A,N/A
">>> sent = 'a b c'.split()
>>> list(everygrams(sent))
[('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c'), ('a', 'b', 'c')]
>>> list(everygrams(sent, max_len=2))
[('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c')]
",list,N/A,N/A,N/A
">>> sent = 'a b c'.split()
>>> list(everygrams(sent))
[('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c'), ('a', 'b', 'c')]
>>> list(everygrams(sent, max_len=2))
[('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c')]
",everygrams,N/A,N/A,N/A
">>> from nltk.util import flatten
>>> flatten(1, 2, ['b', 'a' , ['c', 'd']], 3)
[1, 2, 'b', 'a', 'c', 'd', 3]
",flatten,N/A,N/A,N/A
"locale.setlocale(locale.LC_ALL, '')
",locale.setlocale,N/A,N/A,N/A
">>> from nltk.util import ngrams
>>> list(ngrams([1,2,3,4,5], 3))
[(1, 2, 3), (2, 3, 4), (3, 4, 5)]
",list,N/A,N/A,N/A
">>> from nltk.util import ngrams
>>> list(ngrams([1,2,3,4,5], 3))
[(1, 2, 3), (2, 3, 4), (3, 4, 5)]
",ngrams,N/A,N/A,N/A
">>> list(ngrams([1,2,3,4,5], 2, pad_right=True))
[(1, 2), (2, 3), (3, 4), (4, 5), (5, None)]
>>> list(ngrams([1,2,3,4,5], 2, pad_right=True, right_pad_symbol=''))
[(1, 2), (2, 3), (3, 4), (4, 5), (5, '')]
>>> list(ngrams([1,2,3,4,5], 2, pad_left=True, left_pad_symbol=''))
[('', 1), (1, 2), (2, 3), (3, 4), (4, 5)]
>>> list(ngrams([1,2,3,4,5], 2, pad_left=True, pad_right=True, left_pad_symbol='', right_pad_symbol=''))
[('', 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, '')]
",list,N/A,N/A,N/A
">>> list(ngrams([1,2,3,4,5], 2, pad_right=True))
[(1, 2), (2, 3), (3, 4), (4, 5), (5, None)]
>>> list(ngrams([1,2,3,4,5], 2, pad_right=True, right_pad_symbol=''))
[(1, 2), (2, 3), (3, 4), (4, 5), (5, '')]
>>> list(ngrams([1,2,3,4,5], 2, pad_left=True, left_pad_symbol=''))
[('', 1), (1, 2), (2, 3), (3, 4), (4, 5)]
>>> list(ngrams([1,2,3,4,5], 2, pad_left=True, pad_right=True, left_pad_symbol='', right_pad_symbol=''))
[('', 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, '')]
",ngrams,N/A,N/A,N/A
">>> list(pad_sequence([1,2,3,4,5], 2, pad_left=True, pad_right=True, left_pad_symbol='', right_pad_symbol=''))
['', 1, 2, 3, 4, 5, '']
>>> list(pad_sequence([1,2,3,4,5], 2, pad_left=True, left_pad_symbol=''))
['', 1, 2, 3, 4, 5]
>>> list(pad_sequence([1,2,3,4,5], 2, pad_right=True, right_pad_symbol=''))
[1, 2, 3, 4, 5, '']
",list,N/A,N/A,N/A
">>> list(pad_sequence([1,2,3,4,5], 2, pad_left=True, pad_right=True, left_pad_symbol='', right_pad_symbol=''))
['', 1, 2, 3, 4, 5, '']
>>> list(pad_sequence([1,2,3,4,5], 2, pad_left=True, left_pad_symbol=''))
['', 1, 2, 3, 4, 5]
>>> list(pad_sequence([1,2,3,4,5], 2, pad_right=True, right_pad_symbol=''))
[1, 2, 3, 4, 5, '']
",pad_sequence,N/A,N/A,N/A
">>> sent = ""Insurgents killed in ongoing fighting"".split()
>>> list(skipgrams(sent, 2, 2))
[('Insurgents', 'killed'), ('Insurgents', 'in'), ('Insurgents', 'ongoing'), ('killed', 'in'), ('killed', 'ongoing'), ('killed', 'fighting'), ('in', 'ongoing'), ('in', 'fighting'), ('ongoing', 'fighting')]
>>> list(skipgrams(sent, 3, 2))
[('Insurgents', 'killed', 'in'), ('Insurgents', 'killed', 'ongoing'), ('Insurgents', 'killed', 'fighting'), ('Insurgents', 'in', 'ongoing'), ('Insurgents', 'in', 'fighting'), ('Insurgents', 'ongoing', 'fighting'), ('killed', 'in', 'ongoing'), ('killed', 'in', 'fighting'), ('killed', 'ongoing', 'fighting'), ('in', 'ongoing', 'fighting')]
",split,N/A,N/A,N/A
">>> sent = ""Insurgents killed in ongoing fighting"".split()
>>> list(skipgrams(sent, 2, 2))
[('Insurgents', 'killed'), ('Insurgents', 'in'), ('Insurgents', 'ongoing'), ('killed', 'in'), ('killed', 'ongoing'), ('killed', 'fighting'), ('in', 'ongoing'), ('in', 'fighting'), ('ongoing', 'fighting')]
>>> list(skipgrams(sent, 3, 2))
[('Insurgents', 'killed', 'in'), ('Insurgents', 'killed', 'ongoing'), ('Insurgents', 'killed', 'fighting'), ('Insurgents', 'in', 'ongoing'), ('Insurgents', 'in', 'fighting'), ('Insurgents', 'ongoing', 'fighting'), ('killed', 'in', 'ongoing'), ('killed', 'in', 'fighting'), ('killed', 'ongoing', 'fighting'), ('in', 'ongoing', 'fighting')]
",list,N/A,N/A,N/A
">>> sent = ""Insurgents killed in ongoing fighting"".split()
>>> list(skipgrams(sent, 2, 2))
[('Insurgents', 'killed'), ('Insurgents', 'in'), ('Insurgents', 'ongoing'), ('killed', 'in'), ('killed', 'ongoing'), ('killed', 'fighting'), ('in', 'ongoing'), ('in', 'fighting'), ('ongoing', 'fighting')]
>>> list(skipgrams(sent, 3, 2))
[('Insurgents', 'killed', 'in'), ('Insurgents', 'killed', 'ongoing'), ('Insurgents', 'killed', 'fighting'), ('Insurgents', 'in', 'ongoing'), ('Insurgents', 'in', 'fighting'), ('Insurgents', 'ongoing', 'fighting'), ('killed', 'in', 'ongoing'), ('killed', 'in', 'fighting'), ('killed', 'ongoing', 'fighting'), ('in', 'ongoing', 'fighting')]
",skipgrams,N/A,N/A,N/A
">>> from nltk.util import trigrams
>>> list(trigrams([1,2,3,4,5]))
[(1, 2, 3), (2, 3, 4), (3, 4, 5)]
",list,N/A,N/A,N/A
">>> from nltk.util import trigrams
>>> list(trigrams([1,2,3,4,5]))
[(1, 2, 3), (2, 3, 4), (3, 4, 5)]
",trigrams,N/A,N/A,N/A
">>> lesk(['I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.'], 'bank', 'n')
Synset('savings_bank.n.02')
",lesk,N/A,N/A,N/A
">>> lesk(['I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.'], 'bank', 'n')
Synset('savings_bank.n.02')
",Synset,__init__,nltk\nltk\corpus\reader\wordnet.py,True
"from math import factorial
def binomial_coefficient(n: int, k: int) -> int:
    return factorial(n) // (factorial(k) * factorial(n - k))
",binomial_coefficient,N/A,N/A,N/A
"from math import factorial
def binomial_coefficient(n: int, k: int) -> int:
    return factorial(n) // (factorial(k) * factorial(n - k))
",factorial,N/A,N/A,N/A
"def binomial_coefficient(n: int, k: int) -> int:
    if k < 0 or k > n:
        return 0
    if k == 0 or k == n:
        return 1
    k = min(k, n - k)  # Take advantage of symmetry
    c = 1
    for i in range(k):
        c = c * (n - i) / (i + 1)
    return c
",binomial_coefficient,N/A,N/A,N/A
"def binomial_coefficient(n: int, k: int) -> int:
    if k < 0 or k > n:
        return 0
    if k == 0 or k == n:
        return 1
    k = min(k, n - k)  # Take advantage of symmetry
    c = 1
    for i in range(k):
        c = c * (n - i) / (i + 1)
    return c
",min,N/A,N/A,N/A
"def binomial_coefficient(n: int, k: int) -> int:
    if k < 0 or k > n:
        return 0
    if k == 0 or k == n:
        return 1
    k = min(k, n - k)  # Take advantage of symmetry
    c = 1
    for i in range(k):
        c = c * (n - i) / (i + 1)
    return c
",range,N/A,N/A,N/A
"def binomial_coefficient(n: int, k: int) -> int:
    if k < 0 or k > n:
        return 0
    if k > n - k:  # Take advantage of symmetry
        k = n - k
    if k == 0 or n <= 1:
        return 1
    return binomial_coefficient(n - 1, k) + binomial_coefficient(n - 1, k - 1)
",binomial_coefficient,N/A,N/A,N/A
"(define (binomial n k)
;; Helper function to compute C(n,k) via forward recursion
  (define (binomial-iter n k i prev)
    (if (>= i k)
      prev
     (binomial-iter n k (+ i 1) (/ (* (- n i) prev) (+ i 1)))))
;; Use symmetry property C(n,k)=C(n, n-k)
  (if (< k (-  n k))
    (binomial-iter n k 0 1)
    (binomial-iter n (- n k) 0 1)))
",C,N/A,N/A,N/A
"#include 

unsigned long binomial(unsigned long n, unsigned long k) {
  unsigned long c = 1, i;
  
  if (k > n-k) // take advantage of symmetry
    k = n-k;
  
  for (i = 1; i <= k; i++, n--) {
    if (c/i > UINT_MAX/n) // return 0 on overflow
      return 0;
      
    c = c / i * n + c % i * n / i;  // split c * n / i into (c / i * i + c % i) * n / i
  }
  
  return c;
}
",binomial,N/A,N/A,N/A
"def choose(n, k):
    """"""
    This function is a fast way to calculate binomial coefficients, commonly
    known as nCk, i.e. the number of combinations of n things taken k at a time. 
    (https://en.wikipedia.org/wiki/Binomial_coefficient).

        >>> choose(4, 2)
        6
        >>> choose(6, 2)
        15

    :param n: The number of things.
    :type n: int
    :param r: The number of times a thing is taken.
    :type r: int
    """"""
    if 0 <= k <= n:
        ntok, ktok = 1, 1
        for t in range(1, min(k, n - k) + 1):
            ntok *= n
            ktok *= t
            n -= 1
        return ntok // ktok
    else:
        return 0
",choose,N/A,N/A,N/A
"def choose(n, k):
    """"""
    This function is a fast way to calculate binomial coefficients, commonly
    known as nCk, i.e. the number of combinations of n things taken k at a time. 
    (https://en.wikipedia.org/wiki/Binomial_coefficient).

        >>> choose(4, 2)
        6
        >>> choose(6, 2)
        15

    :param n: The number of things.
    :type n: int
    :param r: The number of times a thing is taken.
    :type r: int
    """"""
    if 0 <= k <= n:
        ntok, ktok = 1, 1
        for t in range(1, min(k, n - k) + 1):
            ntok *= n
            ktok *= t
            n -= 1
        return ntok // ktok
    else:
        return 0
",range,N/A,N/A,N/A
"def choose(n, k):
    """"""
    This function is a fast way to calculate binomial coefficients, commonly
    known as nCk, i.e. the number of combinations of n things taken k at a time. 
    (https://en.wikipedia.org/wiki/Binomial_coefficient).

        >>> choose(4, 2)
        6
        >>> choose(6, 2)
        15

    :param n: The number of things.
    :type n: int
    :param r: The number of times a thing is taken.
    :type r: int
    """"""
    if 0 <= k <= n:
        ntok, ktok = 1, 1
        for t in range(1, min(k, n - k) + 1):
            ntok *= n
            ktok *= t
            n -= 1
        return ntok // ktok
    else:
        return 0
",min,N/A,N/A,N/A
"import itertools
def choose(n,k):
    return len(list(itertools.combinations(range(n),k)))
",choose,N/A,N/A,N/A
"import itertools
def choose(n,k):
    return len(list(itertools.combinations(range(n),k)))
",len,N/A,N/A,N/A
"import itertools
def choose(n,k):
    return len(list(itertools.combinations(range(n),k)))
",list,N/A,N/A,N/A
"import itertools
def choose(n,k):
    return len(list(itertools.combinations(range(n),k)))
",itertools.combinations,N/A,N/A,N/A
"import itertools
def choose(n,k):
    return len(list(itertools.combinations(range(n),k)))
",range,N/A,N/A,N/A
nltk.translate.ribes_score.choose(),ribes_score.choose,N/A,N/A,N/A
"def choose(n, k):
    """"""
    This function is a fast way to calculate binomial coefficients, commonly
    known as nCk, i.e. the number of combinations of n things taken k at a time. 
    (https://en.wikipedia.org/wiki/Binomial_coefficient).

        >>> choose(4, 2)
        6
        >>> choose(6, 2)
        15

    :param n: The number of things.
    :type n: int
    :param r: The number of times a thing is taken.
    :type r: int
    """"""
    if 0 <= k <= n:
        ntok, ktok = 1, 1
        for t in range(1, min(k, n - k) + 1):
            ntok *= n
            ktok *= t
            n -= 1
        return ntok // ktok
    else:
        return 0
",choose,N/A,N/A,N/A
"def choose(n, k):
    """"""
    This function is a fast way to calculate binomial coefficients, commonly
    known as nCk, i.e. the number of combinations of n things taken k at a time. 
    (https://en.wikipedia.org/wiki/Binomial_coefficient).

        >>> choose(4, 2)
        6
        >>> choose(6, 2)
        15

    :param n: The number of things.
    :type n: int
    :param r: The number of times a thing is taken.
    :type r: int
    """"""
    if 0 <= k <= n:
        ntok, ktok = 1, 1
        for t in range(1, min(k, n - k) + 1):
            ntok *= n
            ktok *= t
            n -= 1
        return ntok // ktok
    else:
        return 0
",range,N/A,N/A,N/A
"def choose(n, k):
    """"""
    This function is a fast way to calculate binomial coefficients, commonly
    known as nCk, i.e. the number of combinations of n things taken k at a time. 
    (https://en.wikipedia.org/wiki/Binomial_coefficient).

        >>> choose(4, 2)
        6
        >>> choose(6, 2)
        15

    :param n: The number of things.
    :type n: int
    :param r: The number of times a thing is taken.
    :type r: int
    """"""
    if 0 <= k <= n:
        ntok, ktok = 1, 1
        for t in range(1, min(k, n - k) + 1):
            ntok *= n
            ktok *= t
            n -= 1
        return ntok // ktok
    else:
        return 0
",min,N/A,N/A,N/A
"import itertools
def choose(n,k):
    return len(list(itertools.combinations(range(n),k)))
",choose,N/A,N/A,N/A
"import itertools
def choose(n,k):
    return len(list(itertools.combinations(range(n),k)))
",len,N/A,N/A,N/A
"import itertools
def choose(n,k):
    return len(list(itertools.combinations(range(n),k)))
",list,N/A,N/A,N/A
"import itertools
def choose(n,k):
    return len(list(itertools.combinations(range(n),k)))
",itertools.combinations,N/A,N/A,N/A
"import itertools
def choose(n,k):
    return len(list(itertools.combinations(range(n),k)))
",range,N/A,N/A,N/A
lexicon.fromstring( string>),lexicon.fromstring,N/A,N/A,N/A
"parser = chart.CCGChartParser(, )",CCGChartParser,__init__,nltk\nltk\ccg\chart.py,True
parser.parse(.split()),parser.parse,N/A,N/A,N/A
parser.parse(.split()),split,N/A,N/A,N/A
chart.printCCGDerivation( tree extracted from list>),chart.printCCGDerivation,N/A,N/A,N/A
apply(),apply,N/A,N/A,N/A
tokens[self.start():self.end()],self.start,N/A,N/A,N/A
tokens[self.start():self.end()],self.end,N/A,N/A,N/A
tokens[self.start():self.end()],self.start,N/A,N/A,N/A
tokens[self.start():self.end()],self.end,N/A,N/A,N/A
apply(),apply,N/A,N/A,N/A
parse(),parse,N/A,N/A,N/A
xform(),xform,N/A,N/A,N/A
apply(),apply,N/A,N/A,N/A
leaves(),leaves,N/A,N/A,N/A
parse(),parse,N/A,N/A,N/A
apply(),apply,N/A,N/A,N/A
">>> chunkscore = ChunkScore()           
>>> for correct in correct_sentences:   
...     guess = chunkparser.parse(correct.leaves())   
...     chunkscore.score(correct, guess)              
>>> print('F Measure:', chunkscore.f_measure())       
F Measure: 0.823
",ChunkScore,__init__,nltk\nltk\chunk\util.py,True
">>> chunkscore = ChunkScore()           
>>> for correct in correct_sentences:   
...     guess = chunkparser.parse(correct.leaves())   
...     chunkscore.score(correct, guess)              
>>> print('F Measure:', chunkscore.f_measure())       
F Measure: 0.823
",chunkparser.parse,N/A,N/A,N/A
">>> chunkscore = ChunkScore()           
>>> for correct in correct_sentences:   
...     guess = chunkparser.parse(correct.leaves())   
...     chunkscore.score(correct, guess)              
>>> print('F Measure:', chunkscore.f_measure())       
F Measure: 0.823
",correct.leaves,N/A,N/A,N/A
">>> chunkscore = ChunkScore()           
>>> for correct in correct_sentences:   
...     guess = chunkparser.parse(correct.leaves())   
...     chunkscore.score(correct, guess)              
>>> print('F Measure:', chunkscore.f_measure())       
F Measure: 0.823
",chunkscore.score,N/A,N/A,N/A
">>> chunkscore = ChunkScore()           
>>> for correct in correct_sentences:   
...     guess = chunkparser.parse(correct.leaves())   
...     chunkscore.score(correct, guess)              
>>> print('F Measure:', chunkscore.f_measure())       
F Measure: 0.823
",print,N/A,N/A,N/A
">>> chunkscore = ChunkScore()           
>>> for correct in correct_sentences:   
...     guess = chunkparser.parse(correct.leaves())   
...     chunkscore.score(correct, guess)              
>>> print('F Measure:', chunkscore.f_measure())       
F Measure: 0.823
",chunkscore.f_measure,N/A,N/A,N/A
labels(),labels,N/A,N/A,N/A
classify(),classify,N/A,N/A,N/A
classify_many(),classify_many,N/A,N/A,N/A
prob_classify(),prob_classify,N/A,N/A,N/A
prob_classify_many(),prob_classify_many,N/A,N/A,N/A
self.classify(),self.classify,N/A,N/A,N/A
self.prob_classify(),self.prob_classify,N/A,N/A,N/A
labels(),labels,N/A,N/A,N/A
classify(),classify,N/A,N/A,N/A
classify_many(),classify_many,N/A,N/A,N/A
prob_classify(),prob_classify,N/A,N/A,N/A
prob_classify_many(),prob_classify_many,N/A,N/A,N/A
self.classify(),self.classify,N/A,N/A,N/A
self.prob_classify(),self.prob_classify,N/A,N/A,N/A
train(),train,N/A,N/A,N/A
"is_unseen(fname, fval)",is_unseen,N/A,N/A,N/A
"self.encode(fs,l)",self.encode,N/A,N/A,N/A
"self.encode(fs,l)",self.encode,N/A,N/A,N/A
train(),train,N/A,N/A,N/A
train(),train,N/A,N/A,N/A
"self.encode(fs,l)",self.encode,N/A,N/A,N/A
"self.encode(fs,l)",self.encode,N/A,N/A,N/A
train(),train,N/A,N/A,N/A
"is_unseen(fname, fval)",is_unseen,N/A,N/A,N/A
"self.encode(fs,l)",self.encode,N/A,N/A,N/A
"encoding.encode(fs,l)",encoding.encode,N/A,N/A,N/A
"sum([val for (id,val) in vector])",sum,N/A,N/A,N/A
train_maxent_classifier(),train_maxent_classifier,N/A,N/A,N/A
config_megam(),config_megam,N/A,N/A,N/A
"map(feature_func, toks)",map,N/A,N/A,N/A
feature_func(),feature_func,N/A,N/A,N/A
feature_func(),feature_func,N/A,N/A,N/A
self.classify(),self.classify,N/A,N/A,N/A
self.prob_classify(),self.prob_classify,N/A,N/A,N/A
'contains-word(library)',word,N/A,N/A,N/A
"|  joint_feat(fs, l) = { 1 if (l == label)
|                      {
|                      { 0 otherwise
",joint_feat,N/A,N/A,N/A
"                          dotprod(weights, encode(fs,label))
prob(fs|label) = ---------------------------------------------------
                 sum(dotprod(weights, encode(fs,l)) for l in labels)
",dotprod,N/A,N/A,N/A
"                          dotprod(weights, encode(fs,label))
prob(fs|label) = ---------------------------------------------------
                 sum(dotprod(weights, encode(fs,l)) for l in labels)
",encode,N/A,N/A,N/A
"                          dotprod(weights, encode(fs,label))
prob(fs|label) = ---------------------------------------------------
                 sum(dotprod(weights, encode(fs,l)) for l in labels)
",prob,N/A,N/A,N/A
"                          dotprod(weights, encode(fs,label))
prob(fs|label) = ---------------------------------------------------
                 sum(dotprod(weights, encode(fs,l)) for l in labels)
",sum,N/A,N/A,N/A
"dotprod(a,b) = sum(x*y for (x,y) in zip(a,b))
",dotprod,N/A,N/A,N/A
"dotprod(a,b) = sum(x*y for (x,y) in zip(a,b))
",sum,N/A,N/A,N/A
"dotprod(a,b) = sum(x*y for (x,y) in zip(a,b))
",zip,N/A,N/A,N/A
"ffreq_empirical[i]
       =
SUM[fs,l] (classifier.prob_classify(fs).prob(l) *
           feature_vector(fs,l)[i] *
           exp(delta[i] * nf(feature_vector(fs,l))))
",classifier.prob_classify,N/A,N/A,N/A
"ffreq_empirical[i]
       =
SUM[fs,l] (classifier.prob_classify(fs).prob(l) *
           feature_vector(fs,l)[i] *
           exp(delta[i] * nf(feature_vector(fs,l))))
",prob,N/A,N/A,N/A
"ffreq_empirical[i]
       =
SUM[fs,l] (classifier.prob_classify(fs).prob(l) *
           feature_vector(fs,l)[i] *
           exp(delta[i] * nf(feature_vector(fs,l))))
",feature_vector,N/A,N/A,N/A
"ffreq_empirical[i]
       =
SUM[fs,l] (classifier.prob_classify(fs).prob(l) *
           feature_vector(fs,l)[i] *
           exp(delta[i] * nf(feature_vector(fs,l))))
",exp,N/A,N/A,N/A
"ffreq_empirical[i]
       =
SUM[fs,l] (classifier.prob_classify(fs).prob(l) *
           feature_vector(fs,l)[i] *
           exp(delta[i] * nf(feature_vector(fs,l))))
",nf,N/A,N/A,N/A
">>> from nltk.classify import megam
>>> megam.config_megam() # pass path to megam if not found in PATH 
[Found megam: ...]
",megam.config_megam,N/A,N/A,N/A
">>> def features(sentence):
...     words = sentence.lower().split()
...     return dict(('contains(%s)' % w, True) for w in words)
",features,N/A,N/A,N/A
">>> def features(sentence):
...     words = sentence.lower().split()
...     return dict(('contains(%s)' % w, True) for w in words)
",sentence.lower,N/A,N/A,N/A
">>> def features(sentence):
...     words = sentence.lower().split()
...     return dict(('contains(%s)' % w, True) for w in words)
",split,N/A,N/A,N/A
">>> def features(sentence):
...     words = sentence.lower().split()
...     return dict(('contains(%s)' % w, True) for w in words)
",dict,N/A,N/A,N/A
">>> def features(sentence):
...     words = sentence.lower().split()
...     return dict(('contains(%s)' % w, True) for w in words)
",contains,N/A,N/A,N/A
">>> positive_featuresets = map(features, sports_sentences)
>>> unlabeled_featuresets = map(features, various_sentences)
>>> classifier = PositiveNaiveBayesClassifier.train(positive_featuresets,
...                                                 unlabeled_featuresets)
",map,N/A,N/A,N/A
">>> classifier.classify(features('The cat is on the table'))
False
",classifier.classify,N/A,N/A,N/A
">>> classifier.classify(features('The cat is on the table'))
False
",features,N/A,N/A,N/A
">>> classifier.classify(features('My team lost the game'))
True
",classifier.classify,N/A,N/A,N/A
">>> classifier.classify(features('My team lost the game'))
True
",features,N/A,N/A,N/A
">>> from sklearn.svm import LinearSVC
>>> from nltk.classify.scikitlearn import SklearnClassifier
>>> classif = SklearnClassifier(LinearSVC())
",SklearnClassifier,__init__,nltk\nltk\classify\scikitlearn.py,True
">>> from sklearn.svm import LinearSVC
>>> from nltk.classify.scikitlearn import SklearnClassifier
>>> classif = SklearnClassifier(LinearSVC())
",LinearSVC,N/A,N/A,N/A
">>> from sklearn.feature_extraction.text import TfidfTransformer
>>> from sklearn.feature_selection import SelectKBest, chi2
>>> from sklearn.naive_bayes import MultinomialNB
>>> from sklearn.pipeline import Pipeline
>>> pipeline = Pipeline([('tfidf', TfidfTransformer()),
...                      ('chi2', SelectKBest(chi2, k=1000)),
...                      ('nb', MultinomialNB())])
>>> classif = SklearnClassifier(pipeline)
",Pipeline,N/A,N/A,N/A
">>> from sklearn.feature_extraction.text import TfidfTransformer
>>> from sklearn.feature_selection import SelectKBest, chi2
>>> from sklearn.naive_bayes import MultinomialNB
>>> from sklearn.pipeline import Pipeline
>>> pipeline = Pipeline([('tfidf', TfidfTransformer()),
...                      ('chi2', SelectKBest(chi2, k=1000)),
...                      ('nb', MultinomialNB())])
>>> classif = SklearnClassifier(pipeline)
",TfidfTransformer,N/A,N/A,N/A
">>> from sklearn.feature_extraction.text import TfidfTransformer
>>> from sklearn.feature_selection import SelectKBest, chi2
>>> from sklearn.naive_bayes import MultinomialNB
>>> from sklearn.pipeline import Pipeline
>>> pipeline = Pipeline([('tfidf', TfidfTransformer()),
...                      ('chi2', SelectKBest(chi2, k=1000)),
...                      ('nb', MultinomialNB())])
>>> classif = SklearnClassifier(pipeline)
",SelectKBest,N/A,N/A,N/A
">>> from sklearn.feature_extraction.text import TfidfTransformer
>>> from sklearn.feature_selection import SelectKBest, chi2
>>> from sklearn.naive_bayes import MultinomialNB
>>> from sklearn.pipeline import Pipeline
>>> pipeline = Pipeline([('tfidf', TfidfTransformer()),
...                      ('chi2', SelectKBest(chi2, k=1000)),
...                      ('nb', MultinomialNB())])
>>> classif = SklearnClassifier(pipeline)
",MultinomialNB,N/A,N/A,N/A
">>> from sklearn.feature_extraction.text import TfidfTransformer
>>> from sklearn.feature_selection import SelectKBest, chi2
>>> from sklearn.naive_bayes import MultinomialNB
>>> from sklearn.pipeline import Pipeline
>>> pipeline = Pipeline([('tfidf', TfidfTransformer()),
...                      ('chi2', SelectKBest(chi2, k=1000)),
...                      ('nb', MultinomialNB())])
>>> classif = SklearnClassifier(pipeline)
",SklearnClassifier,__init__,nltk\nltk\classify\scikitlearn.py,True
">>> from nltk.classify import Senna
>>> pipeline = Senna('/usr/share/senna-v3.0', ['pos', 'chk', 'ner'])
>>> sent = 'Dusseldorf is an international business center'.split()
>>> [(token['word'], token['chk'], token['ner'], token['pos']) for token in pipeline.tag(sent)] 
[('Dusseldorf', 'B-NP', 'B-LOC', 'NNP'), ('is', 'B-VP', 'O', 'VBZ'), ('an', 'B-NP', 'O', 'DT'),
('international', 'I-NP', 'O', 'JJ'), ('business', 'I-NP', 'O', 'NN'), ('center', 'I-NP', 'O', 'NN')]
",Senna,__init__,nltk\nltk\classify\senna.py,True
">>> from nltk.classify import Senna
>>> pipeline = Senna('/usr/share/senna-v3.0', ['pos', 'chk', 'ner'])
>>> sent = 'Dusseldorf is an international business center'.split()
>>> [(token['word'], token['chk'], token['ner'], token['pos']) for token in pipeline.tag(sent)] 
[('Dusseldorf', 'B-NP', 'B-LOC', 'NNP'), ('is', 'B-VP', 'O', 'VBZ'), ('an', 'B-NP', 'O', 'DT'),
('international', 'I-NP', 'O', 'JJ'), ('business', 'I-NP', 'O', 'NN'), ('center', 'I-NP', 'O', 'NN')]
",split,N/A,N/A,N/A
">>> from nltk.classify import Senna
>>> pipeline = Senna('/usr/share/senna-v3.0', ['pos', 'chk', 'ner'])
>>> sent = 'Dusseldorf is an international business center'.split()
>>> [(token['word'], token['chk'], token['ner'], token['pos']) for token in pipeline.tag(sent)] 
[('Dusseldorf', 'B-NP', 'B-LOC', 'NNP'), ('is', 'B-VP', 'O', 'VBZ'), ('an', 'B-NP', 'O', 'DT'),
('international', 'I-NP', 'O', 'JJ'), ('business', 'I-NP', 'O', 'NN'), ('center', 'I-NP', 'O', 'NN')]
",pipeline.tag,N/A,N/A,N/A
"[feature_func(tok) for tok in toks]
",feature_func,N/A,N/A,N/A
"[(feature_func(tok), label) for (tok, label) in toks]
",feature_func,N/A,N/A,N/A
">>> # Define a feature detector function.
>>> def document_features(document):
...     return dict([('contains-word(%s)' % w, True) for w in document])
",document_features,N/A,N/A,N/A
">>> # Define a feature detector function.
>>> def document_features(document):
...     return dict([('contains-word(%s)' % w, True) for w in document])
",dict,N/A,N/A,N/A
">>> # Define a feature detector function.
>>> def document_features(document):
...     return dict([('contains-word(%s)' % w, True) for w in document])
",word,N/A,N/A,N/A
">>> # Classify each Gutenberg document.
>>> from nltk.corpus import gutenberg
>>> for fileid in gutenberg.fileids(): 
...     doc = gutenberg.words(fileid) 
...     print(fileid, classifier.classify(document_features(doc))) 
",gutenberg.fileids,N/A,N/A,N/A
">>> # Classify each Gutenberg document.
>>> from nltk.corpus import gutenberg
>>> for fileid in gutenberg.fileids(): 
...     doc = gutenberg.words(fileid) 
...     print(fileid, classifier.classify(document_features(doc))) 
",gutenberg.words,N/A,N/A,N/A
">>> # Classify each Gutenberg document.
>>> from nltk.corpus import gutenberg
>>> for fileid in gutenberg.fileids(): 
...     doc = gutenberg.words(fileid) 
...     print(fileid, classifier.classify(document_features(doc))) 
",print,N/A,N/A,N/A
">>> # Classify each Gutenberg document.
>>> from nltk.corpus import gutenberg
>>> for fileid in gutenberg.fileids(): 
...     doc = gutenberg.words(fileid) 
...     print(fileid, classifier.classify(document_features(doc))) 
",classifier.classify,N/A,N/A,N/A
">>> # Classify each Gutenberg document.
>>> from nltk.corpus import gutenberg
>>> for fileid in gutenberg.fileids(): 
...     doc = gutenberg.words(fileid) 
...     print(fileid, classifier.classify(document_features(doc))) 
",document_features,N/A,N/A,N/A
">>> def wsd_features(sentence, index):
...     featureset = {}
...     for i in range(max(0, index-3), index):
...         featureset['left-context(%s)' % sentence[i]] = True
...     for i in range(index, max(index+3, len(sentence))):
...         featureset['right-context(%s)' % sentence[i]] = True
...     return featureset
",wsd_features,N/A,N/A,N/A
">>> def wsd_features(sentence, index):
...     featureset = {}
...     for i in range(max(0, index-3), index):
...         featureset['left-context(%s)' % sentence[i]] = True
...     for i in range(index, max(index+3, len(sentence))):
...         featureset['right-context(%s)' % sentence[i]] = True
...     return featureset
",range,N/A,N/A,N/A
">>> def wsd_features(sentence, index):
...     featureset = {}
...     for i in range(max(0, index-3), index):
...         featureset['left-context(%s)' % sentence[i]] = True
...     for i in range(index, max(index+3, len(sentence))):
...         featureset['right-context(%s)' % sentence[i]] = True
...     return featureset
",max,N/A,N/A,N/A
">>> def wsd_features(sentence, index):
...     featureset = {}
...     for i in range(max(0, index-3), index):
...         featureset['left-context(%s)' % sentence[i]] = True
...     for i in range(index, max(index+3, len(sentence))):
...         featureset['right-context(%s)' % sentence[i]] = True
...     return featureset
",context,N/A,N/A,N/A
">>> def wsd_features(sentence, index):
...     featureset = {}
...     for i in range(max(0, index-3), index):
...         featureset['left-context(%s)' % sentence[i]] = True
...     for i in range(index, max(index+3, len(sentence))):
...         featureset['right-context(%s)' % sentence[i]] = True
...     return featureset
",len,N/A,N/A,N/A
nltk.corpus.brown.words(),brown.words,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> print("", "".join(brown.words()))
The, Fulton, County, Grand, Jury, said, ...
",print,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> print("", "".join(brown.words()))
The, Fulton, County, Grand, Jury, said, ...
",join,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> print("", "".join(brown.words()))
The, Fulton, County, Grand, Jury, said, ...
",brown.words,N/A,N/A,N/A
categories(),categories,N/A,N/A,N/A
fileids(),fileids,N/A,N/A,N/A
__init__(),__init__,N/A,N/A,N/A
words(),words,N/A,N/A,N/A
parsed_sents(),parsed_sents,N/A,N/A,N/A
xml(),xml,N/A,N/A,N/A
words(),words,N/A,N/A,N/A
sents(),sents,N/A,N/A,N/A
tagged_words(),tagged_words,N/A,N/A,N/A
tagged_sents(),tagged_sents,N/A,N/A,N/A
words(),words,N/A,N/A,N/A
sents(),sents,N/A,N/A,N/A
tagged_words(),tagged_words,N/A,N/A,N/A
tagged_sents(),tagged_sents,N/A,N/A,N/A
frame(),frame,N/A,N/A,N/A
lu(),lu,N/A,N/A,N/A
domains(),domains,N/A,N/A,N/A
channels(),channels,N/A,N/A,N/A
categories(),categories,N/A,N/A,N/A
fileids(channel='prasa'),fileids,N/A,N/A,N/A
fileids(categories='publicystyczny'),fileids,N/A,N/A,N/A
tagged_sents(simplify_tags=True),tagged_sents,N/A,N/A,N/A
tagged_paras(one_tag=False),tagged_paras,N/A,N/A,N/A
tagged_words(disamb_only=False),tagged_words,N/A,N/A,N/A
tagged_words(append_no_space=True),tagged_words,N/A,N/A,N/A
words(append_space=True),words,N/A,N/A,N/A
words(replace_xmlentities=False),words,N/A,N/A,N/A
nombank.roleset() ,nombank.roleset,N/A,N/A,N/A
propbank.roleset() ,propbank.roleset,N/A,N/A,N/A
xml(),xml,N/A,N/A,N/A
words(),words,N/A,N/A,N/A
sents(),sents,N/A,N/A,N/A
tagged_words(),tagged_words,N/A,N/A,N/A
tagged_sents(),tagged_sents,N/A,N/A,N/A
self.paras(),self.paras,N/A,N/A,N/A
self.tagged_paras(),self.tagged_paras,N/A,N/A,N/A
start>=len(self),len,N/A,N/A,N/A
close(),close,N/A,N/A,N/A
stream.seek(),stream.seek,N/A,N/A,N/A
stream.tell(),stream.tell,N/A,N/A,N/A
stream.seek(),stream.seek,N/A,N/A,N/A
start>=len(self),len,N/A,N/A,N/A
nltk.corpus.wordnet_ic.ic(),wordnet_ic.ic,N/A,N/A,N/A
handle_elt(),handle_elt,N/A,N/A,N/A
nltk.corpus.brown.words(),brown.words,N/A,N/A,N/A
xml(),xml,N/A,N/A,N/A
words(),words,N/A,N/A,N/A
sents(),sents,N/A,N/A,N/A
tagged_words(),tagged_words,N/A,N/A,N/A
tagged_sents(),tagged_sents,N/A,N/A,N/A
words(),words,N/A,N/A,N/A
sents(),sents,N/A,N/A,N/A
tagged_words(),tagged_words,N/A,N/A,N/A
tagged_sents(),tagged_sents,N/A,N/A,N/A
categories(),categories,N/A,N/A,N/A
fileids(),fileids,N/A,N/A,N/A
__init__(),__init__,N/A,N/A,N/A
"BNCCorpusReader(root='BNC/Texts/', fileids=r'[A-K]/\w*/\w*\.xml')
",BNCCorpusReader,__init__,nltk\nltk\corpus\reader\bnc.py,True
">>> from nltk.corpus import subjectivity
>>> subjectivity.sents()[23]
['television', 'made', 'him', 'famous', ',', 'but', 'his', 'biggest', 'hits',
'happened', 'off', 'screen', '.']
>>> subjectivity.categories()
['obj', 'subj']
>>> subjectivity.words(categories='subj')
['smart', 'and', 'alert', ',', 'thirteen', ...]
",subjectivity.sents,N/A,N/A,N/A
">>> from nltk.corpus import subjectivity
>>> subjectivity.sents()[23]
['television', 'made', 'him', 'famous', ',', 'but', 'his', 'biggest', 'hits',
'happened', 'off', 'screen', '.']
>>> subjectivity.categories()
['obj', 'subj']
>>> subjectivity.words(categories='subj')
['smart', 'and', 'alert', ',', 'thirteen', ...]
",subjectivity.categories,N/A,N/A,N/A
">>> from nltk.corpus import subjectivity
>>> subjectivity.sents()[23]
['television', 'made', 'him', 'famous', ',', 'but', 'his', 'biggest', 'hits',
'happened', 'off', 'screen', '.']
>>> subjectivity.categories()
['obj', 'subj']
>>> subjectivity.words(categories='subj')
['smart', 'and', 'alert', ',', 'thirteen', ...]
",subjectivity.words,N/A,N/A,N/A
">>> from nltk.corpus import sentence_polarity
>>> sentence_polarity.sents()
[['simplistic', ',', 'silly', 'and', 'tedious', '.'], [""it's"", 'so', 'laddish',
'and', 'juvenile', ',', 'only', 'teenage', 'boys', 'could', 'possibly', 'find',
'it', 'funny', '.'], ...]
>>> sentence_polarity.categories()
['neg', 'pos']
",sentence_polarity.sents,N/A,N/A,N/A
">>> from nltk.corpus import sentence_polarity
>>> sentence_polarity.sents()
[['simplistic', ',', 'silly', 'and', 'tedious', '.'], [""it's"", 'so', 'laddish',
'and', 'juvenile', ',', 'only', 'teenage', 'boys', 'could', 'possibly', 'find',
'it', 'funny', '.'], ...]
>>> sentence_polarity.categories()
['neg', 'pos']
",sentence_polarity.categories,N/A,N/A,N/A
">>> from nltk.corpus import comparative_sentences
>>> comparison = comparative_sentences.comparisons()[0]
>>> comparison.text
['its', 'fast-forward', 'and', 'rewind', 'work', 'much', 'more', 'smoothly',
'and', 'consistently', 'than', 'those', 'of', 'other', 'models', 'i', ""'ve"",
'had', '.']
>>> comparison.entity_2
'models'
>>> (comparison.feature, comparison.keyword)
('rewind', 'more')
>>> len(comparative_sentences.comparisons())
853
",comparative_sentences.comparisons,N/A,N/A,N/A
">>> from nltk.corpus import comparative_sentences
>>> comparison = comparative_sentences.comparisons()[0]
>>> comparison.text
['its', 'fast-forward', 'and', 'rewind', 'work', 'much', 'more', 'smoothly',
'and', 'consistently', 'than', 'those', 'of', 'other', 'models', 'i', ""'ve"",
'had', '.']
>>> comparison.entity_2
'models'
>>> (comparison.feature, comparison.keyword)
('rewind', 'more')
>>> len(comparative_sentences.comparisons())
853
",len,N/A,N/A,N/A
">>> foo = {'a':1, 'b':2, 'c':3}
>>> bar = AttrDict(foo)
>>> pprint(dict(bar))
{'a': 1, 'b': 2, 'c': 3}
>>> bar.b
2
>>> bar.d = 4
>>> pprint(dict(bar))
{'a': 1, 'b': 2, 'c': 3, 'd': 4}
",AttrDict,__init__,nltk\nltk\corpus\reader\framenet.py,True
">>> foo = {'a':1, 'b':2, 'c':3}
>>> bar = AttrDict(foo)
>>> pprint(dict(bar))
{'a': 1, 'b': 2, 'c': 3}
>>> bar.b
2
>>> bar.d = 4
>>> pprint(dict(bar))
{'a': 1, 'b': 2, 'c': 3, 'd': 4}
",pprint,N/A,N/A,N/A
">>> foo = {'a':1, 'b':2, 'c':3}
>>> bar = AttrDict(foo)
>>> pprint(dict(bar))
{'a': 1, 'b': 2, 'c': 3}
>>> bar.b
2
>>> bar.d = 4
>>> pprint(dict(bar))
{'a': 1, 'b': 2, 'c': 3, 'd': 4}
",dict,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> fn.lu(3238).frame.lexUnit['glint.v'] is fn.lu(3238)
True
>>> fn.frame_by_name('Replacing') is fn.lus('replace.v')[0].frame
True
>>> fn.lus('prejudice.n')[0].frame.frameRelations == fn.frame_relations('Partiality')
True
",fn.lu,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> fn.lu(3238).frame.lexUnit['glint.v'] is fn.lu(3238)
True
>>> fn.frame_by_name('Replacing') is fn.lus('replace.v')[0].frame
True
>>> fn.lus('prejudice.n')[0].frame.frameRelations == fn.frame_relations('Partiality')
True
",fn.frame_by_name,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> fn.lu(3238).frame.lexUnit['glint.v'] is fn.lu(3238)
True
>>> fn.frame_by_name('Replacing') is fn.lus('replace.v')[0].frame
True
>>> fn.lus('prejudice.n')[0].frame.frameRelations == fn.frame_relations('Partiality')
True
",fn.lus,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> fn.lu(3238).frame.lexUnit['glint.v'] is fn.lu(3238)
True
>>> fn.frame_by_name('Replacing') is fn.lus('replace.v')[0].frame
True
>>> fn.lus('prejudice.n')[0].frame.frameRelations == fn.frame_relations('Partiality')
True
",fn.frame_relations,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> len(fn.docs()) in (78, 107) # FN 1.5 and 1.7, resp.
True
>>> set([x.corpname for x in fn.docs_metadata()])>=set(['ANC', 'KBEval',                     'LUCorpus-v0.3', 'Miscellaneous', 'NTI', 'PropBank'])
True
",len,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> len(fn.docs()) in (78, 107) # FN 1.5 and 1.7, resp.
True
>>> set([x.corpname for x in fn.docs_metadata()])>=set(['ANC', 'KBEval',                     'LUCorpus-v0.3', 'Miscellaneous', 'NTI', 'PropBank'])
True
",fn.docs,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> len(fn.docs()) in (78, 107) # FN 1.5 and 1.7, resp.
True
>>> set([x.corpname for x in fn.docs_metadata()])>=set(['ANC', 'KBEval',                     'LUCorpus-v0.3', 'Miscellaneous', 'NTI', 'PropBank'])
True
",set,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> len(fn.docs()) in (78, 107) # FN 1.5 and 1.7, resp.
True
>>> set([x.corpname for x in fn.docs_metadata()])>=set(['ANC', 'KBEval',                     'LUCorpus-v0.3', 'Miscellaneous', 'NTI', 'PropBank'])
True
",fn.docs_metadata,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> ferels = fn.fe_relations()
>>> isinstance(ferels, list)
True
>>> len(ferels) in (10020, 12393)   # FN 1.5 and 1.7, resp.
True
>>> PrettyDict(ferels[0], breakLines=True)
{'ID': 14642,
'_type': 'ferelation',
'frameRelation':  Child=Lively_place>,
'subFE': ,
'subFEName': 'Degree',
'subFrame': ,
'subID': 11370,
'supID': 2271,
'superFE': ,
'superFEName': 'Degree',
'superFrame': ,
'type': }
",fn.fe_relations,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> ferels = fn.fe_relations()
>>> isinstance(ferels, list)
True
>>> len(ferels) in (10020, 12393)   # FN 1.5 and 1.7, resp.
True
>>> PrettyDict(ferels[0], breakLines=True)
{'ID': 14642,
'_type': 'ferelation',
'frameRelation':  Child=Lively_place>,
'subFE': ,
'subFEName': 'Degree',
'subFrame': ,
'subID': 11370,
'supID': 2271,
'superFE': ,
'superFEName': 'Degree',
'superFrame': ,
'type': }
",isinstance,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> ferels = fn.fe_relations()
>>> isinstance(ferels, list)
True
>>> len(ferels) in (10020, 12393)   # FN 1.5 and 1.7, resp.
True
>>> PrettyDict(ferels[0], breakLines=True)
{'ID': 14642,
'_type': 'ferelation',
'frameRelation':  Child=Lively_place>,
'subFE': ,
'subFEName': 'Degree',
'subFrame': ,
'subID': 11370,
'supID': 2271,
'superFE': ,
'superFEName': 'Degree',
'superFrame': ,
'type': }
",len,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> ferels = fn.fe_relations()
>>> isinstance(ferels, list)
True
>>> len(ferels) in (10020, 12393)   # FN 1.5 and 1.7, resp.
True
>>> PrettyDict(ferels[0], breakLines=True)
{'ID': 14642,
'_type': 'ferelation',
'frameRelation':  Child=Lively_place>,
'subFE': ,
'subFEName': 'Degree',
'subFrame': ,
'subID': 11370,
'supID': 2271,
'superFE': ,
'superFEName': 'Degree',
'superFrame': ,
'type': }
",PrettyDict,__init__,nltk\nltk\corpus\reader\framenet.py,True
">>> from nltk.corpus import framenet as fn
>>> fn.fes('Noise_maker')
[]
>>> sorted([(fe.frame.name,fe.name) for fe in fn.fes('sound')])
[('Cause_to_make_noise', 'Sound_maker'), ('Make_noise', 'Sound'),
 ('Make_noise', 'Sound_source'), ('Sound_movement', 'Location_of_sound_source'),
 ('Sound_movement', 'Sound'), ('Sound_movement', 'Sound_source'),
 ('Sounds', 'Component_sound'), ('Sounds', 'Location_of_sound_source'),
 ('Sounds', 'Sound_source'), ('Vocalizations', 'Location_of_sound_source'),
 ('Vocalizations', 'Sound_source')]
>>> sorted([(fe.frame.name,fe.name) for fe in fn.fes('sound',r'(?i)make_noise')])
[('Cause_to_make_noise', 'Sound_maker'),
 ('Make_noise', 'Sound'),
 ('Make_noise', 'Sound_source')]
>>> sorted(set(fe.name for fe in fn.fes('^sound')))
['Sound', 'Sound_maker', 'Sound_source']
>>> len(fn.fes('^sound$'))
2
",fn.fes,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> fn.fes('Noise_maker')
[]
>>> sorted([(fe.frame.name,fe.name) for fe in fn.fes('sound')])
[('Cause_to_make_noise', 'Sound_maker'), ('Make_noise', 'Sound'),
 ('Make_noise', 'Sound_source'), ('Sound_movement', 'Location_of_sound_source'),
 ('Sound_movement', 'Sound'), ('Sound_movement', 'Sound_source'),
 ('Sounds', 'Component_sound'), ('Sounds', 'Location_of_sound_source'),
 ('Sounds', 'Sound_source'), ('Vocalizations', 'Location_of_sound_source'),
 ('Vocalizations', 'Sound_source')]
>>> sorted([(fe.frame.name,fe.name) for fe in fn.fes('sound',r'(?i)make_noise')])
[('Cause_to_make_noise', 'Sound_maker'),
 ('Make_noise', 'Sound'),
 ('Make_noise', 'Sound_source')]
>>> sorted(set(fe.name for fe in fn.fes('^sound')))
['Sound', 'Sound_maker', 'Sound_source']
>>> len(fn.fes('^sound$'))
2
",sorted,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> fn.fes('Noise_maker')
[]
>>> sorted([(fe.frame.name,fe.name) for fe in fn.fes('sound')])
[('Cause_to_make_noise', 'Sound_maker'), ('Make_noise', 'Sound'),
 ('Make_noise', 'Sound_source'), ('Sound_movement', 'Location_of_sound_source'),
 ('Sound_movement', 'Sound'), ('Sound_movement', 'Sound_source'),
 ('Sounds', 'Component_sound'), ('Sounds', 'Location_of_sound_source'),
 ('Sounds', 'Sound_source'), ('Vocalizations', 'Location_of_sound_source'),
 ('Vocalizations', 'Sound_source')]
>>> sorted([(fe.frame.name,fe.name) for fe in fn.fes('sound',r'(?i)make_noise')])
[('Cause_to_make_noise', 'Sound_maker'),
 ('Make_noise', 'Sound'),
 ('Make_noise', 'Sound_source')]
>>> sorted(set(fe.name for fe in fn.fes('^sound')))
['Sound', 'Sound_maker', 'Sound_source']
>>> len(fn.fes('^sound$'))
2
",set,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> fn.fes('Noise_maker')
[]
>>> sorted([(fe.frame.name,fe.name) for fe in fn.fes('sound')])
[('Cause_to_make_noise', 'Sound_maker'), ('Make_noise', 'Sound'),
 ('Make_noise', 'Sound_source'), ('Sound_movement', 'Location_of_sound_source'),
 ('Sound_movement', 'Sound'), ('Sound_movement', 'Sound_source'),
 ('Sounds', 'Component_sound'), ('Sounds', 'Location_of_sound_source'),
 ('Sounds', 'Sound_source'), ('Vocalizations', 'Location_of_sound_source'),
 ('Vocalizations', 'Sound_source')]
>>> sorted([(fe.frame.name,fe.name) for fe in fn.fes('sound',r'(?i)make_noise')])
[('Cause_to_make_noise', 'Sound_maker'),
 ('Make_noise', 'Sound'),
 ('Make_noise', 'Sound_source')]
>>> sorted(set(fe.name for fe in fn.fes('^sound')))
['Sound', 'Sound_maker', 'Sound_source']
>>> len(fn.fes('^sound$'))
2
",len,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> f = fn.frame(256)
>>> f.name
'Medical_specialties'
>>> f = fn.frame('Medical_specialties')
>>> f.ID
256
>>> # ensure non-ASCII character in definition doesn't trigger an encoding error:
>>> fn.frame('Imposing_obligation')
frame (1494): Imposing_obligation...
",fn.frame,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> f = fn.frame_by_id(256)
>>> f.ID
256
>>> f.name
'Medical_specialties'
>>> f.definition
""This frame includes words that name ...""
",fn.frame_by_id,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> f = fn.frame_by_name('Medical_specialties')
>>> f.ID
256
>>> f.name
'Medical_specialties'
>>> f.definition
""This frame includes words that name ...""
",fn.frame_by_name,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> frts = sorted(fn.frame_relation_types(), key=itemgetter('ID'))
>>> isinstance(frts, list)
True
>>> len(frts) in (9, 10)    # FN 1.5 and 1.7, resp.
True
>>> PrettyDict(frts[0], breakLines=True)
{'ID': 1,
 '_type': 'framerelationtype',
 'frameRelations': [ Child=Change_of_consistency>,  Child=Rotting>, ...],
 'name': 'Inheritance',
 'subFrameName': 'Child',
 'superFrameName': 'Parent'}
",sorted,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> frts = sorted(fn.frame_relation_types(), key=itemgetter('ID'))
>>> isinstance(frts, list)
True
>>> len(frts) in (9, 10)    # FN 1.5 and 1.7, resp.
True
>>> PrettyDict(frts[0], breakLines=True)
{'ID': 1,
 '_type': 'framerelationtype',
 'frameRelations': [ Child=Change_of_consistency>,  Child=Rotting>, ...],
 'name': 'Inheritance',
 'subFrameName': 'Child',
 'superFrameName': 'Parent'}
",fn.frame_relation_types,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> frts = sorted(fn.frame_relation_types(), key=itemgetter('ID'))
>>> isinstance(frts, list)
True
>>> len(frts) in (9, 10)    # FN 1.5 and 1.7, resp.
True
>>> PrettyDict(frts[0], breakLines=True)
{'ID': 1,
 '_type': 'framerelationtype',
 'frameRelations': [ Child=Change_of_consistency>,  Child=Rotting>, ...],
 'name': 'Inheritance',
 'subFrameName': 'Child',
 'superFrameName': 'Parent'}
",itemgetter,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> frts = sorted(fn.frame_relation_types(), key=itemgetter('ID'))
>>> isinstance(frts, list)
True
>>> len(frts) in (9, 10)    # FN 1.5 and 1.7, resp.
True
>>> PrettyDict(frts[0], breakLines=True)
{'ID': 1,
 '_type': 'framerelationtype',
 'frameRelations': [ Child=Change_of_consistency>,  Child=Rotting>, ...],
 'name': 'Inheritance',
 'subFrameName': 'Child',
 'superFrameName': 'Parent'}
",isinstance,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> frts = sorted(fn.frame_relation_types(), key=itemgetter('ID'))
>>> isinstance(frts, list)
True
>>> len(frts) in (9, 10)    # FN 1.5 and 1.7, resp.
True
>>> PrettyDict(frts[0], breakLines=True)
{'ID': 1,
 '_type': 'framerelationtype',
 'frameRelations': [ Child=Change_of_consistency>,  Child=Rotting>, ...],
 'name': 'Inheritance',
 'subFrameName': 'Child',
 'superFrameName': 'Parent'}
",len,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> frts = sorted(fn.frame_relation_types(), key=itemgetter('ID'))
>>> isinstance(frts, list)
True
>>> len(frts) in (9, 10)    # FN 1.5 and 1.7, resp.
True
>>> PrettyDict(frts[0], breakLines=True)
{'ID': 1,
 '_type': 'framerelationtype',
 'frameRelations': [ Child=Change_of_consistency>,  Child=Rotting>, ...],
 'name': 'Inheritance',
 'subFrameName': 'Child',
 'superFrameName': 'Parent'}
",PrettyDict,__init__,nltk\nltk\corpus\reader\framenet.py,True
">>> from nltk.corpus import framenet as fn
>>> frels = fn.frame_relations()
>>> isinstance(frels, list)
True
>>> len(frels) in (1676, 2070)  # FN 1.5 and 1.7, resp.
True
>>> PrettyList(fn.frame_relations('Cooking_creation'), maxReprSize=0, breakLines=True)
[ Child=Cooking_creation>,
  Child=Cooking_creation>,
  ReferringEntry=Cooking_creation>]
>>> PrettyList(fn.frame_relations(274), breakLines=True)
[ Child=Dodging>,
  Child=Evading>, ...]
>>> PrettyList(fn.frame_relations(fn.frame('Cooking_creation')), breakLines=True)
[ Child=Cooking_creation>,
  Child=Cooking_creation>, ...]
>>> PrettyList(fn.frame_relations('Cooking_creation', type='Inheritance'))
[ Child=Cooking_creation>]
>>> PrettyList(fn.frame_relations('Cooking_creation', 'Apply_heat'), breakLines=True)
[ Child=Cooking_creation>,
 ReferringEntry=Cooking_creation>]
",fn.frame_relations,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> frels = fn.frame_relations()
>>> isinstance(frels, list)
True
>>> len(frels) in (1676, 2070)  # FN 1.5 and 1.7, resp.
True
>>> PrettyList(fn.frame_relations('Cooking_creation'), maxReprSize=0, breakLines=True)
[ Child=Cooking_creation>,
  Child=Cooking_creation>,
  ReferringEntry=Cooking_creation>]
>>> PrettyList(fn.frame_relations(274), breakLines=True)
[ Child=Dodging>,
  Child=Evading>, ...]
>>> PrettyList(fn.frame_relations(fn.frame('Cooking_creation')), breakLines=True)
[ Child=Cooking_creation>,
  Child=Cooking_creation>, ...]
>>> PrettyList(fn.frame_relations('Cooking_creation', type='Inheritance'))
[ Child=Cooking_creation>]
>>> PrettyList(fn.frame_relations('Cooking_creation', 'Apply_heat'), breakLines=True)
[ Child=Cooking_creation>,
 ReferringEntry=Cooking_creation>]
",isinstance,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> frels = fn.frame_relations()
>>> isinstance(frels, list)
True
>>> len(frels) in (1676, 2070)  # FN 1.5 and 1.7, resp.
True
>>> PrettyList(fn.frame_relations('Cooking_creation'), maxReprSize=0, breakLines=True)
[ Child=Cooking_creation>,
  Child=Cooking_creation>,
  ReferringEntry=Cooking_creation>]
>>> PrettyList(fn.frame_relations(274), breakLines=True)
[ Child=Dodging>,
  Child=Evading>, ...]
>>> PrettyList(fn.frame_relations(fn.frame('Cooking_creation')), breakLines=True)
[ Child=Cooking_creation>,
  Child=Cooking_creation>, ...]
>>> PrettyList(fn.frame_relations('Cooking_creation', type='Inheritance'))
[ Child=Cooking_creation>]
>>> PrettyList(fn.frame_relations('Cooking_creation', 'Apply_heat'), breakLines=True)
[ Child=Cooking_creation>,
 ReferringEntry=Cooking_creation>]
",len,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> frels = fn.frame_relations()
>>> isinstance(frels, list)
True
>>> len(frels) in (1676, 2070)  # FN 1.5 and 1.7, resp.
True
>>> PrettyList(fn.frame_relations('Cooking_creation'), maxReprSize=0, breakLines=True)
[ Child=Cooking_creation>,
  Child=Cooking_creation>,
  ReferringEntry=Cooking_creation>]
>>> PrettyList(fn.frame_relations(274), breakLines=True)
[ Child=Dodging>,
  Child=Evading>, ...]
>>> PrettyList(fn.frame_relations(fn.frame('Cooking_creation')), breakLines=True)
[ Child=Cooking_creation>,
  Child=Cooking_creation>, ...]
>>> PrettyList(fn.frame_relations('Cooking_creation', type='Inheritance'))
[ Child=Cooking_creation>]
>>> PrettyList(fn.frame_relations('Cooking_creation', 'Apply_heat'), breakLines=True)
[ Child=Cooking_creation>,
 ReferringEntry=Cooking_creation>]
",PrettyList,__init__,nltk\nltk\corpus\reader\framenet.py,True
">>> from nltk.corpus import framenet as fn
>>> frels = fn.frame_relations()
>>> isinstance(frels, list)
True
>>> len(frels) in (1676, 2070)  # FN 1.5 and 1.7, resp.
True
>>> PrettyList(fn.frame_relations('Cooking_creation'), maxReprSize=0, breakLines=True)
[ Child=Cooking_creation>,
  Child=Cooking_creation>,
  ReferringEntry=Cooking_creation>]
>>> PrettyList(fn.frame_relations(274), breakLines=True)
[ Child=Dodging>,
  Child=Evading>, ...]
>>> PrettyList(fn.frame_relations(fn.frame('Cooking_creation')), breakLines=True)
[ Child=Cooking_creation>,
  Child=Cooking_creation>, ...]
>>> PrettyList(fn.frame_relations('Cooking_creation', type='Inheritance'))
[ Child=Cooking_creation>]
>>> PrettyList(fn.frame_relations('Cooking_creation', 'Apply_heat'), breakLines=True)
[ Child=Cooking_creation>,
 ReferringEntry=Cooking_creation>]
",fn.frame,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> len(fn.frames()) in (1019, 1221)    # FN 1.5 and 1.7, resp.
True
>>> x = PrettyList(fn.frames(r'(?i)crim'), maxReprSize=0, breakLines=True)
>>> x.sort(key=itemgetter('ID'))
>>> x
[,
 ,
 ,
 ]
",len,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> len(fn.frames()) in (1019, 1221)    # FN 1.5 and 1.7, resp.
True
>>> x = PrettyList(fn.frames(r'(?i)crim'), maxReprSize=0, breakLines=True)
>>> x.sort(key=itemgetter('ID'))
>>> x
[,
 ,
 ,
 ]
",fn.frames,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> len(fn.frames()) in (1019, 1221)    # FN 1.5 and 1.7, resp.
True
>>> x = PrettyList(fn.frames(r'(?i)crim'), maxReprSize=0, breakLines=True)
>>> x.sort(key=itemgetter('ID'))
>>> x
[,
 ,
 ,
 ]
",PrettyList,__init__,nltk\nltk\corpus\reader\framenet.py,True
">>> from nltk.corpus import framenet as fn
>>> len(fn.frames()) in (1019, 1221)    # FN 1.5 and 1.7, resp.
True
>>> x = PrettyList(fn.frames(r'(?i)crim'), maxReprSize=0, breakLines=True)
>>> x.sort(key=itemgetter('ID'))
>>> x
[,
 ,
 ,
 ]
",x.sort,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> len(fn.frames()) in (1019, 1221)    # FN 1.5 and 1.7, resp.
True
>>> x = PrettyList(fn.frames(r'(?i)crim'), maxReprSize=0, breakLines=True)
>>> x.sort(key=itemgetter('ID'))
>>> x
[,
 ,
 ,
 ]
",itemgetter,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> from nltk.corpus.reader.framenet import PrettyList
>>> PrettyList(sorted(fn.frames_by_lemma(r'(?i)a little'), key=itemgetter('ID'))) 
[, ]
",PrettyList,__init__,nltk\nltk\corpus\reader\framenet.py,True
">>> from nltk.corpus import framenet as fn
>>> from nltk.corpus.reader.framenet import PrettyList
>>> PrettyList(sorted(fn.frames_by_lemma(r'(?i)a little'), key=itemgetter('ID'))) 
[, ]
",sorted,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> from nltk.corpus.reader.framenet import PrettyList
>>> PrettyList(sorted(fn.frames_by_lemma(r'(?i)a little'), key=itemgetter('ID'))) 
[, ]
",fn.frames_by_lemma,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> from nltk.corpus.reader.framenet import PrettyList
>>> PrettyList(sorted(fn.frames_by_lemma(r'(?i)a little'), key=itemgetter('ID'))) 
[, ]
",itemgetter,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> fn.lu(256).name
'foresee.v'
>>> fn.lu(256).definition
'COD: be aware of beforehand; predict.'
>>> fn.lu(256).frame.name
'Expectation'
>>> pprint(list(map(PrettyDict, fn.lu(256).lexemes)))
[{'POS': 'V', 'breakBefore': 'false', 'headword': 'false', 'name': 'foresee', 'order': 1}]
",fn.lu,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> fn.lu(256).name
'foresee.v'
>>> fn.lu(256).definition
'COD: be aware of beforehand; predict.'
>>> fn.lu(256).frame.name
'Expectation'
>>> pprint(list(map(PrettyDict, fn.lu(256).lexemes)))
[{'POS': 'V', 'breakBefore': 'false', 'headword': 'false', 'name': 'foresee', 'order': 1}]
",pprint,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> fn.lu(256).name
'foresee.v'
>>> fn.lu(256).definition
'COD: be aware of beforehand; predict.'
>>> fn.lu(256).frame.name
'Expectation'
>>> pprint(list(map(PrettyDict, fn.lu(256).lexemes)))
[{'POS': 'V', 'breakBefore': 'false', 'headword': 'false', 'name': 'foresee', 'order': 1}]
",list,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> fn.lu(256).name
'foresee.v'
>>> fn.lu(256).definition
'COD: be aware of beforehand; predict.'
>>> fn.lu(256).frame.name
'Expectation'
>>> pprint(list(map(PrettyDict, fn.lu(256).lexemes)))
[{'POS': 'V', 'breakBefore': 'false', 'headword': 'false', 'name': 'foresee', 'order': 1}]
",map,N/A,N/A,N/A
">>> fn.lu(227).exemplars[23]
exemplar sentence (352962):
[sentNo] 0
[aPos] 59699508

[LU] (227) guess.v in Coming_to_believe

[frame] (23) Coming_to_believe

[annotationSet] 2 annotation sets

[POS] 18 tags

[POS_tagset] BNC

[GF] 3 relations

[PT] 3 phrases

[Other] 1 entry

[text] + [Target] + [FE]

When he was inside the house , Culley noticed the characteristic
                                              ------------------
                                              Content

he would n't have guessed at .
--                ******* --
Co                        C1 [Evidence:INI]
 (Co=Cognizer, C1=Content)

",fn.lu,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> lu = PrettyDict(fn.lu_basic(256), breakLines=True)
>>> # ellipses account for differences between FN 1.5 and 1.7
>>> lu 
{'ID': 256,
 'POS': 'V',
 'URL': 'https://framenet2.icsi.berkeley.edu/fnReports/data/lu/lu256.xml',
 '_type': 'lu',
 'cBy': ...,
 'cDate': '02/08/2001 01:27:50 PST Thu',
 'definition': 'COD: be aware of beforehand; predict.',
 'definitionMarkup': 'COD: be aware of beforehand; predict.',
 'frame': ,
 'lemmaID': 15082,
 'lexemes': [{'POS': 'V', 'breakBefore': 'false', 'headword': 'false', 'name': 'foresee', 'order': 1}],
 'name': 'foresee.v',
 'semTypes': [],
 'sentenceCount': {'annotated': ..., 'total': ...},
 'status': 'FN1_Sent'}
",PrettyDict,__init__,nltk\nltk\corpus\reader\framenet.py,True
">>> from nltk.corpus import framenet as fn
>>> lu = PrettyDict(fn.lu_basic(256), breakLines=True)
>>> # ellipses account for differences between FN 1.5 and 1.7
>>> lu 
{'ID': 256,
 'POS': 'V',
 'URL': 'https://framenet2.icsi.berkeley.edu/fnReports/data/lu/lu256.xml',
 '_type': 'lu',
 'cBy': ...,
 'cDate': '02/08/2001 01:27:50 PST Thu',
 'definition': 'COD: be aware of beforehand; predict.',
 'definitionMarkup': 'COD: be aware of beforehand; predict.',
 'frame': ,
 'lemmaID': 15082,
 'lexemes': [{'POS': 'V', 'breakBefore': 'false', 'headword': 'false', 'name': 'foresee', 'order': 1}],
 'name': 'foresee.v',
 'semTypes': [],
 'sentenceCount': {'annotated': ..., 'total': ...},
 'status': 'FN1_Sent'}
",fn.lu_basic,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> len(fn.lus()) in (11829, 13572) # FN 1.5 and 1.7, resp.
True
>>> PrettyList(sorted(fn.lus(r'(?i)a little'), key=itemgetter('ID')), maxReprSize=0, breakLines=True)
[,
 ,
 ]
>>> PrettyList(sorted(fn.lus(r'interest', r'(?i)stimulus'), key=itemgetter('ID')))
[, ]
",len,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> len(fn.lus()) in (11829, 13572) # FN 1.5 and 1.7, resp.
True
>>> PrettyList(sorted(fn.lus(r'(?i)a little'), key=itemgetter('ID')), maxReprSize=0, breakLines=True)
[,
 ,
 ]
>>> PrettyList(sorted(fn.lus(r'interest', r'(?i)stimulus'), key=itemgetter('ID')))
[, ]
",fn.lus,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> len(fn.lus()) in (11829, 13572) # FN 1.5 and 1.7, resp.
True
>>> PrettyList(sorted(fn.lus(r'(?i)a little'), key=itemgetter('ID')), maxReprSize=0, breakLines=True)
[,
 ,
 ]
>>> PrettyList(sorted(fn.lus(r'interest', r'(?i)stimulus'), key=itemgetter('ID')))
[, ]
",PrettyList,__init__,nltk\nltk\corpus\reader\framenet.py,True
">>> from nltk.corpus import framenet as fn
>>> len(fn.lus()) in (11829, 13572) # FN 1.5 and 1.7, resp.
True
>>> PrettyList(sorted(fn.lus(r'(?i)a little'), key=itemgetter('ID')), maxReprSize=0, breakLines=True)
[,
 ,
 ]
>>> PrettyList(sorted(fn.lus(r'interest', r'(?i)stimulus'), key=itemgetter('ID')))
[, ]
",sorted,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> len(fn.lus()) in (11829, 13572) # FN 1.5 and 1.7, resp.
True
>>> PrettyList(sorted(fn.lus(r'(?i)a little'), key=itemgetter('ID')), maxReprSize=0, breakLines=True)
[,
 ,
 ]
>>> PrettyList(sorted(fn.lus(r'interest', r'(?i)stimulus'), key=itemgetter('ID')))
[, ]
",itemgetter,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> x = sum(1 for f in fn.frames() for fe in f.FE.values() if fe.semType)
>>> fn.propagate_semtypes()
>>> y = sum(1 for f in fn.frames() for fe in f.FE.values() if fe.semType)
>>> y-x > 1000
True
",sum,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> x = sum(1 for f in fn.frames() for fe in f.FE.values() if fe.semType)
>>> fn.propagate_semtypes()
>>> y = sum(1 for f in fn.frames() for fe in f.FE.values() if fe.semType)
>>> y-x > 1000
True
",fn.frames,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> x = sum(1 for f in fn.frames() for fe in f.FE.values() if fe.semType)
>>> fn.propagate_semtypes()
>>> y = sum(1 for f in fn.frames() for fe in f.FE.values() if fe.semType)
>>> y-x > 1000
True
",FE.values,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> x = sum(1 for f in fn.frames() for fe in f.FE.values() if fe.semType)
>>> fn.propagate_semtypes()
>>> y = sum(1 for f in fn.frames() for fe in f.FE.values() if fe.semType)
>>> y-x > 1000
True
",fn.propagate_semtypes,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> fn.semtype(233).name
'Temperature'
>>> fn.semtype(233).abbrev
'Temp'
>>> fn.semtype('Temperature').ID
233
",fn.semtype,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> stypes = fn.semtypes()
>>> len(stypes) in (73, 109) # FN 1.5 and 1.7, resp.
True
>>> sorted(stypes[0].keys())
['ID', '_type', 'abbrev', 'definition', 'definitionMarkup', 'name', 'rootType', 'subTypes', 'superType']
",fn.semtypes,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> stypes = fn.semtypes()
>>> len(stypes) in (73, 109) # FN 1.5 and 1.7, resp.
True
>>> sorted(stypes[0].keys())
['ID', '_type', 'abbrev', 'definition', 'definitionMarkup', 'name', 'rootType', 'subTypes', 'superType']
",len,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> stypes = fn.semtypes()
>>> len(stypes) in (73, 109) # FN 1.5 and 1.7, resp.
True
>>> sorted(stypes[0].keys())
['ID', '_type', 'abbrev', 'definition', 'definitionMarkup', 'name', 'rootType', 'subTypes', 'superType']
",sorted,N/A,N/A,N/A
">>> from nltk.corpus import framenet as fn
>>> stypes = fn.semtypes()
>>> len(stypes) in (73, 109) # FN 1.5 and 1.7, resp.
True
>>> sorted(stypes[0].keys())
['ID', '_type', 'abbrev', 'definition', 'definitionMarkup', 'name', 'rootType', 'subTypes', 'superType']
",keys,N/A,N/A,N/A
">>> from nltk.corpus.util import LazyCorpusLoader
>>> knbc = LazyCorpusLoader(
...     'knbc/corpus1',
...     KNBCorpusReader,
...     r'.*/KN.*',
...     encoding='euc-jp',
... )
",LazyCorpusLoader,__init__,nltk\nltk\corpus\util.py,True
">>> len(knbc.sents()[0])
9
",len,N/A,N/A,N/A
">>> len(knbc.sents()[0])
9
",knbc.sents,N/A,N/A,N/A
">>> from nltk.corpus import opinion_lexicon
>>> opinion_lexicon.words()
['2-faced', '2-faces', 'abnormal', 'abolish', ...]
",opinion_lexicon.words,N/A,N/A,N/A
">>> opinion_lexicon.negative()
['2-faced', '2-faces', 'abnormal', 'abolish', ...]
",opinion_lexicon.negative,N/A,N/A,N/A
">>> opinion_lexicon.words()[0:10]
['2-faced', '2-faces', 'abnormal', 'abolish', 'abominable', 'abominably',
'abominate', 'abomination', 'abort', 'aborted']
>>> sorted(opinion_lexicon.words())[0:10]
['2-faced', '2-faces', 'a+', 'abnormal', 'abolish', 'abominable', 'abominably',
'abominate', 'abomination', 'abort']
",opinion_lexicon.words,N/A,N/A,N/A
">>> opinion_lexicon.words()[0:10]
['2-faced', '2-faces', 'abnormal', 'abolish', 'abominable', 'abominably',
'abominate', 'abomination', 'abort', 'aborted']
>>> sorted(opinion_lexicon.words())[0:10]
['2-faced', '2-faces', 'a+', 'abnormal', 'abolish', 'abominable', 'abominably',
'abominate', 'abomination', 'abort']
",sorted,N/A,N/A,N/A
">>> from nltk.corpus import pros_cons
>>> pros_cons.sents(categories='Cons')
[['East', 'batteries', '!', 'On', '-', 'off', 'switch', 'too', 'easy',
'to', 'maneuver', '.'], ['Eats', '...', 'no', ',', 'GULPS', 'batteries'],
...]
>>> pros_cons.words('IntegratedPros.txt')
['Easy', 'to', 'use', ',', 'economical', '!', ...]
",pros_cons.sents,N/A,N/A,N/A
">>> from nltk.corpus import pros_cons
>>> pros_cons.sents(categories='Cons')
[['East', 'batteries', '!', 'On', '-', 'off', 'switch', 'too', 'easy',
'to', 'maneuver', '.'], ['Eats', '...', 'no', ',', 'GULPS', 'batteries'],
...]
>>> pros_cons.words('IntegratedPros.txt')
['Easy', 'to', 'use', ',', 'economical', '!', ...]
",pros_cons.words,N/A,N/A,N/A
">>> from nltk.corpus import product_reviews_1
>>> camera_reviews = product_reviews_1.reviews('Canon_G3.txt')
>>> review = camera_reviews[0]
>>> review.sents()[0]
['i', 'recently', 'purchased', 'the', 'canon', 'powershot', 'g3', 'and', 'am',
'extremely', 'satisfied', 'with', 'the', 'purchase', '.']
>>> review.features()
[('canon powershot g3', '+3'), ('use', '+2'), ('picture', '+2'),
('picture quality', '+1'), ('picture quality', '+1'), ('camera', '+2'),
('use', '+2'), ('feature', '+1'), ('picture quality', '+3'), ('use', '+1'),
('option', '+1')]
",product_reviews_1.reviews,N/A,N/A,N/A
">>> from nltk.corpus import product_reviews_1
>>> camera_reviews = product_reviews_1.reviews('Canon_G3.txt')
>>> review = camera_reviews[0]
>>> review.sents()[0]
['i', 'recently', 'purchased', 'the', 'canon', 'powershot', 'g3', 'and', 'am',
'extremely', 'satisfied', 'with', 'the', 'purchase', '.']
>>> review.features()
[('canon powershot g3', '+3'), ('use', '+2'), ('picture', '+2'),
('picture quality', '+1'), ('picture quality', '+1'), ('camera', '+2'),
('use', '+2'), ('feature', '+1'), ('picture quality', '+3'), ('use', '+1'),
('option', '+1')]
",review.sents,N/A,N/A,N/A
">>> from nltk.corpus import product_reviews_1
>>> camera_reviews = product_reviews_1.reviews('Canon_G3.txt')
>>> review = camera_reviews[0]
>>> review.sents()[0]
['i', 'recently', 'purchased', 'the', 'canon', 'powershot', 'g3', 'and', 'am',
'extremely', 'satisfied', 'with', 'the', 'purchase', '.']
>>> review.features()
[('canon powershot g3', '+3'), ('use', '+2'), ('picture', '+2'),
('picture quality', '+1'), ('picture quality', '+1'), ('camera', '+2'),
('use', '+2'), ('feature', '+1'), ('picture quality', '+3'), ('use', '+1'),
('option', '+1')]
",review.features,N/A,N/A,N/A
">>> product_reviews_1.features('Canon_G3.txt')
[('canon powershot g3', '+3'), ('use', '+2'), ...]
",product_reviews_1.features,N/A,N/A,N/A
">>> n_reviews = len([(feat,score) for (feat,score) in product_reviews_1.features('Canon_G3.txt') if feat=='picture'])
>>> tot = sum([int(score) for (feat,score) in product_reviews_1.features('Canon_G3.txt') if feat=='picture'])
>>> mean = tot / n_reviews
>>> print(n_reviews, tot, mean)
15 24 1.6
",len,N/A,N/A,N/A
">>> n_reviews = len([(feat,score) for (feat,score) in product_reviews_1.features('Canon_G3.txt') if feat=='picture'])
>>> tot = sum([int(score) for (feat,score) in product_reviews_1.features('Canon_G3.txt') if feat=='picture'])
>>> mean = tot / n_reviews
>>> print(n_reviews, tot, mean)
15 24 1.6
",product_reviews_1.features,N/A,N/A,N/A
">>> n_reviews = len([(feat,score) for (feat,score) in product_reviews_1.features('Canon_G3.txt') if feat=='picture'])
>>> tot = sum([int(score) for (feat,score) in product_reviews_1.features('Canon_G3.txt') if feat=='picture'])
>>> mean = tot / n_reviews
>>> print(n_reviews, tot, mean)
15 24 1.6
",sum,N/A,N/A,N/A
">>> n_reviews = len([(feat,score) for (feat,score) in product_reviews_1.features('Canon_G3.txt') if feat=='picture'])
>>> tot = sum([int(score) for (feat,score) in product_reviews_1.features('Canon_G3.txt') if feat=='picture'])
>>> mean = tot / n_reviews
>>> print(n_reviews, tot, mean)
15 24 1.6
",int,N/A,N/A,N/A
">>> n_reviews = len([(feat,score) for (feat,score) in product_reviews_1.features('Canon_G3.txt') if feat=='picture'])
>>> tot = sum([int(score) for (feat,score) in product_reviews_1.features('Canon_G3.txt') if feat=='picture'])
>>> mean = tot / n_reviews
>>> print(n_reviews, tot, mean)
15 24 1.6
",print,N/A,N/A,N/A
">>> from nltk.corpus import sentiwordnet as swn
>>> print(swn.senti_synset('breakdown.n.03'))

>>> list(swn.senti_synsets('slow'))
[SentiSynset('decelerate.v.01'), SentiSynset('slow.v.02'),
SentiSynset('slow.v.03'), SentiSynset('slow.a.01'),
SentiSynset('slow.a.02'), SentiSynset('dense.s.04'),
SentiSynset('slow.a.04'), SentiSynset('boring.s.01'),
SentiSynset('dull.s.08'), SentiSynset('slowly.r.01'),
SentiSynset('behind.r.03')]
>>> happy = swn.senti_synsets('happy', 'a')
>>> happy0 = list(happy)[0]
>>> happy0.pos_score()
0.875
>>> happy0.neg_score()
0.0
>>> happy0.obj_score()
0.125
",print,N/A,N/A,N/A
">>> from nltk.corpus import sentiwordnet as swn
>>> print(swn.senti_synset('breakdown.n.03'))

>>> list(swn.senti_synsets('slow'))
[SentiSynset('decelerate.v.01'), SentiSynset('slow.v.02'),
SentiSynset('slow.v.03'), SentiSynset('slow.a.01'),
SentiSynset('slow.a.02'), SentiSynset('dense.s.04'),
SentiSynset('slow.a.04'), SentiSynset('boring.s.01'),
SentiSynset('dull.s.08'), SentiSynset('slowly.r.01'),
SentiSynset('behind.r.03')]
>>> happy = swn.senti_synsets('happy', 'a')
>>> happy0 = list(happy)[0]
>>> happy0.pos_score()
0.875
>>> happy0.neg_score()
0.0
>>> happy0.obj_score()
0.125
",swn.senti_synset,N/A,N/A,N/A
">>> from nltk.corpus import sentiwordnet as swn
>>> print(swn.senti_synset('breakdown.n.03'))

>>> list(swn.senti_synsets('slow'))
[SentiSynset('decelerate.v.01'), SentiSynset('slow.v.02'),
SentiSynset('slow.v.03'), SentiSynset('slow.a.01'),
SentiSynset('slow.a.02'), SentiSynset('dense.s.04'),
SentiSynset('slow.a.04'), SentiSynset('boring.s.01'),
SentiSynset('dull.s.08'), SentiSynset('slowly.r.01'),
SentiSynset('behind.r.03')]
>>> happy = swn.senti_synsets('happy', 'a')
>>> happy0 = list(happy)[0]
>>> happy0.pos_score()
0.875
>>> happy0.neg_score()
0.0
>>> happy0.obj_score()
0.125
",list,N/A,N/A,N/A
">>> from nltk.corpus import sentiwordnet as swn
>>> print(swn.senti_synset('breakdown.n.03'))

>>> list(swn.senti_synsets('slow'))
[SentiSynset('decelerate.v.01'), SentiSynset('slow.v.02'),
SentiSynset('slow.v.03'), SentiSynset('slow.a.01'),
SentiSynset('slow.a.02'), SentiSynset('dense.s.04'),
SentiSynset('slow.a.04'), SentiSynset('boring.s.01'),
SentiSynset('dull.s.08'), SentiSynset('slowly.r.01'),
SentiSynset('behind.r.03')]
>>> happy = swn.senti_synsets('happy', 'a')
>>> happy0 = list(happy)[0]
>>> happy0.pos_score()
0.875
>>> happy0.neg_score()
0.0
>>> happy0.obj_score()
0.125
",swn.senti_synsets,N/A,N/A,N/A
">>> from nltk.corpus import sentiwordnet as swn
>>> print(swn.senti_synset('breakdown.n.03'))

>>> list(swn.senti_synsets('slow'))
[SentiSynset('decelerate.v.01'), SentiSynset('slow.v.02'),
SentiSynset('slow.v.03'), SentiSynset('slow.a.01'),
SentiSynset('slow.a.02'), SentiSynset('dense.s.04'),
SentiSynset('slow.a.04'), SentiSynset('boring.s.01'),
SentiSynset('dull.s.08'), SentiSynset('slowly.r.01'),
SentiSynset('behind.r.03')]
>>> happy = swn.senti_synsets('happy', 'a')
>>> happy0 = list(happy)[0]
>>> happy0.pos_score()
0.875
>>> happy0.neg_score()
0.0
>>> happy0.obj_score()
0.125
",SentiSynset,__init__,nltk\nltk\corpus\reader\sentiwordnet.py,True
">>> from nltk.corpus import sentiwordnet as swn
>>> print(swn.senti_synset('breakdown.n.03'))

>>> list(swn.senti_synsets('slow'))
[SentiSynset('decelerate.v.01'), SentiSynset('slow.v.02'),
SentiSynset('slow.v.03'), SentiSynset('slow.a.01'),
SentiSynset('slow.a.02'), SentiSynset('dense.s.04'),
SentiSynset('slow.a.04'), SentiSynset('boring.s.01'),
SentiSynset('dull.s.08'), SentiSynset('slowly.r.01'),
SentiSynset('behind.r.03')]
>>> happy = swn.senti_synsets('happy', 'a')
>>> happy0 = list(happy)[0]
>>> happy0.pos_score()
0.875
>>> happy0.neg_score()
0.0
>>> happy0.obj_score()
0.125
",happy0.pos_score,N/A,N/A,N/A
">>> from nltk.corpus import sentiwordnet as swn
>>> print(swn.senti_synset('breakdown.n.03'))

>>> list(swn.senti_synsets('slow'))
[SentiSynset('decelerate.v.01'), SentiSynset('slow.v.02'),
SentiSynset('slow.v.03'), SentiSynset('slow.a.01'),
SentiSynset('slow.a.02'), SentiSynset('dense.s.04'),
SentiSynset('slow.a.04'), SentiSynset('boring.s.01'),
SentiSynset('dull.s.08'), SentiSynset('slowly.r.01'),
SentiSynset('behind.r.03')]
>>> happy = swn.senti_synsets('happy', 'a')
>>> happy0 = list(happy)[0]
>>> happy0.pos_score()
0.875
>>> happy0.neg_score()
0.0
>>> happy0.obj_score()
0.125
",happy0.neg_score,N/A,N/A,N/A
">>> from nltk.corpus import sentiwordnet as swn
>>> print(swn.senti_synset('breakdown.n.03'))

>>> list(swn.senti_synsets('slow'))
[SentiSynset('decelerate.v.01'), SentiSynset('slow.v.02'),
SentiSynset('slow.v.03'), SentiSynset('slow.a.01'),
SentiSynset('slow.a.02'), SentiSynset('dense.s.04'),
SentiSynset('slow.a.04'), SentiSynset('boring.s.01'),
SentiSynset('dull.s.08'), SentiSynset('slowly.r.01'),
SentiSynset('behind.r.03')]
>>> happy = swn.senti_synsets('happy', 'a')
>>> happy0 = list(happy)[0]
>>> happy0.pos_score()
0.875
>>> happy0.neg_score()
0.0
>>> happy0.obj_score()
0.125
",happy0.obj_score,N/A,N/A,N/A
">>> itemid = 'dr1-fvmh0/sx206'
>>> spkrid , sentid = itemid.split('/')
>>> spkrid
'dr1-fvmh0'
",itemid.split,N/A,N/A,N/A
"from nltk.corpus import TwitterCorpusReader
reader = TwitterCorpusReader(root='/path/to/twitter-files', '.*\.json')
",TwitterCorpusReader,__init__,nltk\nltk\corpus\reader\twitter.py,True
"root = os.environ['TWITTER']
reader = TwitterCorpusReader(root, '.*\.json')
",TwitterCorpusReader,__init__,nltk\nltk\corpus\reader\twitter.py,True
"import json
for tweet in reader.docs():
    print(json.dumps(tweet, indent=1, sort_keys=True))
",reader.docs,N/A,N/A,N/A
"import json
for tweet in reader.docs():
    print(json.dumps(tweet, indent=1, sort_keys=True))
",print,N/A,N/A,N/A
"import json
for tweet in reader.docs():
    print(json.dumps(tweet, indent=1, sort_keys=True))
",json.dumps,N/A,N/A,N/A
">>> from nltk.corpus.reader.util import PickleCorpusView
>>> from nltk.util import LazyMap
>>> feature_corpus = LazyMap(detect_features, corpus) 
>>> PickleCorpusView.write(feature_corpus, some_fileid)  
>>> pcv = PickleCorpusView(some_fileid) 
",LazyMap,__init__,nltk\nltk\collections.py,True
">>> from nltk.corpus.reader.util import PickleCorpusView
>>> from nltk.util import LazyMap
>>> feature_corpus = LazyMap(detect_features, corpus) 
>>> PickleCorpusView.write(feature_corpus, some_fileid)  
>>> pcv = PickleCorpusView(some_fileid) 
",PickleCorpusView.write,PickleCorpusView.write,nltk\nltk\corpus\reader\util.py,True
">>> from nltk.corpus.reader.util import PickleCorpusView
>>> from nltk.util import LazyMap
>>> feature_corpus = LazyMap(detect_features, corpus) 
>>> PickleCorpusView.write(feature_corpus, some_fileid)  
>>> pcv = PickleCorpusView(some_fileid) 
",PickleCorpusView,__init__,nltk\nltk\corpus\reader\util.py,True
">>> def simple_block_reader(stream):
...     return stream.readline().split()
",simple_block_reader,N/A,N/A,N/A
">>> def simple_block_reader(stream):
...     return stream.readline().split()
",stream.readline,N/A,N/A,N/A
">>> def simple_block_reader(stream):
...     return stream.readline().split()
",split,N/A,N/A,N/A
">>> from nltk.corpus import nonbreaking_prefixes as nbp
>>> nbp.words('en')[:10] == [u'A', u'B', u'C', u'D', u'E', u'F', u'G', u'H', u'I', u'J']
True
>>> nbp.words('ta')[:5] == [u'அ', u'ஆ', u'இ', u'ஈ', u'உ']
True
",nbp.words,N/A,N/A,N/A
">>> from nltk.corpus import perluniprops as pup
>>> pup.chars('Open_Punctuation')[:5] == [u'(', u'[', u'{', u'༺', u'༼']
True
>>> pup.chars('Currency_Symbol')[:5] == [u'$', u'¢', u'£', u'¤', u'¥']
True
>>> pup.available_categories
['Close_Punctuation', 'Currency_Symbol', 'IsAlnum', 'IsAlpha', 'IsLower', 'IsN', 'IsSc', 'IsSo', 'IsUpper', 'Line_Separator', 'Number', 'Open_Punctuation', 'Punctuation', 'Separator', 'Symbol']
",pup.chars,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import acyclic_depth_first as acyclic_tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(acyclic_tree(wn.synset('dog.n.01'), lambda s:s.hypernyms(),cut_mark='...'))
[Synset('dog.n.01'),
 [Synset('canine.n.02'),
  [Synset('carnivore.n.01'),
   [Synset('placental.n.01'),
    [Synset('mammal.n.01'),
     [Synset('vertebrate.n.01'),
      [Synset('chordate.n.01'),
       [Synset('animal.n.01'),
        [Synset('organism.n.01'),
         [Synset('living_thing.n.01'),
          [Synset('whole.n.02'),
           [Synset('object.n.01'),
            [Synset('physical_entity.n.01'),
             [Synset('entity.n.01')]]]]]]]]]]]]],
 [Synset('domestic_animal.n.01'), ""Cycle(Synset('animal.n.01'),-3,...)""]]
",pprint,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import acyclic_depth_first as acyclic_tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(acyclic_tree(wn.synset('dog.n.01'), lambda s:s.hypernyms(),cut_mark='...'))
[Synset('dog.n.01'),
 [Synset('canine.n.02'),
  [Synset('carnivore.n.01'),
   [Synset('placental.n.01'),
    [Synset('mammal.n.01'),
     [Synset('vertebrate.n.01'),
      [Synset('chordate.n.01'),
       [Synset('animal.n.01'),
        [Synset('organism.n.01'),
         [Synset('living_thing.n.01'),
          [Synset('whole.n.02'),
           [Synset('object.n.01'),
            [Synset('physical_entity.n.01'),
             [Synset('entity.n.01')]]]]]]]]]]]]],
 [Synset('domestic_animal.n.01'), ""Cycle(Synset('animal.n.01'),-3,...)""]]
",acyclic_tree,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import acyclic_depth_first as acyclic_tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(acyclic_tree(wn.synset('dog.n.01'), lambda s:s.hypernyms(),cut_mark='...'))
[Synset('dog.n.01'),
 [Synset('canine.n.02'),
  [Synset('carnivore.n.01'),
   [Synset('placental.n.01'),
    [Synset('mammal.n.01'),
     [Synset('vertebrate.n.01'),
      [Synset('chordate.n.01'),
       [Synset('animal.n.01'),
        [Synset('organism.n.01'),
         [Synset('living_thing.n.01'),
          [Synset('whole.n.02'),
           [Synset('object.n.01'),
            [Synset('physical_entity.n.01'),
             [Synset('entity.n.01')]]]]]]]]]]]]],
 [Synset('domestic_animal.n.01'), ""Cycle(Synset('animal.n.01'),-3,...)""]]
",wn.synset,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import acyclic_depth_first as acyclic_tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(acyclic_tree(wn.synset('dog.n.01'), lambda s:s.hypernyms(),cut_mark='...'))
[Synset('dog.n.01'),
 [Synset('canine.n.02'),
  [Synset('carnivore.n.01'),
   [Synset('placental.n.01'),
    [Synset('mammal.n.01'),
     [Synset('vertebrate.n.01'),
      [Synset('chordate.n.01'),
       [Synset('animal.n.01'),
        [Synset('organism.n.01'),
         [Synset('living_thing.n.01'),
          [Synset('whole.n.02'),
           [Synset('object.n.01'),
            [Synset('physical_entity.n.01'),
             [Synset('entity.n.01')]]]]]]]]]]]]],
 [Synset('domestic_animal.n.01'), ""Cycle(Synset('animal.n.01'),-3,...)""]]
",s.hypernyms,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import acyclic_depth_first as acyclic_tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(acyclic_tree(wn.synset('dog.n.01'), lambda s:s.hypernyms(),cut_mark='...'))
[Synset('dog.n.01'),
 [Synset('canine.n.02'),
  [Synset('carnivore.n.01'),
   [Synset('placental.n.01'),
    [Synset('mammal.n.01'),
     [Synset('vertebrate.n.01'),
      [Synset('chordate.n.01'),
       [Synset('animal.n.01'),
        [Synset('organism.n.01'),
         [Synset('living_thing.n.01'),
          [Synset('whole.n.02'),
           [Synset('object.n.01'),
            [Synset('physical_entity.n.01'),
             [Synset('entity.n.01')]]]]]]]]]]]]],
 [Synset('domestic_animal.n.01'), ""Cycle(Synset('animal.n.01'),-3,...)""]]
",Synset,__init__,nltk\nltk\corpus\reader\wordnet.py,True
">>> import nltk
>>> from nltk.util import acyclic_depth_first as acyclic_tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(acyclic_tree(wn.synset('dog.n.01'), lambda s:s.hypernyms(),cut_mark='...'))
[Synset('dog.n.01'),
 [Synset('canine.n.02'),
  [Synset('carnivore.n.01'),
   [Synset('placental.n.01'),
    [Synset('mammal.n.01'),
     [Synset('vertebrate.n.01'),
      [Synset('chordate.n.01'),
       [Synset('animal.n.01'),
        [Synset('organism.n.01'),
         [Synset('living_thing.n.01'),
          [Synset('whole.n.02'),
           [Synset('object.n.01'),
            [Synset('physical_entity.n.01'),
             [Synset('entity.n.01')]]]]]]]]]]]]],
 [Synset('domestic_animal.n.01'), ""Cycle(Synset('animal.n.01'),-3,...)""]]
",Cycle,N/A,N/A,N/A
">>> from nltk.corpus import wordnet as wn
>>> computer = wn.synset('computer.n.01')
>>> topic = lambda s:s.topic_domains()
>>> print(list(computer.closure(topic)))
[Synset('computer_science.n.01')]
",wn.synset,N/A,N/A,N/A
">>> from nltk.corpus import wordnet as wn
>>> computer = wn.synset('computer.n.01')
>>> topic = lambda s:s.topic_domains()
>>> print(list(computer.closure(topic)))
[Synset('computer_science.n.01')]
",s.topic_domains,N/A,N/A,N/A
">>> from nltk.corpus import wordnet as wn
>>> computer = wn.synset('computer.n.01')
>>> topic = lambda s:s.topic_domains()
>>> print(list(computer.closure(topic)))
[Synset('computer_science.n.01')]
",print,N/A,N/A,N/A
">>> from nltk.corpus import wordnet as wn
>>> computer = wn.synset('computer.n.01')
>>> topic = lambda s:s.topic_domains()
>>> print(list(computer.closure(topic)))
[Synset('computer_science.n.01')]
",list,N/A,N/A,N/A
">>> from nltk.corpus import wordnet as wn
>>> computer = wn.synset('computer.n.01')
>>> topic = lambda s:s.topic_domains()
>>> print(list(computer.closure(topic)))
[Synset('computer_science.n.01')]
",computer.closure,N/A,N/A,N/A
">>> from nltk.corpus import wordnet as wn
>>> computer = wn.synset('computer.n.01')
>>> topic = lambda s:s.topic_domains()
>>> print(list(computer.closure(topic)))
[Synset('computer_science.n.01')]
",Synset,__init__,nltk\nltk\corpus\reader\wordnet.py,True
">>> dog = wn.synset('dog.n.01')
>>> hyp = lambda s:s.hypernyms()
>>> print(list(dog.closure(hyp)))
[Synset('canine.n.02'), Synset('domestic_animal.n.01'), Synset('carnivore.n.01'),
Synset('animal.n.01'), Synset('placental.n.01'), Synset('organism.n.01'),
Synset('mammal.n.01'), Synset('living_thing.n.01'), Synset('vertebrate.n.01'),
Synset('whole.n.02'), Synset('chordate.n.01'), Synset('object.n.01'),
Synset('physical_entity.n.01'), Synset('entity.n.01')]
",wn.synset,N/A,N/A,N/A
">>> dog = wn.synset('dog.n.01')
>>> hyp = lambda s:s.hypernyms()
>>> print(list(dog.closure(hyp)))
[Synset('canine.n.02'), Synset('domestic_animal.n.01'), Synset('carnivore.n.01'),
Synset('animal.n.01'), Synset('placental.n.01'), Synset('organism.n.01'),
Synset('mammal.n.01'), Synset('living_thing.n.01'), Synset('vertebrate.n.01'),
Synset('whole.n.02'), Synset('chordate.n.01'), Synset('object.n.01'),
Synset('physical_entity.n.01'), Synset('entity.n.01')]
",s.hypernyms,N/A,N/A,N/A
">>> dog = wn.synset('dog.n.01')
>>> hyp = lambda s:s.hypernyms()
>>> print(list(dog.closure(hyp)))
[Synset('canine.n.02'), Synset('domestic_animal.n.01'), Synset('carnivore.n.01'),
Synset('animal.n.01'), Synset('placental.n.01'), Synset('organism.n.01'),
Synset('mammal.n.01'), Synset('living_thing.n.01'), Synset('vertebrate.n.01'),
Synset('whole.n.02'), Synset('chordate.n.01'), Synset('object.n.01'),
Synset('physical_entity.n.01'), Synset('entity.n.01')]
",print,N/A,N/A,N/A
">>> dog = wn.synset('dog.n.01')
>>> hyp = lambda s:s.hypernyms()
>>> print(list(dog.closure(hyp)))
[Synset('canine.n.02'), Synset('domestic_animal.n.01'), Synset('carnivore.n.01'),
Synset('animal.n.01'), Synset('placental.n.01'), Synset('organism.n.01'),
Synset('mammal.n.01'), Synset('living_thing.n.01'), Synset('vertebrate.n.01'),
Synset('whole.n.02'), Synset('chordate.n.01'), Synset('object.n.01'),
Synset('physical_entity.n.01'), Synset('entity.n.01')]
",list,N/A,N/A,N/A
">>> dog = wn.synset('dog.n.01')
>>> hyp = lambda s:s.hypernyms()
>>> print(list(dog.closure(hyp)))
[Synset('canine.n.02'), Synset('domestic_animal.n.01'), Synset('carnivore.n.01'),
Synset('animal.n.01'), Synset('placental.n.01'), Synset('organism.n.01'),
Synset('mammal.n.01'), Synset('living_thing.n.01'), Synset('vertebrate.n.01'),
Synset('whole.n.02'), Synset('chordate.n.01'), Synset('object.n.01'),
Synset('physical_entity.n.01'), Synset('entity.n.01')]
",dog.closure,N/A,N/A,N/A
">>> dog = wn.synset('dog.n.01')
>>> hyp = lambda s:s.hypernyms()
>>> print(list(dog.closure(hyp)))
[Synset('canine.n.02'), Synset('domestic_animal.n.01'), Synset('carnivore.n.01'),
Synset('animal.n.01'), Synset('placental.n.01'), Synset('organism.n.01'),
Synset('mammal.n.01'), Synset('living_thing.n.01'), Synset('vertebrate.n.01'),
Synset('whole.n.02'), Synset('chordate.n.01'), Synset('object.n.01'),
Synset('physical_entity.n.01'), Synset('entity.n.01')]
",Synset,__init__,nltk\nltk\corpus\reader\wordnet.py,True
">>> import nltk
>>> from nltk.util import unweighted_minimum_spanning_tree as mst
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(mst(wn.synset('bound.a.01'), lambda s:s.also_sees()))
[Synset('bound.a.01'),
 [Synset('unfree.a.02'),
  [Synset('confined.a.02')],
  [Synset('dependent.a.01')],
  [Synset('restricted.a.01'), [Synset('classified.a.02')]]]]
",pprint,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import unweighted_minimum_spanning_tree as mst
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(mst(wn.synset('bound.a.01'), lambda s:s.also_sees()))
[Synset('bound.a.01'),
 [Synset('unfree.a.02'),
  [Synset('confined.a.02')],
  [Synset('dependent.a.01')],
  [Synset('restricted.a.01'), [Synset('classified.a.02')]]]]
",mst,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import unweighted_minimum_spanning_tree as mst
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(mst(wn.synset('bound.a.01'), lambda s:s.also_sees()))
[Synset('bound.a.01'),
 [Synset('unfree.a.02'),
  [Synset('confined.a.02')],
  [Synset('dependent.a.01')],
  [Synset('restricted.a.01'), [Synset('classified.a.02')]]]]
",wn.synset,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import unweighted_minimum_spanning_tree as mst
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(mst(wn.synset('bound.a.01'), lambda s:s.also_sees()))
[Synset('bound.a.01'),
 [Synset('unfree.a.02'),
  [Synset('confined.a.02')],
  [Synset('dependent.a.01')],
  [Synset('restricted.a.01'), [Synset('classified.a.02')]]]]
",s.also_sees,N/A,N/A,N/A
">>> import nltk
>>> from nltk.util import unweighted_minimum_spanning_tree as mst
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(mst(wn.synset('bound.a.01'), lambda s:s.also_sees()))
[Synset('bound.a.01'),
 [Synset('unfree.a.02'),
  [Synset('confined.a.02')],
  [Synset('dependent.a.01')],
  [Synset('restricted.a.01'), [Synset('classified.a.02')]]]]
",Synset,__init__,nltk\nltk\corpus\reader\wordnet.py,True
">>> from nltk.corpus import wordnet as wn
>>> from pprint import pprint
>>> computer = wn.synset('computer.n.01')
>>> topic = lambda s:s.topic_domains()
>>> pprint(computer.tree(topic))
[Synset('computer.n.01'), [Synset('computer_science.n.01')]]
",wn.synset,N/A,N/A,N/A
">>> from nltk.corpus import wordnet as wn
>>> from pprint import pprint
>>> computer = wn.synset('computer.n.01')
>>> topic = lambda s:s.topic_domains()
>>> pprint(computer.tree(topic))
[Synset('computer.n.01'), [Synset('computer_science.n.01')]]
",s.topic_domains,N/A,N/A,N/A
">>> from nltk.corpus import wordnet as wn
>>> from pprint import pprint
>>> computer = wn.synset('computer.n.01')
>>> topic = lambda s:s.topic_domains()
>>> pprint(computer.tree(topic))
[Synset('computer.n.01'), [Synset('computer_science.n.01')]]
",pprint,N/A,N/A,N/A
">>> from nltk.corpus import wordnet as wn
>>> from pprint import pprint
>>> computer = wn.synset('computer.n.01')
>>> topic = lambda s:s.topic_domains()
>>> pprint(computer.tree(topic))
[Synset('computer.n.01'), [Synset('computer_science.n.01')]]
",computer.tree,N/A,N/A,N/A
">>> from nltk.corpus import wordnet as wn
>>> from pprint import pprint
>>> computer = wn.synset('computer.n.01')
>>> topic = lambda s:s.topic_domains()
>>> pprint(computer.tree(topic))
[Synset('computer.n.01'), [Synset('computer_science.n.01')]]
",Synset,__init__,nltk\nltk\corpus\reader\wordnet.py,True
">>> dog = wn.synset('dog.n.01')
>>> hyp = lambda s:s.hypernyms()
>>> pprint(dog.tree(hyp))
[Synset('dog.n.01'),
 [Synset('canine.n.02'),
  [Synset('carnivore.n.01'),
   [Synset('placental.n.01'),
    [Synset('mammal.n.01'),
     [Synset('vertebrate.n.01'),
      [Synset('chordate.n.01'),
       [Synset('animal.n.01'),
        [Synset('organism.n.01'),
         [Synset('living_thing.n.01'),
          [Synset('whole.n.02'),
           [Synset('object.n.01'),
            [Synset('physical_entity.n.01'),
             [Synset('entity.n.01')]]]]]]]]]]]]],
 [Synset('domestic_animal.n.01'),
  [Synset('animal.n.01'),
   [Synset('organism.n.01'),
    [Synset('living_thing.n.01'),
     [Synset('whole.n.02'),
      [Synset('object.n.01'),
       [Synset('physical_entity.n.01'), [Synset('entity.n.01')]]]]]]]]]
",wn.synset,N/A,N/A,N/A
">>> dog = wn.synset('dog.n.01')
>>> hyp = lambda s:s.hypernyms()
>>> pprint(dog.tree(hyp))
[Synset('dog.n.01'),
 [Synset('canine.n.02'),
  [Synset('carnivore.n.01'),
   [Synset('placental.n.01'),
    [Synset('mammal.n.01'),
     [Synset('vertebrate.n.01'),
      [Synset('chordate.n.01'),
       [Synset('animal.n.01'),
        [Synset('organism.n.01'),
         [Synset('living_thing.n.01'),
          [Synset('whole.n.02'),
           [Synset('object.n.01'),
            [Synset('physical_entity.n.01'),
             [Synset('entity.n.01')]]]]]]]]]]]]],
 [Synset('domestic_animal.n.01'),
  [Synset('animal.n.01'),
   [Synset('organism.n.01'),
    [Synset('living_thing.n.01'),
     [Synset('whole.n.02'),
      [Synset('object.n.01'),
       [Synset('physical_entity.n.01'), [Synset('entity.n.01')]]]]]]]]]
",s.hypernyms,N/A,N/A,N/A
">>> dog = wn.synset('dog.n.01')
>>> hyp = lambda s:s.hypernyms()
>>> pprint(dog.tree(hyp))
[Synset('dog.n.01'),
 [Synset('canine.n.02'),
  [Synset('carnivore.n.01'),
   [Synset('placental.n.01'),
    [Synset('mammal.n.01'),
     [Synset('vertebrate.n.01'),
      [Synset('chordate.n.01'),
       [Synset('animal.n.01'),
        [Synset('organism.n.01'),
         [Synset('living_thing.n.01'),
          [Synset('whole.n.02'),
           [Synset('object.n.01'),
            [Synset('physical_entity.n.01'),
             [Synset('entity.n.01')]]]]]]]]]]]]],
 [Synset('domestic_animal.n.01'),
  [Synset('animal.n.01'),
   [Synset('organism.n.01'),
    [Synset('living_thing.n.01'),
     [Synset('whole.n.02'),
      [Synset('object.n.01'),
       [Synset('physical_entity.n.01'), [Synset('entity.n.01')]]]]]]]]]
",pprint,N/A,N/A,N/A
">>> dog = wn.synset('dog.n.01')
>>> hyp = lambda s:s.hypernyms()
>>> pprint(dog.tree(hyp))
[Synset('dog.n.01'),
 [Synset('canine.n.02'),
  [Synset('carnivore.n.01'),
   [Synset('placental.n.01'),
    [Synset('mammal.n.01'),
     [Synset('vertebrate.n.01'),
      [Synset('chordate.n.01'),
       [Synset('animal.n.01'),
        [Synset('organism.n.01'),
         [Synset('living_thing.n.01'),
          [Synset('whole.n.02'),
           [Synset('object.n.01'),
            [Synset('physical_entity.n.01'),
             [Synset('entity.n.01')]]]]]]]]]]]]],
 [Synset('domestic_animal.n.01'),
  [Synset('animal.n.01'),
   [Synset('organism.n.01'),
    [Synset('living_thing.n.01'),
     [Synset('whole.n.02'),
      [Synset('object.n.01'),
       [Synset('physical_entity.n.01'), [Synset('entity.n.01')]]]]]]]]]
",dog.tree,N/A,N/A,N/A
">>> dog = wn.synset('dog.n.01')
>>> hyp = lambda s:s.hypernyms()
>>> pprint(dog.tree(hyp))
[Synset('dog.n.01'),
 [Synset('canine.n.02'),
  [Synset('carnivore.n.01'),
   [Synset('placental.n.01'),
    [Synset('mammal.n.01'),
     [Synset('vertebrate.n.01'),
      [Synset('chordate.n.01'),
       [Synset('animal.n.01'),
        [Synset('organism.n.01'),
         [Synset('living_thing.n.01'),
          [Synset('whole.n.02'),
           [Synset('object.n.01'),
            [Synset('physical_entity.n.01'),
             [Synset('entity.n.01')]]]]]]]]]]]]],
 [Synset('domestic_animal.n.01'),
  [Synset('animal.n.01'),
   [Synset('organism.n.01'),
    [Synset('living_thing.n.01'),
     [Synset('whole.n.02'),
      [Synset('object.n.01'),
       [Synset('physical_entity.n.01'), [Synset('entity.n.01')]]]]]]]]]
",Synset,__init__,nltk\nltk\corpus\reader\wordnet.py,True
">>> from nltk.corpus import wordnet as wn
>>> print(wn.morphy('dogs'))
dog
>>> print(wn.morphy('churches'))
church
>>> print(wn.morphy('aardwolves'))
aardwolf
>>> print(wn.morphy('abaci'))
abacus
>>> wn.morphy('hardrock', wn.ADV)
>>> print(wn.morphy('book', wn.NOUN))
book
>>> wn.morphy('book', wn.ADJ)
",print,N/A,N/A,N/A
">>> from nltk.corpus import wordnet as wn
>>> print(wn.morphy('dogs'))
dog
>>> print(wn.morphy('churches'))
church
>>> print(wn.morphy('aardwolves'))
aardwolf
>>> print(wn.morphy('abaci'))
abacus
>>> wn.morphy('hardrock', wn.ADV)
>>> print(wn.morphy('book', wn.NOUN))
book
>>> wn.morphy('book', wn.ADJ)
",wn.morphy,N/A,N/A,N/A
">>> from nltk.corpus import wordnet as wn
>>> print(wn.synset_from_pos_and_offset('n', 1740))
Synset('entity.n.01')
",print,N/A,N/A,N/A
">>> from nltk.corpus import wordnet as wn
>>> print(wn.synset_from_pos_and_offset('n', 1740))
Synset('entity.n.01')
",wn.synset_from_pos_and_offset,N/A,N/A,N/A
">>> from nltk.corpus import wordnet as wn
>>> print(wn.synset_from_pos_and_offset('n', 1740))
Synset('entity.n.01')
",Synset,__init__,nltk\nltk\corpus\reader\wordnet.py,True
">>> import nltk
>>> from nltk.corpus import wordnet as wn
>>> print(wn.synset_from_sense_key(""drive%1:04:03::""))
Synset('drive.n.06')
",print,N/A,N/A,N/A
">>> import nltk
>>> from nltk.corpus import wordnet as wn
>>> print(wn.synset_from_sense_key(""drive%1:04:03::""))
Synset('drive.n.06')
",wn.synset_from_sense_key,N/A,N/A,N/A
">>> import nltk
>>> from nltk.corpus import wordnet as wn
>>> print(wn.synset_from_sense_key(""drive%1:04:03::""))
Synset('drive.n.06')
",Synset,__init__,nltk\nltk\corpus\reader\wordnet.py,True
">>> print(wn.synset_from_sense_key(""driving%1:04:03::""))
Synset('drive.n.06')
",print,N/A,N/A,N/A
">>> print(wn.synset_from_sense_key(""driving%1:04:03::""))
Synset('drive.n.06')
",wn.synset_from_sense_key,N/A,N/A,N/A
">>> print(wn.synset_from_sense_key(""driving%1:04:03::""))
Synset('drive.n.06')
",Synset,__init__,nltk\nltk\corpus\reader\wordnet.py,True
">>> from nltk.corpus import brown
>>> print("", "".join(brown.words()))
The, Fulton, County, Grand, Jury, said, ...
",print,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> print("", "".join(brown.words()))
The, Fulton, County, Grand, Jury, said, ...
",join,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> print("", "".join(brown.words()))
The, Fulton, County, Grand, Jury, said, ...
",brown.words,N/A,N/A,N/A
"BNCCorpusReader(root='BNC/Texts/', fileids=r'[A-K]/\w*/\w*\.xml')
",BNCCorpusReader,__init__,nltk\nltk\corpus\reader\bnc.py,True
get(),get,N/A,N/A,N/A
insert(),insert,N/A,N/A,N/A
show_column(),show_column,N/A,N/A,N/A
self.see(),self.see,N/A,N/A,N/A
hide_column(),hide_column,N/A,N/A,N/A
rowconfig(),rowconfig,N/A,N/A,N/A
columnconfig(),columnconfig,N/A,N/A,N/A
itemconfig(),itemconfig,N/A,N/A,N/A
grid(),grid,N/A,N/A,N/A
pack(),pack,N/A,N/A,N/A
bind(),bind,N/A,N/A,N/A
__getitem__(),__getitem__,N/A,N/A,N/A
__setitem__(),__setitem__,N/A,N/A,N/A
__nonzero__(),__nonzero__,N/A,N/A,N/A
MultiListbox.bind_to_columns(),MultiListbox.bind_to_columns,MultiListbox.bind_to_columns,nltk\nltk\draw\table.py,True
MultiListbox.bind_to_labels(),MultiListbox.bind_to_labels,MultiListbox.bind_to_labels,nltk\nltk\draw\table.py,True
MultiListbox.bind_to_listboxes(),MultiListbox.bind_to_listboxes,MultiListbox.bind_to_listboxes,nltk\nltk\draw\table.py,True
MultiListbox.columnconfigure(),MultiListbox.columnconfigure,MultiListbox.columnconfigure,nltk\nltk\draw\table.py,False
Tkinter.Frame.grid(),Frame.grid,N/A,N/A,N/A
MultiListbox.hide_column(),MultiListbox.hide_column,MultiListbox.hide_column,nltk\nltk\draw\table.py,False
MultiListbox.itemconfigure(),MultiListbox.itemconfigure,MultiListbox.itemconfigure,nltk\nltk\draw\table.py,False
Tkinter.Frame.pack(),Frame.pack,N/A,N/A,N/A
MultiListbox.rowconfigure(),MultiListbox.rowconfigure,MultiListbox.rowconfigure,nltk\nltk\draw\table.py,False
MultiListbox.select(),MultiListbox.select,MultiListbox.select,nltk\nltk\draw\table.py,True
table[table.selected_row()],table.selected_row,N/A,N/A,N/A
MultiListbox.show_column(),MultiListbox.show_column,MultiListbox.show_column,nltk\nltk\draw\table.py,False
sort_by(),sort_by,N/A,N/A,N/A
tree.children()[i1].children()[i2]....children()[in],tree.children,N/A,N/A,N/A
tree.children()[i1].children()[i2]....children()[in],children,N/A,N/A,N/A
tree.children()[i1].children()[i2]....children()[in],tree.children,N/A,N/A,N/A
tree.children()[i1].children()[i2]....children()[in],children,N/A,N/A,N/A
parent.update(self),parent.update,N/A,N/A,N/A
">>> mlb = MultiListbox(master, 5)
>>> mlb.configure(label_foreground='red')
>>> mlb.configure(listbox_foreground='red')
",MultiListbox,__init__,nltk\nltk\draw\table.py,True
">>> mlb = MultiListbox(master, 5)
>>> mlb.configure(label_foreground='red')
>>> mlb.configure(listbox_foreground='red')
",mlb.configure,N/A,N/A,N/A
">>> print(table[3, 'First Name'])
John
",print,N/A,N/A,N/A
">>> from nltk.draw.util import TextWidget
>>> cn = TextWidget(c, 'test', color='red')
",TextWidget,__init__,nltk\nltk\draw\util.py,True
Prover.prove(),Prover.prove,Prover.prove,nltk\nltk\inference\api.py,True
readings(filter=True),readings,N/A,N/A,N/A
"s0 readings:

s0-r1: some x.(boxer(x) & walk(x))
s0-r0: some x.(boxerdog(x) & walk(x))
",boxer,N/A,N/A,N/A
"s0 readings:

s0-r1: some x.(boxer(x) & walk(x))
s0-r0: some x.(boxerdog(x) & walk(x))
",walk,N/A,N/A,N/A
"s0 readings:

s0-r1: some x.(boxer(x) & walk(x))
s0-r0: some x.(boxerdog(x) & walk(x))
",boxerdog,N/A,N/A,N/A
">>> from nltk.metrics.agreement import AnnotationTask
>>> import os.path
>>> t = AnnotationTask(data=[x.split() for x in open(os.path.join(os.path.dirname(__file__), ""artstein_poesio_example.txt""))])
>>> t.avg_Ao()
0.88
>>> t.pi()
0.7995322418977615...
>>> t.S()
0.8199999999999998...
",AnnotationTask,__init__,nltk\nltk\metrics\agreement.py,True
">>> from nltk.metrics.agreement import AnnotationTask
>>> import os.path
>>> t = AnnotationTask(data=[x.split() for x in open(os.path.join(os.path.dirname(__file__), ""artstein_poesio_example.txt""))])
>>> t.avg_Ao()
0.88
>>> t.pi()
0.7995322418977615...
>>> t.S()
0.8199999999999998...
",x.split,N/A,N/A,N/A
">>> from nltk.metrics.agreement import AnnotationTask
>>> import os.path
>>> t = AnnotationTask(data=[x.split() for x in open(os.path.join(os.path.dirname(__file__), ""artstein_poesio_example.txt""))])
>>> t.avg_Ao()
0.88
>>> t.pi()
0.7995322418977615...
>>> t.S()
0.8199999999999998...
",open,N/A,N/A,N/A
">>> from nltk.metrics.agreement import AnnotationTask
>>> import os.path
>>> t = AnnotationTask(data=[x.split() for x in open(os.path.join(os.path.dirname(__file__), ""artstein_poesio_example.txt""))])
>>> t.avg_Ao()
0.88
>>> t.pi()
0.7995322418977615...
>>> t.S()
0.8199999999999998...
",path.join,N/A,N/A,N/A
">>> from nltk.metrics.agreement import AnnotationTask
>>> import os.path
>>> t = AnnotationTask(data=[x.split() for x in open(os.path.join(os.path.dirname(__file__), ""artstein_poesio_example.txt""))])
>>> t.avg_Ao()
0.88
>>> t.pi()
0.7995322418977615...
>>> t.S()
0.8199999999999998...
",path.dirname,N/A,N/A,N/A
">>> from nltk.metrics.agreement import AnnotationTask
>>> import os.path
>>> t = AnnotationTask(data=[x.split() for x in open(os.path.join(os.path.dirname(__file__), ""artstein_poesio_example.txt""))])
>>> t.avg_Ao()
0.88
>>> t.pi()
0.7995322418977615...
>>> t.S()
0.8199999999999998...
",t.avg_Ao,N/A,N/A,N/A
">>> from nltk.metrics.agreement import AnnotationTask
>>> import os.path
>>> t = AnnotationTask(data=[x.split() for x in open(os.path.join(os.path.dirname(__file__), ""artstein_poesio_example.txt""))])
>>> t.avg_Ao()
0.88
>>> t.pi()
0.7995322418977615...
>>> t.S()
0.8199999999999998...
",t.pi,N/A,N/A,N/A
">>> from nltk.metrics.agreement import AnnotationTask
>>> import os.path
>>> t = AnnotationTask(data=[x.split() for x in open(os.path.join(os.path.dirname(__file__), ""artstein_poesio_example.txt""))])
>>> t.avg_Ao()
0.88
>>> t.pi()
0.7995322418977615...
>>> t.S()
0.8199999999999998...
",t.S,N/A,N/A,N/A
">>> align('θin', 'tenwis') 
[[('θ', 't'), ('i', 'e'), ('n', 'n'), ('-', 'w'), ('-', 'i'), ('-', 's')]]
",align,N/A,N/A,N/A
"bigram_score_fn(n_ii, (n_ix, n_xi), n_xx)
",bigram_score_fn,N/A,N/A,N/A
"score_fn(count_of_ngram,
         (count_of_n-1gram_1, ..., count_of_n-1gram_j),
         (count_of_n-2gram_1, ..., count_of_n-2gram_k),
         ...,
         (count_of_1gram_1, ..., count_of_1gram_n),
         count_of_total_words)
",score_fn,N/A,N/A,N/A
"trigram_score_fn(n_iiii,
                (n_iiix, n_iixi, n_ixii, n_xiii),
                (n_iixx, n_ixix, n_ixxi, n_xixi, n_xxii, n_xiix),
                (n_ixxx, n_xixx, n_xxix, n_xxxi),
                n_all)
",trigram_score_fn,N/A,N/A,N/A
"trigram_score_fn(n_iii,
                 (n_iix, n_ixi, n_xii),
                 (n_ixx, n_xix, n_xxi),
                 n_xxx)
",trigram_score_fn,N/A,N/A,N/A
">>> from nltk.metrics import ConfusionMatrix
>>> ref  = 'DET NN VB DET JJ NN NN IN DET NN'.split()
>>> test = 'DET VB VB DET NN NN NN IN DET NN'.split()
>>> cm = ConfusionMatrix(ref, test)
>>> print(cm['NN', 'NN'])
3
",split,N/A,N/A,N/A
">>> from nltk.metrics import ConfusionMatrix
>>> ref  = 'DET NN VB DET JJ NN NN IN DET NN'.split()
>>> test = 'DET VB VB DET NN NN NN IN DET NN'.split()
>>> cm = ConfusionMatrix(ref, test)
>>> print(cm['NN', 'NN'])
3
",ConfusionMatrix,__init__,nltk\nltk\metrics\confusionmatrix.py,True
">>> from nltk.metrics import ConfusionMatrix
>>> ref  = 'DET NN VB DET JJ NN NN IN DET NN'.split()
>>> test = 'DET VB VB DET NN NN NN IN DET NN'.split()
>>> cm = ConfusionMatrix(ref, test)
>>> print(cm['NN', 'NN'])
3
",print,N/A,N/A,N/A
">>> from nltk.metrics import binary_distance
>>> binary_distance(1,1)
0.0
",binary_distance,N/A,N/A,N/A
">>> binary_distance(1,3)
1.0
",binary_distance,N/A,N/A,N/A
">>> from nltk.metrics import interval_distance
>>> interval_distance(1,10)
81
",interval_distance,N/A,N/A,N/A
">>> for (s1, s2), jscore, wscore, p in zip(winkler_examples, jaro_scores, winkler_scores, p_factors):
...     assert round(jaro_similarity(s1, s2), 3) == jscore
...     assert round(jaro_winkler_similarity(s1, s2, p=p), 3) == wscore
",zip,N/A,N/A,N/A
">>> for (s1, s2), jscore, wscore, p in zip(winkler_examples, jaro_scores, winkler_scores, p_factors):
...     assert round(jaro_similarity(s1, s2), 3) == jscore
...     assert round(jaro_winkler_similarity(s1, s2, p=p), 3) == wscore
",round,N/A,N/A,N/A
">>> for (s1, s2), jscore, wscore, p in zip(winkler_examples, jaro_scores, winkler_scores, p_factors):
...     assert round(jaro_similarity(s1, s2), 3) == jscore
...     assert round(jaro_winkler_similarity(s1, s2, p=p), 3) == wscore
",jaro_similarity,N/A,N/A,N/A
">>> for (s1, s2), jscore, wscore, p in zip(winkler_examples, jaro_scores, winkler_scores, p_factors):
...     assert round(jaro_similarity(s1, s2), 3) == jscore
...     assert round(jaro_winkler_similarity(s1, s2, p=p), 3) == wscore
",jaro_winkler_similarity,N/A,N/A,N/A
">>> for (s1, s2), jscore, wscore, p in zip(winkler_examples, jaro_scores, winkler_scores, p_factors):
...     if (s1, s2) in [('JON', 'JAN'), ('1ST', 'IST')]:
...         continue  # Skip bad examples from the paper.
...     assert round(jaro_similarity(s1, s2), 3) == jscore
...     assert round(jaro_winkler_similarity(s1, s2, p=p), 3) == wscore
",zip,N/A,N/A,N/A
">>> for (s1, s2), jscore, wscore, p in zip(winkler_examples, jaro_scores, winkler_scores, p_factors):
...     if (s1, s2) in [('JON', 'JAN'), ('1ST', 'IST')]:
...         continue  # Skip bad examples from the paper.
...     assert round(jaro_similarity(s1, s2), 3) == jscore
...     assert round(jaro_winkler_similarity(s1, s2, p=p), 3) == wscore
",round,N/A,N/A,N/A
">>> for (s1, s2), jscore, wscore, p in zip(winkler_examples, jaro_scores, winkler_scores, p_factors):
...     if (s1, s2) in [('JON', 'JAN'), ('1ST', 'IST')]:
...         continue  # Skip bad examples from the paper.
...     assert round(jaro_similarity(s1, s2), 3) == jscore
...     assert round(jaro_winkler_similarity(s1, s2, p=p), 3) == wscore
",jaro_similarity,N/A,N/A,N/A
">>> for (s1, s2), jscore, wscore, p in zip(winkler_examples, jaro_scores, winkler_scores, p_factors):
...     if (s1, s2) in [('JON', 'JAN'), ('1ST', 'IST')]:
...         continue  # Skip bad examples from the paper.
...     assert round(jaro_similarity(s1, s2), 3) == jscore
...     assert round(jaro_winkler_similarity(s1, s2, p=p), 3) == wscore
",jaro_winkler_similarity,N/A,N/A,N/A
">>> round(jaro_winkler_similarity('TANYA', 'TONYA', p=0.1, max_l=100), 3)
0.88
",round,N/A,N/A,N/A
">>> round(jaro_winkler_similarity('TANYA', 'TONYA', p=0.1, max_l=100), 3)
0.88
",jaro_winkler_similarity,N/A,N/A,N/A
">>> from nltk.metrics import masi_distance
>>> masi_distance(set([1, 2]), set([1, 2, 3, 4]))
0.665
",masi_distance,N/A,N/A,N/A
">>> from nltk.metrics import masi_distance
>>> masi_distance(set([1, 2]), set([1, 2, 3, 4]))
0.665
",set,N/A,N/A,N/A
">>> # Same examples as Kulyukin C++ implementation
>>> ghd('1100100000', '1100010000', 1.0, 1.0, 0.5)
0.5
>>> ghd('1100100000', '1100000001', 1.0, 1.0, 0.5)
2.0
>>> ghd('011', '110', 1.0, 1.0, 0.5)
1.0
>>> ghd('1', '0', 1.0, 1.0, 0.5)
1.0
>>> ghd('111', '000', 1.0, 1.0, 0.5)
3.0
>>> ghd('000', '111', 1.0, 2.0, 0.5)
6.0
",ghd,N/A,N/A,N/A
">>> '%.2f' % pk('0100'*100, '1'*400, 2)
'0.50'
>>> '%.2f' % pk('0100'*100, '0'*400, 2)
'0.50'
>>> '%.2f' % pk('0100'*100, '0100'*100, 2)
'0.00'
",pk,N/A,N/A,N/A
">>> s1 = ""000100000010""
>>> s2 = ""000010000100""
>>> s3 = ""100000010000""
>>> '%.2f' % windowdiff(s1, s1, 3)
'0.00'
>>> '%.2f' % windowdiff(s1, s2, 3)
'0.30'
>>> '%.2f' % windowdiff(s2, s3, 3)
'0.80'
",windowdiff,N/A,N/A,N/A
parse(),parse,N/A,N/A,N/A
parse_sents(),parse_sents,N/A,N/A,N/A
grammar(),grammar,N/A,N/A,N/A
self.parse(),self.parse,N/A,N/A,N/A
bllipparser.RerankingParser.get_unified_model_parameters(),RerankingParser.get_unified_model_parameters,N/A,N/A,N/A
bllipparser.RerankingParser.RerankingParser.load_parser_options(),RerankingParser.load_parser_options,N/A,N/A,N/A
bllipparser.RerankingParser.RerankingParser.load_reranker_model(),RerankingParser.load_reranker_model,N/A,N/A,N/A
apply(),apply,N/A,N/A,N/A
"chart.select(is_complete=True, start=0)",chart.select,N/A,N/A,N/A
e.span()==span,e.span,N/A,N/A,N/A
e.start()==start,e.start,N/A,N/A,N/A
e.end()==end,e.end,N/A,N/A,N/A
e.length()==length,e.length,N/A,N/A,N/A
e.lhs()==lhs,e.lhs,N/A,N/A,N/A
e.rhs()==rhs,e.rhs,N/A,N/A,N/A
e.nextsym()==nextsym,e.nextsym,N/A,N/A,N/A
e.dot()==dot,e.dot,N/A,N/A,N/A
e.is_complete()==is_complete,e.is_complete,N/A,N/A,N/A
e.is_incomplete()==is_incomplete,e.is_incomplete,N/A,N/A,N/A
apply(),apply,N/A,N/A,N/A
tokens[self.start():self.end()],self.start,N/A,N/A,N/A
tokens[self.start():self.end()],self.end,N/A,N/A,N/A
apply(),apply,N/A,N/A,N/A
tokens[self.start():self.end()],self.start,N/A,N/A,N/A
tokens[self.start():self.end()],self.end,N/A,N/A,N/A
apply(),apply,N/A,N/A,N/A
tokens[self.start():self.end()],self.start,N/A,N/A,N/A
tokens[self.start():self.end()],self.end,N/A,N/A,N/A
apply(),apply,N/A,N/A,N/A
e.span()==span,e.span,N/A,N/A,N/A
e.start()==start,e.start,N/A,N/A,N/A
e.end()==end,e.end,N/A,N/A,N/A
e.length()==length,e.length,N/A,N/A,N/A
e.lhs()==lhs,e.lhs,N/A,N/A,N/A
e.rhs()==rhs,e.rhs,N/A,N/A,N/A
e.nextsym()==nextsym,e.nextsym,N/A,N/A,N/A
e.dot()==dot,e.dot,N/A,N/A,N/A
e.is_complete()==is_complete,e.is_complete,N/A,N/A,N/A
e.is_incomplete()==is_incomplete,e.is_incomplete,N/A,N/A,N/A
apply(),apply,N/A,N/A,N/A
apply(),apply,N/A,N/A,N/A
apply(),apply,N/A,N/A,N/A
">>> dep_parser = CoreNLPDependencyParser(url='http://localhost:9000')
",CoreNLPDependencyParser,__init__,nltk\nltk\parse\corenlp.py,True
">>> parse, = dep_parser.raw_parse(
...     'The quick brown fox jumps over the lazy dog.'
... )
>>> print(parse.to_conll(4))  
The     DT      4       det
quick   JJ      4       amod
brown   JJ      4       amod
fox     NN      5       nsubj
jumps   VBZ     0       ROOT
over    IN      9       case
the     DT      9       det
lazy    JJ      9       amod
dog     NN      5       nmod
.       .       5       punct
",dep_parser.raw_parse,N/A,N/A,N/A
">>> parse, = dep_parser.raw_parse(
...     'The quick brown fox jumps over the lazy dog.'
... )
>>> print(parse.to_conll(4))  
The     DT      4       det
quick   JJ      4       amod
brown   JJ      4       amod
fox     NN      5       nsubj
jumps   VBZ     0       ROOT
over    IN      9       case
the     DT      9       det
lazy    JJ      9       amod
dog     NN      5       nmod
.       .       5       punct
",print,N/A,N/A,N/A
">>> parse, = dep_parser.raw_parse(
...     'The quick brown fox jumps over the lazy dog.'
... )
>>> print(parse.to_conll(4))  
The     DT      4       det
quick   JJ      4       amod
brown   JJ      4       amod
fox     NN      5       nsubj
jumps   VBZ     0       ROOT
over    IN      9       case
the     DT      9       det
lazy    JJ      9       amod
dog     NN      5       nmod
.       .       5       punct
",parse.to_conll,N/A,N/A,N/A
">>> print(parse.tree())  
(jumps (fox The quick brown) (dog over the lazy) .)
",print,N/A,N/A,N/A
">>> print(parse.tree())  
(jumps (fox The quick brown) (dog over the lazy) .)
",parse.tree,N/A,N/A,N/A
">>> for governor, dep, dependent in parse.triples():
...     print(governor, dep, dependent)  
    ('jumps', 'VBZ') nsubj ('fox', 'NN')
    ('fox', 'NN') det ('The', 'DT')
    ('fox', 'NN') amod ('quick', 'JJ')
    ('fox', 'NN') amod ('brown', 'JJ')
    ('jumps', 'VBZ') nmod ('dog', 'NN')
    ('dog', 'NN') case ('over', 'IN')
    ('dog', 'NN') det ('the', 'DT')
    ('dog', 'NN') amod ('lazy', 'JJ')
    ('jumps', 'VBZ') punct ('.', '.')
",parse.triples,N/A,N/A,N/A
">>> for governor, dep, dependent in parse.triples():
...     print(governor, dep, dependent)  
    ('jumps', 'VBZ') nsubj ('fox', 'NN')
    ('fox', 'NN') det ('The', 'DT')
    ('fox', 'NN') amod ('quick', 'JJ')
    ('fox', 'NN') amod ('brown', 'JJ')
    ('jumps', 'VBZ') nmod ('dog', 'NN')
    ('dog', 'NN') case ('over', 'IN')
    ('dog', 'NN') det ('the', 'DT')
    ('dog', 'NN') amod ('lazy', 'JJ')
    ('jumps', 'VBZ') punct ('.', '.')
",print,N/A,N/A,N/A
">>> (parse_fox, ), (parse_dog, ) = dep_parser.raw_parse_sents(
...     [
...         'The quick brown fox jumps over the lazy dog.',
...         'The quick grey wolf jumps over the lazy fox.',
...     ]
... )
>>> print(parse_fox.to_conll(4))  
The DT      4       det
quick       JJ      4       amod
brown       JJ      4       amod
fox NN      5       nsubj
jumps       VBZ     0       ROOT
over        IN      9       case
the DT      9       det
lazy        JJ      9       amod
dog NN      5       nmod
.   .       5       punct
",dep_parser.raw_parse_sents,N/A,N/A,N/A
">>> (parse_fox, ), (parse_dog, ) = dep_parser.raw_parse_sents(
...     [
...         'The quick brown fox jumps over the lazy dog.',
...         'The quick grey wolf jumps over the lazy fox.',
...     ]
... )
>>> print(parse_fox.to_conll(4))  
The DT      4       det
quick       JJ      4       amod
brown       JJ      4       amod
fox NN      5       nsubj
jumps       VBZ     0       ROOT
over        IN      9       case
the DT      9       det
lazy        JJ      9       amod
dog NN      5       nmod
.   .       5       punct
",print,N/A,N/A,N/A
">>> (parse_fox, ), (parse_dog, ) = dep_parser.raw_parse_sents(
...     [
...         'The quick brown fox jumps over the lazy dog.',
...         'The quick grey wolf jumps over the lazy fox.',
...     ]
... )
>>> print(parse_fox.to_conll(4))  
The DT      4       det
quick       JJ      4       amod
brown       JJ      4       amod
fox NN      5       nsubj
jumps       VBZ     0       ROOT
over        IN      9       case
the DT      9       det
lazy        JJ      9       amod
dog NN      5       nmod
.   .       5       punct
",parse_fox.to_conll,N/A,N/A,N/A
">>> print(parse_dog.to_conll(4))  
The DT      4       det
quick       JJ      4       amod
grey        JJ      4       amod
wolf        NN      5       nsubj
jumps       VBZ     0       ROOT
over        IN      9       case
the DT      9       det
lazy        JJ      9       amod
fox NN      5       nmod
.   .       5       punct
",print,N/A,N/A,N/A
">>> print(parse_dog.to_conll(4))  
The DT      4       det
quick       JJ      4       amod
grey        JJ      4       amod
wolf        NN      5       nsubj
jumps       VBZ     0       ROOT
over        IN      9       case
the DT      9       det
lazy        JJ      9       amod
fox NN      5       nmod
.   .       5       punct
",parse_dog.to_conll,N/A,N/A,N/A
">>> (parse_dog, ), (parse_friends, ) = dep_parser.parse_sents(
...     [
...         ""I 'm a dog"".split(),
...         ""This is my friends ' cat ( the tabby )"".split(),
...     ]
... )
>>> print(parse_dog.to_conll(4))  
I   PRP     4       nsubj
'm  VBP     4       cop
a   DT      4       det
dog NN      0       ROOT
",dep_parser.parse_sents,N/A,N/A,N/A
">>> (parse_dog, ), (parse_friends, ) = dep_parser.parse_sents(
...     [
...         ""I 'm a dog"".split(),
...         ""This is my friends ' cat ( the tabby )"".split(),
...     ]
... )
>>> print(parse_dog.to_conll(4))  
I   PRP     4       nsubj
'm  VBP     4       cop
a   DT      4       det
dog NN      0       ROOT
",split,N/A,N/A,N/A
">>> (parse_dog, ), (parse_friends, ) = dep_parser.parse_sents(
...     [
...         ""I 'm a dog"".split(),
...         ""This is my friends ' cat ( the tabby )"".split(),
...     ]
... )
>>> print(parse_dog.to_conll(4))  
I   PRP     4       nsubj
'm  VBP     4       cop
a   DT      4       det
dog NN      0       ROOT
",print,N/A,N/A,N/A
">>> (parse_dog, ), (parse_friends, ) = dep_parser.parse_sents(
...     [
...         ""I 'm a dog"".split(),
...         ""This is my friends ' cat ( the tabby )"".split(),
...     ]
... )
>>> print(parse_dog.to_conll(4))  
I   PRP     4       nsubj
'm  VBP     4       cop
a   DT      4       det
dog NN      0       ROOT
",parse_dog.to_conll,N/A,N/A,N/A
">>> print(parse_friends.to_conll(4))  
This        DT      6       nsubj
is  VBZ     6       cop
my  PRP$    4       nmod:poss
friends     NNS     6       nmod:poss
'   POS     4       case
cat NN      0       ROOT
-LRB-       -LRB-   9       punct
the DT      9       det
tabby       NN      6       appos
-RRB-       -RRB-   9       punct
",print,N/A,N/A,N/A
">>> print(parse_friends.to_conll(4))  
This        DT      6       nsubj
is  VBZ     6       cop
my  PRP$    4       nmod:poss
friends     NNS     6       nmod:poss
'   POS     4       case
cat NN      0       ROOT
-LRB-       -LRB-   9       punct
the DT      9       det
tabby       NN      6       appos
-RRB-       -RRB-   9       punct
",parse_friends.to_conll,N/A,N/A,N/A
">>> parse_john, parse_mary, = dep_parser.parse_text(
...     'John loves Mary. Mary walks.'
... )
",dep_parser.parse_text,N/A,N/A,N/A
">>> print(parse_john.to_conll(4))  
John        NNP     2       nsubj
loves       VBZ     0       ROOT
Mary        NNP     2       dobj
.   .       2       punct
",print,N/A,N/A,N/A
">>> print(parse_john.to_conll(4))  
John        NNP     2       nsubj
loves       VBZ     0       ROOT
Mary        NNP     2       dobj
.   .       2       punct
",parse_john.to_conll,N/A,N/A,N/A
">>> print(parse_mary.to_conll(4))  
Mary        NNP     2       nsubj
walks       VBZ     0       ROOT
.   .       2       punct
",print,N/A,N/A,N/A
">>> print(parse_mary.to_conll(4))  
Mary        NNP     2       nsubj
walks       VBZ     0       ROOT
.   .       2       punct
",parse_mary.to_conll,N/A,N/A,N/A
">>> len(
...     next(
...         dep_parser.raw_parse(
...             'Anhalt said children typically treat a 20-ounce soda bottle as one '
...             'serving, while it actually contains 2 1/2 servings.'
...         )
...     ).nodes
... )
21
",len,N/A,N/A,N/A
">>> len(
...     next(
...         dep_parser.raw_parse(
...             'Anhalt said children typically treat a 20-ounce soda bottle as one '
...             'serving, while it actually contains 2 1/2 servings.'
...         )
...     ).nodes
... )
21
",next,N/A,N/A,N/A
">>> len(
...     next(
...         dep_parser.raw_parse(
...             'Anhalt said children typically treat a 20-ounce soda bottle as one '
...             'serving, while it actually contains 2 1/2 servings.'
...         )
...     ).nodes
... )
21
",dep_parser.raw_parse,N/A,N/A,N/A
">>> len(
...     next(
...         dep_parser.raw_parse('This is not going to crash: 01 111 555.')
...     ).nodes
... )
10
",len,N/A,N/A,N/A
">>> len(
...     next(
...         dep_parser.raw_parse('This is not going to crash: 01 111 555.')
...     ).nodes
... )
10
",next,N/A,N/A,N/A
">>> len(
...     next(
...         dep_parser.raw_parse('This is not going to crash: 01 111 555.')
...     ).nodes
... )
10
",dep_parser.raw_parse,N/A,N/A,N/A
">>> print(
...     next(
...         dep_parser.raw_parse('The underscore _ should not simply disappear.')
...     ).to_conll(4)
... )  
The         DT  3   det
underscore  VBP 3   amod
_           NN  7   nsubj
should      MD  7   aux
not         RB  7   neg
simply      RB  7   advmod
disappear   VB  0   ROOT
.           .   7   punct
",print,N/A,N/A,N/A
">>> print(
...     next(
...         dep_parser.raw_parse('The underscore _ should not simply disappear.')
...     ).to_conll(4)
... )  
The         DT  3   det
underscore  VBP 3   amod
_           NN  7   nsubj
should      MD  7   aux
not         RB  7   neg
simply      RB  7   advmod
disappear   VB  0   ROOT
.           .   7   punct
",next,N/A,N/A,N/A
">>> print(
...     next(
...         dep_parser.raw_parse('The underscore _ should not simply disappear.')
...     ).to_conll(4)
... )  
The         DT  3   det
underscore  VBP 3   amod
_           NN  7   nsubj
should      MD  7   aux
not         RB  7   neg
simply      RB  7   advmod
disappear   VB  0   ROOT
.           .   7   punct
",dep_parser.raw_parse,N/A,N/A,N/A
">>> print(
...     next(
...         dep_parser.raw_parse('The underscore _ should not simply disappear.')
...     ).to_conll(4)
... )  
The         DT  3   det
underscore  VBP 3   amod
_           NN  7   nsubj
should      MD  7   aux
not         RB  7   neg
simply      RB  7   advmod
disappear   VB  0   ROOT
.           .   7   punct
",to_conll,N/A,N/A,N/A
">>> print(
...     '\n'.join(
...         next(
...             dep_parser.raw_parse(
...                 'for all of its insights into the dream world of teen life , and its electronic expression through '
...                 'cyber culture , the film gives no quarter to anyone seeking to pull a cohesive story out of its 2 '
...                 '1/2-hour running time .'
...             )
...         ).to_conll(4).split('\n')[-8:]
...     )
... )
its PRP$    40      nmod:poss
2 1/2       CD      40      nummod
-   :       40      punct
hour        NN      31      nmod
running     VBG     42      amod
time        NN      40      dep
.   .       24      punct
",print,N/A,N/A,N/A
">>> print(
...     '\n'.join(
...         next(
...             dep_parser.raw_parse(
...                 'for all of its insights into the dream world of teen life , and its electronic expression through '
...                 'cyber culture , the film gives no quarter to anyone seeking to pull a cohesive story out of its 2 '
...                 '1/2-hour running time .'
...             )
...         ).to_conll(4).split('\n')[-8:]
...     )
... )
its PRP$    40      nmod:poss
2 1/2       CD      40      nummod
-   :       40      punct
hour        NN      31      nmod
running     VBG     42      amod
time        NN      40      dep
.   .       24      punct
",join,N/A,N/A,N/A
">>> print(
...     '\n'.join(
...         next(
...             dep_parser.raw_parse(
...                 'for all of its insights into the dream world of teen life , and its electronic expression through '
...                 'cyber culture , the film gives no quarter to anyone seeking to pull a cohesive story out of its 2 '
...                 '1/2-hour running time .'
...             )
...         ).to_conll(4).split('\n')[-8:]
...     )
... )
its PRP$    40      nmod:poss
2 1/2       CD      40      nummod
-   :       40      punct
hour        NN      31      nmod
running     VBG     42      amod
time        NN      40      dep
.   .       24      punct
",next,N/A,N/A,N/A
">>> print(
...     '\n'.join(
...         next(
...             dep_parser.raw_parse(
...                 'for all of its insights into the dream world of teen life , and its electronic expression through '
...                 'cyber culture , the film gives no quarter to anyone seeking to pull a cohesive story out of its 2 '
...                 '1/2-hour running time .'
...             )
...         ).to_conll(4).split('\n')[-8:]
...     )
... )
its PRP$    40      nmod:poss
2 1/2       CD      40      nummod
-   :       40      punct
hour        NN      31      nmod
running     VBG     42      amod
time        NN      40      dep
.   .       24      punct
",dep_parser.raw_parse,N/A,N/A,N/A
">>> print(
...     '\n'.join(
...         next(
...             dep_parser.raw_parse(
...                 'for all of its insights into the dream world of teen life , and its electronic expression through '
...                 'cyber culture , the film gives no quarter to anyone seeking to pull a cohesive story out of its 2 '
...                 '1/2-hour running time .'
...             )
...         ).to_conll(4).split('\n')[-8:]
...     )
... )
its PRP$    40      nmod:poss
2 1/2       CD      40      nummod
-   :       40      punct
hour        NN      31      nmod
running     VBG     42      amod
time        NN      40      dep
.   .       24      punct
",to_conll,N/A,N/A,N/A
">>> print(
...     '\n'.join(
...         next(
...             dep_parser.raw_parse(
...                 'for all of its insights into the dream world of teen life , and its electronic expression through '
...                 'cyber culture , the film gives no quarter to anyone seeking to pull a cohesive story out of its 2 '
...                 '1/2-hour running time .'
...             )
...         ).to_conll(4).split('\n')[-8:]
...     )
... )
its PRP$    40      nmod:poss
2 1/2       CD      40      nummod
-   :       40      punct
hour        NN      31      nmod
running     VBG     42      amod
time        NN      40      dep
.   .       24      punct
",split,N/A,N/A,N/A
">>> parser = CoreNLPParser(url='http://localhost:9000')
",CoreNLPParser,__init__,nltk\nltk\parse\corenlp.py,True
">>> next(
...     parser.raw_parse('The quick brown fox jumps over the lazy dog.')
... ).pretty_print()  
                     ROOT
                      |
                      S
       _______________|__________________________
      |                         VP               |
      |                _________|___             |
      |               |             PP           |
      |               |     ________|___         |
      NP              |    |            NP       |
  ____|__________     |    |     _______|____    |
 DT   JJ    JJ   NN  VBZ   IN   DT      JJ   NN  .
 |    |     |    |    |    |    |       |    |   |
The quick brown fox jumps over the     lazy dog  .
",next,N/A,N/A,N/A
">>> next(
...     parser.raw_parse('The quick brown fox jumps over the lazy dog.')
... ).pretty_print()  
                     ROOT
                      |
                      S
       _______________|__________________________
      |                         VP               |
      |                _________|___             |
      |               |             PP           |
      |               |     ________|___         |
      NP              |    |            NP       |
  ____|__________     |    |     _______|____    |
 DT   JJ    JJ   NN  VBZ   IN   DT      JJ   NN  .
 |    |     |    |    |    |    |       |    |   |
The quick brown fox jumps over the     lazy dog  .
",parser.raw_parse,N/A,N/A,N/A
">>> next(
...     parser.raw_parse('The quick brown fox jumps over the lazy dog.')
... ).pretty_print()  
                     ROOT
                      |
                      S
       _______________|__________________________
      |                         VP               |
      |                _________|___             |
      |               |             PP           |
      |               |     ________|___         |
      NP              |    |            NP       |
  ____|__________     |    |     _______|____    |
 DT   JJ    JJ   NN  VBZ   IN   DT      JJ   NN  .
 |    |     |    |    |    |    |       |    |   |
The quick brown fox jumps over the     lazy dog  .
",pretty_print,N/A,N/A,N/A
">>> (parse_fox, ), (parse_wolf, ) = parser.raw_parse_sents(
...     [
...         'The quick brown fox jumps over the lazy dog.',
...         'The quick grey wolf jumps over the lazy fox.',
...     ]
... )
",parser.raw_parse_sents,N/A,N/A,N/A
">>> parse_fox.pretty_print()  
                     ROOT
                      |
                      S
       _______________|__________________________
      |                         VP               |
      |                _________|___             |
      |               |             PP           |
      |               |     ________|___         |
      NP              |    |            NP       |
  ____|__________     |    |     _______|____    |
 DT   JJ    JJ   NN  VBZ   IN   DT      JJ   NN  .
 |    |     |    |    |    |    |       |    |   |
The quick brown fox jumps over the     lazy dog  .
",parse_fox.pretty_print,N/A,N/A,N/A
">>> parse_wolf.pretty_print()  
                     ROOT
                      |
                      S
       _______________|__________________________
      |                         VP               |
      |                _________|___             |
      |               |             PP           |
      |               |     ________|___         |
      NP              |    |            NP       |
  ____|_________      |    |     _______|____    |
 DT   JJ   JJ   NN   VBZ   IN   DT      JJ   NN  .
 |    |    |    |     |    |    |       |    |   |
The quick grey wolf jumps over the     lazy fox  .
",parse_wolf.pretty_print,N/A,N/A,N/A
">>> (parse_dog, ), (parse_friends, ) = parser.parse_sents(
...     [
...         ""I 'm a dog"".split(),
...         ""This is my friends ' cat ( the tabby )"".split(),
...     ]
... )
",parser.parse_sents,N/A,N/A,N/A
">>> (parse_dog, ), (parse_friends, ) = parser.parse_sents(
...     [
...         ""I 'm a dog"".split(),
...         ""This is my friends ' cat ( the tabby )"".split(),
...     ]
... )
",split,N/A,N/A,N/A
">>> parse_dog.pretty_print()  
        ROOT
         |
         S
  _______|____
 |            VP
 |    ________|___
 NP  |            NP
 |   |         ___|___
PRP VBP       DT      NN
 |   |        |       |
 I   'm       a      dog
",parse_dog.pretty_print,N/A,N/A,N/A
">>> parse_friends.pretty_print()  
     ROOT
      |
      S
  ____|___________
 |                VP
 |     ___________|_____________
 |    |                         NP
 |    |                  _______|_________
 |    |                 NP               PRN
 |    |            _____|_______      ____|______________
 NP   |           NP            |    |        NP         |
 |    |     ______|_________    |    |     ___|____      |
 DT  VBZ  PRP$   NNS       POS  NN -LRB-  DT       NN  -RRB-
 |    |    |      |         |   |    |    |        |     |
This  is   my  friends      '  cat -LRB- the     tabby -RRB-
",parse_friends.pretty_print,N/A,N/A,N/A
">>> parse_john, parse_mary, = parser.parse_text(
...     'John loves Mary. Mary walks.'
... )
",parser.parse_text,N/A,N/A,N/A
">>> parse_john.pretty_print()  
      ROOT
       |
       S
  _____|_____________
 |          VP       |
 |      ____|___     |
 NP    |        NP   |
 |     |        |    |
NNP   VBZ      NNP   .
 |     |        |    |
John loves     Mary  .
",parse_john.pretty_print,N/A,N/A,N/A
">>> parse_mary.pretty_print()  
      ROOT
       |
       S
  _____|____
 NP    VP   |
 |     |    |
NNP   VBZ   .
 |     |    |
Mary walks  .
",parse_mary.pretty_print,N/A,N/A,N/A
">>> next(
...     parser.raw_parse(
...         'NASIRIYA, Iraq—Iraqi doctors who treated former prisoner of war '
...         'Jessica Lynch have angrily dismissed claims made in her biography '
...         'that she was raped by her Iraqi captors.'
...     )
... ).height()
20
",next,N/A,N/A,N/A
">>> next(
...     parser.raw_parse(
...         'NASIRIYA, Iraq—Iraqi doctors who treated former prisoner of war '
...         'Jessica Lynch have angrily dismissed claims made in her biography '
...         'that she was raped by her Iraqi captors.'
...     )
... ).height()
20
",parser.raw_parse,N/A,N/A,N/A
">>> next(
...     parser.raw_parse(
...         'NASIRIYA, Iraq—Iraqi doctors who treated former prisoner of war '
...         'Jessica Lynch have angrily dismissed claims made in her biography '
...         'that she was raped by her Iraqi captors.'
...     )
... ).height()
20
",height,N/A,N/A,N/A
">>> next(
...     parser.raw_parse(
...         ""The broader Standard & Poor's 500 Index <.SPX> was 0.46 points lower, or ""
...         '0.05 percent, at 997.02.'
...     )
... ).height()
9
",next,N/A,N/A,N/A
">>> next(
...     parser.raw_parse(
...         ""The broader Standard & Poor's 500 Index <.SPX> was 0.46 points lower, or ""
...         '0.05 percent, at 997.02.'
...     )
... ).height()
9
",parser.raw_parse,N/A,N/A,N/A
">>> next(
...     parser.raw_parse(
...         ""The broader Standard & Poor's 500 Index <.SPX> was 0.46 points lower, or ""
...         '0.05 percent, at 997.02.'
...     )
... ).height()
9
",height,N/A,N/A,N/A
">>> parser = CoreNLPParser(url='http://localhost:9000', tagtype='ner')
>>> tokens = 'Rami Eid is studying at Stony Brook University in NY'.split()
>>> parser.tag(tokens)
[('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'),
('Brook', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'O')]
",CoreNLPParser,__init__,nltk\nltk\parse\corenlp.py,True
">>> parser = CoreNLPParser(url='http://localhost:9000', tagtype='ner')
>>> tokens = 'Rami Eid is studying at Stony Brook University in NY'.split()
>>> parser.tag(tokens)
[('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'),
('Brook', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'O')]
",split,N/A,N/A,N/A
">>> parser = CoreNLPParser(url='http://localhost:9000', tagtype='ner')
>>> tokens = 'Rami Eid is studying at Stony Brook University in NY'.split()
>>> parser.tag(tokens)
[('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'),
('Brook', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'O')]
",parser.tag,N/A,N/A,N/A
">>> parser = CoreNLPParser(url='http://localhost:9000', tagtype='pos')
>>> tokens = ""What is the airspeed of an unladen swallow ?"".split()
>>> parser.tag(tokens)
[('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'),
('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'),
('unladen', 'JJ'), ('swallow', 'VB'), ('?', '.')]
",CoreNLPParser,__init__,nltk\nltk\parse\corenlp.py,True
">>> parser = CoreNLPParser(url='http://localhost:9000', tagtype='pos')
>>> tokens = ""What is the airspeed of an unladen swallow ?"".split()
>>> parser.tag(tokens)
[('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'),
('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'),
('unladen', 'JJ'), ('swallow', 'VB'), ('?', '.')]
",split,N/A,N/A,N/A
">>> parser = CoreNLPParser(url='http://localhost:9000', tagtype='pos')
>>> tokens = ""What is the airspeed of an unladen swallow ?"".split()
>>> parser.tag(tokens)
[('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'),
('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'),
('unladen', 'JJ'), ('swallow', 'VB'), ('?', '.')]
",parser.tag,N/A,N/A,N/A
">>> parser = CoreNLPParser(url='http://localhost:9000')
",CoreNLPParser,__init__,nltk\nltk\parse\corenlp.py,True
">>> text = 'Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.\nThanks.'
>>> list(parser.tokenize(text))
['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']
",list,N/A,N/A,N/A
">>> text = 'Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.\nThanks.'
>>> list(parser.tokenize(text))
['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']
",parser.tokenize,N/A,N/A,N/A
">>> s = ""The colour of the wall is blue.""
>>> list(
...     parser.tokenize(
...         'The colour of the wall is blue.',
...             properties={'tokenize.options': 'americanize=true'},
...     )
... )
['The', 'color', 'of', 'the', 'wall', 'is', 'blue', '.']
",list,N/A,N/A,N/A
">>> s = ""The colour of the wall is blue.""
>>> list(
...     parser.tokenize(
...         'The colour of the wall is blue.',
...             properties={'tokenize.options': 'americanize=true'},
...     )
... )
['The', 'color', 'of', 'the', 'wall', 'is', 'blue', '.']
",parser.tokenize,N/A,N/A,N/A
">>> dg = DependencyGraph(treebank_data)
>>> dg.contains_cycle()
False
",DependencyGraph,__init__,nltk\nltk\parse\dependencygraph.py,True
">>> dg = DependencyGraph(treebank_data)
>>> dg.contains_cycle()
False
",dg.contains_cycle,N/A,N/A,N/A
">>> cyclic_dg = DependencyGraph()
>>> top = {'word': None, 'deps': [1], 'rel': 'TOP', 'address': 0}
>>> child1 = {'word': None, 'deps': [2], 'rel': 'NTOP', 'address': 1}
>>> child2 = {'word': None, 'deps': [4], 'rel': 'NTOP', 'address': 2}
>>> child3 = {'word': None, 'deps': [1], 'rel': 'NTOP', 'address': 3}
>>> child4 = {'word': None, 'deps': [3], 'rel': 'NTOP', 'address': 4}
>>> cyclic_dg.nodes = {
...     0: top,
...     1: child1,
...     2: child2,
...     3: child3,
...     4: child4,
... }
>>> cyclic_dg.root = top
",DependencyGraph,__init__,nltk\nltk\parse\dependencygraph.py,True
">>> cyclic_dg.contains_cycle()
[3, 1, 2, 4]
",cyclic_dg.contains_cycle,N/A,N/A,N/A
">>> dg = DependencyGraph(
...     'John N 2\n'
...     'loves V 0\n'
...     'Mary N 2'
... )
>>> print(dg.to_dot())
digraph G{
edge [dir=forward]
node [shape=plaintext]

0 [label=""0 (None)""]
0 -> 2 [label=""ROOT""]
1 [label=""1 (John)""]
2 [label=""2 (loves)""]
2 -> 1 [label=""""]
2 -> 3 [label=""""]
3 [label=""3 (Mary)""]
}
",DependencyGraph,__init__,nltk\nltk\parse\dependencygraph.py,True
">>> dg = DependencyGraph(
...     'John N 2\n'
...     'loves V 0\n'
...     'Mary N 2'
... )
>>> print(dg.to_dot())
digraph G{
edge [dir=forward]
node [shape=plaintext]

0 [label=""0 (None)""]
0 -> 2 [label=""ROOT""]
1 [label=""1 (John)""]
2 [label=""2 (loves)""]
2 -> 1 [label=""""]
2 -> 3 [label=""""]
3 [label=""3 (Mary)""]
}
",print,N/A,N/A,N/A
">>> dg = DependencyGraph(
...     'John N 2\n'
...     'loves V 0\n'
...     'Mary N 2'
... )
>>> print(dg.to_dot())
digraph G{
edge [dir=forward]
node [shape=plaintext]

0 [label=""0 (None)""]
0 -> 2 [label=""ROOT""]
1 [label=""1 (John)""]
2 [label=""2 (loves)""]
2 -> 1 [label=""""]
2 -> 3 [label=""""]
3 [label=""3 (Mary)""]
}
",dg.to_dot,N/A,N/A,N/A
">>> gold_sent = DependencyGraph(""""""
... Pierre  NNP     2       NMOD
... Vinken  NNP     8       SUB
... ,       ,       2       P
... 61      CD      5       NMOD
... years   NNS     6       AMOD
... old     JJ      2       NMOD
... ,       ,       2       P
... will    MD      0       ROOT
... join    VB      8       VC
... the     DT      11      NMOD
... board   NN      9       OBJ
... as      IN      9       VMOD
... a       DT      15      NMOD
... nonexecutive    JJ      15      NMOD
... director        NN      12      PMOD
... Nov.    NNP     9       VMOD
... 29      CD      16      NMOD
... .       .       9       VMOD
... """""")
",DependencyGraph,__init__,nltk\nltk\parse\dependencygraph.py,True
">>> parsed_sent = DependencyGraph(""""""
... Pierre  NNP     8       NMOD
... Vinken  NNP     1       SUB
... ,       ,       3       P
... 61      CD      6       NMOD
... years   NNS     6       AMOD
... old     JJ      2       NMOD
... ,       ,       3       AMOD
... will    MD      0       ROOT
... join    VB      8       VC
... the     DT      11      AMOD
... board   NN      9       OBJECT
... as      IN      9       NMOD
... a       DT      15      NMOD
... nonexecutive    JJ      15      NMOD
... director        NN      12      PMOD
... Nov.    NNP     9       VMOD
... 29      CD      16      NMOD
... .       .       9       VMOD
... """""")
",DependencyGraph,__init__,nltk\nltk\parse\dependencygraph.py,True
">>> de = DependencyEvaluator([parsed_sent],[gold_sent])
>>> las, uas = de.eval()
>>> las
0.6...
>>> uas
0.8...
>>> abs(uas - 0.8) < 0.00001
True
",DependencyEvaluator,__init__,nltk\nltk\parse\evaluate.py,True
">>> de = DependencyEvaluator([parsed_sent],[gold_sent])
>>> las, uas = de.eval()
>>> las
0.6...
>>> uas
0.8...
>>> abs(uas - 0.8) < 0.00001
True
",de.eval,N/A,N/A,N/A
">>> de = DependencyEvaluator([parsed_sent],[gold_sent])
>>> las, uas = de.eval()
>>> las
0.6...
>>> uas
0.8...
>>> abs(uas - 0.8) < 0.00001
True
",abs,N/A,N/A,N/A
">>> from nltk.parse import malt
>>> # With MALT_PARSER and MALT_MODEL environment set.
>>> mp = malt.MaltParser('maltparser-1.7.2', 'engmalt.linear-1.7.mco') 
>>> mp.parse_one('I shot an elephant in my pajamas .'.split()).tree() 
(shot I (elephant an) (in (pajamas my)) .)
>>> # Without MALT_PARSER and MALT_MODEL environment.
>>> mp = malt.MaltParser('/home/user/maltparser-1.7.2/', '/home/user/engmalt.linear-1.7.mco') 
>>> mp.parse_one('I shot an elephant in my pajamas .'.split()).tree() 
(shot I (elephant an) (in (pajamas my)) .)
",MaltParser,__init__,nltk\nltk\parse\malt.py,True
">>> from nltk.parse import malt
>>> # With MALT_PARSER and MALT_MODEL environment set.
>>> mp = malt.MaltParser('maltparser-1.7.2', 'engmalt.linear-1.7.mco') 
>>> mp.parse_one('I shot an elephant in my pajamas .'.split()).tree() 
(shot I (elephant an) (in (pajamas my)) .)
>>> # Without MALT_PARSER and MALT_MODEL environment.
>>> mp = malt.MaltParser('/home/user/maltparser-1.7.2/', '/home/user/engmalt.linear-1.7.mco') 
>>> mp.parse_one('I shot an elephant in my pajamas .'.split()).tree() 
(shot I (elephant an) (in (pajamas my)) .)
",mp.parse_one,N/A,N/A,N/A
">>> from nltk.parse import malt
>>> # With MALT_PARSER and MALT_MODEL environment set.
>>> mp = malt.MaltParser('maltparser-1.7.2', 'engmalt.linear-1.7.mco') 
>>> mp.parse_one('I shot an elephant in my pajamas .'.split()).tree() 
(shot I (elephant an) (in (pajamas my)) .)
>>> # Without MALT_PARSER and MALT_MODEL environment.
>>> mp = malt.MaltParser('/home/user/maltparser-1.7.2/', '/home/user/engmalt.linear-1.7.mco') 
>>> mp.parse_one('I shot an elephant in my pajamas .'.split()).tree() 
(shot I (elephant an) (in (pajamas my)) .)
",split,N/A,N/A,N/A
">>> from nltk.parse import malt
>>> # With MALT_PARSER and MALT_MODEL environment set.
>>> mp = malt.MaltParser('maltparser-1.7.2', 'engmalt.linear-1.7.mco') 
>>> mp.parse_one('I shot an elephant in my pajamas .'.split()).tree() 
(shot I (elephant an) (in (pajamas my)) .)
>>> # Without MALT_PARSER and MALT_MODEL environment.
>>> mp = malt.MaltParser('/home/user/maltparser-1.7.2/', '/home/user/engmalt.linear-1.7.mco') 
>>> mp.parse_one('I shot an elephant in my pajamas .'.split()).tree() 
(shot I (elephant an) (in (pajamas my)) .)
",tree,N/A,N/A,N/A
">>> graphs = [DependencyGraph(entry) for entry in conll_data2.split('\n\n') if entry]
>>> npp = ProbabilisticNonprojectiveParser()
>>> npp.train(graphs, NaiveBayesDependencyScorer())
>>> parses = npp.parse(['Cathy', 'zag', 'hen', 'zwaaien', '.'], ['N', 'V', 'Pron', 'Adj', 'N', 'Punc'])
>>> len(list(parses))
1
",DependencyGraph,__init__,nltk\nltk\parse\dependencygraph.py,True
">>> graphs = [DependencyGraph(entry) for entry in conll_data2.split('\n\n') if entry]
>>> npp = ProbabilisticNonprojectiveParser()
>>> npp.train(graphs, NaiveBayesDependencyScorer())
>>> parses = npp.parse(['Cathy', 'zag', 'hen', 'zwaaien', '.'], ['N', 'V', 'Pron', 'Adj', 'N', 'Punc'])
>>> len(list(parses))
1
",conll_data2.split,N/A,N/A,N/A
">>> graphs = [DependencyGraph(entry) for entry in conll_data2.split('\n\n') if entry]
>>> npp = ProbabilisticNonprojectiveParser()
>>> npp.train(graphs, NaiveBayesDependencyScorer())
>>> parses = npp.parse(['Cathy', 'zag', 'hen', 'zwaaien', '.'], ['N', 'V', 'Pron', 'Adj', 'N', 'Punc'])
>>> len(list(parses))
1
",ProbabilisticNonprojectiveParser,__init__,nltk\nltk\parse\nonprojectivedependencyparser.py,True
">>> graphs = [DependencyGraph(entry) for entry in conll_data2.split('\n\n') if entry]
>>> npp = ProbabilisticNonprojectiveParser()
>>> npp.train(graphs, NaiveBayesDependencyScorer())
>>> parses = npp.parse(['Cathy', 'zag', 'hen', 'zwaaien', '.'], ['N', 'V', 'Pron', 'Adj', 'N', 'Punc'])
>>> len(list(parses))
1
",npp.train,N/A,N/A,N/A
">>> graphs = [DependencyGraph(entry) for entry in conll_data2.split('\n\n') if entry]
>>> npp = ProbabilisticNonprojectiveParser()
>>> npp.train(graphs, NaiveBayesDependencyScorer())
>>> parses = npp.parse(['Cathy', 'zag', 'hen', 'zwaaien', '.'], ['N', 'V', 'Pron', 'Adj', 'N', 'Punc'])
>>> len(list(parses))
1
",NaiveBayesDependencyScorer,__init__,nltk\nltk\parse\nonprojectivedependencyparser.py,True
">>> graphs = [DependencyGraph(entry) for entry in conll_data2.split('\n\n') if entry]
>>> npp = ProbabilisticNonprojectiveParser()
>>> npp.train(graphs, NaiveBayesDependencyScorer())
>>> parses = npp.parse(['Cathy', 'zag', 'hen', 'zwaaien', '.'], ['N', 'V', 'Pron', 'Adj', 'N', 'Punc'])
>>> len(list(parses))
1
",npp.parse,N/A,N/A,N/A
">>> graphs = [DependencyGraph(entry) for entry in conll_data2.split('\n\n') if entry]
>>> npp = ProbabilisticNonprojectiveParser()
>>> npp.train(graphs, NaiveBayesDependencyScorer())
>>> parses = npp.parse(['Cathy', 'zag', 'hen', 'zwaaien', '.'], ['N', 'V', 'Pron', 'Adj', 'N', 'Punc'])
>>> len(list(parses))
1
",len,N/A,N/A,N/A
">>> graphs = [DependencyGraph(entry) for entry in conll_data2.split('\n\n') if entry]
>>> npp = ProbabilisticNonprojectiveParser()
>>> npp.train(graphs, NaiveBayesDependencyScorer())
>>> parses = npp.parse(['Cathy', 'zag', 'hen', 'zwaaien', '.'], ['N', 'V', 'Pron', 'Adj', 'N', 'Punc'])
>>> len(list(parses))
1
",list,N/A,N/A,N/A
">>> class Scorer(DependencyScorerI):
...     def train(self, graphs):
...         pass
...
...     def score(self, graph):
...         return [
...             [[], [5],  [1],  [1]],
...             [[], [],   [11], [4]],
...             [[], [10], [],   [5]],
...             [[], [8],  [8],  []],
...         ]
",Scorer,N/A,N/A,N/A
">>> class Scorer(DependencyScorerI):
...     def train(self, graphs):
...         pass
...
...     def score(self, graph):
...         return [
...             [[], [5],  [1],  [1]],
...             [[], [],   [11], [4]],
...             [[], [10], [],   [5]],
...             [[], [8],  [8],  []],
...         ]
",train,N/A,N/A,N/A
">>> class Scorer(DependencyScorerI):
...     def train(self, graphs):
...         pass
...
...     def score(self, graph):
...         return [
...             [[], [5],  [1],  [1]],
...             [[], [],   [11], [4]],
...             [[], [10], [],   [5]],
...             [[], [8],  [8],  []],
...         ]
",score,N/A,N/A,N/A
">>> npp = ProbabilisticNonprojectiveParser()
>>> npp.train([], Scorer())
",ProbabilisticNonprojectiveParser,__init__,nltk\nltk\parse\nonprojectivedependencyparser.py,True
">>> npp = ProbabilisticNonprojectiveParser()
>>> npp.train([], Scorer())
",npp.train,N/A,N/A,N/A
">>> npp = ProbabilisticNonprojectiveParser()
>>> npp.train([], Scorer())
",Scorer,N/A,N/A,N/A
">>> parses = npp.parse(['v1', 'v2', 'v3'], [None, None, None])
>>> len(list(parses))
1
",npp.parse,N/A,N/A,N/A
">>> parses = npp.parse(['v1', 'v2', 'v3'], [None, None, None])
>>> len(list(parses))
1
",len,N/A,N/A,N/A
">>> parses = npp.parse(['v1', 'v2', 'v3'], [None, None, None])
>>> len(list(parses))
1
",list,N/A,N/A,N/A
">>> ndp = NonprojectiveDependencyParser(grammar)
>>> parses = ndp.parse(['the', 'man', 'in', 'the', 'corner', 'taught', 'his', 'dachshund', 'to', 'play', 'golf'])
>>> len(list(parses))
4
",NonprojectiveDependencyParser,__init__,nltk\nltk\parse\nonprojectivedependencyparser.py,True
">>> ndp = NonprojectiveDependencyParser(grammar)
>>> parses = ndp.parse(['the', 'man', 'in', 'the', 'corner', 'taught', 'his', 'dachshund', 'to', 'play', 'golf'])
>>> len(list(parses))
4
",ndp.parse,N/A,N/A,N/A
">>> ndp = NonprojectiveDependencyParser(grammar)
>>> parses = ndp.parse(['the', 'man', 'in', 'the', 'corner', 'taught', 'his', 'dachshund', 'to', 'play', 'golf'])
>>> len(list(parses))
4
",len,N/A,N/A,N/A
">>> ndp = NonprojectiveDependencyParser(grammar)
>>> parses = ndp.parse(['the', 'man', 'in', 'the', 'corner', 'taught', 'his', 'dachshund', 'to', 'play', 'golf'])
>>> len(list(parses))
4
",list,N/A,N/A,N/A
">>> graphs = [
... DependencyGraph(entry) for entry in conll_data2.split('\n\n') if entry
... ]
",DependencyGraph,__init__,nltk\nltk\parse\dependencygraph.py,True
">>> graphs = [
... DependencyGraph(entry) for entry in conll_data2.split('\n\n') if entry
... ]
",conll_data2.split,N/A,N/A,N/A
">>> ppdp = ProbabilisticProjectiveDependencyParser()
>>> ppdp.train(graphs)
",ProbabilisticProjectiveDependencyParser,__init__,nltk\nltk\parse\projectivedependencyparser.py,True
">>> ppdp = ProbabilisticProjectiveDependencyParser()
>>> ppdp.train(graphs)
",ppdp.train,N/A,N/A,N/A
">>> sent = ['Cathy', 'zag', 'hen', 'wild', 'zwaaien', '.']
>>> list(ppdp.parse(sent))
[Tree('zag', ['Cathy', 'hen', Tree('zwaaien', ['wild', '.'])])]
",list,N/A,N/A,N/A
">>> sent = ['Cathy', 'zag', 'hen', 'wild', 'zwaaien', '.']
>>> list(ppdp.parse(sent))
[Tree('zag', ['Cathy', 'hen', Tree('zwaaien', ['wild', '.'])])]
",ppdp.parse,N/A,N/A,N/A
">>> sent = ['Cathy', 'zag', 'hen', 'wild', 'zwaaien', '.']
>>> list(ppdp.parse(sent))
[Tree('zag', ['Cathy', 'hen', Tree('zwaaien', ['wild', '.'])])]
",Tree,__init__,nltk\nltk\tree\tree.py,True
">>> dep_parser=StanfordDependencyParser(
...     model_path=""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz""
... )
",StanfordDependencyParser,__init__,nltk\nltk\parse\stanford.py,True
">>> [parse.tree() for parse in dep_parser.raw_parse(""The quick brown fox jumps over the lazy dog."")] 
[Tree('jumps', [Tree('fox', ['The', 'quick', 'brown']), Tree('dog', ['over', 'the', 'lazy'])])]
",parse.tree,N/A,N/A,N/A
">>> [parse.tree() for parse in dep_parser.raw_parse(""The quick brown fox jumps over the lazy dog."")] 
[Tree('jumps', [Tree('fox', ['The', 'quick', 'brown']), Tree('dog', ['over', 'the', 'lazy'])])]
",dep_parser.raw_parse,N/A,N/A,N/A
">>> [parse.tree() for parse in dep_parser.raw_parse(""The quick brown fox jumps over the lazy dog."")] 
[Tree('jumps', [Tree('fox', ['The', 'quick', 'brown']), Tree('dog', ['over', 'the', 'lazy'])])]
",Tree,__init__,nltk\nltk\tree\tree.py,True
">>> [list(parse.triples()) for parse in dep_parser.raw_parse(""The quick brown fox jumps over the lazy dog."")] 
[[((u'jumps', u'VBZ'), u'nsubj', (u'fox', u'NN')), ((u'fox', u'NN'), u'det', (u'The', u'DT')),
((u'fox', u'NN'), u'amod', (u'quick', u'JJ')), ((u'fox', u'NN'), u'amod', (u'brown', u'JJ')),
((u'jumps', u'VBZ'), u'nmod', (u'dog', u'NN')), ((u'dog', u'NN'), u'case', (u'over', u'IN')),
((u'dog', u'NN'), u'det', (u'the', u'DT')), ((u'dog', u'NN'), u'amod', (u'lazy', u'JJ'))]]
",list,N/A,N/A,N/A
">>> [list(parse.triples()) for parse in dep_parser.raw_parse(""The quick brown fox jumps over the lazy dog."")] 
[[((u'jumps', u'VBZ'), u'nsubj', (u'fox', u'NN')), ((u'fox', u'NN'), u'det', (u'The', u'DT')),
((u'fox', u'NN'), u'amod', (u'quick', u'JJ')), ((u'fox', u'NN'), u'amod', (u'brown', u'JJ')),
((u'jumps', u'VBZ'), u'nmod', (u'dog', u'NN')), ((u'dog', u'NN'), u'case', (u'over', u'IN')),
((u'dog', u'NN'), u'det', (u'the', u'DT')), ((u'dog', u'NN'), u'amod', (u'lazy', u'JJ'))]]
",parse.triples,N/A,N/A,N/A
">>> [list(parse.triples()) for parse in dep_parser.raw_parse(""The quick brown fox jumps over the lazy dog."")] 
[[((u'jumps', u'VBZ'), u'nsubj', (u'fox', u'NN')), ((u'fox', u'NN'), u'det', (u'The', u'DT')),
((u'fox', u'NN'), u'amod', (u'quick', u'JJ')), ((u'fox', u'NN'), u'amod', (u'brown', u'JJ')),
((u'jumps', u'VBZ'), u'nmod', (u'dog', u'NN')), ((u'dog', u'NN'), u'case', (u'over', u'IN')),
((u'dog', u'NN'), u'det', (u'the', u'DT')), ((u'dog', u'NN'), u'amod', (u'lazy', u'JJ'))]]
",dep_parser.raw_parse,N/A,N/A,N/A
">>> sum([[parse.tree() for parse in dep_graphs] for dep_graphs in dep_parser.raw_parse_sents((
...     ""The quick brown fox jumps over the lazy dog."",
...     ""The quick grey wolf jumps over the lazy fox.""
... ))], []) 
[Tree('jumps', [Tree('fox', ['The', 'quick', 'brown']), Tree('dog', ['over', 'the', 'lazy'])]),
Tree('jumps', [Tree('wolf', ['The', 'quick', 'grey']), Tree('fox', ['over', 'the', 'lazy'])])]
",sum,N/A,N/A,N/A
">>> sum([[parse.tree() for parse in dep_graphs] for dep_graphs in dep_parser.raw_parse_sents((
...     ""The quick brown fox jumps over the lazy dog."",
...     ""The quick grey wolf jumps over the lazy fox.""
... ))], []) 
[Tree('jumps', [Tree('fox', ['The', 'quick', 'brown']), Tree('dog', ['over', 'the', 'lazy'])]),
Tree('jumps', [Tree('wolf', ['The', 'quick', 'grey']), Tree('fox', ['over', 'the', 'lazy'])])]
",parse.tree,N/A,N/A,N/A
">>> sum([[parse.tree() for parse in dep_graphs] for dep_graphs in dep_parser.raw_parse_sents((
...     ""The quick brown fox jumps over the lazy dog."",
...     ""The quick grey wolf jumps over the lazy fox.""
... ))], []) 
[Tree('jumps', [Tree('fox', ['The', 'quick', 'brown']), Tree('dog', ['over', 'the', 'lazy'])]),
Tree('jumps', [Tree('wolf', ['The', 'quick', 'grey']), Tree('fox', ['over', 'the', 'lazy'])])]
",dep_parser.raw_parse_sents,N/A,N/A,N/A
">>> sum([[parse.tree() for parse in dep_graphs] for dep_graphs in dep_parser.raw_parse_sents((
...     ""The quick brown fox jumps over the lazy dog."",
...     ""The quick grey wolf jumps over the lazy fox.""
... ))], []) 
[Tree('jumps', [Tree('fox', ['The', 'quick', 'brown']), Tree('dog', ['over', 'the', 'lazy'])]),
Tree('jumps', [Tree('wolf', ['The', 'quick', 'grey']), Tree('fox', ['over', 'the', 'lazy'])])]
",Tree,__init__,nltk\nltk\tree\tree.py,True
">>> sum([[parse.tree() for parse in dep_graphs] for dep_graphs in dep_parser.parse_sents((
...     ""I 'm a dog"".split(),
...     ""This is my friends ' cat ( the tabby )"".split(),
... ))], []) 
[Tree('dog', ['I', ""'m"", 'a']), Tree('cat', ['This', 'is', Tree('friends', ['my', ""'""]), Tree('tabby', ['the'])])]
",sum,N/A,N/A,N/A
">>> sum([[parse.tree() for parse in dep_graphs] for dep_graphs in dep_parser.parse_sents((
...     ""I 'm a dog"".split(),
...     ""This is my friends ' cat ( the tabby )"".split(),
... ))], []) 
[Tree('dog', ['I', ""'m"", 'a']), Tree('cat', ['This', 'is', Tree('friends', ['my', ""'""]), Tree('tabby', ['the'])])]
",parse.tree,N/A,N/A,N/A
">>> sum([[parse.tree() for parse in dep_graphs] for dep_graphs in dep_parser.parse_sents((
...     ""I 'm a dog"".split(),
...     ""This is my friends ' cat ( the tabby )"".split(),
... ))], []) 
[Tree('dog', ['I', ""'m"", 'a']), Tree('cat', ['This', 'is', Tree('friends', ['my', ""'""]), Tree('tabby', ['the'])])]
",dep_parser.parse_sents,N/A,N/A,N/A
">>> sum([[parse.tree() for parse in dep_graphs] for dep_graphs in dep_parser.parse_sents((
...     ""I 'm a dog"".split(),
...     ""This is my friends ' cat ( the tabby )"".split(),
... ))], []) 
[Tree('dog', ['I', ""'m"", 'a']), Tree('cat', ['This', 'is', Tree('friends', ['my', ""'""]), Tree('tabby', ['the'])])]
",split,N/A,N/A,N/A
">>> sum([[parse.tree() for parse in dep_graphs] for dep_graphs in dep_parser.parse_sents((
...     ""I 'm a dog"".split(),
...     ""This is my friends ' cat ( the tabby )"".split(),
... ))], []) 
[Tree('dog', ['I', ""'m"", 'a']), Tree('cat', ['This', 'is', Tree('friends', ['my', ""'""]), Tree('tabby', ['the'])])]
",Tree,__init__,nltk\nltk\tree\tree.py,True
">>> sum([[list(parse.triples()) for parse in dep_graphs] for dep_graphs in dep_parser.tagged_parse_sents((
...     (
...         (""The"", ""DT""),
...         (""quick"", ""JJ""),
...         (""brown"", ""JJ""),
...         (""fox"", ""NN""),
...         (""jumped"", ""VBD""),
...         (""over"", ""IN""),
...         (""the"", ""DT""),
...         (""lazy"", ""JJ""),
...         (""dog"", ""NN""),
...         (""."", "".""),
...     ),
... ))],[]) 
[[((u'jumped', u'VBD'), u'nsubj', (u'fox', u'NN')), ((u'fox', u'NN'), u'det', (u'The', u'DT')),
((u'fox', u'NN'), u'amod', (u'quick', u'JJ')), ((u'fox', u'NN'), u'amod', (u'brown', u'JJ')),
((u'jumped', u'VBD'), u'nmod', (u'dog', u'NN')), ((u'dog', u'NN'), u'case', (u'over', u'IN')),
((u'dog', u'NN'), u'det', (u'the', u'DT')), ((u'dog', u'NN'), u'amod', (u'lazy', u'JJ'))]]
",sum,N/A,N/A,N/A
">>> sum([[list(parse.triples()) for parse in dep_graphs] for dep_graphs in dep_parser.tagged_parse_sents((
...     (
...         (""The"", ""DT""),
...         (""quick"", ""JJ""),
...         (""brown"", ""JJ""),
...         (""fox"", ""NN""),
...         (""jumped"", ""VBD""),
...         (""over"", ""IN""),
...         (""the"", ""DT""),
...         (""lazy"", ""JJ""),
...         (""dog"", ""NN""),
...         (""."", "".""),
...     ),
... ))],[]) 
[[((u'jumped', u'VBD'), u'nsubj', (u'fox', u'NN')), ((u'fox', u'NN'), u'det', (u'The', u'DT')),
((u'fox', u'NN'), u'amod', (u'quick', u'JJ')), ((u'fox', u'NN'), u'amod', (u'brown', u'JJ')),
((u'jumped', u'VBD'), u'nmod', (u'dog', u'NN')), ((u'dog', u'NN'), u'case', (u'over', u'IN')),
((u'dog', u'NN'), u'det', (u'the', u'DT')), ((u'dog', u'NN'), u'amod', (u'lazy', u'JJ'))]]
",list,N/A,N/A,N/A
">>> sum([[list(parse.triples()) for parse in dep_graphs] for dep_graphs in dep_parser.tagged_parse_sents((
...     (
...         (""The"", ""DT""),
...         (""quick"", ""JJ""),
...         (""brown"", ""JJ""),
...         (""fox"", ""NN""),
...         (""jumped"", ""VBD""),
...         (""over"", ""IN""),
...         (""the"", ""DT""),
...         (""lazy"", ""JJ""),
...         (""dog"", ""NN""),
...         (""."", "".""),
...     ),
... ))],[]) 
[[((u'jumped', u'VBD'), u'nsubj', (u'fox', u'NN')), ((u'fox', u'NN'), u'det', (u'The', u'DT')),
((u'fox', u'NN'), u'amod', (u'quick', u'JJ')), ((u'fox', u'NN'), u'amod', (u'brown', u'JJ')),
((u'jumped', u'VBD'), u'nmod', (u'dog', u'NN')), ((u'dog', u'NN'), u'case', (u'over', u'IN')),
((u'dog', u'NN'), u'det', (u'the', u'DT')), ((u'dog', u'NN'), u'amod', (u'lazy', u'JJ'))]]
",parse.triples,N/A,N/A,N/A
">>> sum([[list(parse.triples()) for parse in dep_graphs] for dep_graphs in dep_parser.tagged_parse_sents((
...     (
...         (""The"", ""DT""),
...         (""quick"", ""JJ""),
...         (""brown"", ""JJ""),
...         (""fox"", ""NN""),
...         (""jumped"", ""VBD""),
...         (""over"", ""IN""),
...         (""the"", ""DT""),
...         (""lazy"", ""JJ""),
...         (""dog"", ""NN""),
...         (""."", "".""),
...     ),
... ))],[]) 
[[((u'jumped', u'VBD'), u'nsubj', (u'fox', u'NN')), ((u'fox', u'NN'), u'det', (u'The', u'DT')),
((u'fox', u'NN'), u'amod', (u'quick', u'JJ')), ((u'fox', u'NN'), u'amod', (u'brown', u'JJ')),
((u'jumped', u'VBD'), u'nmod', (u'dog', u'NN')), ((u'dog', u'NN'), u'case', (u'over', u'IN')),
((u'dog', u'NN'), u'det', (u'the', u'DT')), ((u'dog', u'NN'), u'amod', (u'lazy', u'JJ'))]]
",dep_parser.tagged_parse_sents,N/A,N/A,N/A
">>> from nltk.parse.stanford import StanfordNeuralDependencyParser
>>> dep_parser=StanfordNeuralDependencyParser(java_options='-mx4g')
",StanfordNeuralDependencyParser,__init__,nltk\nltk\parse\stanford.py,True
">>> [parse.tree() for parse in dep_parser.raw_parse(""The quick brown fox jumps over the lazy dog."")] 
[Tree('jumps', [Tree('fox', ['The', 'quick', 'brown']), Tree('dog', ['over', 'the', 'lazy']), '.'])]
",parse.tree,N/A,N/A,N/A
">>> [parse.tree() for parse in dep_parser.raw_parse(""The quick brown fox jumps over the lazy dog."")] 
[Tree('jumps', [Tree('fox', ['The', 'quick', 'brown']), Tree('dog', ['over', 'the', 'lazy']), '.'])]
",dep_parser.raw_parse,N/A,N/A,N/A
">>> [parse.tree() for parse in dep_parser.raw_parse(""The quick brown fox jumps over the lazy dog."")] 
[Tree('jumps', [Tree('fox', ['The', 'quick', 'brown']), Tree('dog', ['over', 'the', 'lazy']), '.'])]
",Tree,__init__,nltk\nltk\tree\tree.py,True
">>> [list(parse.triples()) for parse in dep_parser.raw_parse(""The quick brown fox jumps over the lazy dog."")] 
[[((u'jumps', u'VBZ'), u'nsubj', (u'fox', u'NN')), ((u'fox', u'NN'), u'det',
(u'The', u'DT')), ((u'fox', u'NN'), u'amod', (u'quick', u'JJ')), ((u'fox', u'NN'),
u'amod', (u'brown', u'JJ')), ((u'jumps', u'VBZ'), u'nmod', (u'dog', u'NN')),
((u'dog', u'NN'), u'case', (u'over', u'IN')), ((u'dog', u'NN'), u'det',
(u'the', u'DT')), ((u'dog', u'NN'), u'amod', (u'lazy', u'JJ')), ((u'jumps', u'VBZ'),
u'punct', (u'.', u'.'))]]
",list,N/A,N/A,N/A
">>> [list(parse.triples()) for parse in dep_parser.raw_parse(""The quick brown fox jumps over the lazy dog."")] 
[[((u'jumps', u'VBZ'), u'nsubj', (u'fox', u'NN')), ((u'fox', u'NN'), u'det',
(u'The', u'DT')), ((u'fox', u'NN'), u'amod', (u'quick', u'JJ')), ((u'fox', u'NN'),
u'amod', (u'brown', u'JJ')), ((u'jumps', u'VBZ'), u'nmod', (u'dog', u'NN')),
((u'dog', u'NN'), u'case', (u'over', u'IN')), ((u'dog', u'NN'), u'det',
(u'the', u'DT')), ((u'dog', u'NN'), u'amod', (u'lazy', u'JJ')), ((u'jumps', u'VBZ'),
u'punct', (u'.', u'.'))]]
",parse.triples,N/A,N/A,N/A
">>> [list(parse.triples()) for parse in dep_parser.raw_parse(""The quick brown fox jumps over the lazy dog."")] 
[[((u'jumps', u'VBZ'), u'nsubj', (u'fox', u'NN')), ((u'fox', u'NN'), u'det',
(u'The', u'DT')), ((u'fox', u'NN'), u'amod', (u'quick', u'JJ')), ((u'fox', u'NN'),
u'amod', (u'brown', u'JJ')), ((u'jumps', u'VBZ'), u'nmod', (u'dog', u'NN')),
((u'dog', u'NN'), u'case', (u'over', u'IN')), ((u'dog', u'NN'), u'det',
(u'the', u'DT')), ((u'dog', u'NN'), u'amod', (u'lazy', u'JJ')), ((u'jumps', u'VBZ'),
u'punct', (u'.', u'.'))]]
",dep_parser.raw_parse,N/A,N/A,N/A
">>> sum([[parse.tree() for parse in dep_graphs] for dep_graphs in dep_parser.raw_parse_sents((
...     ""The quick brown fox jumps over the lazy dog."",
...     ""The quick grey wolf jumps over the lazy fox.""
... ))], []) 
[Tree('jumps', [Tree('fox', ['The', 'quick', 'brown']), Tree('dog', ['over',
'the', 'lazy']), '.']), Tree('jumps', [Tree('wolf', ['The', 'quick', 'grey']),
Tree('fox', ['over', 'the', 'lazy']), '.'])]
",sum,N/A,N/A,N/A
">>> sum([[parse.tree() for parse in dep_graphs] for dep_graphs in dep_parser.raw_parse_sents((
...     ""The quick brown fox jumps over the lazy dog."",
...     ""The quick grey wolf jumps over the lazy fox.""
... ))], []) 
[Tree('jumps', [Tree('fox', ['The', 'quick', 'brown']), Tree('dog', ['over',
'the', 'lazy']), '.']), Tree('jumps', [Tree('wolf', ['The', 'quick', 'grey']),
Tree('fox', ['over', 'the', 'lazy']), '.'])]
",parse.tree,N/A,N/A,N/A
">>> sum([[parse.tree() for parse in dep_graphs] for dep_graphs in dep_parser.raw_parse_sents((
...     ""The quick brown fox jumps over the lazy dog."",
...     ""The quick grey wolf jumps over the lazy fox.""
... ))], []) 
[Tree('jumps', [Tree('fox', ['The', 'quick', 'brown']), Tree('dog', ['over',
'the', 'lazy']), '.']), Tree('jumps', [Tree('wolf', ['The', 'quick', 'grey']),
Tree('fox', ['over', 'the', 'lazy']), '.'])]
",dep_parser.raw_parse_sents,N/A,N/A,N/A
">>> sum([[parse.tree() for parse in dep_graphs] for dep_graphs in dep_parser.raw_parse_sents((
...     ""The quick brown fox jumps over the lazy dog."",
...     ""The quick grey wolf jumps over the lazy fox.""
... ))], []) 
[Tree('jumps', [Tree('fox', ['The', 'quick', 'brown']), Tree('dog', ['over',
'the', 'lazy']), '.']), Tree('jumps', [Tree('wolf', ['The', 'quick', 'grey']),
Tree('fox', ['over', 'the', 'lazy']), '.'])]
",Tree,__init__,nltk\nltk\tree\tree.py,True
">>> sum([[parse.tree() for parse in dep_graphs] for dep_graphs in dep_parser.parse_sents((
...     ""I 'm a dog"".split(),
...     ""This is my friends ' cat ( the tabby )"".split(),
... ))], []) 
[Tree('dog', ['I', ""'m"", 'a']), Tree('cat', ['This', 'is', Tree('friends',
['my', ""'""]), Tree('tabby', ['-LRB-', 'the', '-RRB-'])])]
",sum,N/A,N/A,N/A
">>> sum([[parse.tree() for parse in dep_graphs] for dep_graphs in dep_parser.parse_sents((
...     ""I 'm a dog"".split(),
...     ""This is my friends ' cat ( the tabby )"".split(),
... ))], []) 
[Tree('dog', ['I', ""'m"", 'a']), Tree('cat', ['This', 'is', Tree('friends',
['my', ""'""]), Tree('tabby', ['-LRB-', 'the', '-RRB-'])])]
",parse.tree,N/A,N/A,N/A
">>> sum([[parse.tree() for parse in dep_graphs] for dep_graphs in dep_parser.parse_sents((
...     ""I 'm a dog"".split(),
...     ""This is my friends ' cat ( the tabby )"".split(),
... ))], []) 
[Tree('dog', ['I', ""'m"", 'a']), Tree('cat', ['This', 'is', Tree('friends',
['my', ""'""]), Tree('tabby', ['-LRB-', 'the', '-RRB-'])])]
",dep_parser.parse_sents,N/A,N/A,N/A
">>> sum([[parse.tree() for parse in dep_graphs] for dep_graphs in dep_parser.parse_sents((
...     ""I 'm a dog"".split(),
...     ""This is my friends ' cat ( the tabby )"".split(),
... ))], []) 
[Tree('dog', ['I', ""'m"", 'a']), Tree('cat', ['This', 'is', Tree('friends',
['my', ""'""]), Tree('tabby', ['-LRB-', 'the', '-RRB-'])])]
",split,N/A,N/A,N/A
">>> sum([[parse.tree() for parse in dep_graphs] for dep_graphs in dep_parser.parse_sents((
...     ""I 'm a dog"".split(),
...     ""This is my friends ' cat ( the tabby )"".split(),
... ))], []) 
[Tree('dog', ['I', ""'m"", 'a']), Tree('cat', ['This', 'is', Tree('friends',
['my', ""'""]), Tree('tabby', ['-LRB-', 'the', '-RRB-'])])]
",Tree,__init__,nltk\nltk\tree\tree.py,True
">>> parser=StanfordParser(
...     model_path=""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz""
... )
",StanfordParser,__init__,nltk\nltk\parse\stanford.py,True
">>> list(parser.raw_parse(""the quick brown fox jumps over the lazy dog"")) 
[Tree('ROOT', [Tree('NP', [Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['quick']), Tree('JJ', ['brown']),
Tree('NN', ['fox'])]), Tree('NP', [Tree('NP', [Tree('NNS', ['jumps'])]), Tree('PP', [Tree('IN', ['over']),
Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['lazy']), Tree('NN', ['dog'])])])])])])]
",list,N/A,N/A,N/A
">>> list(parser.raw_parse(""the quick brown fox jumps over the lazy dog"")) 
[Tree('ROOT', [Tree('NP', [Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['quick']), Tree('JJ', ['brown']),
Tree('NN', ['fox'])]), Tree('NP', [Tree('NP', [Tree('NNS', ['jumps'])]), Tree('PP', [Tree('IN', ['over']),
Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['lazy']), Tree('NN', ['dog'])])])])])])]
",parser.raw_parse,N/A,N/A,N/A
">>> list(parser.raw_parse(""the quick brown fox jumps over the lazy dog"")) 
[Tree('ROOT', [Tree('NP', [Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['quick']), Tree('JJ', ['brown']),
Tree('NN', ['fox'])]), Tree('NP', [Tree('NP', [Tree('NNS', ['jumps'])]), Tree('PP', [Tree('IN', ['over']),
Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['lazy']), Tree('NN', ['dog'])])])])])])]
",Tree,__init__,nltk\nltk\tree\tree.py,True
">>> sum([list(dep_graphs) for dep_graphs in parser.raw_parse_sents((
...     ""the quick brown fox jumps over the lazy dog"",
...     ""the quick grey wolf jumps over the lazy fox""
... ))], []) 
[Tree('ROOT', [Tree('NP', [Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['quick']), Tree('JJ', ['brown']),
Tree('NN', ['fox'])]), Tree('NP', [Tree('NP', [Tree('NNS', ['jumps'])]), Tree('PP', [Tree('IN', ['over']),
Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['lazy']), Tree('NN', ['dog'])])])])])]), Tree('ROOT', [Tree('NP',
[Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['quick']), Tree('JJ', ['grey']), Tree('NN', ['wolf'])]), Tree('NP',
[Tree('NP', [Tree('NNS', ['jumps'])]), Tree('PP', [Tree('IN', ['over']), Tree('NP', [Tree('DT', ['the']),
Tree('JJ', ['lazy']), Tree('NN', ['fox'])])])])])])]
",sum,N/A,N/A,N/A
">>> sum([list(dep_graphs) for dep_graphs in parser.raw_parse_sents((
...     ""the quick brown fox jumps over the lazy dog"",
...     ""the quick grey wolf jumps over the lazy fox""
... ))], []) 
[Tree('ROOT', [Tree('NP', [Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['quick']), Tree('JJ', ['brown']),
Tree('NN', ['fox'])]), Tree('NP', [Tree('NP', [Tree('NNS', ['jumps'])]), Tree('PP', [Tree('IN', ['over']),
Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['lazy']), Tree('NN', ['dog'])])])])])]), Tree('ROOT', [Tree('NP',
[Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['quick']), Tree('JJ', ['grey']), Tree('NN', ['wolf'])]), Tree('NP',
[Tree('NP', [Tree('NNS', ['jumps'])]), Tree('PP', [Tree('IN', ['over']), Tree('NP', [Tree('DT', ['the']),
Tree('JJ', ['lazy']), Tree('NN', ['fox'])])])])])])]
",list,N/A,N/A,N/A
">>> sum([list(dep_graphs) for dep_graphs in parser.raw_parse_sents((
...     ""the quick brown fox jumps over the lazy dog"",
...     ""the quick grey wolf jumps over the lazy fox""
... ))], []) 
[Tree('ROOT', [Tree('NP', [Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['quick']), Tree('JJ', ['brown']),
Tree('NN', ['fox'])]), Tree('NP', [Tree('NP', [Tree('NNS', ['jumps'])]), Tree('PP', [Tree('IN', ['over']),
Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['lazy']), Tree('NN', ['dog'])])])])])]), Tree('ROOT', [Tree('NP',
[Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['quick']), Tree('JJ', ['grey']), Tree('NN', ['wolf'])]), Tree('NP',
[Tree('NP', [Tree('NNS', ['jumps'])]), Tree('PP', [Tree('IN', ['over']), Tree('NP', [Tree('DT', ['the']),
Tree('JJ', ['lazy']), Tree('NN', ['fox'])])])])])])]
",parser.raw_parse_sents,N/A,N/A,N/A
">>> sum([list(dep_graphs) for dep_graphs in parser.raw_parse_sents((
...     ""the quick brown fox jumps over the lazy dog"",
...     ""the quick grey wolf jumps over the lazy fox""
... ))], []) 
[Tree('ROOT', [Tree('NP', [Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['quick']), Tree('JJ', ['brown']),
Tree('NN', ['fox'])]), Tree('NP', [Tree('NP', [Tree('NNS', ['jumps'])]), Tree('PP', [Tree('IN', ['over']),
Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['lazy']), Tree('NN', ['dog'])])])])])]), Tree('ROOT', [Tree('NP',
[Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['quick']), Tree('JJ', ['grey']), Tree('NN', ['wolf'])]), Tree('NP',
[Tree('NP', [Tree('NNS', ['jumps'])]), Tree('PP', [Tree('IN', ['over']), Tree('NP', [Tree('DT', ['the']),
Tree('JJ', ['lazy']), Tree('NN', ['fox'])])])])])])]
",Tree,__init__,nltk\nltk\tree\tree.py,True
">>> sum([list(dep_graphs) for dep_graphs in parser.parse_sents((
...     ""I 'm a dog"".split(),
...     ""This is my friends ' cat ( the tabby )"".split(),
... ))], []) 
[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('PRP', ['I'])]), Tree('VP', [Tree('VBP', [""'m""]),
Tree('NP', [Tree('DT', ['a']), Tree('NN', ['dog'])])])])]), Tree('ROOT', [Tree('S', [Tree('NP',
[Tree('DT', ['This'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('NP', [Tree('NP', [Tree('NP', [Tree('PRP$', ['my']),
Tree('NNS', ['friends']), Tree('POS', [""'""])]), Tree('NN', ['cat'])]), Tree('PRN', [Tree('-LRB-', [Tree('', []),
Tree('NP', [Tree('DT', ['the']), Tree('NN', ['tabby'])]), Tree('-RRB-', [])])])])])])])]
",sum,N/A,N/A,N/A
">>> sum([list(dep_graphs) for dep_graphs in parser.parse_sents((
...     ""I 'm a dog"".split(),
...     ""This is my friends ' cat ( the tabby )"".split(),
... ))], []) 
[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('PRP', ['I'])]), Tree('VP', [Tree('VBP', [""'m""]),
Tree('NP', [Tree('DT', ['a']), Tree('NN', ['dog'])])])])]), Tree('ROOT', [Tree('S', [Tree('NP',
[Tree('DT', ['This'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('NP', [Tree('NP', [Tree('NP', [Tree('PRP$', ['my']),
Tree('NNS', ['friends']), Tree('POS', [""'""])]), Tree('NN', ['cat'])]), Tree('PRN', [Tree('-LRB-', [Tree('', []),
Tree('NP', [Tree('DT', ['the']), Tree('NN', ['tabby'])]), Tree('-RRB-', [])])])])])])])]
",list,N/A,N/A,N/A
">>> sum([list(dep_graphs) for dep_graphs in parser.parse_sents((
...     ""I 'm a dog"".split(),
...     ""This is my friends ' cat ( the tabby )"".split(),
... ))], []) 
[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('PRP', ['I'])]), Tree('VP', [Tree('VBP', [""'m""]),
Tree('NP', [Tree('DT', ['a']), Tree('NN', ['dog'])])])])]), Tree('ROOT', [Tree('S', [Tree('NP',
[Tree('DT', ['This'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('NP', [Tree('NP', [Tree('NP', [Tree('PRP$', ['my']),
Tree('NNS', ['friends']), Tree('POS', [""'""])]), Tree('NN', ['cat'])]), Tree('PRN', [Tree('-LRB-', [Tree('', []),
Tree('NP', [Tree('DT', ['the']), Tree('NN', ['tabby'])]), Tree('-RRB-', [])])])])])])])]
",parser.parse_sents,N/A,N/A,N/A
">>> sum([list(dep_graphs) for dep_graphs in parser.parse_sents((
...     ""I 'm a dog"".split(),
...     ""This is my friends ' cat ( the tabby )"".split(),
... ))], []) 
[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('PRP', ['I'])]), Tree('VP', [Tree('VBP', [""'m""]),
Tree('NP', [Tree('DT', ['a']), Tree('NN', ['dog'])])])])]), Tree('ROOT', [Tree('S', [Tree('NP',
[Tree('DT', ['This'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('NP', [Tree('NP', [Tree('NP', [Tree('PRP$', ['my']),
Tree('NNS', ['friends']), Tree('POS', [""'""])]), Tree('NN', ['cat'])]), Tree('PRN', [Tree('-LRB-', [Tree('', []),
Tree('NP', [Tree('DT', ['the']), Tree('NN', ['tabby'])]), Tree('-RRB-', [])])])])])])])]
",split,N/A,N/A,N/A
">>> sum([list(dep_graphs) for dep_graphs in parser.parse_sents((
...     ""I 'm a dog"".split(),
...     ""This is my friends ' cat ( the tabby )"".split(),
... ))], []) 
[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('PRP', ['I'])]), Tree('VP', [Tree('VBP', [""'m""]),
Tree('NP', [Tree('DT', ['a']), Tree('NN', ['dog'])])])])]), Tree('ROOT', [Tree('S', [Tree('NP',
[Tree('DT', ['This'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('NP', [Tree('NP', [Tree('NP', [Tree('PRP$', ['my']),
Tree('NNS', ['friends']), Tree('POS', [""'""])]), Tree('NN', ['cat'])]), Tree('PRN', [Tree('-LRB-', [Tree('', []),
Tree('NP', [Tree('DT', ['the']), Tree('NN', ['tabby'])]), Tree('-RRB-', [])])])])])])])]
",Tree,__init__,nltk\nltk\tree\tree.py,True
">>> sum([list(dep_graphs) for dep_graphs in parser.tagged_parse_sents((
...     (
...         (""The"", ""DT""),
...         (""quick"", ""JJ""),
...         (""brown"", ""JJ""),
...         (""fox"", ""NN""),
...         (""jumped"", ""VBD""),
...         (""over"", ""IN""),
...         (""the"", ""DT""),
...         (""lazy"", ""JJ""),
...         (""dog"", ""NN""),
...         (""."", "".""),
...     ),
... ))],[]) 
[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('DT', ['The']), Tree('JJ', ['quick']), Tree('JJ', ['brown']),
Tree('NN', ['fox'])]), Tree('VP', [Tree('VBD', ['jumped']), Tree('PP', [Tree('IN', ['over']), Tree('NP',
[Tree('DT', ['the']), Tree('JJ', ['lazy']), Tree('NN', ['dog'])])])]), Tree('.', ['.'])])])]
",sum,N/A,N/A,N/A
">>> sum([list(dep_graphs) for dep_graphs in parser.tagged_parse_sents((
...     (
...         (""The"", ""DT""),
...         (""quick"", ""JJ""),
...         (""brown"", ""JJ""),
...         (""fox"", ""NN""),
...         (""jumped"", ""VBD""),
...         (""over"", ""IN""),
...         (""the"", ""DT""),
...         (""lazy"", ""JJ""),
...         (""dog"", ""NN""),
...         (""."", "".""),
...     ),
... ))],[]) 
[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('DT', ['The']), Tree('JJ', ['quick']), Tree('JJ', ['brown']),
Tree('NN', ['fox'])]), Tree('VP', [Tree('VBD', ['jumped']), Tree('PP', [Tree('IN', ['over']), Tree('NP',
[Tree('DT', ['the']), Tree('JJ', ['lazy']), Tree('NN', ['dog'])])])]), Tree('.', ['.'])])])]
",list,N/A,N/A,N/A
">>> sum([list(dep_graphs) for dep_graphs in parser.tagged_parse_sents((
...     (
...         (""The"", ""DT""),
...         (""quick"", ""JJ""),
...         (""brown"", ""JJ""),
...         (""fox"", ""NN""),
...         (""jumped"", ""VBD""),
...         (""over"", ""IN""),
...         (""the"", ""DT""),
...         (""lazy"", ""JJ""),
...         (""dog"", ""NN""),
...         (""."", "".""),
...     ),
... ))],[]) 
[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('DT', ['The']), Tree('JJ', ['quick']), Tree('JJ', ['brown']),
Tree('NN', ['fox'])]), Tree('VP', [Tree('VBD', ['jumped']), Tree('PP', [Tree('IN', ['over']), Tree('NP',
[Tree('DT', ['the']), Tree('JJ', ['lazy']), Tree('NN', ['dog'])])])]), Tree('.', ['.'])])])]
",parser.tagged_parse_sents,N/A,N/A,N/A
">>> sum([list(dep_graphs) for dep_graphs in parser.tagged_parse_sents((
...     (
...         (""The"", ""DT""),
...         (""quick"", ""JJ""),
...         (""brown"", ""JJ""),
...         (""fox"", ""NN""),
...         (""jumped"", ""VBD""),
...         (""over"", ""IN""),
...         (""the"", ""DT""),
...         (""lazy"", ""JJ""),
...         (""dog"", ""NN""),
...         (""."", "".""),
...     ),
... ))],[]) 
[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('DT', ['The']), Tree('JJ', ['quick']), Tree('JJ', ['brown']),
Tree('NN', ['fox'])]), Tree('VP', [Tree('VBD', ['jumped']), Tree('PP', [Tree('IN', ['over']), Tree('NP',
[Tree('DT', ['the']), Tree('JJ', ['lazy']), Tree('NN', ['dog'])])])]), Tree('.', ['.'])])])]
",Tree,__init__,nltk\nltk\tree\tree.py,True
">>> from nltk.parse import DependencyGraph, DependencyEvaluator
>>> from nltk.parse.transitionparser import TransitionParser, Configuration, Transition
>>> gold_sent = DependencyGraph(""""""
... Economic  JJ     2      ATT
... news  NN     3       SBJ
... has       VBD       0       ROOT
... little      JJ      5       ATT
... effect   NN     3       OBJ
... on     IN      5       ATT
... financial       JJ       8       ATT
... markets    NNS      6       PC
... .    .      3       PU
... """""")
",DependencyGraph,__init__,nltk\nltk\parse\dependencygraph.py,True
">>> conf = Configuration(gold_sent)
",Configuration,__init__,nltk\nltk\parse\transitionparser.py,True
">>> print(', '.join(conf.extract_features()))
STK_0_POS_TOP, BUF_0_FORM_Economic, BUF_0_LEMMA_Economic, BUF_0_POS_JJ, BUF_1_FORM_news, BUF_1_POS_NN, BUF_2_POS_VBD, BUF_3_POS_JJ
",print,N/A,N/A,N/A
">>> print(', '.join(conf.extract_features()))
STK_0_POS_TOP, BUF_0_FORM_Economic, BUF_0_LEMMA_Economic, BUF_0_POS_JJ, BUF_1_FORM_news, BUF_1_POS_NN, BUF_2_POS_VBD, BUF_3_POS_JJ
",join,N/A,N/A,N/A
">>> print(', '.join(conf.extract_features()))
STK_0_POS_TOP, BUF_0_FORM_Economic, BUF_0_LEMMA_Economic, BUF_0_POS_JJ, BUF_1_FORM_news, BUF_1_POS_NN, BUF_2_POS_VBD, BUF_3_POS_JJ
",conf.extract_features,N/A,N/A,N/A
">>> operation = Transition('arc-standard')
>>> operation.shift(conf)
>>> operation.left_arc(conf, ""ATT"")
>>> operation.shift(conf)
>>> operation.left_arc(conf,""SBJ"")
>>> operation.shift(conf)
>>> operation.shift(conf)
>>> operation.left_arc(conf, ""ATT"")
>>> operation.shift(conf)
>>> operation.shift(conf)
>>> operation.shift(conf)
>>> operation.left_arc(conf, ""ATT"")
",Transition,__init__,nltk\nltk\parse\transitionparser.py,True
">>> operation = Transition('arc-standard')
>>> operation.shift(conf)
>>> operation.left_arc(conf, ""ATT"")
>>> operation.shift(conf)
>>> operation.left_arc(conf,""SBJ"")
>>> operation.shift(conf)
>>> operation.shift(conf)
>>> operation.left_arc(conf, ""ATT"")
>>> operation.shift(conf)
>>> operation.shift(conf)
>>> operation.shift(conf)
>>> operation.left_arc(conf, ""ATT"")
",operation.shift,N/A,N/A,N/A
">>> operation = Transition('arc-standard')
>>> operation.shift(conf)
>>> operation.left_arc(conf, ""ATT"")
>>> operation.shift(conf)
>>> operation.left_arc(conf,""SBJ"")
>>> operation.shift(conf)
>>> operation.shift(conf)
>>> operation.left_arc(conf, ""ATT"")
>>> operation.shift(conf)
>>> operation.shift(conf)
>>> operation.shift(conf)
>>> operation.left_arc(conf, ""ATT"")
",operation.left_arc,N/A,N/A,N/A
">>> print(', '.join(conf.extract_features()))
STK_0_FORM_on, STK_0_LEMMA_on, STK_0_POS_IN, STK_1_POS_NN, BUF_0_FORM_markets, BUF_0_LEMMA_markets, BUF_0_POS_NNS, BUF_1_FORM_., BUF_1_POS_., BUF_0_LDEP_ATT
",print,N/A,N/A,N/A
">>> print(', '.join(conf.extract_features()))
STK_0_FORM_on, STK_0_LEMMA_on, STK_0_POS_IN, STK_1_POS_NN, BUF_0_FORM_markets, BUF_0_LEMMA_markets, BUF_0_POS_NNS, BUF_1_FORM_., BUF_1_POS_., BUF_0_LDEP_ATT
",join,N/A,N/A,N/A
">>> print(', '.join(conf.extract_features()))
STK_0_FORM_on, STK_0_LEMMA_on, STK_0_POS_IN, STK_1_POS_NN, BUF_0_FORM_markets, BUF_0_LEMMA_markets, BUF_0_POS_NNS, BUF_1_FORM_., BUF_1_POS_., BUF_0_LDEP_ATT
",conf.extract_features,N/A,N/A,N/A
">>> operation.right_arc(conf, ""PC"")
>>> operation.right_arc(conf, ""ATT"")
>>> operation.right_arc(conf, ""OBJ"")
>>> operation.shift(conf)
>>> operation.right_arc(conf, ""PU"")
>>> operation.right_arc(conf, ""ROOT"")
>>> operation.shift(conf)
",operation.right_arc,N/A,N/A,N/A
">>> operation.right_arc(conf, ""PC"")
>>> operation.right_arc(conf, ""ATT"")
>>> operation.right_arc(conf, ""OBJ"")
>>> operation.shift(conf)
>>> operation.right_arc(conf, ""PU"")
>>> operation.right_arc(conf, ""ROOT"")
>>> operation.shift(conf)
",operation.shift,N/A,N/A,N/A
">>> conf = Configuration(gold_sent)
>>> operation = Transition('arc-eager')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'ATT')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'SBJ')
>>> operation.right_arc(conf,'ROOT')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'ATT')
>>> operation.right_arc(conf,'OBJ')
>>> operation.right_arc(conf,'ATT')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'ATT')
>>> operation.right_arc(conf,'PC')
>>> operation.reduce(conf)
>>> operation.reduce(conf)
>>> operation.reduce(conf)
>>> operation.right_arc(conf,'PU')
>>> print(conf)
Stack : [0, 3, 9]  Buffer : []   Arcs : [(2, 'ATT', 1), (3, 'SBJ', 2), (0, 'ROOT', 3), (5, 'ATT', 4), (3, 'OBJ', 5), (5, 'ATT', 6), (8, 'ATT', 7), (6, 'PC', 8), (3, 'PU', 9)]
",Configuration,__init__,nltk\nltk\parse\transitionparser.py,True
">>> conf = Configuration(gold_sent)
>>> operation = Transition('arc-eager')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'ATT')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'SBJ')
>>> operation.right_arc(conf,'ROOT')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'ATT')
>>> operation.right_arc(conf,'OBJ')
>>> operation.right_arc(conf,'ATT')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'ATT')
>>> operation.right_arc(conf,'PC')
>>> operation.reduce(conf)
>>> operation.reduce(conf)
>>> operation.reduce(conf)
>>> operation.right_arc(conf,'PU')
>>> print(conf)
Stack : [0, 3, 9]  Buffer : []   Arcs : [(2, 'ATT', 1), (3, 'SBJ', 2), (0, 'ROOT', 3), (5, 'ATT', 4), (3, 'OBJ', 5), (5, 'ATT', 6), (8, 'ATT', 7), (6, 'PC', 8), (3, 'PU', 9)]
",Transition,__init__,nltk\nltk\parse\transitionparser.py,True
">>> conf = Configuration(gold_sent)
>>> operation = Transition('arc-eager')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'ATT')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'SBJ')
>>> operation.right_arc(conf,'ROOT')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'ATT')
>>> operation.right_arc(conf,'OBJ')
>>> operation.right_arc(conf,'ATT')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'ATT')
>>> operation.right_arc(conf,'PC')
>>> operation.reduce(conf)
>>> operation.reduce(conf)
>>> operation.reduce(conf)
>>> operation.right_arc(conf,'PU')
>>> print(conf)
Stack : [0, 3, 9]  Buffer : []   Arcs : [(2, 'ATT', 1), (3, 'SBJ', 2), (0, 'ROOT', 3), (5, 'ATT', 4), (3, 'OBJ', 5), (5, 'ATT', 6), (8, 'ATT', 7), (6, 'PC', 8), (3, 'PU', 9)]
",operation.shift,N/A,N/A,N/A
">>> conf = Configuration(gold_sent)
>>> operation = Transition('arc-eager')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'ATT')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'SBJ')
>>> operation.right_arc(conf,'ROOT')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'ATT')
>>> operation.right_arc(conf,'OBJ')
>>> operation.right_arc(conf,'ATT')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'ATT')
>>> operation.right_arc(conf,'PC')
>>> operation.reduce(conf)
>>> operation.reduce(conf)
>>> operation.reduce(conf)
>>> operation.right_arc(conf,'PU')
>>> print(conf)
Stack : [0, 3, 9]  Buffer : []   Arcs : [(2, 'ATT', 1), (3, 'SBJ', 2), (0, 'ROOT', 3), (5, 'ATT', 4), (3, 'OBJ', 5), (5, 'ATT', 6), (8, 'ATT', 7), (6, 'PC', 8), (3, 'PU', 9)]
",operation.left_arc,N/A,N/A,N/A
">>> conf = Configuration(gold_sent)
>>> operation = Transition('arc-eager')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'ATT')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'SBJ')
>>> operation.right_arc(conf,'ROOT')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'ATT')
>>> operation.right_arc(conf,'OBJ')
>>> operation.right_arc(conf,'ATT')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'ATT')
>>> operation.right_arc(conf,'PC')
>>> operation.reduce(conf)
>>> operation.reduce(conf)
>>> operation.reduce(conf)
>>> operation.right_arc(conf,'PU')
>>> print(conf)
Stack : [0, 3, 9]  Buffer : []   Arcs : [(2, 'ATT', 1), (3, 'SBJ', 2), (0, 'ROOT', 3), (5, 'ATT', 4), (3, 'OBJ', 5), (5, 'ATT', 6), (8, 'ATT', 7), (6, 'PC', 8), (3, 'PU', 9)]
",operation.right_arc,N/A,N/A,N/A
">>> conf = Configuration(gold_sent)
>>> operation = Transition('arc-eager')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'ATT')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'SBJ')
>>> operation.right_arc(conf,'ROOT')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'ATT')
>>> operation.right_arc(conf,'OBJ')
>>> operation.right_arc(conf,'ATT')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'ATT')
>>> operation.right_arc(conf,'PC')
>>> operation.reduce(conf)
>>> operation.reduce(conf)
>>> operation.reduce(conf)
>>> operation.right_arc(conf,'PU')
>>> print(conf)
Stack : [0, 3, 9]  Buffer : []   Arcs : [(2, 'ATT', 1), (3, 'SBJ', 2), (0, 'ROOT', 3), (5, 'ATT', 4), (3, 'OBJ', 5), (5, 'ATT', 6), (8, 'ATT', 7), (6, 'PC', 8), (3, 'PU', 9)]
",operation.reduce,N/A,N/A,N/A
">>> conf = Configuration(gold_sent)
>>> operation = Transition('arc-eager')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'ATT')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'SBJ')
>>> operation.right_arc(conf,'ROOT')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'ATT')
>>> operation.right_arc(conf,'OBJ')
>>> operation.right_arc(conf,'ATT')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'ATT')
>>> operation.right_arc(conf,'PC')
>>> operation.reduce(conf)
>>> operation.reduce(conf)
>>> operation.reduce(conf)
>>> operation.right_arc(conf,'PU')
>>> print(conf)
Stack : [0, 3, 9]  Buffer : []   Arcs : [(2, 'ATT', 1), (3, 'SBJ', 2), (0, 'ROOT', 3), (5, 'ATT', 4), (3, 'OBJ', 5), (5, 'ATT', 6), (8, 'ATT', 7), (6, 'PC', 8), (3, 'PU', 9)]
",print,N/A,N/A,N/A
">>> parser_std = TransitionParser('arc-standard')
>>> print(', '.join(parser_std._create_training_examples_arc_std([gold_sent], input_file)))
 Number of training examples : 1
 Number of valid (projective) examples : 1
SHIFT, LEFTARC:ATT, SHIFT, LEFTARC:SBJ, SHIFT, SHIFT, LEFTARC:ATT, SHIFT, SHIFT, SHIFT, LEFTARC:ATT, RIGHTARC:PC, RIGHTARC:ATT, RIGHTARC:OBJ, SHIFT, RIGHTARC:PU, RIGHTARC:ROOT, SHIFT
",TransitionParser,__init__,nltk\nltk\parse\transitionparser.py,True
">>> parser_std = TransitionParser('arc-standard')
>>> print(', '.join(parser_std._create_training_examples_arc_std([gold_sent], input_file)))
 Number of training examples : 1
 Number of valid (projective) examples : 1
SHIFT, LEFTARC:ATT, SHIFT, LEFTARC:SBJ, SHIFT, SHIFT, LEFTARC:ATT, SHIFT, SHIFT, SHIFT, LEFTARC:ATT, RIGHTARC:PC, RIGHTARC:ATT, RIGHTARC:OBJ, SHIFT, RIGHTARC:PU, RIGHTARC:ROOT, SHIFT
",print,N/A,N/A,N/A
">>> parser_std = TransitionParser('arc-standard')
>>> print(', '.join(parser_std._create_training_examples_arc_std([gold_sent], input_file)))
 Number of training examples : 1
 Number of valid (projective) examples : 1
SHIFT, LEFTARC:ATT, SHIFT, LEFTARC:SBJ, SHIFT, SHIFT, LEFTARC:ATT, SHIFT, SHIFT, SHIFT, LEFTARC:ATT, RIGHTARC:PC, RIGHTARC:ATT, RIGHTARC:OBJ, SHIFT, RIGHTARC:PU, RIGHTARC:ROOT, SHIFT
",join,N/A,N/A,N/A
">>> parser_std = TransitionParser('arc-standard')
>>> print(', '.join(parser_std._create_training_examples_arc_std([gold_sent], input_file)))
 Number of training examples : 1
 Number of valid (projective) examples : 1
SHIFT, LEFTARC:ATT, SHIFT, LEFTARC:SBJ, SHIFT, SHIFT, LEFTARC:ATT, SHIFT, SHIFT, SHIFT, LEFTARC:ATT, RIGHTARC:PC, RIGHTARC:ATT, RIGHTARC:OBJ, SHIFT, RIGHTARC:PU, RIGHTARC:ROOT, SHIFT
",parser_std._create_training_examples_arc_std,N/A,N/A,N/A
">>> parser_std.train([gold_sent],'temp.arcstd.model', verbose=False)
 Number of training examples : 1
 Number of valid (projective) examples : 1
>>> remove(input_file.name)
",parser_std.train,N/A,N/A,N/A
">>> parser_std.train([gold_sent],'temp.arcstd.model', verbose=False)
 Number of training examples : 1
 Number of valid (projective) examples : 1
>>> remove(input_file.name)
",remove,N/A,N/A,N/A
">>> input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(),delete=False)
>>> parser_eager = TransitionParser('arc-eager')
>>> print(', '.join(parser_eager._create_training_examples_arc_eager([gold_sent], input_file)))
 Number of training examples : 1
 Number of valid (projective) examples : 1
SHIFT, LEFTARC:ATT, SHIFT, LEFTARC:SBJ, RIGHTARC:ROOT, SHIFT, LEFTARC:ATT, RIGHTARC:OBJ, RIGHTARC:ATT, SHIFT, LEFTARC:ATT, RIGHTARC:PC, REDUCE, REDUCE, REDUCE, RIGHTARC:PU
",tempfile.NamedTemporaryFile,N/A,N/A,N/A
">>> input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(),delete=False)
>>> parser_eager = TransitionParser('arc-eager')
>>> print(', '.join(parser_eager._create_training_examples_arc_eager([gold_sent], input_file)))
 Number of training examples : 1
 Number of valid (projective) examples : 1
SHIFT, LEFTARC:ATT, SHIFT, LEFTARC:SBJ, RIGHTARC:ROOT, SHIFT, LEFTARC:ATT, RIGHTARC:OBJ, RIGHTARC:ATT, SHIFT, LEFTARC:ATT, RIGHTARC:PC, REDUCE, REDUCE, REDUCE, RIGHTARC:PU
",tempfile.gettempdir,N/A,N/A,N/A
">>> input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(),delete=False)
>>> parser_eager = TransitionParser('arc-eager')
>>> print(', '.join(parser_eager._create_training_examples_arc_eager([gold_sent], input_file)))
 Number of training examples : 1
 Number of valid (projective) examples : 1
SHIFT, LEFTARC:ATT, SHIFT, LEFTARC:SBJ, RIGHTARC:ROOT, SHIFT, LEFTARC:ATT, RIGHTARC:OBJ, RIGHTARC:ATT, SHIFT, LEFTARC:ATT, RIGHTARC:PC, REDUCE, REDUCE, REDUCE, RIGHTARC:PU
",TransitionParser,__init__,nltk\nltk\parse\transitionparser.py,True
">>> input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(),delete=False)
>>> parser_eager = TransitionParser('arc-eager')
>>> print(', '.join(parser_eager._create_training_examples_arc_eager([gold_sent], input_file)))
 Number of training examples : 1
 Number of valid (projective) examples : 1
SHIFT, LEFTARC:ATT, SHIFT, LEFTARC:SBJ, RIGHTARC:ROOT, SHIFT, LEFTARC:ATT, RIGHTARC:OBJ, RIGHTARC:ATT, SHIFT, LEFTARC:ATT, RIGHTARC:PC, REDUCE, REDUCE, REDUCE, RIGHTARC:PU
",print,N/A,N/A,N/A
">>> input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(),delete=False)
>>> parser_eager = TransitionParser('arc-eager')
>>> print(', '.join(parser_eager._create_training_examples_arc_eager([gold_sent], input_file)))
 Number of training examples : 1
 Number of valid (projective) examples : 1
SHIFT, LEFTARC:ATT, SHIFT, LEFTARC:SBJ, RIGHTARC:ROOT, SHIFT, LEFTARC:ATT, RIGHTARC:OBJ, RIGHTARC:ATT, SHIFT, LEFTARC:ATT, RIGHTARC:PC, REDUCE, REDUCE, REDUCE, RIGHTARC:PU
",join,N/A,N/A,N/A
">>> input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(),delete=False)
>>> parser_eager = TransitionParser('arc-eager')
>>> print(', '.join(parser_eager._create_training_examples_arc_eager([gold_sent], input_file)))
 Number of training examples : 1
 Number of valid (projective) examples : 1
SHIFT, LEFTARC:ATT, SHIFT, LEFTARC:SBJ, RIGHTARC:ROOT, SHIFT, LEFTARC:ATT, RIGHTARC:OBJ, RIGHTARC:ATT, SHIFT, LEFTARC:ATT, RIGHTARC:PC, REDUCE, REDUCE, REDUCE, RIGHTARC:PU
",parser_eager._create_training_examples_arc_eager,N/A,N/A,N/A
">>> parser_eager.train([gold_sent],'temp.arceager.model', verbose=False)
 Number of training examples : 1
 Number of valid (projective) examples : 1
",parser_eager.train,N/A,N/A,N/A
">>> remove(input_file.name)
",remove,N/A,N/A,N/A
">>> result = parser_std.parse([gold_sent], 'temp.arcstd.model')
>>> de = DependencyEvaluator(result, [gold_sent])
>>> de.eval() >= (0, 0)
True
",parser_std.parse,N/A,N/A,N/A
">>> result = parser_std.parse([gold_sent], 'temp.arcstd.model')
>>> de = DependencyEvaluator(result, [gold_sent])
>>> de.eval() >= (0, 0)
True
",DependencyEvaluator,__init__,nltk\nltk\parse\evaluate.py,True
">>> result = parser_std.parse([gold_sent], 'temp.arcstd.model')
>>> de = DependencyEvaluator(result, [gold_sent])
>>> de.eval() >= (0, 0)
True
",de.eval,N/A,N/A,N/A
">>> from nltk import word_tokenize, pos_tag
>>> text = ""This is a foobar sentence.""
>>> for line in taggedsent_to_conll(pos_tag(word_tokenize(text))):
...         print(line, end="""")
    1       This    _       DT      DT      _       0       a       _       _
    2       is      _       VBZ     VBZ     _       0       a       _       _
    3       a       _       DT      DT      _       0       a       _       _
    4       foobar  _       JJ      JJ      _       0       a       _       _
    5       sentence        _       NN      NN      _       0       a       _       _
    6       .               _       .       .       _       0       a       _       _
",taggedsent_to_conll,N/A,N/A,N/A
">>> from nltk import word_tokenize, pos_tag
>>> text = ""This is a foobar sentence.""
>>> for line in taggedsent_to_conll(pos_tag(word_tokenize(text))):
...         print(line, end="""")
    1       This    _       DT      DT      _       0       a       _       _
    2       is      _       VBZ     VBZ     _       0       a       _       _
    3       a       _       DT      DT      _       0       a       _       _
    4       foobar  _       JJ      JJ      _       0       a       _       _
    5       sentence        _       NN      NN      _       0       a       _       _
    6       .               _       .       .       _       0       a       _       _
",pos_tag,N/A,N/A,N/A
">>> from nltk import word_tokenize, pos_tag
>>> text = ""This is a foobar sentence.""
>>> for line in taggedsent_to_conll(pos_tag(word_tokenize(text))):
...         print(line, end="""")
    1       This    _       DT      DT      _       0       a       _       _
    2       is      _       VBZ     VBZ     _       0       a       _       _
    3       a       _       DT      DT      _       0       a       _       _
    4       foobar  _       JJ      JJ      _       0       a       _       _
    5       sentence        _       NN      NN      _       0       a       _       _
    6       .               _       .       .       _       0       a       _       _
",word_tokenize,N/A,N/A,N/A
">>> from nltk import word_tokenize, pos_tag
>>> text = ""This is a foobar sentence.""
>>> for line in taggedsent_to_conll(pos_tag(word_tokenize(text))):
...         print(line, end="""")
    1       This    _       DT      DT      _       0       a       _       _
    2       is      _       VBZ     VBZ     _       0       a       _       _
    3       a       _       DT      DT      _       0       a       _       _
    4       foobar  _       JJ      JJ      _       0       a       _       _
    5       sentence        _       NN      NN      _       0       a       _       _
    6       .               _       .       .       _       0       a       _       _
",print,N/A,N/A,N/A
">>> from nltk import word_tokenize, sent_tokenize, pos_tag
>>> text = ""This is a foobar sentence. Is that right?""
>>> sentences = [pos_tag(word_tokenize(sent)) for sent in sent_tokenize(text)]
>>> for line in taggedsents_to_conll(sentences):
...     if line:
...         print(line, end="""")
1   This    _       DT      DT      _       0       a       _       _
2   is      _       VBZ     VBZ     _       0       a       _       _
3   a       _       DT      DT      _       0       a       _       _
4   foobar  _       JJ      JJ      _       0       a       _       _
5   sentence        _       NN      NN      _       0       a       _       _
6   .               _       .       .       _       0       a       _       _


1   Is      _       VBZ     VBZ     _       0       a       _       _
2   that    _       IN      IN      _       0       a       _       _
3   right   _       NN      NN      _       0       a       _       _
4   ?       _       .       .       _       0       a       _       _

",pos_tag,N/A,N/A,N/A
">>> from nltk import word_tokenize, sent_tokenize, pos_tag
>>> text = ""This is a foobar sentence. Is that right?""
>>> sentences = [pos_tag(word_tokenize(sent)) for sent in sent_tokenize(text)]
>>> for line in taggedsents_to_conll(sentences):
...     if line:
...         print(line, end="""")
1   This    _       DT      DT      _       0       a       _       _
2   is      _       VBZ     VBZ     _       0       a       _       _
3   a       _       DT      DT      _       0       a       _       _
4   foobar  _       JJ      JJ      _       0       a       _       _
5   sentence        _       NN      NN      _       0       a       _       _
6   .               _       .       .       _       0       a       _       _


1   Is      _       VBZ     VBZ     _       0       a       _       _
2   that    _       IN      IN      _       0       a       _       _
3   right   _       NN      NN      _       0       a       _       _
4   ?       _       .       .       _       0       a       _       _

",word_tokenize,N/A,N/A,N/A
">>> from nltk import word_tokenize, sent_tokenize, pos_tag
>>> text = ""This is a foobar sentence. Is that right?""
>>> sentences = [pos_tag(word_tokenize(sent)) for sent in sent_tokenize(text)]
>>> for line in taggedsents_to_conll(sentences):
...     if line:
...         print(line, end="""")
1   This    _       DT      DT      _       0       a       _       _
2   is      _       VBZ     VBZ     _       0       a       _       _
3   a       _       DT      DT      _       0       a       _       _
4   foobar  _       JJ      JJ      _       0       a       _       _
5   sentence        _       NN      NN      _       0       a       _       _
6   .               _       .       .       _       0       a       _       _


1   Is      _       VBZ     VBZ     _       0       a       _       _
2   that    _       IN      IN      _       0       a       _       _
3   right   _       NN      NN      _       0       a       _       _
4   ?       _       .       .       _       0       a       _       _

",sent_tokenize,N/A,N/A,N/A
">>> from nltk import word_tokenize, sent_tokenize, pos_tag
>>> text = ""This is a foobar sentence. Is that right?""
>>> sentences = [pos_tag(word_tokenize(sent)) for sent in sent_tokenize(text)]
>>> for line in taggedsents_to_conll(sentences):
...     if line:
...         print(line, end="""")
1   This    _       DT      DT      _       0       a       _       _
2   is      _       VBZ     VBZ     _       0       a       _       _
3   a       _       DT      DT      _       0       a       _       _
4   foobar  _       JJ      JJ      _       0       a       _       _
5   sentence        _       NN      NN      _       0       a       _       _
6   .               _       .       .       _       0       a       _       _


1   Is      _       VBZ     VBZ     _       0       a       _       _
2   that    _       IN      IN      _       0       a       _       _
3   right   _       NN      NN      _       0       a       _       _
4   ?       _       .       .       _       0       a       _       _

",taggedsents_to_conll,N/A,N/A,N/A
">>> from nltk import word_tokenize, sent_tokenize, pos_tag
>>> text = ""This is a foobar sentence. Is that right?""
>>> sentences = [pos_tag(word_tokenize(sent)) for sent in sent_tokenize(text)]
>>> for line in taggedsents_to_conll(sentences):
...     if line:
...         print(line, end="""")
1   This    _       DT      DT      _       0       a       _       _
2   is      _       VBZ     VBZ     _       0       a       _       _
3   a       _       DT      DT      _       0       a       _       _
4   foobar  _       JJ      JJ      _       0       a       _       _
5   sentence        _       NN      NN      _       0       a       _       _
6   .               _       .       .       _       0       a       _       _


1   Is      _       VBZ     VBZ     _       0       a       _       _
2   that    _       IN      IN      _       0       a       _       _
3   right   _       NN      NN      _       0       a       _       _
4   ?       _       .       .       _       0       a       _       _

",print,N/A,N/A,N/A
purge(),purge,N/A,N/A,N/A
clear(),clear,N/A,N/A,N/A
repr(),repr,N/A,N/A,N/A
"relsym(subjsym, objsym)",relsym,N/A,N/A,N/A
"'city(athens,greece,1368).'
",city,N/A,N/A,N/A
"'borders(albania,greece).'

'contains0(africa,central_africa).'
",borders,N/A,N/A,N/A
"'borders(albania,greece).'

'contains0(africa,central_africa).'
",contains0,N/A,N/A,N/A
"cities2table('cities.pl', 'city', 'city.db', verbose=True, setup=True)
",cities2table,N/A,N/A,N/A
"bo(\P.all x.(man(x) -> P(x)),z1)
",bo,N/A,N/A,N/A
"bo(\P.all x.(man(x) -> P(x)),z1)
",man,N/A,N/A,N/A
"bo(\P.all x.(man(x) -> P(x)),z1)
",P,N/A,N/A,N/A
">>> from nltk.sem.evaluate import Assignment
>>> dom = set(['u1', 'u2', 'u3', 'u4'])
>>> g3 = Assignment(dom, [('x', 'u1'), ('y', 'u2')])
>>> g3 == {'x': 'u1', 'y': 'u2'}
True
",set,N/A,N/A,N/A
">>> from nltk.sem.evaluate import Assignment
>>> dom = set(['u1', 'u2', 'u3', 'u4'])
>>> g3 = Assignment(dom, [('x', 'u1'), ('y', 'u2')])
>>> g3 == {'x': 'u1', 'y': 'u2'}
True
",Assignment,__init__,nltk\nltk\sem\evaluate.py,True
">>> print(g3)
g[u1/x][u2/y]
",print,N/A,N/A,N/A
">>> dom = set(['u1', 'u2', 'u3', 'u4'])
>>> g4 = Assignment(dom)
>>> g4.add('x', 'u1')
{'x': 'u1'}
",set,N/A,N/A,N/A
">>> dom = set(['u1', 'u2', 'u3', 'u4'])
>>> g4 = Assignment(dom)
>>> g4.add('x', 'u1')
{'x': 'u1'}
",Assignment,__init__,nltk\nltk\sem\evaluate.py,True
">>> dom = set(['u1', 'u2', 'u3', 'u4'])
>>> g4 = Assignment(dom)
>>> g4.add('x', 'u1')
{'x': 'u1'}
",g4.add,N/A,N/A,N/A
">>> g4.purge()
>>> g4
{}
",g4.purge,N/A,N/A,N/A
">>> from nltk.sem import Valuation, Model
>>> v = [('adam', 'b1'), ('betty', 'g1'), ('fido', 'd1'),
... ('girl', set(['g1', 'g2'])), ('boy', set(['b1', 'b2'])),
... ('dog', set(['d1'])),
... ('love', set([('b1', 'g1'), ('b2', 'g2'), ('g1', 'b1'), ('g2', 'b1')]))]
>>> val = Valuation(v)
>>> dom = val.domain
>>> m = Model(dom, val)
",set,N/A,N/A,N/A
">>> from nltk.sem import Valuation, Model
>>> v = [('adam', 'b1'), ('betty', 'g1'), ('fido', 'd1'),
... ('girl', set(['g1', 'g2'])), ('boy', set(['b1', 'b2'])),
... ('dog', set(['d1'])),
... ('love', set([('b1', 'g1'), ('b2', 'g2'), ('g1', 'b1'), ('g2', 'b1')]))]
>>> val = Valuation(v)
>>> dom = val.domain
>>> m = Model(dom, val)
",Valuation,__init__,nltk\nltk\sem\evaluate.py,True
">>> from nltk.sem import Valuation, Model
>>> v = [('adam', 'b1'), ('betty', 'g1'), ('fido', 'd1'),
... ('girl', set(['g1', 'g2'])), ('boy', set(['b1', 'b2'])),
... ('dog', set(['d1'])),
... ('love', set([('b1', 'g1'), ('b2', 'g2'), ('g1', 'b1'), ('g2', 'b1')]))]
>>> val = Valuation(v)
>>> dom = val.domain
>>> m = Model(dom, val)
",Model,__init__,nltk\nltk\sem\evaluate.py,True
">>> from nltk.stem import arlstem2
>>> stemmer = ARLSTem2()
>>> word = stemmer.stem('يعمل')
>>> print(word)
",ARLSTem2,__init__,nltk\nltk\stem\arlstem2.py,True
">>> from nltk.stem import arlstem2
>>> stemmer = ARLSTem2()
>>> word = stemmer.stem('يعمل')
>>> print(word)
",stemmer.stem,N/A,N/A,N/A
">>> from nltk.stem import arlstem2
>>> stemmer = ARLSTem2()
>>> word = stemmer.stem('يعمل')
>>> print(word)
",print,N/A,N/A,N/A
">>> from nltk.stem.cistem import Cistem
>>> stemmer = Cistem()
>>> s1 = ""Speicherbehältern""
>>> print(""('"" + stemmer.segment(s1)[0] + ""', '"" + stemmer.segment(s1)[1] + ""')"")
('speicherbehält', 'ern')
>>> s2 = ""Grenzpostens""
>>> stemmer.segment(s2)
('grenzpost', 'ens')
>>> s3 = ""Ausgefeiltere""
>>> stemmer.segment(s3)
('ausgefeilt', 'ere')
>>> stemmer = Cistem(True)
>>> print(""('"" + stemmer.segment(s1)[0] + ""', '"" + stemmer.segment(s1)[1] + ""')"")
('speicherbehäl', 'tern')
>>> stemmer.segment(s2)
('grenzpo', 'stens')
>>> stemmer.segment(s3)
('ausgefeil', 'tere')
",Cistem,__init__,nltk\nltk\stem\cistem.py,True
">>> from nltk.stem.cistem import Cistem
>>> stemmer = Cistem()
>>> s1 = ""Speicherbehältern""
>>> print(""('"" + stemmer.segment(s1)[0] + ""', '"" + stemmer.segment(s1)[1] + ""')"")
('speicherbehält', 'ern')
>>> s2 = ""Grenzpostens""
>>> stemmer.segment(s2)
('grenzpost', 'ens')
>>> s3 = ""Ausgefeiltere""
>>> stemmer.segment(s3)
('ausgefeilt', 'ere')
>>> stemmer = Cistem(True)
>>> print(""('"" + stemmer.segment(s1)[0] + ""', '"" + stemmer.segment(s1)[1] + ""')"")
('speicherbehäl', 'tern')
>>> stemmer.segment(s2)
('grenzpo', 'stens')
>>> stemmer.segment(s3)
('ausgefeil', 'tere')
",print,N/A,N/A,N/A
">>> from nltk.stem.cistem import Cistem
>>> stemmer = Cistem()
>>> s1 = ""Speicherbehältern""
>>> print(""('"" + stemmer.segment(s1)[0] + ""', '"" + stemmer.segment(s1)[1] + ""')"")
('speicherbehält', 'ern')
>>> s2 = ""Grenzpostens""
>>> stemmer.segment(s2)
('grenzpost', 'ens')
>>> s3 = ""Ausgefeiltere""
>>> stemmer.segment(s3)
('ausgefeilt', 'ere')
>>> stemmer = Cistem(True)
>>> print(""('"" + stemmer.segment(s1)[0] + ""', '"" + stemmer.segment(s1)[1] + ""')"")
('speicherbehäl', 'tern')
>>> stemmer.segment(s2)
('grenzpo', 'stens')
>>> stemmer.segment(s3)
('ausgefeil', 'tere')
",stemmer.segment,N/A,N/A,N/A
">>> from nltk.stem.cistem import Cistem
>>> stemmer = Cistem()
>>> s1 = ""Speicherbehältern""
>>> stemmer.stem(s1)
'speicherbehalt'
>>> s2 = ""Grenzpostens""
>>> stemmer.stem(s2)
'grenzpost'
>>> s3 = ""Ausgefeiltere""
>>> stemmer.stem(s3)
'ausgefeilt'
>>> stemmer = Cistem(True)
>>> stemmer.stem(s1)
'speicherbehal'
>>> stemmer.stem(s2)
'grenzpo'
>>> stemmer.stem(s3)
'ausgefeil'
",Cistem,__init__,nltk\nltk\stem\cistem.py,True
">>> from nltk.stem.cistem import Cistem
>>> stemmer = Cistem()
>>> s1 = ""Speicherbehältern""
>>> stemmer.stem(s1)
'speicherbehalt'
>>> s2 = ""Grenzpostens""
>>> stemmer.stem(s2)
'grenzpost'
>>> s3 = ""Ausgefeiltere""
>>> stemmer.stem(s3)
'ausgefeilt'
>>> stemmer = Cistem(True)
>>> stemmer.stem(s1)
'speicherbehal'
>>> stemmer.stem(s2)
'grenzpo'
>>> stemmer.stem(s3)
'ausgefeil'
",stemmer.stem,N/A,N/A,N/A
">>> from nltk.stem.lancaster import LancasterStemmer
>>> st = LancasterStemmer()
>>> st.stem('maximum')     # Remove ""-um"" when word is intact
'maxim'
>>> st.stem('presumably')  # Don't remove ""-um"" when word is not intact
'presum'
>>> st.stem('multiply')    # No action taken if word ends with ""-ply""
'multiply'
>>> st.stem('provision')   # Replace ""-sion"" with ""-j"" to trigger ""j"" set of rules
'provid'
>>> st.stem('owed')        # Word starting with vowel must contain at least 2 letters
'ow'
>>> st.stem('ear')         # ditto
'ear'
>>> st.stem('saying')      # Words starting with consonant must contain at least 3
'say'
>>> st.stem('crying')      #     letters and one of those letters must be a vowel
'cry'
>>> st.stem('string')      # ditto
'string'
>>> st.stem('meant')       # ditto
'meant'
>>> st.stem('cement')      # ditto
'cem'
>>> st_pre = LancasterStemmer(strip_prefix_flag=True)
>>> st_pre.stem('kilometer') # Test Prefix
'met'
>>> st_custom = LancasterStemmer(rule_tuple=(""ssen4>"", ""s1t.""))
>>> st_custom.stem(""ness"") # Change s to t
'nest'
",LancasterStemmer,__init__,nltk\nltk\stem\lancaster.py,True
">>> from nltk.stem.lancaster import LancasterStemmer
>>> st = LancasterStemmer()
>>> st.stem('maximum')     # Remove ""-um"" when word is intact
'maxim'
>>> st.stem('presumably')  # Don't remove ""-um"" when word is not intact
'presum'
>>> st.stem('multiply')    # No action taken if word ends with ""-ply""
'multiply'
>>> st.stem('provision')   # Replace ""-sion"" with ""-j"" to trigger ""j"" set of rules
'provid'
>>> st.stem('owed')        # Word starting with vowel must contain at least 2 letters
'ow'
>>> st.stem('ear')         # ditto
'ear'
>>> st.stem('saying')      # Words starting with consonant must contain at least 3
'say'
>>> st.stem('crying')      #     letters and one of those letters must be a vowel
'cry'
>>> st.stem('string')      # ditto
'string'
>>> st.stem('meant')       # ditto
'meant'
>>> st.stem('cement')      # ditto
'cem'
>>> st_pre = LancasterStemmer(strip_prefix_flag=True)
>>> st_pre.stem('kilometer') # Test Prefix
'met'
>>> st_custom = LancasterStemmer(rule_tuple=(""ssen4>"", ""s1t.""))
>>> st_custom.stem(""ness"") # Change s to t
'nest'
",st.stem,N/A,N/A,N/A
">>> from nltk.stem.lancaster import LancasterStemmer
>>> st = LancasterStemmer()
>>> st.stem('maximum')     # Remove ""-um"" when word is intact
'maxim'
>>> st.stem('presumably')  # Don't remove ""-um"" when word is not intact
'presum'
>>> st.stem('multiply')    # No action taken if word ends with ""-ply""
'multiply'
>>> st.stem('provision')   # Replace ""-sion"" with ""-j"" to trigger ""j"" set of rules
'provid'
>>> st.stem('owed')        # Word starting with vowel must contain at least 2 letters
'ow'
>>> st.stem('ear')         # ditto
'ear'
>>> st.stem('saying')      # Words starting with consonant must contain at least 3
'say'
>>> st.stem('crying')      #     letters and one of those letters must be a vowel
'cry'
>>> st.stem('string')      # ditto
'string'
>>> st.stem('meant')       # ditto
'meant'
>>> st.stem('cement')      # ditto
'cem'
>>> st_pre = LancasterStemmer(strip_prefix_flag=True)
>>> st_pre.stem('kilometer') # Test Prefix
'met'
>>> st_custom = LancasterStemmer(rule_tuple=(""ssen4>"", ""s1t.""))
>>> st_custom.stem(""ness"") # Change s to t
'nest'
",st_pre.stem,N/A,N/A,N/A
">>> from nltk.stem.lancaster import LancasterStemmer
>>> st = LancasterStemmer()
>>> st.stem('maximum')     # Remove ""-um"" when word is intact
'maxim'
>>> st.stem('presumably')  # Don't remove ""-um"" when word is not intact
'presum'
>>> st.stem('multiply')    # No action taken if word ends with ""-ply""
'multiply'
>>> st.stem('provision')   # Replace ""-sion"" with ""-j"" to trigger ""j"" set of rules
'provid'
>>> st.stem('owed')        # Word starting with vowel must contain at least 2 letters
'ow'
>>> st.stem('ear')         # ditto
'ear'
>>> st.stem('saying')      # Words starting with consonant must contain at least 3
'say'
>>> st.stem('crying')      #     letters and one of those letters must be a vowel
'cry'
>>> st.stem('string')      # ditto
'string'
>>> st.stem('meant')       # ditto
'meant'
>>> st.stem('cement')      # ditto
'cem'
>>> st_pre = LancasterStemmer(strip_prefix_flag=True)
>>> st_pre.stem('kilometer') # Test Prefix
'met'
>>> st_custom = LancasterStemmer(rule_tuple=(""ssen4>"", ""s1t.""))
>>> st_custom.stem(""ness"") # Change s to t
'nest'
",st_custom.stem,N/A,N/A,N/A
">>> from nltk.stem import RegexpStemmer
>>> st = RegexpStemmer('ing$|s$|e$|able$', min=4)
>>> st.stem('cars')
'car'
>>> st.stem('mass')
'mas'
>>> st.stem('was')
'was'
>>> st.stem('bee')
'bee'
>>> st.stem('compute')
'comput'
>>> st.stem('advisable')
'advis'
",RegexpStemmer,__init__,nltk\nltk\stem\regexp.py,True
">>> from nltk.stem import RegexpStemmer
>>> st = RegexpStemmer('ing$|s$|e$|able$', min=4)
>>> st.stem('cars')
'car'
>>> st.stem('mass')
'mas'
>>> st.stem('was')
'was'
>>> st.stem('bee')
'bee'
>>> st.stem('compute')
'comput'
>>> st.stem('advisable')
'advis'
",st.stem,N/A,N/A,N/A
">>> from nltk.stem import RSLPStemmer
>>> st = RSLPStemmer()
>>> # opening lines of Erico Verissimo's ""Música ao Longe""
>>> text = '''
... Clarissa risca com giz no quadro-negro a paisagem que os alunos
... devem copiar . Uma casinha de porta e janela , em cima duma
... coxilha .'''
>>> for token in text.split():
...     print(st.stem(token))
clariss risc com giz no quadro-negr a pais que os alun dev copi .
uma cas de port e janel , em cim dum coxilh .
",RSLPStemmer,__init__,nltk\nltk\stem\rslp.py,True
">>> from nltk.stem import RSLPStemmer
>>> st = RSLPStemmer()
>>> # opening lines of Erico Verissimo's ""Música ao Longe""
>>> text = '''
... Clarissa risca com giz no quadro-negro a paisagem que os alunos
... devem copiar . Uma casinha de porta e janela , em cima duma
... coxilha .'''
>>> for token in text.split():
...     print(st.stem(token))
clariss risc com giz no quadro-negr a pais que os alun dev copi .
uma cas de port e janel , em cim dum coxilh .
",text.split,N/A,N/A,N/A
">>> from nltk.stem import RSLPStemmer
>>> st = RSLPStemmer()
>>> # opening lines of Erico Verissimo's ""Música ao Longe""
>>> text = '''
... Clarissa risca com giz no quadro-negro a paisagem que os alunos
... devem copiar . Uma casinha de porta e janela , em cima duma
... coxilha .'''
>>> for token in text.split():
...     print(st.stem(token))
clariss risc com giz no quadro-negr a pais que os alun dev copi .
uma cas de port e janel , em cim dum coxilh .
",print,N/A,N/A,N/A
">>> from nltk.stem import RSLPStemmer
>>> st = RSLPStemmer()
>>> # opening lines of Erico Verissimo's ""Música ao Longe""
>>> text = '''
... Clarissa risca com giz no quadro-negro a paisagem que os alunos
... devem copiar . Uma casinha de porta e janela , em cima duma
... coxilha .'''
>>> for token in text.split():
...     print(st.stem(token))
clariss risc com giz no quadro-negr a pais que os alun dev copi .
uma cas de port e janel , em cim dum coxilh .
",st.stem,N/A,N/A,N/A
">>> from nltk.stem import SnowballStemmer
>>> print("" "".join(SnowballStemmer.languages)) # See which languages are supported
arabic danish dutch english finnish french german hungarian
italian norwegian porter portuguese romanian russian
spanish swedish
>>> stemmer = SnowballStemmer(""german"") # Choose a language
>>> stemmer.stem(""Autobahnen"") # Stem a word
'autobahn'
",print,N/A,N/A,N/A
">>> from nltk.stem import SnowballStemmer
>>> print("" "".join(SnowballStemmer.languages)) # See which languages are supported
arabic danish dutch english finnish french german hungarian
italian norwegian porter portuguese romanian russian
spanish swedish
>>> stemmer = SnowballStemmer(""german"") # Choose a language
>>> stemmer.stem(""Autobahnen"") # Stem a word
'autobahn'
",join,N/A,N/A,N/A
">>> from nltk.stem import SnowballStemmer
>>> print("" "".join(SnowballStemmer.languages)) # See which languages are supported
arabic danish dutch english finnish french german hungarian
italian norwegian porter portuguese romanian russian
spanish swedish
>>> stemmer = SnowballStemmer(""german"") # Choose a language
>>> stemmer.stem(""Autobahnen"") # Stem a word
'autobahn'
",SnowballStemmer,__init__,nltk\nltk\stem\snowball.py,True
">>> from nltk.stem import SnowballStemmer
>>> print("" "".join(SnowballStemmer.languages)) # See which languages are supported
arabic danish dutch english finnish french german hungarian
italian norwegian porter portuguese romanian russian
spanish swedish
>>> stemmer = SnowballStemmer(""german"") # Choose a language
>>> stemmer.stem(""Autobahnen"") # Stem a word
'autobahn'
",stemmer.stem,N/A,N/A,N/A
">>> from nltk.stem.snowball import GermanStemmer
>>> stemmer = GermanStemmer()
>>> stemmer.stem(""Autobahnen"")
'autobahn'
",GermanStemmer,__init__,nltk\nltk\stem\snowball.py,True
">>> from nltk.stem.snowball import GermanStemmer
>>> stemmer = GermanStemmer()
>>> stemmer.stem(""Autobahnen"")
'autobahn'
",stemmer.stem,N/A,N/A,N/A
">>> from nltk.stem import WordNetLemmatizer
>>> wnl = WordNetLemmatizer()
>>> print(wnl.lemmatize('dogs'))
dog
>>> print(wnl.lemmatize('churches'))
church
>>> print(wnl.lemmatize('aardwolves'))
aardwolf
>>> print(wnl.lemmatize('abaci'))
abacus
>>> print(wnl.lemmatize('hardrock'))
hardrock
",WordNetLemmatizer,__init__,nltk\nltk\stem\wordnet.py,True
">>> from nltk.stem import WordNetLemmatizer
>>> wnl = WordNetLemmatizer()
>>> print(wnl.lemmatize('dogs'))
dog
>>> print(wnl.lemmatize('churches'))
church
>>> print(wnl.lemmatize('aardwolves'))
aardwolf
>>> print(wnl.lemmatize('abaci'))
abacus
>>> print(wnl.lemmatize('hardrock'))
hardrock
",print,N/A,N/A,N/A
">>> from nltk.stem import WordNetLemmatizer
>>> wnl = WordNetLemmatizer()
>>> print(wnl.lemmatize('dogs'))
dog
>>> print(wnl.lemmatize('churches'))
church
>>> print(wnl.lemmatize('aardwolves'))
aardwolf
>>> print(wnl.lemmatize('abaci'))
abacus
>>> print(wnl.lemmatize('hardrock'))
hardrock
",wnl.lemmatize,N/A,N/A,N/A
tag(),tag,N/A,N/A,N/A
tag_sents(),tag_sents,N/A,N/A,N/A
self.tag(),self.tag,N/A,N/A,N/A
transform(),transform,N/A,N/A,N/A
choose_tag(),choose_tag,N/A,N/A,N/A
classifier(),classifier,N/A,N/A,N/A
feature_detector(),feature_detector,N/A,N/A,N/A
classifier(),classifier,N/A,N/A,N/A
_train(),_train,N/A,N/A,N/A
choose_tag(),choose_tag,N/A,N/A,N/A
self.tag(),self.tag,N/A,N/A,N/A
">>> backoff = RegexpTagger([
... (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers
... (r'(The|the|A|a|An|an)$', 'AT'),   # articles
... (r'.*able$', 'JJ'),                # adjectives
... (r'.*ness$', 'NN'),                # nouns formed from adjectives
... (r'.*ly$', 'RB'),                  # adverbs
... (r'.*s$', 'NNS'),                  # plural nouns
... (r'.*ing$', 'VBG'),                # gerunds
... (r'.*ed$', 'VBD'),                 # past tense verbs
... (r'.*', 'NN')                      # nouns (default)
... ])
",RegexpTagger,__init__,nltk\nltk\tag\sequential.py,True
">>> baseline.evaluate(gold_data) 
0.2450142...
",baseline.evaluate,N/A,N/A,N/A
">>> tagger1 = tt.train(training_data, max_rules=10)
TBL train (fast) (seqs: 100; tokens: 2417; tpls: 2; min score: 2; min acc: None)
Finding initial useful rules...
    Found 845 useful rules.

           B      |
   S   F   r   O  |        Score = Fixed - Broken
   c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct
   o   x   k   h  |  u     Broken = num tags changed correct -> incorrect
   r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect
   e   d   n   r  |  e
------------------+-------------------------------------------------------
 132 132   0   0  | AT->DT if Pos:NN@[-1]
  85  85   0   0  | NN->, if Pos:NN@[-1] & Word:,@[0]
  69  69   0   0  | NN->. if Pos:NN@[-1] & Word:.@[0]
  51  51   0   0  | NN->IN if Pos:NN@[-1] & Word:of@[0]
  47  63  16 161  | NN->IN if Pos:NNS@[-1]
  33  33   0   0  | NN->TO if Pos:NN@[-1] & Word:to@[0]
  26  26   0   0  | IN->. if Pos:NNS@[-1] & Word:.@[0]
  24  24   0   0  | IN->, if Pos:NNS@[-1] & Word:,@[0]
  22  27   5  24  | NN->-NONE- if Pos:VBD@[-1]
  17  17   0   0  | NN->CC if Pos:NN@[-1] & Word:and@[0]
",tt.train,N/A,N/A,N/A
">>> tagger1.rules()[1:3]
(Rule('001', 'NN', ',', [(Pos([-1]),'NN'), (Word([0]),',')]), Rule('001', 'NN', '.', [(Pos([-1]),'NN'), (Word([0]),'.')]))
",tagger1.rules,N/A,N/A,N/A
">>> tagger1.rules()[1:3]
(Rule('001', 'NN', ',', [(Pos([-1]),'NN'), (Word([0]),',')]), Rule('001', 'NN', '.', [(Pos([-1]),'NN'), (Word([0]),'.')]))
",Rule,__init__,nltk\nltk\tbl\rule.py,True
">>> tagger1.rules()[1:3]
(Rule('001', 'NN', ',', [(Pos([-1]),'NN'), (Word([0]),',')]), Rule('001', 'NN', '.', [(Pos([-1]),'NN'), (Word([0]),'.')]))
",Pos,__init__,nltk\nltk\tag\brill.py,True
">>> tagger1.rules()[1:3]
(Rule('001', 'NN', ',', [(Pos([-1]),'NN'), (Word([0]),',')]), Rule('001', 'NN', '.', [(Pos([-1]),'NN'), (Word([0]),'.')]))
",Word,__init__,nltk\nltk\tag\brill.py,True
">>> train_stats = tagger1.train_stats()
>>> [train_stats[stat] for stat in ['initialerrors', 'finalerrors', 'rulescores']]
[1775, 1269, [132, 85, 69, 51, 47, 33, 26, 24, 22, 17]]
",tagger1.train_stats,N/A,N/A,N/A
">>> tagger1.print_template_statistics(printunused=False)
TEMPLATE STATISTICS (TRAIN)  2 templates, 10 rules)
TRAIN (   2417 tokens) initial  1775 0.2656 final:  1269 0.4750
#ID | Score (train) |  #Rules     | Template
--------------------------------------------
001 |   305   0.603 |   7   0.700 | Template(Pos([-1]),Word([0]))
000 |   201   0.397 |   3   0.300 | Template(Pos([-1]))

",tagger1.print_template_statistics,N/A,N/A,N/A
">>> tagger1.print_template_statistics(printunused=False)
TEMPLATE STATISTICS (TRAIN)  2 templates, 10 rules)
TRAIN (   2417 tokens) initial  1775 0.2656 final:  1269 0.4750
#ID | Score (train) |  #Rules     | Template
--------------------------------------------
001 |   305   0.603 |   7   0.700 | Template(Pos([-1]),Word([0]))
000 |   201   0.397 |   3   0.300 | Template(Pos([-1]))

",Template,__init__,nltk\nltk\tbl\template.py,True
">>> tagger1.print_template_statistics(printunused=False)
TEMPLATE STATISTICS (TRAIN)  2 templates, 10 rules)
TRAIN (   2417 tokens) initial  1775 0.2656 final:  1269 0.4750
#ID | Score (train) |  #Rules     | Template
--------------------------------------------
001 |   305   0.603 |   7   0.700 | Template(Pos([-1]),Word([0]))
000 |   201   0.397 |   3   0.300 | Template(Pos([-1]))

",Pos,__init__,nltk\nltk\tag\brill.py,True
">>> tagger1.print_template_statistics(printunused=False)
TEMPLATE STATISTICS (TRAIN)  2 templates, 10 rules)
TRAIN (   2417 tokens) initial  1775 0.2656 final:  1269 0.4750
#ID | Score (train) |  #Rules     | Template
--------------------------------------------
001 |   305   0.603 |   7   0.700 | Template(Pos([-1]),Word([0]))
000 |   201   0.397 |   3   0.300 | Template(Pos([-1]))

",Word,__init__,nltk\nltk\tag\brill.py,True
">>> tagger1.evaluate(gold_data) 
0.43996...
",tagger1.evaluate,N/A,N/A,N/A
">>> tagged, test_stats = tagger1.batch_tag_incremental(testing_data, gold_data)
",tagger1.batch_tag_incremental,N/A,N/A,N/A
">>> tagger2.evaluate(gold_data)  
0.44159544...
>>> tagger2.rules()[2:4]
(Rule('001', 'NN', '.', [(Pos([-1]),'NN'), (Word([0]),'.')]), Rule('001', 'NN', 'IN', [(Pos([-1]),'NN'), (Word([0]),'of')]))
",tagger2.evaluate,N/A,N/A,N/A
">>> tagger2.evaluate(gold_data)  
0.44159544...
>>> tagger2.rules()[2:4]
(Rule('001', 'NN', '.', [(Pos([-1]),'NN'), (Word([0]),'.')]), Rule('001', 'NN', 'IN', [(Pos([-1]),'NN'), (Word([0]),'of')]))
",tagger2.rules,N/A,N/A,N/A
">>> tagger2.evaluate(gold_data)  
0.44159544...
>>> tagger2.rules()[2:4]
(Rule('001', 'NN', '.', [(Pos([-1]),'NN'), (Word([0]),'.')]), Rule('001', 'NN', 'IN', [(Pos([-1]),'NN'), (Word([0]),'of')]))
",Rule,__init__,nltk\nltk\tbl\rule.py,True
">>> tagger2.evaluate(gold_data)  
0.44159544...
>>> tagger2.rules()[2:4]
(Rule('001', 'NN', '.', [(Pos([-1]),'NN'), (Word([0]),'.')]), Rule('001', 'NN', 'IN', [(Pos([-1]),'NN'), (Word([0]),'of')]))
",Pos,__init__,nltk\nltk\tag\brill.py,True
">>> tagger2.evaluate(gold_data)  
0.44159544...
>>> tagger2.rules()[2:4]
(Rule('001', 'NN', '.', [(Pos([-1]),'NN'), (Word([0]),'.')]), Rule('001', 'NN', 'IN', [(Pos([-1]),'NN'), (Word([0]),'of')]))
",Word,__init__,nltk\nltk\tag\brill.py,True
">>> from nltk.tag import CRFTagger
>>> ct = CRFTagger()
",CRFTagger,__init__,nltk\nltk\tag\crf.py,True
">>> ct.train(train_data,'model.crf.tagger')
>>> ct.tag_sents([['dog','is','good'], ['Cat','eat','meat']])
[[('dog', 'Noun'), ('is', 'Verb'), ('good', 'Adj')], [('Cat', 'Noun'), ('eat', 'Verb'), ('meat', 'Noun')]]
",ct.train,N/A,N/A,N/A
">>> ct.train(train_data,'model.crf.tagger')
>>> ct.tag_sents([['dog','is','good'], ['Cat','eat','meat']])
[[('dog', 'Noun'), ('is', 'Verb'), ('good', 'Adj')], [('Cat', 'Noun'), ('eat', 'Verb'), ('meat', 'Noun')]]
",ct.tag_sents,N/A,N/A,N/A
">>> gold_sentences = [[('dog','Noun'),('is','Verb'),('good','Adj')] , [('Cat','Noun'),('eat','Verb'), ('meat','Noun')]]
>>> ct.evaluate(gold_sentences)
1.0
",ct.evaluate,N/A,N/A,N/A
"H(O) = - sum_S Pr(S | O) log Pr(S | O)
",H,N/A,N/A,N/A
"H(O) = - sum_S Pr(S | O) log Pr(S | O)
",Pr,N/A,N/A,N/A
"H = - sum_S Pr(S | O) log [ Pr(S, O) / Z ]
= log Z - sum_S Pr(S | O) log Pr(S, 0)
= log Z - sum_S Pr(S | O) [ log Pr(S_0) + sum_t Pr(S_t | S_{t-1}) + sum_t Pr(O_t | S_t) ]
",Pr,N/A,N/A,N/A
"H = log Z - sum_s0 alpha_0(s0) beta_0(s0) / Z * log Pr(s0)
+ sum_t,si,sj alpha_t(si) Pr(sj | si) Pr(O_t+1 | sj) beta_t(sj) / Z * log Pr(sj | si)
+ sum_t,st alpha_t(st) beta_t(st) / Z * log Pr(O_t | st)
",alpha_0,N/A,N/A,N/A
"H = log Z - sum_s0 alpha_0(s0) beta_0(s0) / Z * log Pr(s0)
+ sum_t,si,sj alpha_t(si) Pr(sj | si) Pr(O_t+1 | sj) beta_t(sj) / Z * log Pr(sj | si)
+ sum_t,st alpha_t(st) beta_t(st) / Z * log Pr(O_t | st)
",beta_0,N/A,N/A,N/A
"H = log Z - sum_s0 alpha_0(s0) beta_0(s0) / Z * log Pr(s0)
+ sum_t,si,sj alpha_t(si) Pr(sj | si) Pr(O_t+1 | sj) beta_t(sj) / Z * log Pr(sj | si)
+ sum_t,st alpha_t(st) beta_t(st) / Z * log Pr(O_t | st)
",Pr,N/A,N/A,N/A
"H = log Z - sum_s0 alpha_0(s0) beta_0(s0) / Z * log Pr(s0)
+ sum_t,si,sj alpha_t(si) Pr(sj | si) Pr(O_t+1 | sj) beta_t(sj) / Z * log Pr(sj | si)
+ sum_t,st alpha_t(st) beta_t(st) / Z * log Pr(O_t | st)
",alpha_t,N/A,N/A,N/A
"H = log Z - sum_s0 alpha_0(s0) beta_0(s0) / Z * log Pr(s0)
+ sum_t,si,sj alpha_t(si) Pr(sj | si) Pr(O_t+1 | sj) beta_t(sj) / Z * log Pr(sj | si)
+ sum_t,st alpha_t(st) beta_t(st) / Z * log Pr(O_t | st)
",beta_t,N/A,N/A,N/A
">>> from nltk.tag import HunposTagger
>>> ht = HunposTagger('en_wsj.model')
>>> ht.tag('What is the airspeed of an unladen swallow ?'.split())
[('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'), ('unladen', 'NN'), ('swallow', 'VB'), ('?', '.')]
>>> ht.close()
",HunposTagger,__init__,nltk\nltk\tag\hunpos.py,True
">>> from nltk.tag import HunposTagger
>>> ht = HunposTagger('en_wsj.model')
>>> ht.tag('What is the airspeed of an unladen swallow ?'.split())
[('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'), ('unladen', 'NN'), ('swallow', 'VB'), ('?', '.')]
>>> ht.close()
",ht.tag,N/A,N/A,N/A
">>> from nltk.tag import HunposTagger
>>> ht = HunposTagger('en_wsj.model')
>>> ht.tag('What is the airspeed of an unladen swallow ?'.split())
[('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'), ('unladen', 'NN'), ('swallow', 'VB'), ('?', '.')]
>>> ht.close()
",split,N/A,N/A,N/A
">>> from nltk.tag import HunposTagger
>>> ht = HunposTagger('en_wsj.model')
>>> ht.tag('What is the airspeed of an unladen swallow ?'.split())
[('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'), ('unladen', 'NN'), ('swallow', 'VB'), ('?', '.')]
>>> ht.close()
",ht.close,N/A,N/A,N/A
">>> with HunposTagger('en_wsj.model') as ht:
...     ht.tag('What is the airspeed of an unladen swallow ?'.split())
...
[('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'), ('unladen', 'NN'), ('swallow', 'VB'), ('?', '.')]
",HunposTagger,__init__,nltk\nltk\tag\hunpos.py,True
">>> with HunposTagger('en_wsj.model') as ht:
...     ht.tag('What is the airspeed of an unladen swallow ?'.split())
...
[('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'), ('unladen', 'NN'), ('swallow', 'VB'), ('?', '.')]
",ht.tag,N/A,N/A,N/A
">>> with HunposTagger('en_wsj.model') as ht:
...     ht.tag('What is the airspeed of an unladen swallow ?'.split())
...
[('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'), ('unladen', 'NN'), ('swallow', 'VB'), ('?', '.')]
",split,N/A,N/A,N/A
">>> map_tag('en-ptb', 'universal', 'VBZ')
'VERB'
>>> map_tag('en-ptb', 'universal', 'VBP')
'VERB'
>>> map_tag('en-ptb', 'universal', '``')
'.'
",map_tag,N/A,N/A,N/A
">>> tagset_mapping('ru-rnc', 'universal') == {'!': '.', 'A': 'ADJ', 'C': 'CONJ', 'AD': 'ADV',            'NN': 'NOUN', 'VG': 'VERB', 'COMP': 'CONJ', 'NC': 'NUM', 'VP': 'VERB', 'P': 'ADP',            'IJ': 'X', 'V': 'VERB', 'Z': 'X', 'VI': 'VERB', 'YES_NO_SENT': 'X', 'PTCL': 'PRT'}
True
",tagset_mapping,N/A,N/A,N/A
">>> tagger = PerceptronTagger(load=False)
",PerceptronTagger,__init__,nltk\nltk\tag\perceptron.py,True
">>> tagger.train([[('today','NN'),('is','VBZ'),('good','JJ'),('day','NN')],
... [('yes','NNS'),('it','PRP'),('beautiful','JJ')]])
",tagger.train,N/A,N/A,N/A
">>> tagger.tag(['today','is','a','beautiful','day'])
[('today', 'NN'), ('is', 'PRP'), ('a', 'PRP'), ('beautiful', 'JJ'), ('day', 'NN')]
",tagger.tag,N/A,N/A,N/A
">>> pretrain = PerceptronTagger()
",PerceptronTagger,__init__,nltk\nltk\tag\perceptron.py,True
">>> pretrain.tag('The quick brown fox jumps over the lazy dog'.split())
[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]
",pretrain.tag,N/A,N/A,N/A
">>> pretrain.tag('The quick brown fox jumps over the lazy dog'.split())
[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]
",split,N/A,N/A,N/A
">>> pretrain.tag(""The red cat"".split())
[('The', 'DT'), ('red', 'JJ'), ('cat', 'NN')]
",pretrain.tag,N/A,N/A,N/A
">>> pretrain.tag(""The red cat"".split())
[('The', 'DT'), ('red', 'JJ'), ('cat', 'NN')]
",split,N/A,N/A,N/A
">>> from nltk.tag import SennaTagger
>>> tagger = SennaTagger('/usr/share/senna-v3.0')
>>> tagger.tag('What is the airspeed of an unladen swallow ?'.split()) 
[('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('airspeed', 'NN'),
('of', 'IN'), ('an', 'DT'), ('unladen', 'NN'), ('swallow', 'NN'), ('?', '.')]
",SennaTagger,__init__,nltk\nltk\tag\senna.py,True
">>> from nltk.tag import SennaTagger
>>> tagger = SennaTagger('/usr/share/senna-v3.0')
>>> tagger.tag('What is the airspeed of an unladen swallow ?'.split()) 
[('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('airspeed', 'NN'),
('of', 'IN'), ('an', 'DT'), ('unladen', 'NN'), ('swallow', 'NN'), ('?', '.')]
",tagger.tag,N/A,N/A,N/A
">>> from nltk.tag import SennaTagger
>>> tagger = SennaTagger('/usr/share/senna-v3.0')
>>> tagger.tag('What is the airspeed of an unladen swallow ?'.split()) 
[('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('airspeed', 'NN'),
('of', 'IN'), ('an', 'DT'), ('unladen', 'NN'), ('swallow', 'NN'), ('?', '.')]
",split,N/A,N/A,N/A
">>> from nltk.tag import SennaChunkTagger
>>> chktagger = SennaChunkTagger('/usr/share/senna-v3.0')
>>> chktagger.tag('What is the airspeed of an unladen swallow ?'.split()) 
[('What', 'B-NP'), ('is', 'B-VP'), ('the', 'B-NP'), ('airspeed', 'I-NP'),
('of', 'B-PP'), ('an', 'B-NP'), ('unladen', 'I-NP'), ('swallow', 'I-NP'),
('?', 'O')]
",SennaChunkTagger,__init__,nltk\nltk\tag\senna.py,True
">>> from nltk.tag import SennaChunkTagger
>>> chktagger = SennaChunkTagger('/usr/share/senna-v3.0')
>>> chktagger.tag('What is the airspeed of an unladen swallow ?'.split()) 
[('What', 'B-NP'), ('is', 'B-VP'), ('the', 'B-NP'), ('airspeed', 'I-NP'),
('of', 'B-PP'), ('an', 'B-NP'), ('unladen', 'I-NP'), ('swallow', 'I-NP'),
('?', 'O')]
",chktagger.tag,N/A,N/A,N/A
">>> from nltk.tag import SennaChunkTagger
>>> chktagger = SennaChunkTagger('/usr/share/senna-v3.0')
>>> chktagger.tag('What is the airspeed of an unladen swallow ?'.split()) 
[('What', 'B-NP'), ('is', 'B-VP'), ('the', 'B-NP'), ('airspeed', 'I-NP'),
('of', 'B-PP'), ('an', 'B-NP'), ('unladen', 'I-NP'), ('swallow', 'I-NP'),
('?', 'O')]
",split,N/A,N/A,N/A
">>> from nltk.tag import SennaNERTagger
>>> nertagger = SennaNERTagger('/usr/share/senna-v3.0')
>>> nertagger.tag('Shakespeare theatre was in London .'.split()) 
[('Shakespeare', 'B-PER'), ('theatre', 'O'), ('was', 'O'), ('in', 'O'),
('London', 'B-LOC'), ('.', 'O')]
>>> nertagger.tag('UN headquarters are in NY , USA .'.split()) 
[('UN', 'B-ORG'), ('headquarters', 'O'), ('are', 'O'), ('in', 'O'),
('NY', 'B-LOC'), (',', 'O'), ('USA', 'B-LOC'), ('.', 'O')]
",SennaNERTagger,__init__,nltk\nltk\tag\senna.py,True
">>> from nltk.tag import SennaNERTagger
>>> nertagger = SennaNERTagger('/usr/share/senna-v3.0')
>>> nertagger.tag('Shakespeare theatre was in London .'.split()) 
[('Shakespeare', 'B-PER'), ('theatre', 'O'), ('was', 'O'), ('in', 'O'),
('London', 'B-LOC'), ('.', 'O')]
>>> nertagger.tag('UN headquarters are in NY , USA .'.split()) 
[('UN', 'B-ORG'), ('headquarters', 'O'), ('are', 'O'), ('in', 'O'),
('NY', 'B-LOC'), (',', 'O'), ('USA', 'B-LOC'), ('.', 'O')]
",nertagger.tag,N/A,N/A,N/A
">>> from nltk.tag import SennaNERTagger
>>> nertagger = SennaNERTagger('/usr/share/senna-v3.0')
>>> nertagger.tag('Shakespeare theatre was in London .'.split()) 
[('Shakespeare', 'B-PER'), ('theatre', 'O'), ('was', 'O'), ('in', 'O'),
('London', 'B-LOC'), ('.', 'O')]
>>> nertagger.tag('UN headquarters are in NY , USA .'.split()) 
[('UN', 'B-ORG'), ('headquarters', 'O'), ('are', 'O'), ('in', 'O'),
('NY', 'B-LOC'), (',', 'O'), ('USA', 'B-LOC'), ('.', 'O')]
",split,N/A,N/A,N/A
">>> from nltk.tag import SennaChunkTagger
>>> chktagger = SennaChunkTagger('/usr/share/senna-v3.0')
>>> sent = 'What is the airspeed of an unladen swallow ?'.split()
>>> tagged_sent = chktagger.tag(sent) 
>>> tagged_sent 
[('What', 'B-NP'), ('is', 'B-VP'), ('the', 'B-NP'), ('airspeed', 'I-NP'),
('of', 'B-PP'), ('an', 'B-NP'), ('unladen', 'I-NP'), ('swallow', 'I-NP'),
('?', 'O')]
>>> list(chktagger.bio_to_chunks(tagged_sent, chunk_type='NP')) 
[('What', '0'), ('the airspeed', '2-3'), ('an unladen swallow', '5-6-7')]
",SennaChunkTagger,__init__,nltk\nltk\tag\senna.py,True
">>> from nltk.tag import SennaChunkTagger
>>> chktagger = SennaChunkTagger('/usr/share/senna-v3.0')
>>> sent = 'What is the airspeed of an unladen swallow ?'.split()
>>> tagged_sent = chktagger.tag(sent) 
>>> tagged_sent 
[('What', 'B-NP'), ('is', 'B-VP'), ('the', 'B-NP'), ('airspeed', 'I-NP'),
('of', 'B-PP'), ('an', 'B-NP'), ('unladen', 'I-NP'), ('swallow', 'I-NP'),
('?', 'O')]
>>> list(chktagger.bio_to_chunks(tagged_sent, chunk_type='NP')) 
[('What', '0'), ('the airspeed', '2-3'), ('an unladen swallow', '5-6-7')]
",split,N/A,N/A,N/A
">>> from nltk.tag import SennaChunkTagger
>>> chktagger = SennaChunkTagger('/usr/share/senna-v3.0')
>>> sent = 'What is the airspeed of an unladen swallow ?'.split()
>>> tagged_sent = chktagger.tag(sent) 
>>> tagged_sent 
[('What', 'B-NP'), ('is', 'B-VP'), ('the', 'B-NP'), ('airspeed', 'I-NP'),
('of', 'B-PP'), ('an', 'B-NP'), ('unladen', 'I-NP'), ('swallow', 'I-NP'),
('?', 'O')]
>>> list(chktagger.bio_to_chunks(tagged_sent, chunk_type='NP')) 
[('What', '0'), ('the airspeed', '2-3'), ('an unladen swallow', '5-6-7')]
",chktagger.tag,N/A,N/A,N/A
">>> from nltk.tag import SennaChunkTagger
>>> chktagger = SennaChunkTagger('/usr/share/senna-v3.0')
>>> sent = 'What is the airspeed of an unladen swallow ?'.split()
>>> tagged_sent = chktagger.tag(sent) 
>>> tagged_sent 
[('What', 'B-NP'), ('is', 'B-VP'), ('the', 'B-NP'), ('airspeed', 'I-NP'),
('of', 'B-PP'), ('an', 'B-NP'), ('unladen', 'I-NP'), ('swallow', 'I-NP'),
('?', 'O')]
>>> list(chktagger.bio_to_chunks(tagged_sent, chunk_type='NP')) 
[('What', '0'), ('the airspeed', '2-3'), ('an unladen swallow', '5-6-7')]
",list,N/A,N/A,N/A
">>> from nltk.tag import SennaChunkTagger
>>> chktagger = SennaChunkTagger('/usr/share/senna-v3.0')
>>> sent = 'What is the airspeed of an unladen swallow ?'.split()
>>> tagged_sent = chktagger.tag(sent) 
>>> tagged_sent 
[('What', 'B-NP'), ('is', 'B-VP'), ('the', 'B-NP'), ('airspeed', 'I-NP'),
('of', 'B-PP'), ('an', 'B-NP'), ('unladen', 'I-NP'), ('swallow', 'I-NP'),
('?', 'O')]
>>> list(chktagger.bio_to_chunks(tagged_sent, chunk_type='NP')) 
[('What', '0'), ('the airspeed', '2-3'), ('an unladen swallow', '5-6-7')]
",chktagger.bio_to_chunks,N/A,N/A,N/A
"feature_detector(tokens, index, history) -> featureset
",feature_detector,N/A,N/A,N/A
">>> from nltk.tag import DefaultTagger
>>> default_tagger = DefaultTagger('NN')
>>> list(default_tagger.tag('This is a test'.split()))
[('This', 'NN'), ('is', 'NN'), ('a', 'NN'), ('test', 'NN')]
",DefaultTagger,__init__,nltk\nltk\tag\sequential.py,True
">>> from nltk.tag import DefaultTagger
>>> default_tagger = DefaultTagger('NN')
>>> list(default_tagger.tag('This is a test'.split()))
[('This', 'NN'), ('is', 'NN'), ('a', 'NN'), ('test', 'NN')]
",list,N/A,N/A,N/A
">>> from nltk.tag import DefaultTagger
>>> default_tagger = DefaultTagger('NN')
>>> list(default_tagger.tag('This is a test'.split()))
[('This', 'NN'), ('is', 'NN'), ('a', 'NN'), ('test', 'NN')]
",default_tagger.tag,N/A,N/A,N/A
">>> from nltk.tag import DefaultTagger
>>> default_tagger = DefaultTagger('NN')
>>> list(default_tagger.tag('This is a test'.split()))
[('This', 'NN'), ('is', 'NN'), ('a', 'NN'), ('test', 'NN')]
",split,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> from nltk.tag import RegexpTagger
>>> test_sent = brown.sents(categories='news')[0]
>>> regexp_tagger = RegexpTagger(
...     [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers
...      (r'(The|the|A|a|An|an)$', 'AT'),   # articles
...      (r'.*able$', 'JJ'),                # adjectives
...      (r'.*ness$', 'NN'),                # nouns formed from adjectives
...      (r'.*ly$', 'RB'),                  # adverbs
...      (r'.*s$', 'NNS'),                  # plural nouns
...      (r'.*ing$', 'VBG'),                # gerunds
...      (r'.*ed$', 'VBD'),                 # past tense verbs
...      (r'.*', 'NN')                      # nouns (default)
... ])
>>> regexp_tagger

>>> regexp_tagger.tag(test_sent)
[('The', 'AT'), ('Fulton', 'NN'), ('County', 'NN'), ('Grand', 'NN'), ('Jury', 'NN'),
('said', 'NN'), ('Friday', 'NN'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'NN'),
(""Atlanta's"", 'NNS'), ('recent', 'NN'), ('primary', 'NN'), ('election', 'NN'),
('produced', 'VBD'), ('``', 'NN'), ('no', 'NN'), ('evidence', 'NN'), (""''"", 'NN'),
('that', 'NN'), ('any', 'NN'), ('irregularities', 'NNS'), ('took', 'NN'),
('place', 'NN'), ('.', 'NN')]
",brown.sents,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> from nltk.tag import RegexpTagger
>>> test_sent = brown.sents(categories='news')[0]
>>> regexp_tagger = RegexpTagger(
...     [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers
...      (r'(The|the|A|a|An|an)$', 'AT'),   # articles
...      (r'.*able$', 'JJ'),                # adjectives
...      (r'.*ness$', 'NN'),                # nouns formed from adjectives
...      (r'.*ly$', 'RB'),                  # adverbs
...      (r'.*s$', 'NNS'),                  # plural nouns
...      (r'.*ing$', 'VBG'),                # gerunds
...      (r'.*ed$', 'VBD'),                 # past tense verbs
...      (r'.*', 'NN')                      # nouns (default)
... ])
>>> regexp_tagger

>>> regexp_tagger.tag(test_sent)
[('The', 'AT'), ('Fulton', 'NN'), ('County', 'NN'), ('Grand', 'NN'), ('Jury', 'NN'),
('said', 'NN'), ('Friday', 'NN'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'NN'),
(""Atlanta's"", 'NNS'), ('recent', 'NN'), ('primary', 'NN'), ('election', 'NN'),
('produced', 'VBD'), ('``', 'NN'), ('no', 'NN'), ('evidence', 'NN'), (""''"", 'NN'),
('that', 'NN'), ('any', 'NN'), ('irregularities', 'NNS'), ('took', 'NN'),
('place', 'NN'), ('.', 'NN')]
",RegexpTagger,__init__,nltk\nltk\tag\sequential.py,True
">>> from nltk.corpus import brown
>>> from nltk.tag import RegexpTagger
>>> test_sent = brown.sents(categories='news')[0]
>>> regexp_tagger = RegexpTagger(
...     [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers
...      (r'(The|the|A|a|An|an)$', 'AT'),   # articles
...      (r'.*able$', 'JJ'),                # adjectives
...      (r'.*ness$', 'NN'),                # nouns formed from adjectives
...      (r'.*ly$', 'RB'),                  # adverbs
...      (r'.*s$', 'NNS'),                  # plural nouns
...      (r'.*ing$', 'VBG'),                # gerunds
...      (r'.*ed$', 'VBD'),                 # past tense verbs
...      (r'.*', 'NN')                      # nouns (default)
... ])
>>> regexp_tagger

>>> regexp_tagger.tag(test_sent)
[('The', 'AT'), ('Fulton', 'NN'), ('County', 'NN'), ('Grand', 'NN'), ('Jury', 'NN'),
('said', 'NN'), ('Friday', 'NN'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'NN'),
(""Atlanta's"", 'NNS'), ('recent', 'NN'), ('primary', 'NN'), ('election', 'NN'),
('produced', 'VBD'), ('``', 'NN'), ('no', 'NN'), ('evidence', 'NN'), (""''"", 'NN'),
('that', 'NN'), ('any', 'NN'), ('irregularities', 'NNS'), ('took', 'NN'),
('place', 'NN'), ('.', 'NN')]
",regexp_tagger.tag,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> from nltk.tag import UnigramTagger
>>> test_sent = brown.sents(categories='news')[0]
>>> unigram_tagger = UnigramTagger(brown.tagged_sents(categories='news')[:500])
>>> for tok, tag in unigram_tagger.tag(test_sent):
...     print(""({}, {}), "".format(tok, tag))
(The, AT), (Fulton, NP-TL), (County, NN-TL), (Grand, JJ-TL),
(Jury, NN-TL), (said, VBD), (Friday, NR), (an, AT),
(investigation, NN), (of, IN), (Atlanta's, NP$), (recent, JJ),
(primary, NN), (election, NN), (produced, VBD), (``, ``),
(no, AT), (evidence, NN), ('', ''), (that, CS), (any, DTI),
(irregularities, NNS), (took, VBD), (place, NN), (., .),
",brown.sents,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> from nltk.tag import UnigramTagger
>>> test_sent = brown.sents(categories='news')[0]
>>> unigram_tagger = UnigramTagger(brown.tagged_sents(categories='news')[:500])
>>> for tok, tag in unigram_tagger.tag(test_sent):
...     print(""({}, {}), "".format(tok, tag))
(The, AT), (Fulton, NP-TL), (County, NN-TL), (Grand, JJ-TL),
(Jury, NN-TL), (said, VBD), (Friday, NR), (an, AT),
(investigation, NN), (of, IN), (Atlanta's, NP$), (recent, JJ),
(primary, NN), (election, NN), (produced, VBD), (``, ``),
(no, AT), (evidence, NN), ('', ''), (that, CS), (any, DTI),
(irregularities, NNS), (took, VBD), (place, NN), (., .),
",UnigramTagger,__init__,nltk\nltk\tag\sequential.py,True
">>> from nltk.corpus import brown
>>> from nltk.tag import UnigramTagger
>>> test_sent = brown.sents(categories='news')[0]
>>> unigram_tagger = UnigramTagger(brown.tagged_sents(categories='news')[:500])
>>> for tok, tag in unigram_tagger.tag(test_sent):
...     print(""({}, {}), "".format(tok, tag))
(The, AT), (Fulton, NP-TL), (County, NN-TL), (Grand, JJ-TL),
(Jury, NN-TL), (said, VBD), (Friday, NR), (an, AT),
(investigation, NN), (of, IN), (Atlanta's, NP$), (recent, JJ),
(primary, NN), (election, NN), (produced, VBD), (``, ``),
(no, AT), (evidence, NN), ('', ''), (that, CS), (any, DTI),
(irregularities, NNS), (took, VBD), (place, NN), (., .),
",brown.tagged_sents,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> from nltk.tag import UnigramTagger
>>> test_sent = brown.sents(categories='news')[0]
>>> unigram_tagger = UnigramTagger(brown.tagged_sents(categories='news')[:500])
>>> for tok, tag in unigram_tagger.tag(test_sent):
...     print(""({}, {}), "".format(tok, tag))
(The, AT), (Fulton, NP-TL), (County, NN-TL), (Grand, JJ-TL),
(Jury, NN-TL), (said, VBD), (Friday, NR), (an, AT),
(investigation, NN), (of, IN), (Atlanta's, NP$), (recent, JJ),
(primary, NN), (election, NN), (produced, VBD), (``, ``),
(no, AT), (evidence, NN), ('', ''), (that, CS), (any, DTI),
(irregularities, NNS), (took, VBD), (place, NN), (., .),
",unigram_tagger.tag,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> from nltk.tag import UnigramTagger
>>> test_sent = brown.sents(categories='news')[0]
>>> unigram_tagger = UnigramTagger(brown.tagged_sents(categories='news')[:500])
>>> for tok, tag in unigram_tagger.tag(test_sent):
...     print(""({}, {}), "".format(tok, tag))
(The, AT), (Fulton, NP-TL), (County, NN-TL), (Grand, JJ-TL),
(Jury, NN-TL), (said, VBD), (Friday, NR), (an, AT),
(investigation, NN), (of, IN), (Atlanta's, NP$), (recent, JJ),
(primary, NN), (election, NN), (produced, VBD), (``, ``),
(no, AT), (evidence, NN), ('', ''), (that, CS), (any, DTI),
(irregularities, NNS), (took, VBD), (place, NN), (., .),
",print,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> from nltk.tag import UnigramTagger
>>> test_sent = brown.sents(categories='news')[0]
>>> unigram_tagger = UnigramTagger(brown.tagged_sents(categories='news')[:500])
>>> for tok, tag in unigram_tagger.tag(test_sent):
...     print(""({}, {}), "".format(tok, tag))
(The, AT), (Fulton, NP-TL), (County, NN-TL), (Grand, JJ-TL),
(Jury, NN-TL), (said, VBD), (Friday, NR), (an, AT),
(investigation, NN), (of, IN), (Atlanta's, NP$), (recent, JJ),
(primary, NN), (election, NN), (produced, VBD), (``, ``),
(no, AT), (evidence, NN), ('', ''), (that, CS), (any, DTI),
(irregularities, NNS), (took, VBD), (place, NN), (., .),
",format,N/A,N/A,N/A
">>> from nltk.tag import StanfordNERTagger
>>> st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') 
>>> st.tag('Rami Eid is studying at Stony Brook University in NY'.split()) 
[('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'),
 ('at', 'O'), ('Stony', 'ORGANIZATION'), ('Brook', 'ORGANIZATION'),
 ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'LOCATION')]
",StanfordNERTagger,__init__,nltk\nltk\tag\stanford.py,True
">>> from nltk.tag import StanfordNERTagger
>>> st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') 
>>> st.tag('Rami Eid is studying at Stony Brook University in NY'.split()) 
[('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'),
 ('at', 'O'), ('Stony', 'ORGANIZATION'), ('Brook', 'ORGANIZATION'),
 ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'LOCATION')]
",st.tag,N/A,N/A,N/A
">>> from nltk.tag import StanfordNERTagger
>>> st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') 
>>> st.tag('Rami Eid is studying at Stony Brook University in NY'.split()) 
[('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'),
 ('at', 'O'), ('Stony', 'ORGANIZATION'), ('Brook', 'ORGANIZATION'),
 ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'LOCATION')]
",split,N/A,N/A,N/A
">>> from nltk.tag import StanfordPOSTagger
>>> st = StanfordPOSTagger('english-bidirectional-distsim.tagger')
>>> st.tag('What is the airspeed of an unladen swallow ?'.split())
[('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'), ('unladen', 'JJ'), ('swallow', 'VB'), ('?', '.')]
",StanfordPOSTagger,__init__,nltk\nltk\tag\stanford.py,True
">>> from nltk.tag import StanfordPOSTagger
>>> st = StanfordPOSTagger('english-bidirectional-distsim.tagger')
>>> st.tag('What is the airspeed of an unladen swallow ?'.split())
[('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'), ('unladen', 'JJ'), ('swallow', 'VB'), ('?', '.')]
",st.tag,N/A,N/A,N/A
">>> from nltk.tag import StanfordPOSTagger
>>> st = StanfordPOSTagger('english-bidirectional-distsim.tagger')
>>> st.tag('What is the airspeed of an unladen swallow ?'.split())
[('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'), ('unladen', 'JJ'), ('swallow', 'VB'), ('?', '.')]
",split,N/A,N/A,N/A
">>> from nltk.tag.util import str2tuple
>>> str2tuple('fly/NN')
('fly', 'NN')
",str2tuple,N/A,N/A,N/A
">>> from nltk.tag.util import tuple2str
>>> tagged_token = ('fly', 'NN')
>>> tuple2str(tagged_token)
'fly/NN'
",tuple2str,N/A,N/A,N/A
">>> from nltk.tag.util import untag
>>> untag([('John', 'NNP'), ('saw', 'VBD'), ('Mary', 'NNP')])
['John', 'saw', 'Mary']
",untag,N/A,N/A,N/A
">>> from nltk import pos_tag, word_tokenize
>>> pos_tag(word_tokenize(""John's big idea isn't all that bad.""))
[('John', 'NNP'), (""'s"", 'POS'), ('big', 'JJ'), ('idea', 'NN'), ('is', 'VBZ'),
(""n't"", 'RB'), ('all', 'PDT'), ('that', 'DT'), ('bad', 'JJ'), ('.', '.')]
",pos_tag,N/A,N/A,N/A
">>> from nltk import pos_tag, word_tokenize
>>> pos_tag(word_tokenize(""John's big idea isn't all that bad.""))
[('John', 'NNP'), (""'s"", 'POS'), ('big', 'JJ'), ('idea', 'NN'), ('is', 'VBZ'),
(""n't"", 'RB'), ('all', 'PDT'), ('that', 'DT'), ('bad', 'JJ'), ('.', '.')]
",word_tokenize,N/A,N/A,N/A
">>> pos_tag(word_tokenize(""Илья оторопел и дважды перечитал бумажку.""), lang='rus')    
[('Илья', 'S'), ('оторопел', 'V'), ('и', 'CONJ'), ('дважды', 'ADV'), ('перечитал', 'V'),
('бумажку', 'S'), ('.', 'NONLEX')]
",pos_tag,N/A,N/A,N/A
">>> pos_tag(word_tokenize(""Илья оторопел и дважды перечитал бумажку.""), lang='rus')    
[('Илья', 'S'), ('оторопел', 'V'), ('и', 'CONJ'), ('дважды', 'ADV'), ('перечитал', 'V'),
('бумажку', 'S'), ('.', 'NONLEX')]
",word_tokenize,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> from nltk.tag import UnigramTagger
>>> tagger = UnigramTagger(brown.tagged_sents(categories='news')[:500])
>>> sent = ['Mitchell', 'decried', 'the', 'high', 'rate', 'of', 'unemployment']
>>> for word, tag in tagger.tag(sent):
...     print(word, '->', tag)
Mitchell -> NP
decried -> None
the -> AT
high -> JJ
rate -> NN
of -> IN
unemployment -> None
",UnigramTagger,__init__,nltk\nltk\tag\sequential.py,True
">>> from nltk.corpus import brown
>>> from nltk.tag import UnigramTagger
>>> tagger = UnigramTagger(brown.tagged_sents(categories='news')[:500])
>>> sent = ['Mitchell', 'decried', 'the', 'high', 'rate', 'of', 'unemployment']
>>> for word, tag in tagger.tag(sent):
...     print(word, '->', tag)
Mitchell -> NP
decried -> None
the -> AT
high -> JJ
rate -> NN
of -> IN
unemployment -> None
",brown.tagged_sents,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> from nltk.tag import UnigramTagger
>>> tagger = UnigramTagger(brown.tagged_sents(categories='news')[:500])
>>> sent = ['Mitchell', 'decried', 'the', 'high', 'rate', 'of', 'unemployment']
>>> for word, tag in tagger.tag(sent):
...     print(word, '->', tag)
Mitchell -> NP
decried -> None
the -> AT
high -> JJ
rate -> NN
of -> IN
unemployment -> None
",tagger.tag,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> from nltk.tag import UnigramTagger
>>> tagger = UnigramTagger(brown.tagged_sents(categories='news')[:500])
>>> sent = ['Mitchell', 'decried', 'the', 'high', 'rate', 'of', 'unemployment']
>>> for word, tag in tagger.tag(sent):
...     print(word, '->', tag)
Mitchell -> NP
decried -> None
the -> AT
high -> JJ
rate -> NN
of -> IN
unemployment -> None
",print,N/A,N/A,N/A
">>> tagger.evaluate(brown.tagged_sents(categories='news')[500:600])
0.7...
",tagger.evaluate,N/A,N/A,N/A
">>> tagger.evaluate(brown.tagged_sents(categories='news')[500:600])
0.7...
",brown.tagged_sents,N/A,N/A,N/A
">>> from nltk.tag import pos_tag
>>> from nltk.tokenize import word_tokenize
>>> pos_tag(word_tokenize(""John's big idea isn't all that bad.""))
[('John', 'NNP'), (""'s"", 'POS'), ('big', 'JJ'), ('idea', 'NN'), ('is', 'VBZ'),
(""n't"", 'RB'), ('all', 'PDT'), ('that', 'DT'), ('bad', 'JJ'), ('.', '.')]
>>> pos_tag(word_tokenize(""John's big idea isn't all that bad.""), tagset='universal')
[('John', 'NOUN'), (""'s"", 'PRT'), ('big', 'ADJ'), ('idea', 'NOUN'), ('is', 'VERB'),
(""n't"", 'ADV'), ('all', 'DET'), ('that', 'DET'), ('bad', 'ADJ'), ('.', '.')]
",pos_tag,N/A,N/A,N/A
">>> from nltk.tag import pos_tag
>>> from nltk.tokenize import word_tokenize
>>> pos_tag(word_tokenize(""John's big idea isn't all that bad.""))
[('John', 'NNP'), (""'s"", 'POS'), ('big', 'JJ'), ('idea', 'NN'), ('is', 'VBZ'),
(""n't"", 'RB'), ('all', 'PDT'), ('that', 'DT'), ('bad', 'JJ'), ('.', '.')]
>>> pos_tag(word_tokenize(""John's big idea isn't all that bad.""), tagset='universal')
[('John', 'NOUN'), (""'s"", 'PRT'), ('big', 'ADJ'), ('idea', 'NOUN'), ('is', 'VERB'),
(""n't"", 'ADV'), ('all', 'DET'), ('that', 'DET'), ('bad', 'ADJ'), ('.', '.')]
",word_tokenize,N/A,N/A,N/A
tokenize(),tokenize,N/A,N/A,N/A
tokenize_sents(),tokenize_sents,N/A,N/A,N/A
self.span_tokenize(),self.span_tokenize,N/A,N/A,N/A
self.tokenize(),self.tokenize,N/A,N/A,N/A
PunktSentenceTokenizer(text),PunktSentenceTokenizer,__init__,nltk\nltk\tokenize\punkt.py,True
regexp_tokenize(),regexp_tokenize,N/A,N/A,N/A
split(),split,N/A,N/A,N/A
split(),split,N/A,N/A,N/A
s.split('\n'),s.split,N/A,N/A,N/A
s.split(' '),s.split,N/A,N/A,N/A
s.split('\t'),s.split,N/A,N/A,N/A
word_tokenize(),word_tokenize,N/A,N/A,N/A
sent_tokenize(),sent_tokenize,N/A,N/A,N/A
"s.decode(""utf8"")",s.decode,N/A,N/A,N/A
">>> from nltk.tokenize import TweetTokenizer
>>> tknzr = TweetTokenizer()
>>> s0 = ""This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--""
>>> tknzr.tokenize(s0)
['This', 'is', 'a', 'cooool', '#dummysmiley', ':', ':-)', ':-P', '<3', 'and', 'some', 'arrows', '<', '>', '->', '<--']
",TweetTokenizer,__init__,nltk\nltk\tokenize\casual.py,True
">>> from nltk.tokenize import TweetTokenizer
>>> tknzr = TweetTokenizer()
>>> s0 = ""This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--""
>>> tknzr.tokenize(s0)
['This', 'is', 'a', 'cooool', '#dummysmiley', ':', ':-)', ':-P', '<3', 'and', 'some', 'arrows', '<', '>', '->', '<--']
",tknzr.tokenize,N/A,N/A,N/A
">>> tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)
>>> s1 = '@remy: This is waaaaayyyy too much for you!!!!!!'
>>> tknzr.tokenize(s1)
[':', 'This', 'is', 'waaayyy', 'too', 'much', 'for', 'you', '!', '!', '!']
",TweetTokenizer,__init__,nltk\nltk\tokenize\casual.py,True
">>> tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)
>>> s1 = '@remy: This is waaaaayyyy too much for you!!!!!!'
>>> tknzr.tokenize(s1)
[':', 'This', 'is', 'waaayyy', 'too', 'much', 'for', 'you', '!', '!', '!']
",tknzr.tokenize,N/A,N/A,N/A
">>> from nltk.tokenize import LegalitySyllableTokenizer
>>> from nltk import word_tokenize
>>> from nltk.corpus import words
>>> text = ""This is a wonderful sentence.""
>>> text_words = word_tokenize(text)
>>> LP = LegalitySyllableTokenizer(words.words())
>>> [LP.tokenize(word) for word in text_words]
[['This'], ['is'], ['a'], ['won', 'der', 'ful'], ['sen', 'ten', 'ce'], ['.']]
",word_tokenize,N/A,N/A,N/A
">>> from nltk.tokenize import LegalitySyllableTokenizer
>>> from nltk import word_tokenize
>>> from nltk.corpus import words
>>> text = ""This is a wonderful sentence.""
>>> text_words = word_tokenize(text)
>>> LP = LegalitySyllableTokenizer(words.words())
>>> [LP.tokenize(word) for word in text_words]
[['This'], ['is'], ['a'], ['won', 'der', 'ful'], ['sen', 'ten', 'ce'], ['.']]
",LegalitySyllableTokenizer,__init__,nltk\nltk\tokenize\legality_principle.py,True
">>> from nltk.tokenize import LegalitySyllableTokenizer
>>> from nltk import word_tokenize
>>> from nltk.corpus import words
>>> text = ""This is a wonderful sentence.""
>>> text_words = word_tokenize(text)
>>> LP = LegalitySyllableTokenizer(words.words())
>>> [LP.tokenize(word) for word in text_words]
[['This'], ['is'], ['a'], ['won', 'der', 'ful'], ['sen', 'ten', 'ce'], ['.']]
",words.words,N/A,N/A,N/A
">>> from nltk.tokenize import LegalitySyllableTokenizer
>>> from nltk import word_tokenize
>>> from nltk.corpus import words
>>> text = ""This is a wonderful sentence.""
>>> text_words = word_tokenize(text)
>>> LP = LegalitySyllableTokenizer(words.words())
>>> [LP.tokenize(word) for word in text_words]
[['This'], ['is'], ['a'], ['won', 'der', 'ful'], ['sen', 'ten', 'ce'], ['.']]
",LP.tokenize,N/A,N/A,N/A
">>> tokenizer = MWETokenizer([('a', 'little'), ('a', 'little', 'bit'), ('a', 'lot')])
>>> tokenizer.add_mwe(('in', 'spite', 'of'))
",MWETokenizer,__init__,nltk\nltk\tokenize\mwe.py,True
">>> tokenizer = MWETokenizer([('a', 'little'), ('a', 'little', 'bit'), ('a', 'lot')])
>>> tokenizer.add_mwe(('in', 'spite', 'of'))
",tokenizer.add_mwe,N/A,N/A,N/A
">>> tokenizer.tokenize('Testing testing testing one two three'.split())
['Testing', 'testing', 'testing', 'one', 'two', 'three']
",tokenizer.tokenize,N/A,N/A,N/A
">>> tokenizer.tokenize('Testing testing testing one two three'.split())
['Testing', 'testing', 'testing', 'one', 'two', 'three']
",split,N/A,N/A,N/A
">>> tokenizer.tokenize('This is a test in spite'.split())
['This', 'is', 'a', 'test', 'in', 'spite']
",tokenizer.tokenize,N/A,N/A,N/A
">>> tokenizer.tokenize('This is a test in spite'.split())
['This', 'is', 'a', 'test', 'in', 'spite']
",split,N/A,N/A,N/A
">>> tokenizer.tokenize('In a little or a little bit or a lot in spite of'.split())
['In', 'a_little', 'or', 'a_little_bit', 'or', 'a_lot', 'in_spite_of']
",tokenizer.tokenize,N/A,N/A,N/A
">>> tokenizer.tokenize('In a little or a little bit or a lot in spite of'.split())
['In', 'a_little', 'or', 'a_little_bit', 'or', 'a_lot', 'in_spite_of']
",split,N/A,N/A,N/A
">>> tokenizer = MWETokenizer()
>>> tokenizer.add_mwe(('a', 'b'))
>>> tokenizer.add_mwe(('a', 'b', 'c'))
>>> tokenizer.add_mwe(('a', 'x'))
>>> expected = {'a': {'x': {True: None}, 'b': {True: None, 'c': {True: None}}}}
>>> tokenizer._mwes == expected
True
",MWETokenizer,__init__,nltk\nltk\tokenize\mwe.py,True
">>> tokenizer = MWETokenizer()
>>> tokenizer.add_mwe(('a', 'b'))
>>> tokenizer.add_mwe(('a', 'b', 'c'))
>>> tokenizer.add_mwe(('a', 'x'))
>>> expected = {'a': {'x': {True: None}, 'b': {True: None, 'c': {True: None}}}}
>>> tokenizer._mwes == expected
True
",tokenizer.add_mwe,N/A,N/A,N/A
">>> tokenizer = MWETokenizer([('hors', ""d'oeuvre"")], separator='+')
>>> tokenizer.tokenize(""An hors d'oeuvre tonight, sir?"".split())
['An', ""hors+d'oeuvre"", 'tonight,', 'sir?']
",MWETokenizer,__init__,nltk\nltk\tokenize\mwe.py,True
">>> tokenizer = MWETokenizer([('hors', ""d'oeuvre"")], separator='+')
>>> tokenizer.tokenize(""An hors d'oeuvre tonight, sir?"".split())
['An', ""hors+d'oeuvre"", 'tonight,', 'sir?']
",tokenizer.tokenize,N/A,N/A,N/A
">>> tokenizer = MWETokenizer([('hors', ""d'oeuvre"")], separator='+')
>>> tokenizer.tokenize(""An hors d'oeuvre tonight, sir?"".split())
['An', ""hors+d'oeuvre"", 'tonight,', 'sir?']
",split,N/A,N/A,N/A
">>> import nltk.data
>>> text = '''
... Punkt knows that the periods in Mr. Smith and Johann S. Bach
... do not mark sentence boundaries.  And sometimes sentences
... can start with non-capitalized words.  i is a good variable
... name.
... '''
>>> sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')
>>> print('\n-----\n'.join(sent_detector.tokenize(text.strip())))
Punkt knows that the periods in Mr. Smith and Johann S. Bach
do not mark sentence boundaries.
-----
And sometimes sentences
can start with non-capitalized words.
-----
i is a good variable
name.
",data.load,N/A,N/A,N/A
">>> import nltk.data
>>> text = '''
... Punkt knows that the periods in Mr. Smith and Johann S. Bach
... do not mark sentence boundaries.  And sometimes sentences
... can start with non-capitalized words.  i is a good variable
... name.
... '''
>>> sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')
>>> print('\n-----\n'.join(sent_detector.tokenize(text.strip())))
Punkt knows that the periods in Mr. Smith and Johann S. Bach
do not mark sentence boundaries.
-----
And sometimes sentences
can start with non-capitalized words.
-----
i is a good variable
name.
",print,N/A,N/A,N/A
">>> import nltk.data
>>> text = '''
... Punkt knows that the periods in Mr. Smith and Johann S. Bach
... do not mark sentence boundaries.  And sometimes sentences
... can start with non-capitalized words.  i is a good variable
... name.
... '''
>>> sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')
>>> print('\n-----\n'.join(sent_detector.tokenize(text.strip())))
Punkt knows that the periods in Mr. Smith and Johann S. Bach
do not mark sentence boundaries.
-----
And sometimes sentences
can start with non-capitalized words.
-----
i is a good variable
name.
",join,N/A,N/A,N/A
">>> import nltk.data
>>> text = '''
... Punkt knows that the periods in Mr. Smith and Johann S. Bach
... do not mark sentence boundaries.  And sometimes sentences
... can start with non-capitalized words.  i is a good variable
... name.
... '''
>>> sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')
>>> print('\n-----\n'.join(sent_detector.tokenize(text.strip())))
Punkt knows that the periods in Mr. Smith and Johann S. Bach
do not mark sentence boundaries.
-----
And sometimes sentences
can start with non-capitalized words.
-----
i is a good variable
name.
",sent_detector.tokenize,N/A,N/A,N/A
">>> import nltk.data
>>> text = '''
... Punkt knows that the periods in Mr. Smith and Johann S. Bach
... do not mark sentence boundaries.  And sometimes sentences
... can start with non-capitalized words.  i is a good variable
... name.
... '''
>>> sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')
>>> print('\n-----\n'.join(sent_detector.tokenize(text.strip())))
Punkt knows that the periods in Mr. Smith and Johann S. Bach
do not mark sentence boundaries.
-----
And sometimes sentences
can start with non-capitalized words.
-----
i is a good variable
name.
",text.strip,N/A,N/A,N/A
">>> text = '''
... (How does it deal with this parenthesis?)  ""It should be part of the
... previous sentence."" ""(And the same with this one.)"" ('And this one!')
... ""('(And (this)) '?)"" [(and this. )]
... '''
>>> print('\n-----\n'.join(
...     sent_detector.tokenize(text.strip())))
(How does it deal with this parenthesis?)
-----
""It should be part of the
previous sentence.""
-----
""(And the same with this one.)""
-----
('And this one!')
-----
""('(And (this)) '?)""
-----
[(and this. )]
>>> print('\n-----\n'.join(
...     sent_detector.tokenize(text.strip(), realign_boundaries=False)))
(How does it deal with this parenthesis?
-----
)  ""It should be part of the
previous sentence.
-----
"" ""(And the same with this one.
-----
)"" ('And this one!
-----
')
""('(And (this)) '?
-----
)"" [(and this.
-----
)]
",print,N/A,N/A,N/A
">>> text = '''
... (How does it deal with this parenthesis?)  ""It should be part of the
... previous sentence."" ""(And the same with this one.)"" ('And this one!')
... ""('(And (this)) '?)"" [(and this. )]
... '''
>>> print('\n-----\n'.join(
...     sent_detector.tokenize(text.strip())))
(How does it deal with this parenthesis?)
-----
""It should be part of the
previous sentence.""
-----
""(And the same with this one.)""
-----
('And this one!')
-----
""('(And (this)) '?)""
-----
[(and this. )]
>>> print('\n-----\n'.join(
...     sent_detector.tokenize(text.strip(), realign_boundaries=False)))
(How does it deal with this parenthesis?
-----
)  ""It should be part of the
previous sentence.
-----
"" ""(And the same with this one.
-----
)"" ('And this one!
-----
')
""('(And (this)) '?
-----
)"" [(and this.
-----
)]
",join,N/A,N/A,N/A
">>> text = '''
... (How does it deal with this parenthesis?)  ""It should be part of the
... previous sentence."" ""(And the same with this one.)"" ('And this one!')
... ""('(And (this)) '?)"" [(and this. )]
... '''
>>> print('\n-----\n'.join(
...     sent_detector.tokenize(text.strip())))
(How does it deal with this parenthesis?)
-----
""It should be part of the
previous sentence.""
-----
""(And the same with this one.)""
-----
('And this one!')
-----
""('(And (this)) '?)""
-----
[(and this. )]
>>> print('\n-----\n'.join(
...     sent_detector.tokenize(text.strip(), realign_boundaries=False)))
(How does it deal with this parenthesis?
-----
)  ""It should be part of the
previous sentence.
-----
"" ""(And the same with this one.
-----
)"" ('And this one!
-----
')
""('(And (this)) '?
-----
)"" [(and this.
-----
)]
",sent_detector.tokenize,N/A,N/A,N/A
">>> text = '''
... (How does it deal with this parenthesis?)  ""It should be part of the
... previous sentence."" ""(And the same with this one.)"" ('And this one!')
... ""('(And (this)) '?)"" [(and this. )]
... '''
>>> print('\n-----\n'.join(
...     sent_detector.tokenize(text.strip())))
(How does it deal with this parenthesis?)
-----
""It should be part of the
previous sentence.""
-----
""(And the same with this one.)""
-----
('And this one!')
-----
""('(And (this)) '?)""
-----
[(and this. )]
>>> print('\n-----\n'.join(
...     sent_detector.tokenize(text.strip(), realign_boundaries=False)))
(How does it deal with this parenthesis?
-----
)  ""It should be part of the
previous sentence.
-----
"" ""(And the same with this one.
-----
)"" ('And this one!
-----
')
""('(And (this)) '?
-----
)"" [(and this.
-----
)]
",text.strip,N/A,N/A,N/A
">>> from nltk.tokenize import RegexpTokenizer
>>> s = ""Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.\n\nThanks.""
>>> tokenizer = RegexpTokenizer('\w+|\$[\d\.]+|\S+')
>>> tokenizer.tokenize(s)
['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York', '.',
'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']
",RegexpTokenizer,__init__,nltk\nltk\tokenize\regexp.py,True
">>> from nltk.tokenize import RegexpTokenizer
>>> s = ""Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.\n\nThanks.""
>>> tokenizer = RegexpTokenizer('\w+|\$[\d\.]+|\S+')
>>> tokenizer.tokenize(s)
['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York', '.',
'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']
",tokenizer.tokenize,N/A,N/A,N/A
">>> tokenizer = RegexpTokenizer('\s+', gaps=True)
>>> tokenizer.tokenize(s)
['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York.',
'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks.']
",RegexpTokenizer,__init__,nltk\nltk\tokenize\regexp.py,True
">>> tokenizer = RegexpTokenizer('\s+', gaps=True)
>>> tokenizer.tokenize(s)
['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York.',
'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks.']
",tokenizer.tokenize,N/A,N/A,N/A
">>> capword_tokenizer = RegexpTokenizer('[A-Z]\w+')
>>> capword_tokenizer.tokenize(s)
['Good', 'New', 'York', 'Please', 'Thanks']
",RegexpTokenizer,__init__,nltk\nltk\tokenize\regexp.py,True
">>> capword_tokenizer = RegexpTokenizer('[A-Z]\w+')
>>> capword_tokenizer.tokenize(s)
['Good', 'New', 'York', 'Please', 'Thanks']
",capword_tokenizer.tokenize,N/A,N/A,N/A
">>> from nltk.tokenize import BlanklineTokenizer
>>> # Uses '\s*\n\s*\n\s*':
>>> BlanklineTokenizer().tokenize(s)
['Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.',
'Thanks.']
",BlanklineTokenizer,__init__,nltk\nltk\tokenize\regexp.py,True
">>> from nltk.tokenize import BlanklineTokenizer
>>> # Uses '\s*\n\s*\n\s*':
>>> BlanklineTokenizer().tokenize(s)
['Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.',
'Thanks.']
",tokenize,N/A,N/A,N/A
">>> from nltk.tokenize import regexp_tokenize, wordpunct_tokenize, blankline_tokenize
>>> regexp_tokenize(s, pattern='\w+|\$[\d\.]+|\S+')
['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York', '.',
'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']
>>> wordpunct_tokenize(s)
['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York',
 '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']
>>> blankline_tokenize(s)
['Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.', 'Thanks.']
",regexp_tokenize,N/A,N/A,N/A
">>> from nltk.tokenize import regexp_tokenize, wordpunct_tokenize, blankline_tokenize
>>> regexp_tokenize(s, pattern='\w+|\$[\d\.]+|\S+')
['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York', '.',
'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']
>>> wordpunct_tokenize(s)
['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York',
 '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']
>>> blankline_tokenize(s)
['Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.', 'Thanks.']
",wordpunct_tokenize,N/A,N/A,N/A
">>> from nltk.tokenize import regexp_tokenize, wordpunct_tokenize, blankline_tokenize
>>> regexp_tokenize(s, pattern='\w+|\$[\d\.]+|\S+')
['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York', '.',
'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']
>>> wordpunct_tokenize(s)
['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York',
 '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']
>>> blankline_tokenize(s)
['Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.', 'Thanks.']
",blankline_tokenize,N/A,N/A,N/A
">>> tokenizer = RegexpTokenizer('\w+|\$[\d\.]+|\S+')
",RegexpTokenizer,__init__,nltk\nltk\tokenize\regexp.py,True
">>> from nltk.tokenize import WhitespaceTokenizer
>>> s = ""Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.\n\nThanks.""
>>> WhitespaceTokenizer().tokenize(s)
['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York.',
'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks.']
",WhitespaceTokenizer,__init__,nltk\nltk\tokenize\regexp.py,True
">>> from nltk.tokenize import WhitespaceTokenizer
>>> s = ""Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.\n\nThanks.""
>>> WhitespaceTokenizer().tokenize(s)
['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York.',
'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks.']
",tokenize,N/A,N/A,N/A
">>> from nltk.tokenize import WordPunctTokenizer
>>> s = ""Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.\n\nThanks.""
>>> WordPunctTokenizer().tokenize(s)
['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York',
'.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']
",WordPunctTokenizer,__init__,nltk\nltk\tokenize\regexp.py,True
">>> from nltk.tokenize import WordPunctTokenizer
>>> s = ""Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.\n\nThanks.""
>>> WordPunctTokenizer().tokenize(s)
['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York',
'.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']
",tokenize,N/A,N/A,N/A
">>> sents = ['Tokenization is widely regarded as a solved problem due to the high accuracy that rulebased tokenizers achieve.' ,
... 'But rule-based tokenizers are hard to maintain and their rules language specific.' ,
... 'We evaluated our method on three languages and obtained error rates of 0.27% (English), 0.35% (Dutch) and 0.76% (Italian) for our best models.'
... ]
>>> tokenizer = ReppTokenizer('/home/alvas/repp/') 
>>> for sent in sents:                             
...     tokenizer.tokenize(sent)                   
...
(u'Tokenization', u'is', u'widely', u'regarded', u'as', u'a', u'solved', u'problem', u'due', u'to', u'the', u'high', u'accuracy', u'that', u'rulebased', u'tokenizers', u'achieve', u'.')
(u'But', u'rule-based', u'tokenizers', u'are', u'hard', u'to', u'maintain', u'and', u'their', u'rules', u'language', u'specific', u'.')
(u'We', u'evaluated', u'our', u'method', u'on', u'three', u'languages', u'and', u'obtained', u'error', u'rates', u'of', u'0.27', u'%', u'(', u'English', u')', u',', u'0.35', u'%', u'(', u'Dutch', u')', u'and', u'0.76', u'%', u'(', u'Italian', u')', u'for', u'our', u'best', u'models', u'.')
",ReppTokenizer,__init__,nltk\nltk\tokenize\repp.py,True
">>> sents = ['Tokenization is widely regarded as a solved problem due to the high accuracy that rulebased tokenizers achieve.' ,
... 'But rule-based tokenizers are hard to maintain and their rules language specific.' ,
... 'We evaluated our method on three languages and obtained error rates of 0.27% (English), 0.35% (Dutch) and 0.76% (Italian) for our best models.'
... ]
>>> tokenizer = ReppTokenizer('/home/alvas/repp/') 
>>> for sent in sents:                             
...     tokenizer.tokenize(sent)                   
...
(u'Tokenization', u'is', u'widely', u'regarded', u'as', u'a', u'solved', u'problem', u'due', u'to', u'the', u'high', u'accuracy', u'that', u'rulebased', u'tokenizers', u'achieve', u'.')
(u'But', u'rule-based', u'tokenizers', u'are', u'hard', u'to', u'maintain', u'and', u'their', u'rules', u'language', u'specific', u'.')
(u'We', u'evaluated', u'our', u'method', u'on', u'three', u'languages', u'and', u'obtained', u'error', u'rates', u'of', u'0.27', u'%', u'(', u'English', u')', u',', u'0.35', u'%', u'(', u'Dutch', u')', u'and', u'0.76', u'%', u'(', u'Italian', u')', u'for', u'our', u'best', u'models', u'.')
",tokenizer.tokenize,N/A,N/A,N/A
">>> for sent in tokenizer.tokenize_sents(sents): 
...     print(sent)                              
...
(u'Tokenization', u'is', u'widely', u'regarded', u'as', u'a', u'solved', u'problem', u'due', u'to', u'the', u'high', u'accuracy', u'that', u'rulebased', u'tokenizers', u'achieve', u'.')
(u'But', u'rule-based', u'tokenizers', u'are', u'hard', u'to', u'maintain', u'and', u'their', u'rules', u'language', u'specific', u'.')
(u'We', u'evaluated', u'our', u'method', u'on', u'three', u'languages', u'and', u'obtained', u'error', u'rates', u'of', u'0.27', u'%', u'(', u'English', u')', u',', u'0.35', u'%', u'(', u'Dutch', u')', u'and', u'0.76', u'%', u'(', u'Italian', u')', u'for', u'our', u'best', u'models', u'.')
>>> for sent in tokenizer.tokenize_sents(sents, keep_token_positions=True): 
...     print(sent)                                                         
...
[(u'Tokenization', 0, 12), (u'is', 13, 15), (u'widely', 16, 22), (u'regarded', 23, 31), (u'as', 32, 34), (u'a', 35, 36), (u'solved', 37, 43), (u'problem', 44, 51), (u'due', 52, 55), (u'to', 56, 58), (u'the', 59, 62), (u'high', 63, 67), (u'accuracy', 68, 76), (u'that', 77, 81), (u'rulebased', 82, 91), (u'tokenizers', 92, 102), (u'achieve', 103, 110), (u'.', 110, 111)]
[(u'But', 0, 3), (u'rule-based', 4, 14), (u'tokenizers', 15, 25), (u'are', 26, 29), (u'hard', 30, 34), (u'to', 35, 37), (u'maintain', 38, 46), (u'and', 47, 50), (u'their', 51, 56), (u'rules', 57, 62), (u'language', 63, 71), (u'specific', 72, 80), (u'.', 80, 81)]
[(u'We', 0, 2), (u'evaluated', 3, 12), (u'our', 13, 16), (u'method', 17, 23), (u'on', 24, 26), (u'three', 27, 32), (u'languages', 33, 42), (u'and', 43, 46), (u'obtained', 47, 55), (u'error', 56, 61), (u'rates', 62, 67), (u'of', 68, 70), (u'0.27', 71, 75), (u'%', 75, 76), (u'(', 77, 78), (u'English', 78, 85), (u')', 85, 86), (u',', 86, 87), (u'0.35', 88, 92), (u'%', 92, 93), (u'(', 94, 95), (u'Dutch', 95, 100), (u')', 100, 101), (u'and', 102, 105), (u'0.76', 106, 110), (u'%', 110, 111), (u'(', 112, 113), (u'Italian', 113, 120), (u')', 120, 121), (u'for', 122, 125), (u'our', 126, 129), (u'best', 130, 134), (u'models', 135, 141), (u'.', 141, 142)]
",tokenizer.tokenize_sents,N/A,N/A,N/A
">>> for sent in tokenizer.tokenize_sents(sents): 
...     print(sent)                              
...
(u'Tokenization', u'is', u'widely', u'regarded', u'as', u'a', u'solved', u'problem', u'due', u'to', u'the', u'high', u'accuracy', u'that', u'rulebased', u'tokenizers', u'achieve', u'.')
(u'But', u'rule-based', u'tokenizers', u'are', u'hard', u'to', u'maintain', u'and', u'their', u'rules', u'language', u'specific', u'.')
(u'We', u'evaluated', u'our', u'method', u'on', u'three', u'languages', u'and', u'obtained', u'error', u'rates', u'of', u'0.27', u'%', u'(', u'English', u')', u',', u'0.35', u'%', u'(', u'Dutch', u')', u'and', u'0.76', u'%', u'(', u'Italian', u')', u'for', u'our', u'best', u'models', u'.')
>>> for sent in tokenizer.tokenize_sents(sents, keep_token_positions=True): 
...     print(sent)                                                         
...
[(u'Tokenization', 0, 12), (u'is', 13, 15), (u'widely', 16, 22), (u'regarded', 23, 31), (u'as', 32, 34), (u'a', 35, 36), (u'solved', 37, 43), (u'problem', 44, 51), (u'due', 52, 55), (u'to', 56, 58), (u'the', 59, 62), (u'high', 63, 67), (u'accuracy', 68, 76), (u'that', 77, 81), (u'rulebased', 82, 91), (u'tokenizers', 92, 102), (u'achieve', 103, 110), (u'.', 110, 111)]
[(u'But', 0, 3), (u'rule-based', 4, 14), (u'tokenizers', 15, 25), (u'are', 26, 29), (u'hard', 30, 34), (u'to', 35, 37), (u'maintain', 38, 46), (u'and', 47, 50), (u'their', 51, 56), (u'rules', 57, 62), (u'language', 63, 71), (u'specific', 72, 80), (u'.', 80, 81)]
[(u'We', 0, 2), (u'evaluated', 3, 12), (u'our', 13, 16), (u'method', 17, 23), (u'on', 24, 26), (u'three', 27, 32), (u'languages', 33, 42), (u'and', 43, 46), (u'obtained', 47, 55), (u'error', 56, 61), (u'rates', 62, 67), (u'of', 68, 70), (u'0.27', 71, 75), (u'%', 75, 76), (u'(', 77, 78), (u'English', 78, 85), (u')', 85, 86), (u',', 86, 87), (u'0.35', 88, 92), (u'%', 92, 93), (u'(', 94, 95), (u'Dutch', 95, 100), (u')', 100, 101), (u'and', 102, 105), (u'0.76', 106, 110), (u'%', 110, 111), (u'(', 112, 113), (u'Italian', 113, 120), (u')', 120, 121), (u'for', 122, 125), (u'our', 126, 129), (u'best', 130, 134), (u'models', 135, 141), (u'.', 141, 142)]
",print,N/A,N/A,N/A
">>> from nltk.tokenize import SExprTokenizer
>>> SExprTokenizer().tokenize('(a b (c d)) e f (g)')
['(a b (c d))', 'e', 'f', '(g)']
",SExprTokenizer,__init__,nltk\nltk\tokenize\sexpr.py,True
">>> from nltk.tokenize import SExprTokenizer
>>> SExprTokenizer().tokenize('(a b (c d)) e f (g)')
['(a b (c d))', 'e', 'f', '(g)']
",tokenize,N/A,N/A,N/A
">>> SExprTokenizer().tokenize('c) d) e (f (g')
Traceback (most recent call last):
  ...
ValueError: Un-matched close paren at char 1
",SExprTokenizer,__init__,nltk\nltk\tokenize\sexpr.py,True
">>> SExprTokenizer().tokenize('c) d) e (f (g')
Traceback (most recent call last):
  ...
ValueError: Un-matched close paren at char 1
",tokenize,N/A,N/A,N/A
">>> SExprTokenizer(strict=False).tokenize('c) d) e (f (g')
['c', ')', 'd', ')', 'e', '(f (g']
",SExprTokenizer,__init__,nltk\nltk\tokenize\sexpr.py,True
">>> SExprTokenizer(strict=False).tokenize('c) d) e (f (g')
['c', ')', 'd', ')', 'e', '(f (g']
",tokenize,N/A,N/A,N/A
">>> SExprTokenizer(parens='{}').tokenize('{a b {c d}} e f {g}')
['{a b {c d}}', 'e', 'f', '{g}']
",SExprTokenizer,__init__,nltk\nltk\tokenize\sexpr.py,True
">>> SExprTokenizer(parens='{}').tokenize('{a b {c d}} e f {g}')
['{a b {c d}}', 'e', 'f', '{g}']
",tokenize,N/A,N/A,N/A
">>> from nltk.tokenize import sexpr_tokenize
>>> sexpr_tokenize('(a b (c d)) e f (g)')
['(a b (c d))', 'e', 'f', '(g)']
",sexpr_tokenize,N/A,N/A,N/A
">>> SExprTokenizer().tokenize('(a b (c d)) e f (g)')
['(a b (c d))', 'e', 'f', '(g)']
",SExprTokenizer,__init__,nltk\nltk\tokenize\sexpr.py,True
">>> SExprTokenizer().tokenize('(a b (c d)) e f (g)')
['(a b (c d))', 'e', 'f', '(g)']
",tokenize,N/A,N/A,N/A
">>> SExprTokenizer(strict=False).tokenize('c) d) e (f (g')
['c', ')', 'd', ')', 'e', '(f (g']
",SExprTokenizer,__init__,nltk\nltk\tokenize\sexpr.py,True
">>> SExprTokenizer(strict=False).tokenize('c) d) e (f (g')
['c', ')', 'd', ')', 'e', '(f (g']
",tokenize,N/A,N/A,N/A
">>> SExprTokenizer().tokenize('(a b (c d)) e f (g)')
['(a b (c d))', 'e', 'f', '(g)']
",SExprTokenizer,__init__,nltk\nltk\tokenize\sexpr.py,True
">>> SExprTokenizer().tokenize('(a b (c d)) e f (g)')
['(a b (c d))', 'e', 'f', '(g)']
",tokenize,N/A,N/A,N/A
">>> SExprTokenizer(strict=False).tokenize('c) d) e (f (g')
['c', ')', 'd', ')', 'e', '(f (g']
",SExprTokenizer,__init__,nltk\nltk\tokenize\sexpr.py,True
">>> SExprTokenizer(strict=False).tokenize('c) d) e (f (g')
['c', ')', 'd', ')', 'e', '(f (g']
",tokenize,N/A,N/A,N/A
">>> s = ""Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.\n\nThanks.""
>>> s.split()
['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York.',
'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks.']
>>> s.split(' ')
['Good', 'muffins', 'cost', '$3.88\nin', 'New', 'York.', '',
'Please', 'buy', 'me\ntwo', 'of', 'them.\n\nThanks.']
>>> s.split('\n')
['Good muffins cost $3.88', 'in New York.  Please buy me',
'two of them.', '', 'Thanks.']
",s.split,N/A,N/A,N/A
">>> from nltk.tokenize import LineTokenizer
>>> s = ""Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.\n\nThanks.""
>>> LineTokenizer(blanklines='keep').tokenize(s)
['Good muffins cost $3.88', 'in New York.  Please buy me',
'two of them.', '', 'Thanks.']
>>> # same as [l for l in s.split('\n') if l.strip()]:
>>> LineTokenizer(blanklines='discard').tokenize(s)
['Good muffins cost $3.88', 'in New York.  Please buy me',
'two of them.', 'Thanks.']
",LineTokenizer,__init__,nltk\nltk\tokenize\simple.py,True
">>> from nltk.tokenize import LineTokenizer
>>> s = ""Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.\n\nThanks.""
>>> LineTokenizer(blanklines='keep').tokenize(s)
['Good muffins cost $3.88', 'in New York.  Please buy me',
'two of them.', '', 'Thanks.']
>>> # same as [l for l in s.split('\n') if l.strip()]:
>>> LineTokenizer(blanklines='discard').tokenize(s)
['Good muffins cost $3.88', 'in New York.  Please buy me',
'two of them.', 'Thanks.']
",tokenize,N/A,N/A,N/A
">>> from nltk.tokenize import LineTokenizer
>>> s = ""Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.\n\nThanks.""
>>> LineTokenizer(blanklines='keep').tokenize(s)
['Good muffins cost $3.88', 'in New York.  Please buy me',
'two of them.', '', 'Thanks.']
>>> # same as [l for l in s.split('\n') if l.strip()]:
>>> LineTokenizer(blanklines='discard').tokenize(s)
['Good muffins cost $3.88', 'in New York.  Please buy me',
'two of them.', 'Thanks.']
",s.split,N/A,N/A,N/A
">>> from nltk.tokenize import LineTokenizer
>>> s = ""Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.\n\nThanks.""
>>> LineTokenizer(blanklines='keep').tokenize(s)
['Good muffins cost $3.88', 'in New York.  Please buy me',
'two of them.', '', 'Thanks.']
>>> # same as [l for l in s.split('\n') if l.strip()]:
>>> LineTokenizer(blanklines='discard').tokenize(s)
['Good muffins cost $3.88', 'in New York.  Please buy me',
'two of them.', 'Thanks.']
",l.strip,N/A,N/A,N/A
">>> from nltk.tokenize import SpaceTokenizer
>>> s = ""Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.\n\nThanks.""
>>> SpaceTokenizer().tokenize(s)
['Good', 'muffins', 'cost', '$3.88\nin', 'New', 'York.', '',
'Please', 'buy', 'me\ntwo', 'of', 'them.\n\nThanks.']
",SpaceTokenizer,N/A,N/A,N/A
">>> from nltk.tokenize import SpaceTokenizer
>>> s = ""Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.\n\nThanks.""
>>> SpaceTokenizer().tokenize(s)
['Good', 'muffins', 'cost', '$3.88\nin', 'New', 'York.', '',
'Please', 'buy', 'me\ntwo', 'of', 'them.\n\nThanks.']
",tokenize,N/A,N/A,N/A
">>> from nltk.tokenize import TabTokenizer
>>> TabTokenizer().tokenize('a\tb c\n\t d')
['a', 'b c\n', ' d']
",TabTokenizer,N/A,N/A,N/A
">>> from nltk.tokenize import TabTokenizer
>>> TabTokenizer().tokenize('a\tb c\n\t d')
['a', 'b c\n', ' d']
",tokenize,N/A,N/A,N/A
">>> from nltk.tokenize import SyllableTokenizer
>>> from nltk import word_tokenize
>>> SSP = SyllableTokenizer()
>>> SSP.tokenize('justification')
['jus', 'ti', 'fi', 'ca', 'tion']
>>> text = ""This is a foobar-like sentence.""
>>> [SSP.tokenize(token) for token in word_tokenize(text)]
[['This'], ['is'], ['a'], ['foo', 'bar', '-', 'li', 'ke'], ['sen', 'ten', 'ce'], ['.']]
",SyllableTokenizer,__init__,nltk\nltk\tokenize\sonority_sequencing.py,True
">>> from nltk.tokenize import SyllableTokenizer
>>> from nltk import word_tokenize
>>> SSP = SyllableTokenizer()
>>> SSP.tokenize('justification')
['jus', 'ti', 'fi', 'ca', 'tion']
>>> text = ""This is a foobar-like sentence.""
>>> [SSP.tokenize(token) for token in word_tokenize(text)]
[['This'], ['is'], ['a'], ['foo', 'bar', '-', 'li', 'ke'], ['sen', 'ten', 'ce'], ['.']]
",SSP.tokenize,N/A,N/A,N/A
">>> from nltk.tokenize import SyllableTokenizer
>>> from nltk import word_tokenize
>>> SSP = SyllableTokenizer()
>>> SSP.tokenize('justification')
['jus', 'ti', 'fi', 'ca', 'tion']
>>> text = ""This is a foobar-like sentence.""
>>> [SSP.tokenize(token) for token in word_tokenize(text)]
[['This'], ['is'], ['a'], ['foo', 'bar', '-', 'li', 'ke'], ['sen', 'ten', 'ce'], ['.']]
",word_tokenize,N/A,N/A,N/A
">>> from nltk.tokenize.stanford import StanfordTokenizer
>>> s = ""Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.\nThanks.""
>>> StanfordTokenizer().tokenize(s)
['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']
>>> s = ""The colour of the wall is blue.""
>>> StanfordTokenizer(options={""americanize"": True}).tokenize(s)
['The', 'color', 'of', 'the', 'wall', 'is', 'blue', '.']
",StanfordTokenizer,__init__,nltk\nltk\tokenize\stanford.py,True
">>> from nltk.tokenize.stanford import StanfordTokenizer
>>> s = ""Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.\nThanks.""
>>> StanfordTokenizer().tokenize(s)
['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']
>>> s = ""The colour of the wall is blue.""
>>> StanfordTokenizer(options={""americanize"": True}).tokenize(s)
['The', 'color', 'of', 'the', 'wall', 'is', 'blue', '.']
",tokenize,N/A,N/A,N/A
"seg = StanfordSegmenter(path_to_slf4j='/YOUR_PATH/slf4j-api.jar')
",StanfordSegmenter,__init__,nltk\nltk\tokenize\stanford_segmenter.py,True
">>> from nltk.tokenize.stanford_segmenter import StanfordSegmenter
>>> seg = StanfordSegmenter()
>>> seg.default_config('zh')
>>> sent = u'这是斯坦福中文分词器测试'
>>> print(seg.segment(sent))
这 是 斯坦福 中文 分词器 测试

>>> seg.default_config('ar')
>>> sent = u'هذا هو تصنيف ستانفورد العربي للكلمات'
>>> print(seg.segment(sent.split()))
هذا هو تصنيف ستانفورد العربي ل الكلمات
",StanfordSegmenter,__init__,nltk\nltk\tokenize\stanford_segmenter.py,True
">>> from nltk.tokenize.stanford_segmenter import StanfordSegmenter
>>> seg = StanfordSegmenter()
>>> seg.default_config('zh')
>>> sent = u'这是斯坦福中文分词器测试'
>>> print(seg.segment(sent))
这 是 斯坦福 中文 分词器 测试

>>> seg.default_config('ar')
>>> sent = u'هذا هو تصنيف ستانفورد العربي للكلمات'
>>> print(seg.segment(sent.split()))
هذا هو تصنيف ستانفورد العربي ل الكلمات
",seg.default_config,N/A,N/A,N/A
">>> from nltk.tokenize.stanford_segmenter import StanfordSegmenter
>>> seg = StanfordSegmenter()
>>> seg.default_config('zh')
>>> sent = u'这是斯坦福中文分词器测试'
>>> print(seg.segment(sent))
这 是 斯坦福 中文 分词器 测试

>>> seg.default_config('ar')
>>> sent = u'هذا هو تصنيف ستانفورد العربي للكلمات'
>>> print(seg.segment(sent.split()))
هذا هو تصنيف ستانفورد العربي ل الكلمات
",print,N/A,N/A,N/A
">>> from nltk.tokenize.stanford_segmenter import StanfordSegmenter
>>> seg = StanfordSegmenter()
>>> seg.default_config('zh')
>>> sent = u'这是斯坦福中文分词器测试'
>>> print(seg.segment(sent))
这 是 斯坦福 中文 分词器 测试

>>> seg.default_config('ar')
>>> sent = u'هذا هو تصنيف ستانفورد العربي للكلمات'
>>> print(seg.segment(sent.split()))
هذا هو تصنيف ستانفورد العربي ل الكلمات
",seg.segment,N/A,N/A,N/A
">>> from nltk.tokenize.stanford_segmenter import StanfordSegmenter
>>> seg = StanfordSegmenter()
>>> seg.default_config('zh')
>>> sent = u'这是斯坦福中文分词器测试'
>>> print(seg.segment(sent))
这 是 斯坦福 中文 分词器 测试

>>> seg.default_config('ar')
>>> sent = u'هذا هو تصنيف ستانفورد العربي للكلمات'
>>> print(seg.segment(sent.split()))
هذا هو تصنيف ستانفورد العربي ل الكلمات
",sent.split,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> tt = TextTilingTokenizer(demo_mode=True)
>>> text = brown.raw()[:4000]
>>> s, ss, d, b = tt.tokenize(text)
>>> b
[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]
",TextTilingTokenizer,__init__,nltk\nltk\tokenize\texttiling.py,True
">>> from nltk.corpus import brown
>>> tt = TextTilingTokenizer(demo_mode=True)
>>> text = brown.raw()[:4000]
>>> s, ss, d, b = tt.tokenize(text)
>>> b
[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]
",brown.raw,N/A,N/A,N/A
">>> from nltk.corpus import brown
>>> tt = TextTilingTokenizer(demo_mode=True)
>>> text = brown.raw()[:4000]
>>> s, ss, d, b = tt.tokenize(text)
>>> b
[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]
",tt.tokenize,N/A,N/A,N/A
"t=linspace(-2,2,0.1)
x=sin(t)+randn(len(t))*0.1
y=smooth(x)
",linspace,N/A,N/A,N/A
"t=linspace(-2,2,0.1)
x=sin(t)+randn(len(t))*0.1
y=smooth(x)
",sin,N/A,N/A,N/A
"t=linspace(-2,2,0.1)
x=sin(t)+randn(len(t))*0.1
y=smooth(x)
",randn,N/A,N/A,N/A
"t=linspace(-2,2,0.1)
x=sin(t)+randn(len(t))*0.1
y=smooth(x)
",len,N/A,N/A,N/A
"t=linspace(-2,2,0.1)
x=sin(t)+randn(len(t))*0.1
y=smooth(x)
",smooth,N/A,N/A,N/A
">>> toktok = ToktokTokenizer()
>>> text = u'Is 9.5 or 525,600 my favorite number?'
>>> print(toktok.tokenize(text, return_str=True))
Is 9.5 or 525,600 my favorite number ?
>>> text = u'The https://github.com/jonsafari/tok-tok/blob/master/tok-tok.pl is a website with/and/or slashes and sort of weird : things'
>>> print(toktok.tokenize(text, return_str=True))
The https://github.com/jonsafari/tok-tok/blob/master/tok-tok.pl is a website with/and/or slashes and sort of weird : things
>>> text = u'¡This, is a sentence with weird» symbols… appearing everywhere¿'
>>> expected = u'¡ This , is a sentence with weird » symbols … appearing everywhere ¿'
>>> assert toktok.tokenize(text, return_str=True) == expected
>>> toktok.tokenize(text) == [u'¡', u'This', u',', u'is', u'a', u'sentence', u'with', u'weird', u'»', u'symbols', u'…', u'appearing', u'everywhere', u'¿']
True
",ToktokTokenizer,__init__,nltk\nltk\tokenize\toktok.py,True
">>> toktok = ToktokTokenizer()
>>> text = u'Is 9.5 or 525,600 my favorite number?'
>>> print(toktok.tokenize(text, return_str=True))
Is 9.5 or 525,600 my favorite number ?
>>> text = u'The https://github.com/jonsafari/tok-tok/blob/master/tok-tok.pl is a website with/and/or slashes and sort of weird : things'
>>> print(toktok.tokenize(text, return_str=True))
The https://github.com/jonsafari/tok-tok/blob/master/tok-tok.pl is a website with/and/or slashes and sort of weird : things
>>> text = u'¡This, is a sentence with weird» symbols… appearing everywhere¿'
>>> expected = u'¡ This , is a sentence with weird » symbols … appearing everywhere ¿'
>>> assert toktok.tokenize(text, return_str=True) == expected
>>> toktok.tokenize(text) == [u'¡', u'This', u',', u'is', u'a', u'sentence', u'with', u'weird', u'»', u'symbols', u'…', u'appearing', u'everywhere', u'¿']
True
",print,N/A,N/A,N/A
">>> toktok = ToktokTokenizer()
>>> text = u'Is 9.5 or 525,600 my favorite number?'
>>> print(toktok.tokenize(text, return_str=True))
Is 9.5 or 525,600 my favorite number ?
>>> text = u'The https://github.com/jonsafari/tok-tok/blob/master/tok-tok.pl is a website with/and/or slashes and sort of weird : things'
>>> print(toktok.tokenize(text, return_str=True))
The https://github.com/jonsafari/tok-tok/blob/master/tok-tok.pl is a website with/and/or slashes and sort of weird : things
>>> text = u'¡This, is a sentence with weird» symbols… appearing everywhere¿'
>>> expected = u'¡ This , is a sentence with weird » symbols … appearing everywhere ¿'
>>> assert toktok.tokenize(text, return_str=True) == expected
>>> toktok.tokenize(text) == [u'¡', u'This', u',', u'is', u'a', u'sentence', u'with', u'weird', u'»', u'symbols', u'…', u'appearing', u'everywhere', u'¿']
True
",toktok.tokenize,N/A,N/A,N/A
">>> from nltk.tokenize.treebank import TreebankWordTokenizer, TreebankWordDetokenizer
>>> s = '''Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.\nThanks.'''
>>> d = TreebankWordDetokenizer()
>>> t = TreebankWordTokenizer()
>>> toks = t.tokenize(s)
>>> d.detokenize(toks)
'Good muffins cost $3.88 in New York. Please buy me two of them. Thanks.'
",TreebankWordDetokenizer,__init__,nltk\nltk\tokenize\treebank.py,True
">>> from nltk.tokenize.treebank import TreebankWordTokenizer, TreebankWordDetokenizer
>>> s = '''Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.\nThanks.'''
>>> d = TreebankWordDetokenizer()
>>> t = TreebankWordTokenizer()
>>> toks = t.tokenize(s)
>>> d.detokenize(toks)
'Good muffins cost $3.88 in New York. Please buy me two of them. Thanks.'
",TreebankWordTokenizer,__init__,nltk\nltk\tokenize\treebank.py,True
">>> from nltk.tokenize.treebank import TreebankWordTokenizer, TreebankWordDetokenizer
>>> s = '''Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.\nThanks.'''
>>> d = TreebankWordDetokenizer()
>>> t = TreebankWordTokenizer()
>>> toks = t.tokenize(s)
>>> d.detokenize(toks)
'Good muffins cost $3.88 in New York. Please buy me two of them. Thanks.'
",t.tokenize,N/A,N/A,N/A
">>> from nltk.tokenize.treebank import TreebankWordTokenizer, TreebankWordDetokenizer
>>> s = '''Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.\nThanks.'''
>>> d = TreebankWordDetokenizer()
>>> t = TreebankWordTokenizer()
>>> toks = t.tokenize(s)
>>> d.detokenize(toks)
'Good muffins cost $3.88 in New York. Please buy me two of them. Thanks.'
",d.detokenize,N/A,N/A,N/A
">>> s = '''Good muffins cost $3.88\nin New (York).  Please (buy) me\ntwo of them.\n(Thanks).'''
>>> expected_tokens = ['Good', 'muffins', 'cost', '$', '3.88', 'in',
... 'New', '-LRB-', 'York', '-RRB-', '.', 'Please', '-LRB-', 'buy',
... '-RRB-', 'me', 'two', 'of', 'them.', '-LRB-', 'Thanks', '-RRB-', '.']
>>> expected_tokens == t.tokenize(s, convert_parentheses=True)
True
>>> expected_detoken = 'Good muffins cost $3.88 in New (York). Please (buy) me two of them. (Thanks).'
>>> expected_detoken == d.detokenize(t.tokenize(s, convert_parentheses=True), convert_parentheses=True)
True
",n,N/A,N/A,N/A
">>> s = '''Good muffins cost $3.88\nin New (York).  Please (buy) me\ntwo of them.\n(Thanks).'''
>>> expected_tokens = ['Good', 'muffins', 'cost', '$', '3.88', 'in',
... 'New', '-LRB-', 'York', '-RRB-', '.', 'Please', '-LRB-', 'buy',
... '-RRB-', 'me', 'two', 'of', 'them.', '-LRB-', 'Thanks', '-RRB-', '.']
>>> expected_tokens == t.tokenize(s, convert_parentheses=True)
True
>>> expected_detoken = 'Good muffins cost $3.88 in New (York). Please (buy) me two of them. (Thanks).'
>>> expected_detoken == d.detokenize(t.tokenize(s, convert_parentheses=True), convert_parentheses=True)
True
",t.tokenize,N/A,N/A,N/A
">>> s = '''Good muffins cost $3.88\nin New (York).  Please (buy) me\ntwo of them.\n(Thanks).'''
>>> expected_tokens = ['Good', 'muffins', 'cost', '$', '3.88', 'in',
... 'New', '-LRB-', 'York', '-RRB-', '.', 'Please', '-LRB-', 'buy',
... '-RRB-', 'me', 'two', 'of', 'them.', '-LRB-', 'Thanks', '-RRB-', '.']
>>> expected_tokens == t.tokenize(s, convert_parentheses=True)
True
>>> expected_detoken = 'Good muffins cost $3.88 in New (York). Please (buy) me two of them. (Thanks).'
>>> expected_detoken == d.detokenize(t.tokenize(s, convert_parentheses=True), convert_parentheses=True)
True
",d.detokenize,N/A,N/A,N/A
">>> from nltk.tokenize.treebank import TreebankWordDetokenizer
>>> toks = ['hello', ',', 'i', 'ca', ""n't"", 'feel', 'my', 'feet', '!', 'Help', '!', '!']
>>> twd = TreebankWordDetokenizer()
>>> twd.detokenize(toks)
""hello, i can't feel my feet! Help!!""
",TreebankWordDetokenizer,__init__,nltk\nltk\tokenize\treebank.py,True
">>> from nltk.tokenize.treebank import TreebankWordDetokenizer
>>> toks = ['hello', ',', 'i', 'ca', ""n't"", 'feel', 'my', 'feet', '!', 'Help', '!', '!']
>>> twd = TreebankWordDetokenizer()
>>> twd.detokenize(toks)
""hello, i can't feel my feet! Help!!""
",twd.detokenize,N/A,N/A,N/A
">>> toks = ['hello', ',', 'i', ""can't"", 'feel', ';', 'my', 'feet', '!',
... 'Help', '!', '!', 'He', 'said', ':', 'Help', ',', 'help', '?', '!']
>>> twd.detokenize(toks)
""hello, i can't feel; my feet! Help!! He said: Help, help?!""
",twd.detokenize,N/A,N/A,N/A
">>> from nltk.tokenize import TreebankWordTokenizer
>>> s = '''Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.\nThanks.'''
>>> TreebankWordTokenizer().tokenize(s)
['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York.', 'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks', '.']
>>> s = ""They'll save and invest more.""
>>> TreebankWordTokenizer().tokenize(s)
['They', ""'ll"", 'save', 'and', 'invest', 'more', '.']
>>> s = ""hi, my name can't hello,""
>>> TreebankWordTokenizer().tokenize(s)
['hi', ',', 'my', 'name', 'ca', ""n't"", 'hello', ',']
",TreebankWordTokenizer,__init__,nltk\nltk\tokenize\treebank.py,True
">>> from nltk.tokenize import TreebankWordTokenizer
>>> s = '''Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.\nThanks.'''
>>> TreebankWordTokenizer().tokenize(s)
['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York.', 'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks', '.']
>>> s = ""They'll save and invest more.""
>>> TreebankWordTokenizer().tokenize(s)
['They', ""'ll"", 'save', 'and', 'invest', 'more', '.']
>>> s = ""hi, my name can't hello,""
>>> TreebankWordTokenizer().tokenize(s)
['hi', ',', 'my', 'name', 'ca', ""n't"", 'hello', ',']
",tokenize,N/A,N/A,N/A
">>> from nltk.tokenize import TreebankWordTokenizer
>>> s = '''Good muffins cost $3.88\nin New (York).  Please (buy) me\ntwo of them.\n(Thanks).'''
>>> expected = [(0, 4), (5, 12), (13, 17), (18, 19), (19, 23),
... (24, 26), (27, 30), (31, 32), (32, 36), (36, 37), (37, 38),
... (40, 46), (47, 48), (48, 51), (51, 52), (53, 55), (56, 59),
... (60, 62), (63, 68), (69, 70), (70, 76), (76, 77), (77, 78)]
>>> list(TreebankWordTokenizer().span_tokenize(s)) == expected
True
>>> expected = ['Good', 'muffins', 'cost', '$', '3.88', 'in',
... 'New', '(', 'York', ')', '.', 'Please', '(', 'buy', ')',
... 'me', 'two', 'of', 'them.', '(', 'Thanks', ')', '.']
>>> [s[start:end] for start, end in TreebankWordTokenizer().span_tokenize(s)] == expected
True
",n,N/A,N/A,N/A
">>> from nltk.tokenize import TreebankWordTokenizer
>>> s = '''Good muffins cost $3.88\nin New (York).  Please (buy) me\ntwo of them.\n(Thanks).'''
>>> expected = [(0, 4), (5, 12), (13, 17), (18, 19), (19, 23),
... (24, 26), (27, 30), (31, 32), (32, 36), (36, 37), (37, 38),
... (40, 46), (47, 48), (48, 51), (51, 52), (53, 55), (56, 59),
... (60, 62), (63, 68), (69, 70), (70, 76), (76, 77), (77, 78)]
>>> list(TreebankWordTokenizer().span_tokenize(s)) == expected
True
>>> expected = ['Good', 'muffins', 'cost', '$', '3.88', 'in',
... 'New', '(', 'York', ')', '.', 'Please', '(', 'buy', ')',
... 'me', 'two', 'of', 'them.', '(', 'Thanks', ')', '.']
>>> [s[start:end] for start, end in TreebankWordTokenizer().span_tokenize(s)] == expected
True
",list,N/A,N/A,N/A
">>> from nltk.tokenize import TreebankWordTokenizer
>>> s = '''Good muffins cost $3.88\nin New (York).  Please (buy) me\ntwo of them.\n(Thanks).'''
>>> expected = [(0, 4), (5, 12), (13, 17), (18, 19), (19, 23),
... (24, 26), (27, 30), (31, 32), (32, 36), (36, 37), (37, 38),
... (40, 46), (47, 48), (48, 51), (51, 52), (53, 55), (56, 59),
... (60, 62), (63, 68), (69, 70), (70, 76), (76, 77), (77, 78)]
>>> list(TreebankWordTokenizer().span_tokenize(s)) == expected
True
>>> expected = ['Good', 'muffins', 'cost', '$', '3.88', 'in',
... 'New', '(', 'York', ')', '.', 'Please', '(', 'buy', ')',
... 'me', 'two', 'of', 'them.', '(', 'Thanks', ')', '.']
>>> [s[start:end] for start, end in TreebankWordTokenizer().span_tokenize(s)] == expected
True
",TreebankWordTokenizer,__init__,nltk\nltk\tokenize\treebank.py,True
">>> from nltk.tokenize import TreebankWordTokenizer
>>> s = '''Good muffins cost $3.88\nin New (York).  Please (buy) me\ntwo of them.\n(Thanks).'''
>>> expected = [(0, 4), (5, 12), (13, 17), (18, 19), (19, 23),
... (24, 26), (27, 30), (31, 32), (32, 36), (36, 37), (37, 38),
... (40, 46), (47, 48), (48, 51), (51, 52), (53, 55), (56, 59),
... (60, 62), (63, 68), (69, 70), (70, 76), (76, 77), (77, 78)]
>>> list(TreebankWordTokenizer().span_tokenize(s)) == expected
True
>>> expected = ['Good', 'muffins', 'cost', '$', '3.88', 'in',
... 'New', '(', 'York', ')', '.', 'Please', '(', 'buy', ')',
... 'me', 'two', 'of', 'them.', '(', 'Thanks', ')', '.']
>>> [s[start:end] for start, end in TreebankWordTokenizer().span_tokenize(s)] == expected
True
",span_tokenize,N/A,N/A,N/A
">>> from nltk.tokenize import TreebankWordTokenizer
>>> from nltk.tokenize.util import align_tokens
>>> s = str(""The plane, bound for St Petersburg, crashed in Egypt's ""
... ""Sinai desert just 23 minutes after take-off from Sharm el-Sheikh ""
... ""on Saturday."")
>>> tokens = TreebankWordTokenizer().tokenize(s)
>>> expected = [(0, 3), (4, 9), (9, 10), (11, 16), (17, 20), (21, 23),
... (24, 34), (34, 35), (36, 43), (44, 46), (47, 52), (52, 54),
... (55, 60), (61, 67), (68, 72), (73, 75), (76, 83), (84, 89),
... (90, 98), (99, 103), (104, 109), (110, 119), (120, 122),
... (123, 131), (131, 132)]
>>> output = list(align_tokens(tokens, s))
>>> len(tokens) == len(expected) == len(output)  # Check that length of tokens and tuples are the same.
True
>>> expected == list(align_tokens(tokens, s))  # Check that the output is as expected.
True
>>> tokens == [s[start:end] for start, end in output]  # Check that the slices of the string corresponds to the tokens.
True
",str,N/A,N/A,N/A
">>> from nltk.tokenize import TreebankWordTokenizer
>>> from nltk.tokenize.util import align_tokens
>>> s = str(""The plane, bound for St Petersburg, crashed in Egypt's ""
... ""Sinai desert just 23 minutes after take-off from Sharm el-Sheikh ""
... ""on Saturday."")
>>> tokens = TreebankWordTokenizer().tokenize(s)
>>> expected = [(0, 3), (4, 9), (9, 10), (11, 16), (17, 20), (21, 23),
... (24, 34), (34, 35), (36, 43), (44, 46), (47, 52), (52, 54),
... (55, 60), (61, 67), (68, 72), (73, 75), (76, 83), (84, 89),
... (90, 98), (99, 103), (104, 109), (110, 119), (120, 122),
... (123, 131), (131, 132)]
>>> output = list(align_tokens(tokens, s))
>>> len(tokens) == len(expected) == len(output)  # Check that length of tokens and tuples are the same.
True
>>> expected == list(align_tokens(tokens, s))  # Check that the output is as expected.
True
>>> tokens == [s[start:end] for start, end in output]  # Check that the slices of the string corresponds to the tokens.
True
",TreebankWordTokenizer,__init__,nltk\nltk\tokenize\treebank.py,True
">>> from nltk.tokenize import TreebankWordTokenizer
>>> from nltk.tokenize.util import align_tokens
>>> s = str(""The plane, bound for St Petersburg, crashed in Egypt's ""
... ""Sinai desert just 23 minutes after take-off from Sharm el-Sheikh ""
... ""on Saturday."")
>>> tokens = TreebankWordTokenizer().tokenize(s)
>>> expected = [(0, 3), (4, 9), (9, 10), (11, 16), (17, 20), (21, 23),
... (24, 34), (34, 35), (36, 43), (44, 46), (47, 52), (52, 54),
... (55, 60), (61, 67), (68, 72), (73, 75), (76, 83), (84, 89),
... (90, 98), (99, 103), (104, 109), (110, 119), (120, 122),
... (123, 131), (131, 132)]
>>> output = list(align_tokens(tokens, s))
>>> len(tokens) == len(expected) == len(output)  # Check that length of tokens and tuples are the same.
True
>>> expected == list(align_tokens(tokens, s))  # Check that the output is as expected.
True
>>> tokens == [s[start:end] for start, end in output]  # Check that the slices of the string corresponds to the tokens.
True
",tokenize,N/A,N/A,N/A
">>> from nltk.tokenize import TreebankWordTokenizer
>>> from nltk.tokenize.util import align_tokens
>>> s = str(""The plane, bound for St Petersburg, crashed in Egypt's ""
... ""Sinai desert just 23 minutes after take-off from Sharm el-Sheikh ""
... ""on Saturday."")
>>> tokens = TreebankWordTokenizer().tokenize(s)
>>> expected = [(0, 3), (4, 9), (9, 10), (11, 16), (17, 20), (21, 23),
... (24, 34), (34, 35), (36, 43), (44, 46), (47, 52), (52, 54),
... (55, 60), (61, 67), (68, 72), (73, 75), (76, 83), (84, 89),
... (90, 98), (99, 103), (104, 109), (110, 119), (120, 122),
... (123, 131), (131, 132)]
>>> output = list(align_tokens(tokens, s))
>>> len(tokens) == len(expected) == len(output)  # Check that length of tokens and tuples are the same.
True
>>> expected == list(align_tokens(tokens, s))  # Check that the output is as expected.
True
>>> tokens == [s[start:end] for start, end in output]  # Check that the slices of the string corresponds to the tokens.
True
",list,N/A,N/A,N/A
">>> from nltk.tokenize import TreebankWordTokenizer
>>> from nltk.tokenize.util import align_tokens
>>> s = str(""The plane, bound for St Petersburg, crashed in Egypt's ""
... ""Sinai desert just 23 minutes after take-off from Sharm el-Sheikh ""
... ""on Saturday."")
>>> tokens = TreebankWordTokenizer().tokenize(s)
>>> expected = [(0, 3), (4, 9), (9, 10), (11, 16), (17, 20), (21, 23),
... (24, 34), (34, 35), (36, 43), (44, 46), (47, 52), (52, 54),
... (55, 60), (61, 67), (68, 72), (73, 75), (76, 83), (84, 89),
... (90, 98), (99, 103), (104, 109), (110, 119), (120, 122),
... (123, 131), (131, 132)]
>>> output = list(align_tokens(tokens, s))
>>> len(tokens) == len(expected) == len(output)  # Check that length of tokens and tuples are the same.
True
>>> expected == list(align_tokens(tokens, s))  # Check that the output is as expected.
True
>>> tokens == [s[start:end] for start, end in output]  # Check that the slices of the string corresponds to the tokens.
True
",align_tokens,N/A,N/A,N/A
">>> from nltk.tokenize import TreebankWordTokenizer
>>> from nltk.tokenize.util import align_tokens
>>> s = str(""The plane, bound for St Petersburg, crashed in Egypt's ""
... ""Sinai desert just 23 minutes after take-off from Sharm el-Sheikh ""
... ""on Saturday."")
>>> tokens = TreebankWordTokenizer().tokenize(s)
>>> expected = [(0, 3), (4, 9), (9, 10), (11, 16), (17, 20), (21, 23),
... (24, 34), (34, 35), (36, 43), (44, 46), (47, 52), (52, 54),
... (55, 60), (61, 67), (68, 72), (73, 75), (76, 83), (84, 89),
... (90, 98), (99, 103), (104, 109), (110, 119), (120, 122),
... (123, 131), (131, 132)]
>>> output = list(align_tokens(tokens, s))
>>> len(tokens) == len(expected) == len(output)  # Check that length of tokens and tuples are the same.
True
>>> expected == list(align_tokens(tokens, s))  # Check that the output is as expected.
True
>>> tokens == [s[start:end] for start, end in output]  # Check that the slices of the string corresponds to the tokens.
True
",len,N/A,N/A,N/A
">>> CJKChars().ranges
[(4352, 4607), (11904, 42191), (43072, 43135), (44032, 55215), (63744, 64255), (65072, 65103), (65381, 65500), (131072, 196607)]
>>> is_cjk(u'㏾')
True
>>> is_cjk(u'﹟')
False
",CJKChars,N/A,N/A,N/A
">>> CJKChars().ranges
[(4352, 4607), (11904, 42191), (43072, 43135), (44032, 55215), (63744, 64255), (65072, 65103), (65381, 65500), (131072, 196607)]
>>> is_cjk(u'㏾')
True
>>> is_cjk(u'﹟')
False
",is_cjk,N/A,N/A,N/A
">>> from nltk.tokenize.util import regexp_span_tokenize
>>> s = '''Good muffins cost $3.88\nin New York.  Please buy me
... two of them.\n\nThanks.'''
>>> list(regexp_span_tokenize(s, r'\s'))
[(0, 4), (5, 12), (13, 17), (18, 23), (24, 26), (27, 30), (31, 36),
(38, 44), (45, 48), (49, 51), (52, 55), (56, 58), (59, 64), (66, 73)]
",list,N/A,N/A,N/A
">>> from nltk.tokenize.util import regexp_span_tokenize
>>> s = '''Good muffins cost $3.88\nin New York.  Please buy me
... two of them.\n\nThanks.'''
>>> list(regexp_span_tokenize(s, r'\s'))
[(0, 4), (5, 12), (13, 17), (18, 23), (24, 26), (27, 30), (31, 36),
(38, 44), (45, 48), (49, 51), (52, 55), (56, 58), (59, 64), (66, 73)]
",regexp_span_tokenize,N/A,N/A,N/A
">>> from nltk.tokenize import WhitespaceTokenizer
>>> from nltk.tokenize.util import spans_to_relative
>>> s = '''Good muffins cost $3.88\nin New York.  Please buy me
... two of them.\n\nThanks.'''
>>> list(spans_to_relative(WhitespaceTokenizer().span_tokenize(s)))
[(0, 4), (1, 7), (1, 4), (1, 5), (1, 2), (1, 3), (1, 5), (2, 6),
(1, 3), (1, 2), (1, 3), (1, 2), (1, 5), (2, 7)]
",list,N/A,N/A,N/A
">>> from nltk.tokenize import WhitespaceTokenizer
>>> from nltk.tokenize.util import spans_to_relative
>>> s = '''Good muffins cost $3.88\nin New York.  Please buy me
... two of them.\n\nThanks.'''
>>> list(spans_to_relative(WhitespaceTokenizer().span_tokenize(s)))
[(0, 4), (1, 7), (1, 4), (1, 5), (1, 2), (1, 3), (1, 5), (2, 6),
(1, 3), (1, 2), (1, 3), (1, 2), (1, 5), (2, 7)]
",spans_to_relative,N/A,N/A,N/A
">>> from nltk.tokenize import WhitespaceTokenizer
>>> from nltk.tokenize.util import spans_to_relative
>>> s = '''Good muffins cost $3.88\nin New York.  Please buy me
... two of them.\n\nThanks.'''
>>> list(spans_to_relative(WhitespaceTokenizer().span_tokenize(s)))
[(0, 4), (1, 7), (1, 4), (1, 5), (1, 2), (1, 3), (1, 5), (2, 6),
(1, 3), (1, 2), (1, 3), (1, 2), (1, 5), (2, 7)]
",WhitespaceTokenizer,__init__,nltk\nltk\tokenize\regexp.py,True
">>> from nltk.tokenize import WhitespaceTokenizer
>>> from nltk.tokenize.util import spans_to_relative
>>> s = '''Good muffins cost $3.88\nin New York.  Please buy me
... two of them.\n\nThanks.'''
>>> list(spans_to_relative(WhitespaceTokenizer().span_tokenize(s)))
[(0, 4), (1, 7), (1, 4), (1, 5), (1, 2), (1, 3), (1, 5), (2, 6),
(1, 3), (1, 2), (1, 3), (1, 2), (1, 5), (2, 7)]
",span_tokenize,N/A,N/A,N/A
">>> from nltk.tokenize.util import string_span_tokenize
>>> s = '''Good muffins cost $3.88\nin New York.  Please buy me
... two of them.\n\nThanks.'''
>>> list(string_span_tokenize(s, "" ""))
[(0, 4), (5, 12), (13, 17), (18, 26), (27, 30), (31, 36), (37, 37),
(38, 44), (45, 48), (49, 55), (56, 58), (59, 73)]
",list,N/A,N/A,N/A
">>> from nltk.tokenize.util import string_span_tokenize
>>> s = '''Good muffins cost $3.88\nin New York.  Please buy me
... two of them.\n\nThanks.'''
>>> list(string_span_tokenize(s, "" ""))
[(0, 4), (5, 12), (13, 17), (18, 26), (27, 30), (31, 36), (37, 37),
(38, 44), (45, 48), (49, 55), (56, 58), (59, 73)]
",string_span_tokenize,N/A,N/A,N/A
">>> input_str = ''')| & < > ' "" ] ['''
>>> expected_output =  ''')| & < > ' "" ] ['''
>>> escape(input_str) == expected_output
True
>>> xml_escape(input_str)
')| & < > ' "" ] ['
",escape,N/A,N/A,N/A
">>> input_str = ''')| & < > ' "" ] ['''
>>> expected_output =  ''')| & < > ' "" ] ['''
>>> escape(input_str) == expected_output
True
>>> xml_escape(input_str)
')| & < > ' "" ] ['
",xml_escape,N/A,N/A,N/A
">>> from xml.sax.saxutils import unescape
>>> s = ')| & < > ' "" ] ['
>>> expected = ''')| & < > ' "" ] ['''
>>> xml_unescape(s) == expected
True
",xml_unescape,N/A,N/A,N/A
">>> from nltk.tokenize import word_tokenize
>>> s = '''Good muffins cost $3.88\nin New York.  Please buy me
... two of them.\n\nThanks.'''
>>> word_tokenize(s)
['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.',
'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']
",word_tokenize,N/A,N/A,N/A
">>> from nltk.tokenize import wordpunct_tokenize
>>> wordpunct_tokenize(s)
['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York', '.',
'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']
",wordpunct_tokenize,N/A,N/A,N/A
">>> from nltk.tokenize import sent_tokenize, word_tokenize
>>> sent_tokenize(s)
['Good muffins cost $3.88\nin New York.', 'Please buy me\ntwo of them.', 'Thanks.']
>>> [word_tokenize(t) for t in sent_tokenize(s)]
[['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.'],
['Please', 'buy', 'me', 'two', 'of', 'them', '.'], ['Thanks', '.']]
",sent_tokenize,N/A,N/A,N/A
">>> from nltk.tokenize import sent_tokenize, word_tokenize
>>> sent_tokenize(s)
['Good muffins cost $3.88\nin New York.', 'Please buy me\ntwo of them.', 'Thanks.']
>>> [word_tokenize(t) for t in sent_tokenize(s)]
[['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.'],
['Please', 'buy', 'me', 'two', 'of', 'them', '.'], ['Thanks', '.']]
",word_tokenize,N/A,N/A,N/A
">>> from nltk.tokenize import WhitespaceTokenizer
>>> list(WhitespaceTokenizer().span_tokenize(s))
[(0, 4), (5, 12), (13, 17), (18, 23), (24, 26), (27, 30), (31, 36), (38, 44),
(45, 48), (49, 51), (52, 55), (56, 58), (59, 64), (66, 73)]
",list,N/A,N/A,N/A
">>> from nltk.tokenize import WhitespaceTokenizer
>>> list(WhitespaceTokenizer().span_tokenize(s))
[(0, 4), (5, 12), (13, 17), (18, 23), (24, 26), (27, 30), (31, 36), (38, 44),
(45, 48), (49, 51), (52, 55), (56, 58), (59, 64), (66, 73)]
",WhitespaceTokenizer,__init__,nltk\nltk\tokenize\regexp.py,True
">>> from nltk.tokenize import WhitespaceTokenizer
>>> list(WhitespaceTokenizer().span_tokenize(s))
[(0, 4), (5, 12), (13, 17), (18, 23), (24, 26), (27, 30), (31, 36), (38, 44),
(45, 48), (49, 51), (52, 55), (56, 58), (59, 64), (66, 73)]
",span_tokenize,N/A,N/A,N/A
