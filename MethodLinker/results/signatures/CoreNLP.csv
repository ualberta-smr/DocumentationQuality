Example,Extracted Signature,Linked Signature,Source file,Matched
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}",main,"BasicPipelineExample.main,StanfordCoreNLP.main","CoreNLP\src\edu\stanford\nlp\examples\BasicPipelineExample.java
CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java",False
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}",Properties,N/A,N/A,N/A
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}","('StanfordCoreNLP', 'StanfordCoreNLP(props)')","('StanfordCoreNLP', ""[['props'], ['props', 'enforceRequirements'], ['propsFileNamePrefix'], ['propsFileNamePrefix', 'enforceRequirements'], ['props', 'enforceRequirements', 'annotatorPool']]"")",CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}","('CoreDocument', 'CoreDocument(text)')","('CoreDocument', ""['documentText', ['annotation']]"")",CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java,False
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}",document.tokens,"Sentence.tokens,CoreDocument.tokens,CoreEntityMention.tokens,CoreSentence.tokens","CoreNLP\src\edu\stanford\nlp\simple\Sentence.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreEntityMention.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreSentence.java",False
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}",document.sentences,"CoreQuote.sentences,Document.sentences,CoreDocument.sentences","CoreNLP\src\edu\stanford\nlp\pipeline\CoreQuote.java
CoreNLP\src\edu\stanford\nlp\simple\Document.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java",False
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}",text,"CoreEntityMention.text,Document.text,Sentence.text,CoreQuote.text,CoreSentence.text,CoreDocument.text","CoreNLP\src\edu\stanford\nlp\pipeline\CoreEntityMention.java
CoreNLP\src\edu\stanford\nlp\simple\Document.java
CoreNLP\src\edu\stanford\nlp\simple\Sentence.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreQuote.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreSentence.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java",False
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}",sentence.posTags,"CoreSentence.posTags,Sentence.posTags","CoreNLP\src\edu\stanford\nlp\pipeline\CoreSentence.java
CoreNLP\src\edu\stanford\nlp\simple\Sentence.java",False
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}",sentence.nerTags,"Sentence.nerTags,CoreSentence.nerTags","CoreNLP\src\edu\stanford\nlp\simple\Sentence.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreSentence.java",False
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}","('sentence.constituencyParse', 'sentence.constituencyParse()')","('CoreSentence.constituencyParse', '[]')",CoreNLP\src\edu\stanford\nlp\pipeline\CoreSentence.java,True
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}",sentence.entityMentions,"CoreSentence.entityMentions,CoreDocument.entityMentions","CoreNLP\src\edu\stanford\nlp\pipeline\CoreSentence.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java",False
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}","('originalEntityMention.canonicalEntityMention', 'originalEntityMention.canonicalEntityMention()')","('CoreEntityMention.canonicalEntityMention', '[]')",CoreNLP\src\edu\stanford\nlp\pipeline\CoreEntityMention.java,True
"import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

  public static String text = ""Joe Smith was born in California. "" +
      ""In 2017, he went to Paris, France in the summer. "" +
      ""His flight left at 3:00pm on July 10th, 2017. "" +
      ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
      ""He sent a postcard to his sister Jane Smith. "" +
      ""After hearing about Joe's trip, Jane decided she might go to France one day."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);

    // list of the part-of-speech tags for the second sentence
    List<String> posTags = sentence.posTags();
    System.out.println(""Example: pos tags"");
    System.out.println(posTags);
    System.out.println();

    // list of the ner tags for the second sentence
    List<String> nerTags = sentence.nerTags();
    System.out.println(""Example: ner tags"");
    System.out.println(nerTags);
    System.out.println();

    // constituency parse for the second sentence
    Tree constituencyParse = sentence.constituencyParse();
    System.out.println(""Example: constituency parse"");
    System.out.println(constituencyParse);
    System.out.println();

    // dependency parse for the second sentence
    SemanticGraph dependencyParse = sentence.dependencyParse();
    System.out.println(""Example: dependency parse"");
    System.out.println(dependencyParse);
    System.out.println();

    // kbp relations found in fifth sentence
    List<RelationTriple> relations =
        document.sentences().get(4).relations();
    System.out.println(""Example: relation"");
    System.out.println(relations.get(0));
    System.out.println();

    // entity mentions in the second sentence
    List<CoreEntityMention> entityMentions = sentence.entityMentions();
    System.out.println(""Example: entity mentions"");
    System.out.println(entityMentions);
    System.out.println();

    // coreference between entity mentions
    CoreEntityMention originalEntityMention = document.sentences().get(3).entityMentions().get(1);
    System.out.println(""Example: original entity mention"");
    System.out.println(originalEntityMention);
    System.out.println(""Example: canonical entity mention"");
    System.out.println(originalEntityMention.canonicalEntityMention().get());
    System.out.println();

    // get document wide coref info
    Map<Integer, CorefChain> corefChains = document.corefChains();
    System.out.println(""Example: coref chains for document"");
    System.out.println(corefChains);
    System.out.println();

    // get quotes in document
    List<CoreQuote> quotes = document.quotes();
    CoreQuote quote = quotes.get(0);
    System.out.println(""Example: quote"");
    System.out.println(quote);
    System.out.println();

    // original speaker of quote
    // note that quote.speaker() returns an Optional
    System.out.println(""Example: original speaker of quote"");
    System.out.println(quote.speaker().get());
    System.out.println();

    // canonical speaker of quote
    System.out.println(""Example: canonical speaker of quote"");
    System.out.println(quote.canonicalSpeaker().get());
    System.out.println();

  }

}","('quote.speaker', 'quote.speaker()')","('CoreQuote.speaker', '[]')",CoreNLP\src\edu\stanford\nlp\pipeline\CoreQuote.java,True
StanfordCoreNLP(Properties props),"('StanfordCoreNLP', 'StanfordCoreNLP(Properties props)')","('StanfordCoreNLP', ""[['props'], ['props', 'enforceRequirements'], ['propsFileNamePrefix'], ['propsFileNamePrefix', 'enforceRequirements'], ['props', 'enforceRequirements', 'annotatorPool']]"")",CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
annotate(Annotation document),annotate,N/A,N/A,N/A
"import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class BasicPipelineExample {

    public static void main(String[] args) {

        // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

        // read some text in the text variable
        String text = ""..."";

        // create an empty Annotation just with the given text
        Annotation document = new Annotation(text);

        // run all Annotators on this text
        pipeline.annotate(document);

    }

}",main,"BasicPipelineExample.main,StanfordCoreNLP.main","CoreNLP\src\edu\stanford\nlp\examples\BasicPipelineExample.java
CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java",False
"import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class BasicPipelineExample {

    public static void main(String[] args) {

        // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

        // read some text in the text variable
        String text = ""..."";

        // create an empty Annotation just with the given text
        Annotation document = new Annotation(text);

        // run all Annotators on this text
        pipeline.annotate(document);

    }

}",Properties,N/A,N/A,N/A
"import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class BasicPipelineExample {

    public static void main(String[] args) {

        // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

        // read some text in the text variable
        String text = ""..."";

        // create an empty Annotation just with the given text
        Annotation document = new Annotation(text);

        // run all Annotators on this text
        pipeline.annotate(document);

    }

}","('StanfordCoreNLP', 'StanfordCoreNLP(props)')","('StanfordCoreNLP', ""[['props'], ['props', 'enforceRequirements'], ['propsFileNamePrefix'], ['propsFileNamePrefix', 'enforceRequirements'], ['props', 'enforceRequirements', 'annotatorPool']]"")",CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class BasicPipelineExample {

    public static void main(String[] args) {

        // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

        // read some text in the text variable
        String text = ""..."";

        // create an empty Annotation just with the given text
        Annotation document = new Annotation(text);

        // run all Annotators on this text
        pipeline.annotate(document);

    }

}","('Annotation', 'Annotation(text)')","('Annotation', ""['map', ['text'], ['sentences'], []]"")",CoreNLP\src\edu\stanford\nlp\pipeline\Annotation.java,False
PropertiesUtils.asProperties(String ...),"('PropertiesUtils.asProperties', 'PropertiesUtils.asProperties(String ...)')","('PropertiesUtils.asProperties', ""['args']"")",CoreNLP\src\edu\stanford\nlp\util\PropertiesUtils.java,False
"// build pipeline
StanfordCoreNLP pipeline = new StanfordCoreNLP(
	PropertiesUtils.asProperties(
		""annotators"", ""tokenize,ssplit,pos,lemma,parse,natlog"",
		""ssplit.isOneSentence"", ""true"",
		""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"",
		""tokenize.language"", ""en""));

// read some text in the text variable
String text = ... // Add your text here!
Annotation document = new Annotation(text);

// run all Annotators on this text
pipeline.annotate(document);","('Annotation', 'Annotation(text)')","('Annotation', ""['map', ['text'], ['sentences'], []]"")",CoreNLP\src\edu\stanford\nlp\pipeline\Annotation.java,False
"// build pipeline
StanfordCoreNLP pipeline = new StanfordCoreNLP(
	PropertiesUtils.asProperties(
		""annotators"", ""tokenize,ssplit,pos,lemma,parse,natlog"",
		""ssplit.isOneSentence"", ""true"",
		""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"",
		""tokenize.language"", ""en""));

// read some text in the text variable
String text = ... // Add your text here!
Annotation document = new Annotation(text);

// run all Annotators on this text
pipeline.annotate(document);",pipeline.annotate,"StanfordCoreNLP.annotate,Util.annotate","CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java
CoreNLP\src\edu\stanford\nlp\naturalli\Util.java",False
"// these are all the sentences in this document
// a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
List<CoreMap> sentences = document.get(SentencesAnnotation.class);

for(CoreMap sentence: sentences) {
  // traversing the words in the current sentence
  // a CoreLabel is a CoreMap with additional token-specific methods
  for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
    // this is the text of the token
    String word = token.get(TextAnnotation.class);
    // this is the POS tag of the token
    String pos = token.get(PartOfSpeechAnnotation.class);
    // this is the NER label of the token
    String ne = token.get(NamedEntityTagAnnotation.class);
  }

  // this is the parse tree of the current sentence
  Tree tree = sentence.get(TreeAnnotation.class);

  // this is the Stanford dependency graph of the current sentence
  SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
}

// This is the coreference link graph
// Each chain stores a set of mentions that link to each other,
// along with a method for getting the most representative mention
// Both sentence and token offsets start at 1!
Map<Integer, CorefChain> graph = 
  document.get(CorefChainAnnotation.class);",document.get,N/A,N/A,N/A
"import edu.stanford.nlp.simple.*;

Sentence sent = new Sentence(""Lucy is in the sky with diamonds."");
List<String> nerTags = sent.nerTags();  // [PERSON, O, O, O, O, O, O, O]
String firstPOSTag = sent.posTag(0);   // NNP
...","('Sentence', 'Sentence(""Lucy is in the sky with diamonds."")')","('Sentence', ""['doc', 'props', ['text', 'props'], ['text'], ['doc', 'tokens', 'props'], ['tokens'], ['docFn', 'proto', 'props'], ['proto'], ['doc', 'sentenceIndex'], ['doc', 'proto', 'defaultProps'], ['doc', 'proto', 'text', 'defaultProps'], ['doc', 'sentence'], ['sentence'], ['sentence']]"")",CoreNLP\src\edu\stanford\nlp\simple\Sentence.java,False
"import edu.stanford.nlp.simple.*;

public class SimpleCoreNLPDemo {
    public static void main(String[] args) { 
        // Create a document. No computation is done yet.
        Document doc = new Document(""add your text here! It can contain multiple sentences."");
        for (Sentence sent : doc.sentences()) {  // Will iterate over two sentences
            // We're only asking for words -- no need to load any models yet
            System.out.println(""The second word of the sentence '"" + sent + ""' is "" + sent.word(1));
            // When we ask for the lemma, it will load and run the part of speech tagger
            System.out.println(""The third lemma of the sentence '"" + sent + ""' is "" + sent.lemma(2));
            // When we ask for the parse, it will load and run the parser
            System.out.println(""The parse of the sentence '"" + sent + ""' is "" + sent.parse());
            // ...
        }
    }
}",main,N/A,N/A,N/A
"import edu.stanford.nlp.simple.*;

public class SimpleCoreNLPDemo {
    public static void main(String[] args) { 
        // Create a document. No computation is done yet.
        Document doc = new Document(""add your text here! It can contain multiple sentences."");
        for (Sentence sent : doc.sentences()) {  // Will iterate over two sentences
            // We're only asking for words -- no need to load any models yet
            System.out.println(""The second word of the sentence '"" + sent + ""' is "" + sent.word(1));
            // When we ask for the lemma, it will load and run the part of speech tagger
            System.out.println(""The third lemma of the sentence '"" + sent + ""' is "" + sent.lemma(2));
            // When we ask for the parse, it will load and run the parser
            System.out.println(""The parse of the sentence '"" + sent + ""' is "" + sent.parse());
            // ...
        }
    }
}","('doc.sentences', 'doc.sentences()')","('Document.sentences', ""['props', []]"")",CoreNLP\src\edu\stanford\nlp\simple\Document.java,False
"import edu.stanford.nlp.simple.*;

public class SimpleCoreNLPDemo {
    public static void main(String[] args) { 
        // Create a document. No computation is done yet.
        Document doc = new Document(""add your text here! It can contain multiple sentences."");
        for (Sentence sent : doc.sentences()) {  // Will iterate over two sentences
            // We're only asking for words -- no need to load any models yet
            System.out.println(""The second word of the sentence '"" + sent + ""' is "" + sent.word(1));
            // When we ask for the lemma, it will load and run the part of speech tagger
            System.out.println(""The third lemma of the sentence '"" + sent + ""' is "" + sent.lemma(2));
            // When we ask for the parse, it will load and run the parser
            System.out.println(""The parse of the sentence '"" + sent + ""' is "" + sent.parse());
            // ...
        }
    }
}","('sent.word', 'sent.word(1)')","('Sentence.word', ""['index']"")",CoreNLP\src\edu\stanford\nlp\simple\Sentence.java,False
.words(),words,N/A,N/A,N/A
.word(int),word,N/A,N/A,N/A
.sentences(),sentences,N/A,N/A,N/A
.sentence(int),sentence,N/A,N/A,N/A
.posTags(),posTags,N/A,N/A,N/A
.posTag(int),posTag,N/A,N/A,N/A
.lemmas(),lemmas,N/A,N/A,N/A
.lemma(int),lemma,N/A,N/A,N/A
.nerTags(),nerTags,N/A,N/A,N/A
.nerTag(int),nerTag,N/A,N/A,N/A
.parse(),parse,N/A,N/A,N/A
.governor(int),governor,N/A,N/A,N/A
.incomingDependencyLabel(int),incomingDependencyLabel,N/A,N/A,N/A
.coref(),coref,N/A,N/A,N/A
.natlogPolarities(),natlogPolarities,N/A,N/A,N/A
natlogPolarity(int),natlogPolarity,N/A,N/A,N/A
.openie(),openie,N/A,N/A,N/A
.openieTriples(),openieTriples,N/A,N/A,N/A
"Sentence sent = new Sentence(""your text should go here"");
sent.algorithms().headOfSpan(new Span(0, 2));  // Should return 1","('Sentence', 'Sentence(""your text should go here"")')","('Sentence', ""['doc', 'props', ['text', 'props'], ['text'], ['doc', 'tokens', 'props'], ['tokens'], ['docFn', 'proto', 'props'], ['proto'], ['doc', 'sentenceIndex'], ['doc', 'proto', 'defaultProps'], ['doc', 'proto', 'text', 'defaultProps'], ['doc', 'sentence'], ['sentence'], ['sentence']]"")",CoreNLP\src\edu\stanford\nlp\simple\Sentence.java,False
"Sentence sent = new Sentence(""your text should go here"");
sent.algorithms().headOfSpan(new Span(0, 2));  // Should return 1",headOfSpan,N/A,N/A,N/A
"Sentence sent = new Sentence(""your text should go here"");
sent.algorithms().headOfSpan(new Span(0, 2));  // Should return 1","('Span', 'Span(new Span(0,2)')","('Span', ""[['s', 'e'], ['spans'], [], ['start', 'end']]"")",CoreNLP\src\edu\stanford\nlp\ie\machinereading\structure\Span.java,False
headOfSpan(Span),headOfSpan,N/A,N/A,N/A
"dependencyPathBetween(int, int)",dependencyPathBetween,N/A,N/A,N/A
"import requests
print(requests.post('http://[::]:9000/?properties={""annotators"":""tokenize,ssplit,pos"",""outputFormat"":""json""}', data = {'data':'The quick brown fox jumped over the lazy dog.'}).text)",print,N/A,N/A,N/A
"// creates a StanfordCoreNLP object with POS tagging, lemmatization, NER, parsing, and coreference resolution
Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
StanfordCoreNLPClient pipeline = new StanfordCoreNLPClient(props, ""http://localhost"", 9000, 2);
// read some text in the text variable
String text = ... // Add your text here!
// create an empty Annotation just with the given text
Annotation document = new Annotation(text);
// run all Annotators on this text
pipeline.annotate(document);",Properties,N/A,N/A,N/A
"// creates a StanfordCoreNLP object with POS tagging, lemmatization, NER, parsing, and coreference resolution
Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
StanfordCoreNLPClient pipeline = new StanfordCoreNLPClient(props, ""http://localhost"", 9000, 2);
// read some text in the text variable
String text = ... // Add your text here!
// create an empty Annotation just with the given text
Annotation document = new Annotation(text);
// run all Annotators on this text
pipeline.annotate(document);","('StanfordCoreNLPClient', 'StanfordCoreNLPClient(props,""http://localhost"",9000,2)')","('StanfordCoreNLPClient', ""['properties', 'backends', 'apiKey', 'apiSecret', ['properties', 'backends'], ['properties'], ['properties', 'host', 'port'], ['properties', 'host', 'port', 'apiKey', 'apiSecret'], ['properties', 'host', 'apiKey', 'apiSecret'], ['properties', 'host', 'port', 'threads'], ['properties', 'host', 'port', 'threads', 'apiKey', 'apiSecret']]"")",CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLPClient.java,False
"// creates a StanfordCoreNLP object with POS tagging, lemmatization, NER, parsing, and coreference resolution
Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
StanfordCoreNLPClient pipeline = new StanfordCoreNLPClient(props, ""http://localhost"", 9000, 2);
// read some text in the text variable
String text = ... // Add your text here!
// create an empty Annotation just with the given text
Annotation document = new Annotation(text);
// run all Annotators on this text
pipeline.annotate(document);","('Annotation', 'Annotation(text)')","('Annotation', ""['map', ['text'], ['sentences'], []]"")",CoreNLP\src\edu\stanford\nlp\pipeline\Annotation.java,False
"// creates a StanfordCoreNLP object with POS tagging, lemmatization, NER, parsing, and coreference resolution
Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
StanfordCoreNLPClient pipeline = new StanfordCoreNLPClient(props, ""http://localhost"", 9000, 2);
// read some text in the text variable
String text = ... // Add your text here!
// create an empty Annotation just with the given text
Annotation document = new Annotation(text);
// run all Annotators on this text
pipeline.annotate(document);",pipeline.annotate,"StanfordCoreNLP.annotate,StanfordCoreNLPClient.annotate","CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java
CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLPClient.java",False
"System.getProperty(""java.io.tmpdir"");",System.getProperty,N/A,N/A,N/A
"String text = ""La Universidad de Stanford se encuentra en Palo Alto."";
StanfordCoreNLP pipeline = new StanfordCoreNLP(""spanish"");
CoreDocument doc = pipeline.processToCoreDocument(text);","('StanfordCoreNLP', 'StanfordCoreNLP(""spanish"")')","('StanfordCoreNLP', ""[['props'], ['props', 'enforceRequirements'], ['propsFileNamePrefix'], ['propsFileNamePrefix', 'enforceRequirements'], ['props', 'enforceRequirements', 'annotatorPool']]"")",CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;


public class PipelineExample {

    public static String text = ""Marie was born in Paris."";

    public static void main(String[] args) {
        // set up pipeline properties
        Properties props = new Properties();
        // set the list of annotators to run
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,depparse"");
        // build pipeline
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        // create a document object
        CoreDocument document = pipeline.processToCoreDocument(text);
    }

}","('main', 'main(String[] args)')","('StanfordCoreNLP.main', ""['args']"")",CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;


public class PipelineExample {

    public static String text = ""Marie was born in Paris."";

    public static void main(String[] args) {
        // set up pipeline properties
        Properties props = new Properties();
        // set the list of annotators to run
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,depparse"");
        // build pipeline
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        // create a document object
        CoreDocument document = pipeline.processToCoreDocument(text);
    }

}",Properties,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.util.ArraySet;

import java.util.*;


public class CustomLemmaAnnotator implements Annotator {

  HashMap<String,String> wordToLemma = new HashMap<String,String>();

  public CustomLemmaAnnotator(String name, Properties props) {
    // load the lemma file
    // format should be tsv with word and lemma
    String lemmaFile = props.getProperty(""custom.lemma.lemmaFile"");
    List<String> lemmaEntries = IOUtils.linesFromFile(lemmaFile);
    for (String lemmaEntry : lemmaEntries) {
      wordToLemma.put(lemmaEntry.split(""\\t"")[0], lemmaEntry.split(""\\t"")[1]);
    }
  }

  public void annotate(Annotation annotation) {
    for (CoreLabel token : annotation.get(CoreAnnotations.TokensAnnotation.class)) {
      String lemma = wordToLemma.getOrDefault(token.word(), token.word());
      token.set(CoreAnnotations.LemmaAnnotation.class, lemma);
    }
  }

  @Override
  public Set<Class extends CoreAnnotation>> requires() {
    return Collections.unmodifiableSet(new ArraySet<>(Arrays.asList(
        CoreAnnotations.TextAnnotation.class,
        CoreAnnotations.TokensAnnotation.class,
        CoreAnnotations.SentencesAnnotation.class,
        CoreAnnotations.PartOfSpeechAnnotation.class
    )));
  }

  @Override
  public Set<Class extends CoreAnnotation>> requirementsSatisfied() {
    return Collections.singleton(CoreAnnotations.LemmaAnnotation.class);
  }
  
}",CustomLemmaAnnotator,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.util.ArraySet;

import java.util.*;


public class CustomLemmaAnnotator implements Annotator {

  HashMap<String,String> wordToLemma = new HashMap<String,String>();

  public CustomLemmaAnnotator(String name, Properties props) {
    // load the lemma file
    // format should be tsv with word and lemma
    String lemmaFile = props.getProperty(""custom.lemma.lemmaFile"");
    List<String> lemmaEntries = IOUtils.linesFromFile(lemmaFile);
    for (String lemmaEntry : lemmaEntries) {
      wordToLemma.put(lemmaEntry.split(""\\t"")[0], lemmaEntry.split(""\\t"")[1]);
    }
  }

  public void annotate(Annotation annotation) {
    for (CoreLabel token : annotation.get(CoreAnnotations.TokensAnnotation.class)) {
      String lemma = wordToLemma.getOrDefault(token.word(), token.word());
      token.set(CoreAnnotations.LemmaAnnotation.class, lemma);
    }
  }

  @Override
  public Set<Class extends CoreAnnotation>> requires() {
    return Collections.unmodifiableSet(new ArraySet<>(Arrays.asList(
        CoreAnnotations.TextAnnotation.class,
        CoreAnnotations.TokensAnnotation.class,
        CoreAnnotations.SentencesAnnotation.class,
        CoreAnnotations.PartOfSpeechAnnotation.class
    )));
  }

  @Override
  public Set<Class extends CoreAnnotation>> requirementsSatisfied() {
    return Collections.singleton(CoreAnnotations.LemmaAnnotation.class);
  }
  
}","('IOUtils.linesFromFile', 'IOUtils.linesFromFile(lemmaFile)')","('IOUtils.linesFromFile', ""['filename', ['filename', 'encoding'], ['filename', 'encoding', 'ignoreHeader']]"")",CoreNLP\src\edu\stanford\nlp\io\IOUtils.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.util.ArraySet;

import java.util.*;


public class CustomLemmaAnnotator implements Annotator {

  HashMap<String,String> wordToLemma = new HashMap<String,String>();

  public CustomLemmaAnnotator(String name, Properties props) {
    // load the lemma file
    // format should be tsv with word and lemma
    String lemmaFile = props.getProperty(""custom.lemma.lemmaFile"");
    List<String> lemmaEntries = IOUtils.linesFromFile(lemmaFile);
    for (String lemmaEntry : lemmaEntries) {
      wordToLemma.put(lemmaEntry.split(""\\t"")[0], lemmaEntry.split(""\\t"")[1]);
    }
  }

  public void annotate(Annotation annotation) {
    for (CoreLabel token : annotation.get(CoreAnnotations.TokensAnnotation.class)) {
      String lemma = wordToLemma.getOrDefault(token.word(), token.word());
      token.set(CoreAnnotations.LemmaAnnotation.class, lemma);
    }
  }

  @Override
  public Set<Class extends CoreAnnotation>> requires() {
    return Collections.unmodifiableSet(new ArraySet<>(Arrays.asList(
        CoreAnnotations.TextAnnotation.class,
        CoreAnnotations.TokensAnnotation.class,
        CoreAnnotations.SentencesAnnotation.class,
        CoreAnnotations.PartOfSpeechAnnotation.class
    )));
  }

  @Override
  public Set<Class extends CoreAnnotation>> requirementsSatisfied() {
    return Collections.singleton(CoreAnnotations.LemmaAnnotation.class);
  }
  
}","('annotate', 'annotate(Annotation annotation)')","('Util.annotate', ""['sentence', 'pipeline']"")",CoreNLP\src\edu\stanford\nlp\naturalli\Util.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.util.ArraySet;

import java.util.*;


public class CustomLemmaAnnotator implements Annotator {

  HashMap<String,String> wordToLemma = new HashMap<String,String>();

  public CustomLemmaAnnotator(String name, Properties props) {
    // load the lemma file
    // format should be tsv with word and lemma
    String lemmaFile = props.getProperty(""custom.lemma.lemmaFile"");
    List<String> lemmaEntries = IOUtils.linesFromFile(lemmaFile);
    for (String lemmaEntry : lemmaEntries) {
      wordToLemma.put(lemmaEntry.split(""\\t"")[0], lemmaEntry.split(""\\t"")[1]);
    }
  }

  public void annotate(Annotation annotation) {
    for (CoreLabel token : annotation.get(CoreAnnotations.TokensAnnotation.class)) {
      String lemma = wordToLemma.getOrDefault(token.word(), token.word());
      token.set(CoreAnnotations.LemmaAnnotation.class, lemma);
    }
  }

  @Override
  public Set<Class extends CoreAnnotation>> requires() {
    return Collections.unmodifiableSet(new ArraySet<>(Arrays.asList(
        CoreAnnotations.TextAnnotation.class,
        CoreAnnotations.TokensAnnotation.class,
        CoreAnnotations.SentencesAnnotation.class,
        CoreAnnotations.PartOfSpeechAnnotation.class
    )));
  }

  @Override
  public Set<Class extends CoreAnnotation>> requirementsSatisfied() {
    return Collections.singleton(CoreAnnotations.LemmaAnnotation.class);
  }
  
}",token.word,"CoreLabel.word,Sentence.word,Token.word","CoreNLP\src\edu\stanford\nlp\ling\CoreLabel.java
CoreNLP\src\edu\stanford\nlp\simple\Sentence.java
CoreNLP\src\edu\stanford\nlp\simple\Token.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class PipelineExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize"");
    // example of how to customize the PTBTokenizer (these are just random example settings!!)
    props.setProperty(""tokenize.options"", ""splitHyphenated=false,americanize=false"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s\t%d\t%d"", tok.word(), tok.beginPosition(), tok.endPosition()));
    }
  }
}",main,"PTBTokenizer.main,StanfordCoreNLP.main","CoreNLP\src\edu\stanford\nlp\process\PTBTokenizer.java
CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class PipelineExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize"");
    // example of how to customize the PTBTokenizer (these are just random example settings!!)
    props.setProperty(""tokenize.options"", ""splitHyphenated=false,americanize=false"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s\t%d\t%d"", tok.word(), tok.beginPosition(), tok.endPosition()));
    }
  }
}",Properties,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class PipelineExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize"");
    // example of how to customize the PTBTokenizer (these are just random example settings!!)
    props.setProperty(""tokenize.options"", ""splitHyphenated=false,americanize=false"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s\t%d\t%d"", tok.word(), tok.beginPosition(), tok.endPosition()));
    }
  }
}","('StanfordCoreNLP', 'StanfordCoreNLP(props)')","('StanfordCoreNLP', ""[['props'], ['props', 'enforceRequirements'], ['propsFileNamePrefix'], ['propsFileNamePrefix', 'enforceRequirements'], ['props', 'enforceRequirements', 'annotatorPool']]"")",CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class PipelineExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize"");
    // example of how to customize the PTBTokenizer (these are just random example settings!!)
    props.setProperty(""tokenize.options"", ""splitHyphenated=false,americanize=false"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s\t%d\t%d"", tok.word(), tok.beginPosition(), tok.endPosition()));
    }
  }
}","('CoreDocument', 'CoreDocument(text)')","('CoreDocument', ""['documentText', ['annotation']]"")",CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class PipelineExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize"");
    // example of how to customize the PTBTokenizer (these are just random example settings!!)
    props.setProperty(""tokenize.options"", ""splitHyphenated=false,americanize=false"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s\t%d\t%d"", tok.word(), tok.beginPosition(), tok.endPosition()));
    }
  }
}",tok.word,"CoreLabel.word,Token.word","CoreNLP\src\edu\stanford\nlp\ling\CoreLabel.java
CoreNLP\src\edu\stanford\nlp\simple\Token.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class PipelineExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize"");
    // example of how to customize the PTBTokenizer (these are just random example settings!!)
    props.setProperty(""tokenize.options"", ""splitHyphenated=false,americanize=false"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s\t%d\t%d"", tok.word(), tok.beginPosition(), tok.endPosition()));
    }
  }
}",tok.beginPosition,"Token.beginPosition,CoreLabel.beginPosition","CoreNLP\src\edu\stanford\nlp\simple\Token.java
CoreNLP\src\edu\stanford\nlp\ling\CoreLabel.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;

import java.util.*;
import java.util.stream.Collectors;


public class ForumPostExample {

  public static void main(String args[]) {
    // properties
    Properties props = StringUtils.argsToProperties(""-props"", args[0]);
    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // set up document
    Annotation testDocument = new Annotation(IOUtils.stringFromFile(args[1]));
    // annotate document
    pipeline.annotate(testDocument);
    // print out the forum posts
    for (CoreMap discussionForumPost : testDocument.get(CoreAnnotations.SectionsAnnotation.class)) {
      System.err.println(""---"");
      System.err.println(""author: "" + discussionForumPost.get(CoreAnnotations.AuthorAnnotation.class));
      System.err.println(""date: "" +
          discussionForumPost.get(CoreAnnotations.SectionDateAnnotation.class));
      System.err.println(""char begin: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
      System.err.println(""char end: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
      System.err.println(""author start offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetBeginAnnotation.class));
      System.err.println(""author end offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetEndAnnotation.class));
      System.err.println(""section start tag: "" +
          discussionForumPost.get(CoreAnnotations.SectionTagAnnotation.class));
      // print out the sentences
      System.err.println(""sentences: "");
      for (CoreMap sentence : discussionForumPost.get(CoreAnnotations.SentencesAnnotation.class)) {
        System.err.println(""\t***"");
        boolean sentenceQuoted = (sentence.get(CoreAnnotations.QuotedAnnotation.class) != null) &&
            sentence.get(CoreAnnotations.QuotedAnnotation.class);
        String sentenceAuthor = sentence.get(CoreAnnotations.AuthorAnnotation.class);
        String potentialQuoteText = sentenceQuoted ? ""(QUOTING: ""+sentenceAuthor+"")"" : """" ;
        System.err.println(""\t"" + potentialQuoteText + "" "" +
            sentence.get(CoreAnnotations.TokensAnnotation.class).stream().
            map(token -> token.word()).collect(Collectors.joining("" "")));
      }
    }
  }
}",main,"StanfordCoreNLP.main,StringUtils.main","CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java
CoreNLP\src\edu\stanford\nlp\util\StringUtils.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;

import java.util.*;
import java.util.stream.Collectors;


public class ForumPostExample {

  public static void main(String args[]) {
    // properties
    Properties props = StringUtils.argsToProperties(""-props"", args[0]);
    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // set up document
    Annotation testDocument = new Annotation(IOUtils.stringFromFile(args[1]));
    // annotate document
    pipeline.annotate(testDocument);
    // print out the forum posts
    for (CoreMap discussionForumPost : testDocument.get(CoreAnnotations.SectionsAnnotation.class)) {
      System.err.println(""---"");
      System.err.println(""author: "" + discussionForumPost.get(CoreAnnotations.AuthorAnnotation.class));
      System.err.println(""date: "" +
          discussionForumPost.get(CoreAnnotations.SectionDateAnnotation.class));
      System.err.println(""char begin: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
      System.err.println(""char end: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
      System.err.println(""author start offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetBeginAnnotation.class));
      System.err.println(""author end offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetEndAnnotation.class));
      System.err.println(""section start tag: "" +
          discussionForumPost.get(CoreAnnotations.SectionTagAnnotation.class));
      // print out the sentences
      System.err.println(""sentences: "");
      for (CoreMap sentence : discussionForumPost.get(CoreAnnotations.SentencesAnnotation.class)) {
        System.err.println(""\t***"");
        boolean sentenceQuoted = (sentence.get(CoreAnnotations.QuotedAnnotation.class) != null) &&
            sentence.get(CoreAnnotations.QuotedAnnotation.class);
        String sentenceAuthor = sentence.get(CoreAnnotations.AuthorAnnotation.class);
        String potentialQuoteText = sentenceQuoted ? ""(QUOTING: ""+sentenceAuthor+"")"" : """" ;
        System.err.println(""\t"" + potentialQuoteText + "" "" +
            sentence.get(CoreAnnotations.TokensAnnotation.class).stream().
            map(token -> token.word()).collect(Collectors.joining("" "")));
      }
    }
  }
}","('StanfordCoreNLP', 'StanfordCoreNLP(props)')","('StanfordCoreNLP', ""[['props'], ['props', 'enforceRequirements'], ['propsFileNamePrefix'], ['propsFileNamePrefix', 'enforceRequirements'], ['props', 'enforceRequirements', 'annotatorPool']]"")",CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;

import java.util.*;
import java.util.stream.Collectors;


public class ForumPostExample {

  public static void main(String args[]) {
    // properties
    Properties props = StringUtils.argsToProperties(""-props"", args[0]);
    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // set up document
    Annotation testDocument = new Annotation(IOUtils.stringFromFile(args[1]));
    // annotate document
    pipeline.annotate(testDocument);
    // print out the forum posts
    for (CoreMap discussionForumPost : testDocument.get(CoreAnnotations.SectionsAnnotation.class)) {
      System.err.println(""---"");
      System.err.println(""author: "" + discussionForumPost.get(CoreAnnotations.AuthorAnnotation.class));
      System.err.println(""date: "" +
          discussionForumPost.get(CoreAnnotations.SectionDateAnnotation.class));
      System.err.println(""char begin: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
      System.err.println(""char end: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
      System.err.println(""author start offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetBeginAnnotation.class));
      System.err.println(""author end offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetEndAnnotation.class));
      System.err.println(""section start tag: "" +
          discussionForumPost.get(CoreAnnotations.SectionTagAnnotation.class));
      // print out the sentences
      System.err.println(""sentences: "");
      for (CoreMap sentence : discussionForumPost.get(CoreAnnotations.SentencesAnnotation.class)) {
        System.err.println(""\t***"");
        boolean sentenceQuoted = (sentence.get(CoreAnnotations.QuotedAnnotation.class) != null) &&
            sentence.get(CoreAnnotations.QuotedAnnotation.class);
        String sentenceAuthor = sentence.get(CoreAnnotations.AuthorAnnotation.class);
        String potentialQuoteText = sentenceQuoted ? ""(QUOTING: ""+sentenceAuthor+"")"" : """" ;
        System.err.println(""\t"" + potentialQuoteText + "" "" +
            sentence.get(CoreAnnotations.TokensAnnotation.class).stream().
            map(token -> token.word()).collect(Collectors.joining("" "")));
      }
    }
  }
}","('Annotation', 'Annotation(IOUtils.stringFromFile(args[1])')","('Annotation', ""['map', ['text'], ['sentences'], []]"")",CoreNLP\src\edu\stanford\nlp\pipeline\Annotation.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;

import java.util.*;
import java.util.stream.Collectors;


public class ForumPostExample {

  public static void main(String args[]) {
    // properties
    Properties props = StringUtils.argsToProperties(""-props"", args[0]);
    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // set up document
    Annotation testDocument = new Annotation(IOUtils.stringFromFile(args[1]));
    // annotate document
    pipeline.annotate(testDocument);
    // print out the forum posts
    for (CoreMap discussionForumPost : testDocument.get(CoreAnnotations.SectionsAnnotation.class)) {
      System.err.println(""---"");
      System.err.println(""author: "" + discussionForumPost.get(CoreAnnotations.AuthorAnnotation.class));
      System.err.println(""date: "" +
          discussionForumPost.get(CoreAnnotations.SectionDateAnnotation.class));
      System.err.println(""char begin: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
      System.err.println(""char end: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
      System.err.println(""author start offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetBeginAnnotation.class));
      System.err.println(""author end offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetEndAnnotation.class));
      System.err.println(""section start tag: "" +
          discussionForumPost.get(CoreAnnotations.SectionTagAnnotation.class));
      // print out the sentences
      System.err.println(""sentences: "");
      for (CoreMap sentence : discussionForumPost.get(CoreAnnotations.SentencesAnnotation.class)) {
        System.err.println(""\t***"");
        boolean sentenceQuoted = (sentence.get(CoreAnnotations.QuotedAnnotation.class) != null) &&
            sentence.get(CoreAnnotations.QuotedAnnotation.class);
        String sentenceAuthor = sentence.get(CoreAnnotations.AuthorAnnotation.class);
        String potentialQuoteText = sentenceQuoted ? ""(QUOTING: ""+sentenceAuthor+"")"" : """" ;
        System.err.println(""\t"" + potentialQuoteText + "" "" +
            sentence.get(CoreAnnotations.TokensAnnotation.class).stream().
            map(token -> token.word()).collect(Collectors.joining("" "")));
      }
    }
  }
}","('IOUtils.stringFromFile', 'IOUtils.stringFromFile(args[1])')","('IOUtils.stringFromFile', ""['filename', ['filename', 'encoding']]"")",CoreNLP\src\edu\stanford\nlp\io\IOUtils.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;

import java.util.*;
import java.util.stream.Collectors;


public class ForumPostExample {

  public static void main(String args[]) {
    // properties
    Properties props = StringUtils.argsToProperties(""-props"", args[0]);
    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // set up document
    Annotation testDocument = new Annotation(IOUtils.stringFromFile(args[1]));
    // annotate document
    pipeline.annotate(testDocument);
    // print out the forum posts
    for (CoreMap discussionForumPost : testDocument.get(CoreAnnotations.SectionsAnnotation.class)) {
      System.err.println(""---"");
      System.err.println(""author: "" + discussionForumPost.get(CoreAnnotations.AuthorAnnotation.class));
      System.err.println(""date: "" +
          discussionForumPost.get(CoreAnnotations.SectionDateAnnotation.class));
      System.err.println(""char begin: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
      System.err.println(""char end: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
      System.err.println(""author start offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetBeginAnnotation.class));
      System.err.println(""author end offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetEndAnnotation.class));
      System.err.println(""section start tag: "" +
          discussionForumPost.get(CoreAnnotations.SectionTagAnnotation.class));
      // print out the sentences
      System.err.println(""sentences: "");
      for (CoreMap sentence : discussionForumPost.get(CoreAnnotations.SentencesAnnotation.class)) {
        System.err.println(""\t***"");
        boolean sentenceQuoted = (sentence.get(CoreAnnotations.QuotedAnnotation.class) != null) &&
            sentence.get(CoreAnnotations.QuotedAnnotation.class);
        String sentenceAuthor = sentence.get(CoreAnnotations.AuthorAnnotation.class);
        String potentialQuoteText = sentenceQuoted ? ""(QUOTING: ""+sentenceAuthor+"")"" : """" ;
        System.err.println(""\t"" + potentialQuoteText + "" "" +
            sentence.get(CoreAnnotations.TokensAnnotation.class).stream().
            map(token -> token.word()).collect(Collectors.joining("" "")));
      }
    }
  }
}",pipeline.annotate,"StanfordCoreNLP.annotate,Util.annotate","CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java
CoreNLP\src\edu\stanford\nlp\naturalli\Util.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;

import java.util.*;
import java.util.stream.Collectors;


public class ForumPostExample {

  public static void main(String args[]) {
    // properties
    Properties props = StringUtils.argsToProperties(""-props"", args[0]);
    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // set up document
    Annotation testDocument = new Annotation(IOUtils.stringFromFile(args[1]));
    // annotate document
    pipeline.annotate(testDocument);
    // print out the forum posts
    for (CoreMap discussionForumPost : testDocument.get(CoreAnnotations.SectionsAnnotation.class)) {
      System.err.println(""---"");
      System.err.println(""author: "" + discussionForumPost.get(CoreAnnotations.AuthorAnnotation.class));
      System.err.println(""date: "" +
          discussionForumPost.get(CoreAnnotations.SectionDateAnnotation.class));
      System.err.println(""char begin: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
      System.err.println(""char end: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
      System.err.println(""author start offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetBeginAnnotation.class));
      System.err.println(""author end offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetEndAnnotation.class));
      System.err.println(""section start tag: "" +
          discussionForumPost.get(CoreAnnotations.SectionTagAnnotation.class));
      // print out the sentences
      System.err.println(""sentences: "");
      for (CoreMap sentence : discussionForumPost.get(CoreAnnotations.SentencesAnnotation.class)) {
        System.err.println(""\t***"");
        boolean sentenceQuoted = (sentence.get(CoreAnnotations.QuotedAnnotation.class) != null) &&
            sentence.get(CoreAnnotations.QuotedAnnotation.class);
        String sentenceAuthor = sentence.get(CoreAnnotations.AuthorAnnotation.class);
        String potentialQuoteText = sentenceQuoted ? ""(QUOTING: ""+sentenceAuthor+"")"" : """" ;
        System.err.println(""\t"" + potentialQuoteText + "" "" +
            sentence.get(CoreAnnotations.TokensAnnotation.class).stream().
            map(token -> token.word()).collect(Collectors.joining("" "")));
      }
    }
  }
}",testDocument.get,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;

import java.util.*;
import java.util.stream.Collectors;


public class ForumPostExample {

  public static void main(String args[]) {
    // properties
    Properties props = StringUtils.argsToProperties(""-props"", args[0]);
    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // set up document
    Annotation testDocument = new Annotation(IOUtils.stringFromFile(args[1]));
    // annotate document
    pipeline.annotate(testDocument);
    // print out the forum posts
    for (CoreMap discussionForumPost : testDocument.get(CoreAnnotations.SectionsAnnotation.class)) {
      System.err.println(""---"");
      System.err.println(""author: "" + discussionForumPost.get(CoreAnnotations.AuthorAnnotation.class));
      System.err.println(""date: "" +
          discussionForumPost.get(CoreAnnotations.SectionDateAnnotation.class));
      System.err.println(""char begin: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
      System.err.println(""char end: "" +
          discussionForumPost.get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
      System.err.println(""author start offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetBeginAnnotation.class));
      System.err.println(""author end offset: "" +
          discussionForumPost.get(CoreAnnotations.SectionAuthorCharacterOffsetEndAnnotation.class));
      System.err.println(""section start tag: "" +
          discussionForumPost.get(CoreAnnotations.SectionTagAnnotation.class));
      // print out the sentences
      System.err.println(""sentences: "");
      for (CoreMap sentence : discussionForumPost.get(CoreAnnotations.SentencesAnnotation.class)) {
        System.err.println(""\t***"");
        boolean sentenceQuoted = (sentence.get(CoreAnnotations.QuotedAnnotation.class) != null) &&
            sentence.get(CoreAnnotations.QuotedAnnotation.class);
        String sentenceAuthor = sentence.get(CoreAnnotations.AuthorAnnotation.class);
        String potentialQuoteText = sentenceQuoted ? ""(QUOTING: ""+sentenceAuthor+"")"" : """" ;
        System.err.println(""\t"" + potentialQuoteText + "" "" +
            sentence.get(CoreAnnotations.TokensAnnotation.class).stream().
            map(token -> token.word()).collect(Collectors.joining("" "")));
      }
    }
  }
}",token.word,"Sentence.word,Token.word","CoreNLP\src\edu\stanford\nlp\simple\Sentence.java
CoreNLP\src\edu\stanford\nlp\simple\Token.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class SentenceSplittingExample {

  public static String text = ""Hello world. Hello world again."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display sentences
    for (CoreSentence sent : doc.sentences()) {
        System.out.println(sent.text());
    }
  }
}","('main', 'main(String[] args)')","('StanfordCoreNLP.main', ""['args']"")",CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class SentenceSplittingExample {

  public static String text = ""Hello world. Hello world again."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display sentences
    for (CoreSentence sent : doc.sentences()) {
        System.out.println(sent.text());
    }
  }
}",Properties,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class SentenceSplittingExample {

  public static String text = ""Hello world. Hello world again."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display sentences
    for (CoreSentence sent : doc.sentences()) {
        System.out.println(sent.text());
    }
  }
}","('CoreDocument', 'CoreDocument(text)')","('CoreDocument', ""['documentText', ['annotation']]"")",CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class SentenceSplittingExample {

  public static String text = ""Hello world. Hello world again."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display sentences
    for (CoreSentence sent : doc.sentences()) {
        System.out.println(sent.text());
    }
  }
}",doc.sentences,"Document.sentences,CoreDocument.sentences","CoreNLP\src\edu\stanford\nlp\simple\Document.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class SentenceSplittingExample {

  public static String text = ""Hello world. Hello world again."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display sentences
    for (CoreSentence sent : doc.sentences()) {
        System.out.println(sent.text());
    }
  }
}",sent.text,"Sentence.text,Document.text,CoreSentence.text,CoreDocument.text","CoreNLP\src\edu\stanford\nlp\simple\Sentence.java
CoreNLP\src\edu\stanford\nlp\simple\Document.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreSentence.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.StringUtils;

import java.util.*;

public class MWTExpansionExample {

  public static String text = ""Pude haber querido escribirlo."";

  public static void main(String[] args) {
    // set the list of annotators to run
    Properties props = StringUtils.argsToProperties(""-props"", ""spanish"");
    props.setProperty(""annotators"", ""tokenize,ssplit,mwt"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s"", tok.word()));
    }
  }
}",main,"StanfordCoreNLP.main,StringUtils.main","CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java
CoreNLP\src\edu\stanford\nlp\util\StringUtils.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.StringUtils;

import java.util.*;

public class MWTExpansionExample {

  public static String text = ""Pude haber querido escribirlo."";

  public static void main(String[] args) {
    // set the list of annotators to run
    Properties props = StringUtils.argsToProperties(""-props"", ""spanish"");
    props.setProperty(""annotators"", ""tokenize,ssplit,mwt"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s"", tok.word()));
    }
  }
}",props.setProperty,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.StringUtils;

import java.util.*;

public class MWTExpansionExample {

  public static String text = ""Pude haber querido escribirlo."";

  public static void main(String[] args) {
    // set the list of annotators to run
    Properties props = StringUtils.argsToProperties(""-props"", ""spanish"");
    props.setProperty(""annotators"", ""tokenize,ssplit,mwt"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s"", tok.word()));
    }
  }
}","('StanfordCoreNLP', 'StanfordCoreNLP(props)')","('StanfordCoreNLP', ""[['props'], ['props', 'enforceRequirements'], ['propsFileNamePrefix'], ['propsFileNamePrefix', 'enforceRequirements'], ['props', 'enforceRequirements', 'annotatorPool']]"")",CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.StringUtils;

import java.util.*;

public class MWTExpansionExample {

  public static String text = ""Pude haber querido escribirlo."";

  public static void main(String[] args) {
    // set the list of annotators to run
    Properties props = StringUtils.argsToProperties(""-props"", ""spanish"");
    props.setProperty(""annotators"", ""tokenize,ssplit,mwt"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s"", tok.word()));
    }
  }
}","('CoreDocument', 'CoreDocument(text)')","('CoreDocument', ""['documentText', ['annotation']]"")",CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.StringUtils;

import java.util.*;

public class MWTExpansionExample {

  public static String text = ""Pude haber querido escribirlo."";

  public static void main(String[] args) {
    // set the list of annotators to run
    Properties props = StringUtils.argsToProperties(""-props"", ""spanish"");
    props.setProperty(""annotators"", ""tokenize,ssplit,mwt"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s"", tok.word()));
    }
  }
}",pipeline.annotate,"StanfordCoreNLP.annotate,Util.annotate","CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java
CoreNLP\src\edu\stanford\nlp\naturalli\Util.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.StringUtils;

import java.util.*;

public class MWTExpansionExample {

  public static String text = ""Pude haber querido escribirlo."";

  public static void main(String[] args) {
    // set the list of annotators to run
    Properties props = StringUtils.argsToProperties(""-props"", ""spanish"");
    props.setProperty(""annotators"", ""tokenize,ssplit,mwt"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument doc = new CoreDocument(text);
    // annotate
    pipeline.annotate(doc);
    // display tokens
    for (CoreLabel tok : doc.tokens()) {
      System.out.println(String.format(""%s"", tok.word()));
    }
  }
}","('tok.word', 'tok.word()')","('CoreLabel.word', '[]')",CoreNLP\src\edu\stanford\nlp\ling\CoreLabel.java,True
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class POSTaggingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.tag()));
    }
  }
}","('main', 'main(String[] args)')","('StanfordCoreNLP.main', ""['args']"")",CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class POSTaggingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.tag()));
    }
  }
}",Properties,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class POSTaggingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.tag()));
    }
  }
}","('document.tokens', 'document.tokens()')","('CoreDocument.tokens', '[]')",CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java,True
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class POSTaggingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.tag()));
    }
  }
}","('tok.word', 'tok.word()')","('CoreLabel.word', '[]')",CoreNLP\src\edu\stanford\nlp\ling\CoreLabel.java,True
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class POSTaggingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.tag()));
    }
  }
}",tok.tag,"CoreLabel.tag,Tag.tag","CoreNLP\src\edu\stanford\nlp\ling\CoreLabel.java
CoreNLP\src\edu\stanford\nlp\ling\Tag.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;


public class LemmatizingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.lemma()));
    }
  }

}","('main', 'main(String[] args)')","('StanfordCoreNLP.main', ""['args']"")",CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;


public class LemmatizingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.lemma()));
    }
  }

}",Properties,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;


public class LemmatizingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.lemma()));
    }
  }

}","('document.tokens', 'document.tokens()')","('CoreDocument.tokens', '[]')",CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java,True
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;


public class LemmatizingExample {

  public static String text = ""Marie was born in Paris."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", tok.word(), tok.lemma()));
    }
  }

}","('tok.word', 'tok.word()')","('CoreLabel.word', '[]')",CoreNLP\src\edu\stanford\nlp\ling\CoreLabel.java,True
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.Properties;
import java.util.stream.Collectors;

public class NERPipelineDemo {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
    // example customizations (these are commented out but you can uncomment them to see the results

    // disable fine grained ner
    // props.setProperty(""ner.applyFineGrained"", ""false"");

    // customize fine grained ner
    // props.setProperty(""ner.fine.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.fine.regexner.ignorecase"", ""true"");

    // add additional rules, customize TokensRegexNER annotator
    // props.setProperty(""ner.additional.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.additional.regexner.ignorecase"", ""true"");

    // add 2 additional rules files ; set the first one to be case-insensitive
    // props.setProperty(""ner.additional.regexner.mapping"", ""ignorecase=true,example_one.rules;example_two.rules"");

    // set document date to be a specific date (other options are explained in the document date section)
    // props.setProperty(""ner.docdate.useFixedDate"", ""2019-01-01"");

    // only run rules based NER
    // props.setProperty(""ner.rulesOnly"", ""true"");

    // only run statistical NER
    // props.setProperty(""ner.statisticalOnly"", ""true"");

    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // make an example document
    CoreDocument doc = new CoreDocument(""Joe Smith is from Seattle."");
    // annotate the document
    pipeline.annotate(doc);
    // view results
    System.out.println(""---"");
    System.out.println(""entities found"");
    for (CoreEntityMention em : doc.entityMentions())
      System.out.println(""\tdetected entity: \t""+em.text()+""\t""+em.entityType());
    System.out.println(""---"");
    System.out.println(""tokens and ner tags"");
    String tokensAndNERTags = doc.tokens().stream().map(token -> ""(""+token.word()+"",""+token.ner()+"")"").collect(
        Collectors.joining("" ""));
    System.out.println(tokensAndNERTags);
  }

}","('main', 'main(String[] args)')","('StanfordCoreNLP.main', ""['args']"")",CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.Properties;
import java.util.stream.Collectors;

public class NERPipelineDemo {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
    // example customizations (these are commented out but you can uncomment them to see the results

    // disable fine grained ner
    // props.setProperty(""ner.applyFineGrained"", ""false"");

    // customize fine grained ner
    // props.setProperty(""ner.fine.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.fine.regexner.ignorecase"", ""true"");

    // add additional rules, customize TokensRegexNER annotator
    // props.setProperty(""ner.additional.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.additional.regexner.ignorecase"", ""true"");

    // add 2 additional rules files ; set the first one to be case-insensitive
    // props.setProperty(""ner.additional.regexner.mapping"", ""ignorecase=true,example_one.rules;example_two.rules"");

    // set document date to be a specific date (other options are explained in the document date section)
    // props.setProperty(""ner.docdate.useFixedDate"", ""2019-01-01"");

    // only run rules based NER
    // props.setProperty(""ner.rulesOnly"", ""true"");

    // only run statistical NER
    // props.setProperty(""ner.statisticalOnly"", ""true"");

    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // make an example document
    CoreDocument doc = new CoreDocument(""Joe Smith is from Seattle."");
    // annotate the document
    pipeline.annotate(doc);
    // view results
    System.out.println(""---"");
    System.out.println(""entities found"");
    for (CoreEntityMention em : doc.entityMentions())
      System.out.println(""\tdetected entity: \t""+em.text()+""\t""+em.entityType());
    System.out.println(""---"");
    System.out.println(""tokens and ner tags"");
    String tokensAndNERTags = doc.tokens().stream().map(token -> ""(""+token.word()+"",""+token.ner()+"")"").collect(
        Collectors.joining("" ""));
    System.out.println(tokensAndNERTags);
  }

}",Properties,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.Properties;
import java.util.stream.Collectors;

public class NERPipelineDemo {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
    // example customizations (these are commented out but you can uncomment them to see the results

    // disable fine grained ner
    // props.setProperty(""ner.applyFineGrained"", ""false"");

    // customize fine grained ner
    // props.setProperty(""ner.fine.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.fine.regexner.ignorecase"", ""true"");

    // add additional rules, customize TokensRegexNER annotator
    // props.setProperty(""ner.additional.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.additional.regexner.ignorecase"", ""true"");

    // add 2 additional rules files ; set the first one to be case-insensitive
    // props.setProperty(""ner.additional.regexner.mapping"", ""ignorecase=true,example_one.rules;example_two.rules"");

    // set document date to be a specific date (other options are explained in the document date section)
    // props.setProperty(""ner.docdate.useFixedDate"", ""2019-01-01"");

    // only run rules based NER
    // props.setProperty(""ner.rulesOnly"", ""true"");

    // only run statistical NER
    // props.setProperty(""ner.statisticalOnly"", ""true"");

    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // make an example document
    CoreDocument doc = new CoreDocument(""Joe Smith is from Seattle."");
    // annotate the document
    pipeline.annotate(doc);
    // view results
    System.out.println(""---"");
    System.out.println(""entities found"");
    for (CoreEntityMention em : doc.entityMentions())
      System.out.println(""\tdetected entity: \t""+em.text()+""\t""+em.entityType());
    System.out.println(""---"");
    System.out.println(""tokens and ner tags"");
    String tokensAndNERTags = doc.tokens().stream().map(token -> ""(""+token.word()+"",""+token.ner()+"")"").collect(
        Collectors.joining("" ""));
    System.out.println(tokensAndNERTags);
  }

}","('CoreDocument', 'CoreDocument(""Joe Smith is from Seattle."")')","('CoreDocument', ""['documentText', ['annotation']]"")",CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.Properties;
import java.util.stream.Collectors;

public class NERPipelineDemo {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
    // example customizations (these are commented out but you can uncomment them to see the results

    // disable fine grained ner
    // props.setProperty(""ner.applyFineGrained"", ""false"");

    // customize fine grained ner
    // props.setProperty(""ner.fine.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.fine.regexner.ignorecase"", ""true"");

    // add additional rules, customize TokensRegexNER annotator
    // props.setProperty(""ner.additional.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.additional.regexner.ignorecase"", ""true"");

    // add 2 additional rules files ; set the first one to be case-insensitive
    // props.setProperty(""ner.additional.regexner.mapping"", ""ignorecase=true,example_one.rules;example_two.rules"");

    // set document date to be a specific date (other options are explained in the document date section)
    // props.setProperty(""ner.docdate.useFixedDate"", ""2019-01-01"");

    // only run rules based NER
    // props.setProperty(""ner.rulesOnly"", ""true"");

    // only run statistical NER
    // props.setProperty(""ner.statisticalOnly"", ""true"");

    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // make an example document
    CoreDocument doc = new CoreDocument(""Joe Smith is from Seattle."");
    // annotate the document
    pipeline.annotate(doc);
    // view results
    System.out.println(""---"");
    System.out.println(""entities found"");
    for (CoreEntityMention em : doc.entityMentions())
      System.out.println(""\tdetected entity: \t""+em.text()+""\t""+em.entityType());
    System.out.println(""---"");
    System.out.println(""tokens and ner tags"");
    String tokensAndNERTags = doc.tokens().stream().map(token -> ""(""+token.word()+"",""+token.ner()+"")"").collect(
        Collectors.joining("" ""));
    System.out.println(tokensAndNERTags);
  }

}",em.text,"CoreEntityMention.text,Document.text,CoreDocument.text","CoreNLP\src\edu\stanford\nlp\pipeline\CoreEntityMention.java
CoreNLP\src\edu\stanford\nlp\simple\Document.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.Properties;
import java.util.stream.Collectors;

public class NERPipelineDemo {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
    // example customizations (these are commented out but you can uncomment them to see the results

    // disable fine grained ner
    // props.setProperty(""ner.applyFineGrained"", ""false"");

    // customize fine grained ner
    // props.setProperty(""ner.fine.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.fine.regexner.ignorecase"", ""true"");

    // add additional rules, customize TokensRegexNER annotator
    // props.setProperty(""ner.additional.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.additional.regexner.ignorecase"", ""true"");

    // add 2 additional rules files ; set the first one to be case-insensitive
    // props.setProperty(""ner.additional.regexner.mapping"", ""ignorecase=true,example_one.rules;example_two.rules"");

    // set document date to be a specific date (other options are explained in the document date section)
    // props.setProperty(""ner.docdate.useFixedDate"", ""2019-01-01"");

    // only run rules based NER
    // props.setProperty(""ner.rulesOnly"", ""true"");

    // only run statistical NER
    // props.setProperty(""ner.statisticalOnly"", ""true"");

    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // make an example document
    CoreDocument doc = new CoreDocument(""Joe Smith is from Seattle."");
    // annotate the document
    pipeline.annotate(doc);
    // view results
    System.out.println(""---"");
    System.out.println(""entities found"");
    for (CoreEntityMention em : doc.entityMentions())
      System.out.println(""\tdetected entity: \t""+em.text()+""\t""+em.entityType());
    System.out.println(""---"");
    System.out.println(""tokens and ner tags"");
    String tokensAndNERTags = doc.tokens().stream().map(token -> ""(""+token.word()+"",""+token.ner()+"")"").collect(
        Collectors.joining("" ""));
    System.out.println(tokensAndNERTags);
  }

}","('em.entityType', 'em.entityType()')","('CoreEntityMention.entityType', '[]')",CoreNLP\src\edu\stanford\nlp\pipeline\CoreEntityMention.java,True
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.Properties;
import java.util.stream.Collectors;

public class NERPipelineDemo {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
    // example customizations (these are commented out but you can uncomment them to see the results

    // disable fine grained ner
    // props.setProperty(""ner.applyFineGrained"", ""false"");

    // customize fine grained ner
    // props.setProperty(""ner.fine.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.fine.regexner.ignorecase"", ""true"");

    // add additional rules, customize TokensRegexNER annotator
    // props.setProperty(""ner.additional.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.additional.regexner.ignorecase"", ""true"");

    // add 2 additional rules files ; set the first one to be case-insensitive
    // props.setProperty(""ner.additional.regexner.mapping"", ""ignorecase=true,example_one.rules;example_two.rules"");

    // set document date to be a specific date (other options are explained in the document date section)
    // props.setProperty(""ner.docdate.useFixedDate"", ""2019-01-01"");

    // only run rules based NER
    // props.setProperty(""ner.rulesOnly"", ""true"");

    // only run statistical NER
    // props.setProperty(""ner.statisticalOnly"", ""true"");

    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // make an example document
    CoreDocument doc = new CoreDocument(""Joe Smith is from Seattle."");
    // annotate the document
    pipeline.annotate(doc);
    // view results
    System.out.println(""---"");
    System.out.println(""entities found"");
    for (CoreEntityMention em : doc.entityMentions())
      System.out.println(""\tdetected entity: \t""+em.text()+""\t""+em.entityType());
    System.out.println(""---"");
    System.out.println(""tokens and ner tags"");
    String tokensAndNERTags = doc.tokens().stream().map(token -> ""(""+token.word()+"",""+token.ner()+"")"").collect(
        Collectors.joining("" ""));
    System.out.println(tokensAndNERTags);
  }

}",doc.tokens,"CoreDocument.tokens,CoreEntityMention.tokens","CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreEntityMention.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.Properties;
import java.util.stream.Collectors;

public class NERPipelineDemo {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
    // example customizations (these are commented out but you can uncomment them to see the results

    // disable fine grained ner
    // props.setProperty(""ner.applyFineGrained"", ""false"");

    // customize fine grained ner
    // props.setProperty(""ner.fine.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.fine.regexner.ignorecase"", ""true"");

    // add additional rules, customize TokensRegexNER annotator
    // props.setProperty(""ner.additional.regexner.mapping"", ""example.rules"");
    // props.setProperty(""ner.additional.regexner.ignorecase"", ""true"");

    // add 2 additional rules files ; set the first one to be case-insensitive
    // props.setProperty(""ner.additional.regexner.mapping"", ""ignorecase=true,example_one.rules;example_two.rules"");

    // set document date to be a specific date (other options are explained in the document date section)
    // props.setProperty(""ner.docdate.useFixedDate"", ""2019-01-01"");

    // only run rules based NER
    // props.setProperty(""ner.rulesOnly"", ""true"");

    // only run statistical NER
    // props.setProperty(""ner.statisticalOnly"", ""true"");

    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // make an example document
    CoreDocument doc = new CoreDocument(""Joe Smith is from Seattle."");
    // annotate the document
    pipeline.annotate(doc);
    // view results
    System.out.println(""---"");
    System.out.println(""entities found"");
    for (CoreEntityMention em : doc.entityMentions())
      System.out.println(""\tdetected entity: \t""+em.text()+""\t""+em.entityType());
    System.out.println(""---"");
    System.out.println(""tokens and ner tags"");
    String tokensAndNERTags = doc.tokens().stream().map(token -> ""(""+token.word()+"",""+token.ner()+"")"").collect(
        Collectors.joining("" ""));
    System.out.println(tokensAndNERTags);
  }

}","('token.word', 'token.word()')","('Token.word', '[]')",CoreNLP\src\edu\stanford\nlp\simple\Token.java,True
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class NERConfidenceExample {

    public static void main(String[] args) {
        String exampleText = ""Joe Smith lives in California."";
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        CoreDocument document = new CoreDocument(exampleText);
        pipeline.annotate(document);
        // get confidences for entities
        for (CoreEntityMention em : document.entityMentions()) {
            System.out.println(em.text() + ""\t"" + em.entityTypeConfidences());
        }
        // get confidences for tokens
        for (CoreLabel token : document.tokens()) {
            System.out.println(token.word() + ""\t"" + token.get(CoreAnnotations.NamedEntityTagProbsAnnotation.class));
        }
    }
}","('main', 'main(String[] args)')","('StanfordCoreNLP.main', ""['args']"")",CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class NERConfidenceExample {

    public static void main(String[] args) {
        String exampleText = ""Joe Smith lives in California."";
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        CoreDocument document = new CoreDocument(exampleText);
        pipeline.annotate(document);
        // get confidences for entities
        for (CoreEntityMention em : document.entityMentions()) {
            System.out.println(em.text() + ""\t"" + em.entityTypeConfidences());
        }
        // get confidences for tokens
        for (CoreLabel token : document.tokens()) {
            System.out.println(token.word() + ""\t"" + token.get(CoreAnnotations.NamedEntityTagProbsAnnotation.class));
        }
    }
}",Properties,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class NERConfidenceExample {

    public static void main(String[] args) {
        String exampleText = ""Joe Smith lives in California."";
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        CoreDocument document = new CoreDocument(exampleText);
        pipeline.annotate(document);
        // get confidences for entities
        for (CoreEntityMention em : document.entityMentions()) {
            System.out.println(em.text() + ""\t"" + em.entityTypeConfidences());
        }
        // get confidences for tokens
        for (CoreLabel token : document.tokens()) {
            System.out.println(token.word() + ""\t"" + token.get(CoreAnnotations.NamedEntityTagProbsAnnotation.class));
        }
    }
}","('CoreDocument', 'CoreDocument(exampleText)')","('CoreDocument', ""['documentText', ['annotation']]"")",CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class NERConfidenceExample {

    public static void main(String[] args) {
        String exampleText = ""Joe Smith lives in California."";
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        CoreDocument document = new CoreDocument(exampleText);
        pipeline.annotate(document);
        // get confidences for entities
        for (CoreEntityMention em : document.entityMentions()) {
            System.out.println(em.text() + ""\t"" + em.entityTypeConfidences());
        }
        // get confidences for tokens
        for (CoreLabel token : document.tokens()) {
            System.out.println(token.word() + ""\t"" + token.get(CoreAnnotations.NamedEntityTagProbsAnnotation.class));
        }
    }
}",em.text,"CoreEntityMention.text,Document.text,CoreDocument.text","CoreNLP\src\edu\stanford\nlp\pipeline\CoreEntityMention.java
CoreNLP\src\edu\stanford\nlp\simple\Document.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class NERConfidenceExample {

    public static void main(String[] args) {
        String exampleText = ""Joe Smith lives in California."";
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        CoreDocument document = new CoreDocument(exampleText);
        pipeline.annotate(document);
        // get confidences for entities
        for (CoreEntityMention em : document.entityMentions()) {
            System.out.println(em.text() + ""\t"" + em.entityTypeConfidences());
        }
        // get confidences for tokens
        for (CoreLabel token : document.tokens()) {
            System.out.println(token.word() + ""\t"" + token.get(CoreAnnotations.NamedEntityTagProbsAnnotation.class));
        }
    }
}","('em.entityTypeConfidences', 'em.entityTypeConfidences()')","('CoreEntityMention.entityTypeConfidences', '[]')",CoreNLP\src\edu\stanford\nlp\pipeline\CoreEntityMention.java,True
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class NERConfidenceExample {

    public static void main(String[] args) {
        String exampleText = ""Joe Smith lives in California."";
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        CoreDocument document = new CoreDocument(exampleText);
        pipeline.annotate(document);
        // get confidences for entities
        for (CoreEntityMention em : document.entityMentions()) {
            System.out.println(em.text() + ""\t"" + em.entityTypeConfidences());
        }
        // get confidences for tokens
        for (CoreLabel token : document.tokens()) {
            System.out.println(token.word() + ""\t"" + token.get(CoreAnnotations.NamedEntityTagProbsAnnotation.class));
        }
    }
}",document.tokens,"CoreDocument.tokens,CoreEntityMention.tokens","CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreEntityMention.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import java.util.*;

public class NERConfidenceExample {

    public static void main(String[] args) {
        String exampleText = ""Joe Smith lives in California."";
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        CoreDocument document = new CoreDocument(exampleText);
        pipeline.annotate(document);
        // get confidences for entities
        for (CoreEntityMention em : document.entityMentions()) {
            System.out.println(em.text() + ""\t"" + em.entityTypeConfidences());
        }
        // get confidences for tokens
        for (CoreLabel token : document.tokens()) {
            System.out.println(token.word() + ""\t"" + token.get(CoreAnnotations.NamedEntityTagProbsAnnotation.class));
        }
    }
}","('token.word', 'token.word()')","('CoreLabel.word', '[]')",CoreNLP\src\edu\stanford\nlp\ling\CoreLabel.java,True
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}","('main', 'main(String[] args)')","('StanfordCoreNLP.main', ""['args']"")",CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}",Properties,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}","('Annotation', 'Annotation(""The small red car turned very quickly around the corner."")')","('Annotation', ""['map', ['text'], ['sentences'], []]"")",CoreNLP\src\edu\stanford\nlp\pipeline\Annotation.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}","('tree.constituents', 'tree.constituents(new LabeledScoredConstituentFactory()')","('Tree.constituents', ""[['cf'], ['cf', 'maxDepth'], ['cf', 'charLevel'], ['cf', 'charLevel', 'filter']]"")",CoreNLP\src\edu\stanford\nlp\trees\Tree.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}",LabeledScoredConstituentFactory,LabeledScoredConstituentFactory,CoreNLP\src\edu\stanford\nlp\trees\LabeledScoredConstituentFactory.java,True
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}",constituent.label,"Constituent.label,Tree.label","CoreNLP\src\edu\stanford\nlp\trees\Constituent.java
CoreNLP\src\edu\stanford\nlp\trees\Tree.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}",toString,"Sentence.toString,Constituent.toString,Tree.toString,Annotation.toString","CoreNLP\src\edu\stanford\nlp\simple\Sentence.java
CoreNLP\src\edu\stanford\nlp\trees\Constituent.java
CoreNLP\src\edu\stanford\nlp\trees\Tree.java
CoreNLP\src\edu\stanford\nlp\pipeline\Annotation.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}",equals,"Constituent.equals,Tree.equals,Sentence.equals","CoreNLP\src\edu\stanford\nlp\trees\Constituent.java
CoreNLP\src\edu\stanford\nlp\trees\Tree.java
CoreNLP\src\edu\stanford\nlp\simple\Sentence.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""The small red car turned very quickly around the corner."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set<Constituent> treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &&
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}","('constituent.start', 'constituent.start()')","('Constituent.start', '[]')",CoreNLP\src\edu\stanford\nlp\trees\Constituent.java,True
"import java.util.Properties;

import edu.stanford.nlp.coref.CorefCoreAnnotations;
import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.coref.data.Mention;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;

public class CorefExample {
  public static void main(String[] args) throws Exception {
    Annotation document = new Annotation(""Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008."");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,coref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);
    System.out.println(""---"");
    System.out.println(""coref chains"");
    for (CorefChain cc : document.get(CorefCoreAnnotations.CorefChainAnnotation.class).values()) {
      System.out.println(""\t"" + cc);
    }
    for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""mentions"");
      for (Mention m : sentence.get(CorefCoreAnnotations.CorefMentionsAnnotation.class)) {
        System.out.println(""\t"" + m);
       }
    }
  }
}","('main', 'main(String[] args)')","('StanfordCoreNLP.main', ""['args']"")",CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"import java.util.Properties;

import edu.stanford.nlp.coref.CorefCoreAnnotations;
import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.coref.data.Mention;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;

public class CorefExample {
  public static void main(String[] args) throws Exception {
    Annotation document = new Annotation(""Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008."");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,coref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);
    System.out.println(""---"");
    System.out.println(""coref chains"");
    for (CorefChain cc : document.get(CorefCoreAnnotations.CorefChainAnnotation.class).values()) {
      System.out.println(""\t"" + cc);
    }
    for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""mentions"");
      for (Mention m : sentence.get(CorefCoreAnnotations.CorefMentionsAnnotation.class)) {
        System.out.println(""\t"" + m);
       }
    }
  }
}","('Annotation', 'Annotation(""Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008."")')","('Annotation', ""['map', ['text'], ['sentences'], []]"")",CoreNLP\src\edu\stanford\nlp\pipeline\Annotation.java,False
"import java.util.Properties;

import edu.stanford.nlp.coref.CorefCoreAnnotations;
import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.coref.data.Mention;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;

public class CorefExample {
  public static void main(String[] args) throws Exception {
    Annotation document = new Annotation(""Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008."");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,coref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);
    System.out.println(""---"");
    System.out.println(""coref chains"");
    for (CorefChain cc : document.get(CorefCoreAnnotations.CorefChainAnnotation.class).values()) {
      System.out.println(""\t"" + cc);
    }
    for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""mentions"");
      for (Mention m : sentence.get(CorefCoreAnnotations.CorefMentionsAnnotation.class)) {
        System.out.println(""\t"" + m);
       }
    }
  }
}",Properties,N/A,N/A,N/A
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.naturalli.NaturalLogicAnnotations;
import edu.stanford.nlp.util.CoreMap;

import java.util.Collection;
import java.util.Properties;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create the Stanford CoreNLP pipeline
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,depparse,natlog,openie"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // Annotate an example document.
    Annotation doc = new Annotation(""Obama was born in Hawaii. He is our president."");
    pipeline.annotate(doc);

    // Loop over sentences in the document
    for (CoreMap sentence : doc.get(CoreAnnotations.SentencesAnnotation.class)) {
      // Get the OpenIE triples for the sentence
      Collection<RelationTriple> triples =
	          sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class);
      // Print the triples
      for (RelationTriple triple : triples) {
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}",main,"OpenIEDemo.main,StanfordCoreNLP.main","CoreNLP\src\edu\stanford\nlp\naturalli\OpenIEDemo.java
CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java",False
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.naturalli.NaturalLogicAnnotations;
import edu.stanford.nlp.util.CoreMap;

import java.util.Collection;
import java.util.Properties;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create the Stanford CoreNLP pipeline
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,depparse,natlog,openie"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // Annotate an example document.
    Annotation doc = new Annotation(""Obama was born in Hawaii. He is our president."");
    pipeline.annotate(doc);

    // Loop over sentences in the document
    for (CoreMap sentence : doc.get(CoreAnnotations.SentencesAnnotation.class)) {
      // Get the OpenIE triples for the sentence
      Collection<RelationTriple> triples =
	          sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class);
      // Print the triples
      for (RelationTriple triple : triples) {
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}",Properties,N/A,N/A,N/A
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.naturalli.NaturalLogicAnnotations;
import edu.stanford.nlp.util.CoreMap;

import java.util.Collection;
import java.util.Properties;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create the Stanford CoreNLP pipeline
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,depparse,natlog,openie"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // Annotate an example document.
    Annotation doc = new Annotation(""Obama was born in Hawaii. He is our president."");
    pipeline.annotate(doc);

    // Loop over sentences in the document
    for (CoreMap sentence : doc.get(CoreAnnotations.SentencesAnnotation.class)) {
      // Get the OpenIE triples for the sentence
      Collection<RelationTriple> triples =
	          sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class);
      // Print the triples
      for (RelationTriple triple : triples) {
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}","('StanfordCoreNLP', 'StanfordCoreNLP(props)')","('StanfordCoreNLP', ""[['props'], ['props', 'enforceRequirements'], ['propsFileNamePrefix'], ['propsFileNamePrefix', 'enforceRequirements'], ['props', 'enforceRequirements', 'annotatorPool']]"")",CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.naturalli.NaturalLogicAnnotations;
import edu.stanford.nlp.util.CoreMap;

import java.util.Collection;
import java.util.Properties;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create the Stanford CoreNLP pipeline
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,depparse,natlog,openie"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // Annotate an example document.
    Annotation doc = new Annotation(""Obama was born in Hawaii. He is our president."");
    pipeline.annotate(doc);

    // Loop over sentences in the document
    for (CoreMap sentence : doc.get(CoreAnnotations.SentencesAnnotation.class)) {
      // Get the OpenIE triples for the sentence
      Collection<RelationTriple> triples =
	          sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class);
      // Print the triples
      for (RelationTriple triple : triples) {
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}","('Annotation', 'Annotation(""Obama was born in Hawaii. He is our president."")')","('Annotation', ""['map', ['text'], ['sentences'], []]"")",CoreNLP\src\edu\stanford\nlp\pipeline\Annotation.java,False
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.naturalli.NaturalLogicAnnotations;
import edu.stanford.nlp.util.CoreMap;

import java.util.Collection;
import java.util.Properties;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create the Stanford CoreNLP pipeline
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,depparse,natlog,openie"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // Annotate an example document.
    Annotation doc = new Annotation(""Obama was born in Hawaii. He is our president."");
    pipeline.annotate(doc);

    // Loop over sentences in the document
    for (CoreMap sentence : doc.get(CoreAnnotations.SentencesAnnotation.class)) {
      // Get the OpenIE triples for the sentence
      Collection<RelationTriple> triples =
	          sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class);
      // Print the triples
      for (RelationTriple triple : triples) {
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}","('triple.subjectLemmaGloss', 'triple.subjectLemmaGloss()')","('RelationTriple.subjectLemmaGloss', '[]')",CoreNLP\src\edu\stanford\nlp\ie\util\RelationTriple.java,True
Sentence.openieTriples(),"('Sentence.openieTriples', 'Sentence.openieTriples()')","('Sentence.openieTriples', ""['props', []]"")",CoreNLP\src\edu\stanford\nlp\simple\Sentence.java,False
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.simple.*;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create a CoreNLP document
    Document doc = new Document(""Obama was born in Hawaii. He is our president."");

    // Iterate over the sentences in the document
    for (Sentence sent : doc.sentences()) {
      // Iterate over the triples in the sentence
      for (RelationTriple triple : sent.openieTriples()) {
        // Print the triple
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}","('main', 'main(String[] args)')","('OpenIEDemo.main', ""['args']"")",CoreNLP\src\edu\stanford\nlp\naturalli\OpenIEDemo.java,False
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.simple.*;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create a CoreNLP document
    Document doc = new Document(""Obama was born in Hawaii. He is our president."");

    // Iterate over the sentences in the document
    for (Sentence sent : doc.sentences()) {
      // Iterate over the triples in the sentence
      for (RelationTriple triple : sent.openieTriples()) {
        // Print the triple
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}","('Document', 'Document(""Obama was born in Hawaii. He is our president."")')","('Document', ""[['anno', 'predictedMentions', 'goldMentions'], ['input', 'mentions'], [], ['anno', 'predictedMentions', 'goldMentions', 'dict'], ['props', 'text'], ['text'], ['props', 'ann'], ['ann'], ['props', 'proto'], ['proto']]"")",CoreNLP\src\edu\stanford\nlp\coref\data\Document.java,False
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.simple.*;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create a CoreNLP document
    Document doc = new Document(""Obama was born in Hawaii. He is our president."");

    // Iterate over the sentences in the document
    for (Sentence sent : doc.sentences()) {
      // Iterate over the triples in the sentence
      for (RelationTriple triple : sent.openieTriples()) {
        // Print the triple
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}","('doc.sentences', 'doc.sentences()')","('Document.sentences', ""['props', []]"")",CoreNLP\src\edu\stanford\nlp\simple\Document.java,False
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.simple.*;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create a CoreNLP document
    Document doc = new Document(""Obama was born in Hawaii. He is our president."");

    // Iterate over the sentences in the document
    for (Sentence sent : doc.sentences()) {
      // Iterate over the triples in the sentence
      for (RelationTriple triple : sent.openieTriples()) {
        // Print the triple
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}","('sent.openieTriples', 'sent.openieTriples()')","('Sentence.openieTriples', ""['props', []]"")",CoreNLP\src\edu\stanford\nlp\simple\Sentence.java,False
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.simple.*;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create a CoreNLP document
    Document doc = new Document(""Obama was born in Hawaii. He is our president."");

    // Iterate over the sentences in the document
    for (Sentence sent : doc.sentences()) {
      // Iterate over the triples in the sentence
      for (RelationTriple triple : sent.openieTriples()) {
        // Print the triple
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}",out.println,N/A,N/A,N/A
"import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.simple.*;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create a CoreNLP document
    Document doc = new Document(""Obama was born in Hawaii. He is our president."");

    // Iterate over the sentences in the document
    for (Sentence sent : doc.sentences()) {
      // Iterate over the triples in the sentence
      for (RelationTriple triple : sent.openieTriples()) {
        // Print the triple
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}","('triple.subjectLemmaGloss', 'triple.subjectLemmaGloss()')","('RelationTriple.subjectLemmaGloss', '[]')",CoreNLP\src\edu\stanford\nlp\ie\util\RelationTriple.java,True
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.time.*;

import java.util.Properties;

public class SUTimeBasicExample {

    public static String[] examples = {
            ""The concert will be on February 4th at the ampitheater."",
            ""The meeting will be held at 4:00pm in the library"",
            ""The conflict has lasted for over 15 years and shows no signs of abating.""
    };


    public static void main(String[] args) {
        // set up pipeline properties
        Properties props = new Properties();
        // general properties
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        props.setProperty(""ner.docdate.usePresent"", ""true"");
        props.setProperty(""sutime.includeRange"", ""true"");
        props.setProperty(""sutime.markTimeRanges"", ""true"");
        // build pipeline
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        for (String example : examples) {
            CoreDocument document = new CoreDocument(example);
            pipeline.annotate(document);
            for (CoreEntityMention cem : document.entityMentions()) {
                System.out.println(""temporal expression: ""+cem.text());
                System.out.println(""temporal value: "" +
                                   cem.coreMap().get(TimeAnnotations.TimexAnnotation.class));
            }
        }
    }
}","('main', 'main(String[] args)')","('StanfordCoreNLP.main', ""['args']"")",CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.time.*;

import java.util.Properties;

public class SUTimeBasicExample {

    public static String[] examples = {
            ""The concert will be on February 4th at the ampitheater."",
            ""The meeting will be held at 4:00pm in the library"",
            ""The conflict has lasted for over 15 years and shows no signs of abating.""
    };


    public static void main(String[] args) {
        // set up pipeline properties
        Properties props = new Properties();
        // general properties
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        props.setProperty(""ner.docdate.usePresent"", ""true"");
        props.setProperty(""sutime.includeRange"", ""true"");
        props.setProperty(""sutime.markTimeRanges"", ""true"");
        // build pipeline
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        for (String example : examples) {
            CoreDocument document = new CoreDocument(example);
            pipeline.annotate(document);
            for (CoreEntityMention cem : document.entityMentions()) {
                System.out.println(""temporal expression: ""+cem.text());
                System.out.println(""temporal value: "" +
                                   cem.coreMap().get(TimeAnnotations.TimexAnnotation.class));
            }
        }
    }
}",Properties,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.time.*;

import java.util.Properties;

public class SUTimeBasicExample {

    public static String[] examples = {
            ""The concert will be on February 4th at the ampitheater."",
            ""The meeting will be held at 4:00pm in the library"",
            ""The conflict has lasted for over 15 years and shows no signs of abating.""
    };


    public static void main(String[] args) {
        // set up pipeline properties
        Properties props = new Properties();
        // general properties
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        props.setProperty(""ner.docdate.usePresent"", ""true"");
        props.setProperty(""sutime.includeRange"", ""true"");
        props.setProperty(""sutime.markTimeRanges"", ""true"");
        // build pipeline
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        for (String example : examples) {
            CoreDocument document = new CoreDocument(example);
            pipeline.annotate(document);
            for (CoreEntityMention cem : document.entityMentions()) {
                System.out.println(""temporal expression: ""+cem.text());
                System.out.println(""temporal value: "" +
                                   cem.coreMap().get(TimeAnnotations.TimexAnnotation.class));
            }
        }
    }
}","('CoreDocument', 'CoreDocument(example)')","('CoreDocument', ""['documentText', ['annotation']]"")",CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.time.*;

import java.util.Properties;

public class SUTimeBasicExample {

    public static String[] examples = {
            ""The concert will be on February 4th at the ampitheater."",
            ""The meeting will be held at 4:00pm in the library"",
            ""The conflict has lasted for over 15 years and shows no signs of abating.""
    };


    public static void main(String[] args) {
        // set up pipeline properties
        Properties props = new Properties();
        // general properties
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        props.setProperty(""ner.docdate.usePresent"", ""true"");
        props.setProperty(""sutime.includeRange"", ""true"");
        props.setProperty(""sutime.markTimeRanges"", ""true"");
        // build pipeline
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        for (String example : examples) {
            CoreDocument document = new CoreDocument(example);
            pipeline.annotate(document);
            for (CoreEntityMention cem : document.entityMentions()) {
                System.out.println(""temporal expression: ""+cem.text());
                System.out.println(""temporal value: "" +
                                   cem.coreMap().get(TimeAnnotations.TimexAnnotation.class));
            }
        }
    }
}",cem.text,"Timex.text,CoreEntityMention.text,Document.text,CoreDocument.text","CoreNLP\src\edu\stanford\nlp\time\Timex.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreEntityMention.java
CoreNLP\src\edu\stanford\nlp\simple\Document.java
CoreNLP\src\edu\stanford\nlp\pipeline\CoreDocument.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.time.*;

import java.util.Properties;

public class SUTimeBasicExample {

    public static String[] examples = {
            ""The concert will be on February 4th at the ampitheater."",
            ""The meeting will be held at 4:00pm in the library"",
            ""The conflict has lasted for over 15 years and shows no signs of abating.""
    };


    public static void main(String[] args) {
        // set up pipeline properties
        Properties props = new Properties();
        // general properties
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        props.setProperty(""ner.docdate.usePresent"", ""true"");
        props.setProperty(""sutime.includeRange"", ""true"");
        props.setProperty(""sutime.markTimeRanges"", ""true"");
        // build pipeline
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        for (String example : examples) {
            CoreDocument document = new CoreDocument(example);
            pipeline.annotate(document);
            for (CoreEntityMention cem : document.entityMentions()) {
                System.out.println(""temporal expression: ""+cem.text());
                System.out.println(""temporal value: "" +
                                   cem.coreMap().get(TimeAnnotations.TimexAnnotation.class));
            }
        }
    }
}","('cem.coreMap', 'cem.coreMap()')","('CoreEntityMention.coreMap', '[]')",CoreNLP\src\edu\stanford\nlp\pipeline\CoreEntityMention.java,True
"// Financial Quarters
  FYQ1 = {
      type: QUARTER_OF_YEAR,
      label: ""FYQ1"",
      value: TimeWithRange(TimeRange(IsoDate(ANY,10,1), IsoDate(ANY,12,31), QUARTER))
  }
  FYQ2 = {
      type: QUARTER_OF_YEAR,
      label: ""FYQ2"",
      value: TimeWithRange(TimeRange(IsoDate(ANY,1,1), IsoDate(ANY,3,31), QUARTER))
  }
  FYQ3 = {
      type: QUARTER_OF_YEAR,
      label: ""FYQ3"",
      value: TimeWithRange(TimeRange(IsoDate(ANY,4,1), IsoDate(ANY,6,30), QUARTER))
  }
  FYQ4 = {
      type: QUARTER_OF_YEAR,
      label: ""FYQ4"",
      value: TimeWithRange(TimeRange(IsoDate(ANY,7,1), IsoDate(ANY,9,30), QUARTER))
  }",TimeWithRange,N/A,N/A,N/A
"# Finanical Quarters
  FISCAL_YEAR_QUARTER_MAP = {
    ""Q1"": FYQ1,
    ""Q2"": FYQ2,
    ""Q3"": FYQ3,
    ""Q4"": FYQ4
  }
  FISCAL_YEAR_QUARTER_YEAR_OFFSETS_MAP = {
    ""Q1"": 1,
    ""Q2"": 0,
    ""Q3"": 0,
    ""Q4"": 0
  }
  $FiscalYearQuarterTerm = CreateRegex(Keys(FISCAL_YEAR_QUARTER_MAP))",CreateRegex,N/A,N/A,N/A
"{
    matchWithResults: TRUE,
    pattern: ((/$FiscalYearQuarterTerm/) (FY)? (/(FY)?([0-9]{4})/)),
    result:  TemporalCompose(INTERSECT, IsoDate(Subtract({type: ""NUMBER"", value: $$3.matchResults[0].word.group(2)}, FISCAL_YEAR_QUARTER_YEAR_OFFSETS_MAP[$1[0].word]), ANY, ANY), FISCAL_YEAR_QUARTER_MAP[$1[0].word])
  }

  {
    pattern: ((/$FiscalYearQuarterTerm/)),
    result: FISCAL_YEAR_QUARTER_MAP[$1[0].word]
  }",TemporalCompose,N/A,N/A,N/A
"TemporalCompose(INTERSECT, IsoDate(Subtract({type: ""NUMBER"", value: $$3.matchResults[0].word.group(2)}, FISCAL_YEAR_QUARTER_YEAR_OFFSETS_MAP[$1[0].word]), ANY, ANY), FISCAL_YEAR_QUARTER_MAP[$1[0].word])",TemporalCompose,N/A,N/A,N/A
"TemporalCompose(INTERSECT, ... , ...)",TemporalCompose,N/A,N/A,N/A
$$3.matchResults[0].word.group(2),word.group,N/A,N/A,N/A
"IsoDate($Year, $Month, $Day)",IsoDate,N/A,N/A,N/A
"IsoDate(2019, ANY, ANY)",IsoDate,N/A,N/A,N/A
$$3.matchResults[0].word.group(1),word.group,N/A,N/A,N/A
"Subtract(..., ...)",Subtract,N/A,N/A,N/A
"IsoDate(..., ..., ...)",IsoDate,N/A,N/A,N/A
"TemporalCompose(INTERSECT, IsoDate(...), ...)",TemporalCompose,N/A,N/A,N/A
"// Dates are rough with respect to northern hemisphere (actual
  // solstice/equinox days depend on the year)
  SPRING_EQUINOX = {
    type: DAY_OF_YEAR,
	value: InexactTime( TimeRange( IsoDate(ANY, 3, 20), IsoDate(ANY, 3, 21) ) )
  }
  SUMMER_SOLSTICE = {
    type: DAY_OF_YEAR,
	value: InexactTime( TimeRange( IsoDate(ANY, 6, 20), IsoDate(ANY, 6, 21) ) )
  }
  FALL_EQUINOX = {
    type: DAY_OF_YEAR,
	value: InexactTime( TimeRange( IsoDate(ANY, 9, 22), IsoDate(ANY, 9, 23) ) )
  }
  WINTER_SOLSTICE = {
    type: DAY_OF_YEAR,
	value: InexactTime( TimeRange( IsoDate(ANY, 12, 21), IsoDate(ANY, 12, 22) ) )
  }

  ...
  
  // Dates for seasons are rough with respect to northern hemisphere
  SPRING = {
      type: SEASON_OF_YEAR,
      label: ""SP"",
      value: InexactTime( SPRING_EQUINOX, QUARTER, TimeRange( MARCH, JUNE, QUARTER ) ) }
  SUMMER = {
      type: SEASON_OF_YEAR,
      label: ""SU"",
      value: InexactTime( SUMMER_SOLSTICE, QUARTER, TimeRange( JUNE, SEPTEMBER, QUARTER ) )
  }
  FALL = {
      type: SEASON_OF_YEAR,
      label: ""FA"",
      value: InexactTime( FALL_EQUINOX, QUARTER, TimeRange( SEPTEMBER, DECEMBER, QUARTER ) )
  }
  WINTER = {
      type: SEASON_OF_YEAR,
      label: ""WI"",
      value: InexactTime( WINTER_SOLSTICE, QUARTER, TimeRange( DECEMBER, MARCH, QUARTER ) )
  }",InexactTime,N/A,N/A,N/A
"// Dates are rough with respect to northern hemisphere (actual
// solstice/equinox days depend on the year)
SPRING_EQUINOX = {
   type: DAY_OF_YEAR,
   value: InexactTime( TimeRange( IsoDate(ANY, 9, 22), IsoDate(ANY, 9, 23) ) )
}
SUMMER_SOLSTICE = {
   type: DAY_OF_YEAR,
   value: InexactTime( TimeRange( IsoDate(ANY, 12, 21), IsoDate(ANY, 12, 22) ) )
}
FALL_EQUINOX = {
   type: DAY_OF_YEAR,
   value: InexactTime( TimeRange( IsoDate(ANY, 3, 20), IsoDate(ANY, 3, 21) ) )
}
WINTER_SOLSTICE = {
   type: DAY_OF_YEAR,
   value: InexactTime( TimeRange( IsoDate(ANY, 6, 20), IsoDate(ANY, 6, 21) ) )
}

// Dates for seasons are rough with respect to northern hemisphere
SPRING = {
    type: SEASON_OF_YEAR,
    label: ""SP"",
    value: InexactTime( SPRING_EQUINOX, QUARTER, TimeRange( SEPTEMBER, DECEMBER, QUARTER ) ) }
SUMMER = {
    type: SEASON_OF_YEAR,
    label: ""SU"",
    value: InexactTime( SUMMER_SOLSTICE, QUARTER, TimeRange( DECEMBER, MARCH, QUARTER ) )
}
FALL = {
    type: SEASON_OF_YEAR,
    label: ""FA"",
    value: InexactTime( FALL_EQUINOX, QUARTER, TimeRange( MARCH, JUNE, QUARTER ) )
}
WINTER = {
    type: SEASON_OF_YEAR,
    label: ""WI"",
    value: InexactTime( WINTER_SOLSTICE, QUARTER, TimeRange( JUNE, SEPTEMBER, QUARTER ) )
}",InexactTime,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}",getType,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}","('uncheckedCast', 'uncheckedCast(List.class)')","('ErasureUtils.uncheckedCast', ""['o']"")",CoreNLP\src\edu\stanford\nlp\util\ErasureUtils.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}",main,"TokensRegexDemo.main,StanfordCoreNLP.main,StringUtils.main","CoreNLP\src\edu\stanford\nlp\ling\tokensregex\demo\TokensRegexDemo.java
CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java
CoreNLP\src\edu\stanford\nlp\util\StringUtils.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}","('StringUtils.argsToProperties', 'StringUtils.argsToProperties(args)')","('StringUtils.argsToProperties', ""['args', ['args', 'flagsToNumArgs']]"")",CoreNLP\src\edu\stanford\nlp\util\StringUtils.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}","('IOUtils.stringFromFile', 'IOUtils.stringFromFile(props.getProperty(""inputText"")')","('IOUtils.stringFromFile', ""['filename', ['filename', 'encoding']]"")",CoreNLP\src\edu\stanford\nlp\io\IOUtils.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}","('StanfordCoreNLP', 'StanfordCoreNLP(pipelineProps)')","('StanfordCoreNLP', ""[['props'], ['props', 'enforceRequirements'], ['propsFileNamePrefix'], ['propsFileNamePrefix', 'enforceRequirements'], ['props', 'enforceRequirements', 'annotatorPool']]"")",CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}","('Annotation', 'Annotation(exampleSentences)')","('Annotation', ""['map', ['text'], ['sentences'], []]"")",CoreNLP\src\edu\stanford\nlp\pipeline\Annotation.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}",pipeline.annotate,"StanfordCoreNLP.annotate,Util.annotate","CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java
CoreNLP\src\edu\stanford\nlp\naturalli\Util.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}","('TokenSequencePattern.getNewEnv', 'TokenSequencePattern.getNewEnv()')","('TokenSequencePattern.getNewEnv', '[]')",CoreNLP\src\edu\stanford\nlp\ling\tokensregex\TokenSequencePattern.java,True
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}","('env.setDefaultStringMatchFlags', 'env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE)')","('Env.setDefaultStringMatchFlags', ""['defaultStringMatchFlags']"")",CoreNLP\src\edu\stanford\nlp\ling\tokensregex\Env.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}","('CoreMapExpressionExtractor.createExtractorFromFiles', 'CoreMapExpressionExtractor.createExtractorFromFiles(env,rulesFiles)')","('CoreMapExpressionExtractor.createExtractorFromFiles', ""['env', 'filenames', ['env', 'filenames']]"")",CoreNLP\src\edu\stanford\nlp\ling\tokensregex\CoreMapExpressionExtractor.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}",token.word,"CoreLabel.word,Sentence.word,Token.word","CoreNLP\src\edu\stanford\nlp\ling\CoreLabel.java
CoreNLP\src\edu\stanford\nlp\simple\Sentence.java
CoreNLP\src\edu\stanford\nlp\simple\Token.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}",token.tag,"Token.tag,CoreLabel.tag","CoreNLP\src\edu\stanford\nlp\simple\Token.java
CoreNLP\src\edu\stanford\nlp\ling\CoreLabel.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}","('me.getText', 'me.getText()')","('MatchedExpression.getText', '[]')",CoreNLP\src\edu\stanford\nlp\ling\tokensregex\MatchedExpression.java,True
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;
import java.util.regex.*;

public class TokensRegexDemo {

  // My custom tokens
  public static class MyTokensAnnotation implements CoreAnnotation<List extends CoreMap>> {
    @Override
    public Class<List extends CoreMap>> getType() {
      return ErasureUtils.<Class<List extends CoreMap>>> uncheckedCast(List.class);
    }
  }

  // My custom type
  public static class MyTypeAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  // My custom value
  public static class MyValueAnnotation implements CoreAnnotation<String> {
    @Override
    public Class<String> getType() {
      return ErasureUtils.<Class<String>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) {

    // load settings from the command line
    Properties props = StringUtils.argsToProperties(args);

    // get the text to process

    // load sentences
    String exampleSentences = IOUtils.stringFromFile(props.getProperty(""inputText""));

    // build pipeline to get sentences and do basic tagging
    Properties pipelineProps = new Properties();
    pipelineProps.setProperty(""annotators"", props.getProperty(""annotators""));
    pipelineProps.setProperty(""ner.applyFineGrained"", ""false"");
    pipelineProps.setProperty(""ssplit.eolonly"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);

    // get sentences
    Annotation exampleSentencesAnnotation = new Annotation(exampleSentences);
    pipeline.annotate(exampleSentencesAnnotation);

    // set up the TokensRegex pipeline

    // get the rules files
    String[] rulesFiles = props.getProperty(""rulesFiles"").split("","");

    // set up an environment with reasonable defaults
    Env env = TokenSequencePattern.getNewEnv();
    // set to case insensitive
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CASE);

    // build the CoreMapExpressionExtractor
    CoreMapExpressionExtractor
        extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, rulesFiles);

    // for each sentence in the input text, run the TokensRegex pipeline
    int sentNum = 0;
    for (CoreMap sentence : exampleSentencesAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""sentence number: ""+sentNum);
      System.out.println(""sentence text: ""+sentence.get(CoreAnnotations.TextAnnotation.class));
      sentNum++;
      List<MatchedExpression> matchedExpressions = extractor.extractExpressions(sentence);
      // print out the results of the rules actions
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.tag() + ""\t"" + token.ner());
      }
      // print out the matched expressions
      for (MatchedExpression me : matchedExpressions) {
        System.out.println(""matched expression: ""+me.getText());
        System.out.println(""matched expression value: ""+me.getValue());
        System.out.println(""matched expression char offsets: ""+me.getCharOffsets());
        System.out.println(""matched expression tokens:"" +
            me.getAnnotation().get(CoreAnnotations.TokensAnnotation.class));
      }
    }
  }

}",me.getValue,"Token.getValue,MatchedExpression.getValue,CoreMapExpressionExtractor.getValue,Match.getValue","CoreNLP\src\edu\stanford\nlp\ling\tokensregex\parser\Token.java
CoreNLP\src\edu\stanford\nlp\ling\tokensregex\MatchedExpression.java
CoreNLP\src\edu\stanford\nlp\ling\tokensregex\CoreMapExpressionExtractor.java
CoreNLP\src\edu\stanford\nlp\ling\tokensregex\matcher\Match.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;

public class TokensRegexAnnotatorDemo {

  // key for matched expressions
  public static class MyMatchedExpressionAnnotation implements CoreAnnotation<List<CoreMap>> {
    @Override
    public Class<List<CoreMap>> getType() {
      return ErasureUtils.<Class<List<CoreMap>>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) throws ClassNotFoundException {
    // set properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,tokensregex"");
    props.setProperty(""tokensregex.rules"", ""basic_ner.rules"");
    props.setProperty(""tokensregex.matchedExpressionsAnnotationKey"",
        ""edu.stanford.nlp.examples.TokensRegexAnnotatorDemo$MyMatchedExpressionAnnotation"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // annotate
    Annotation ann = new Annotation(""There will be a big announcement by Apple Inc today at 5:00pm.  "" +
        ""She has worked at Miller Corp. for 5 years."");
    pipeline.annotate(ann);
    // show results
    System.out.println(""---"");
    System.out.println(""tokens\n"");
    for (CoreMap sentence : ann.get(CoreAnnotations.SentencesAnnotation.class)) {
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.ner());
      }
      System.out.println("""");
    }
    System.out.println(""---"");
    System.out.println(""matched expressions\n"");
    for (CoreMap me : ann.get(MyMatchedExpressionAnnotation.class)) {
      System.out.println(me);
    }
  }
}",getType,N/A,N/A,N/A
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;

public class TokensRegexAnnotatorDemo {

  // key for matched expressions
  public static class MyMatchedExpressionAnnotation implements CoreAnnotation<List<CoreMap>> {
    @Override
    public Class<List<CoreMap>> getType() {
      return ErasureUtils.<Class<List<CoreMap>>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) throws ClassNotFoundException {
    // set properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,tokensregex"");
    props.setProperty(""tokensregex.rules"", ""basic_ner.rules"");
    props.setProperty(""tokensregex.matchedExpressionsAnnotationKey"",
        ""edu.stanford.nlp.examples.TokensRegexAnnotatorDemo$MyMatchedExpressionAnnotation"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // annotate
    Annotation ann = new Annotation(""There will be a big announcement by Apple Inc today at 5:00pm.  "" +
        ""She has worked at Miller Corp. for 5 years."");
    pipeline.annotate(ann);
    // show results
    System.out.println(""---"");
    System.out.println(""tokens\n"");
    for (CoreMap sentence : ann.get(CoreAnnotations.SentencesAnnotation.class)) {
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.ner());
      }
      System.out.println("""");
    }
    System.out.println(""---"");
    System.out.println(""matched expressions\n"");
    for (CoreMap me : ann.get(MyMatchedExpressionAnnotation.class)) {
      System.out.println(me);
    }
  }
}","('uncheckedCast', 'uncheckedCast(String.class)')","('ErasureUtils.uncheckedCast', ""['o']"")",CoreNLP\src\edu\stanford\nlp\util\ErasureUtils.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;

public class TokensRegexAnnotatorDemo {

  // key for matched expressions
  public static class MyMatchedExpressionAnnotation implements CoreAnnotation<List<CoreMap>> {
    @Override
    public Class<List<CoreMap>> getType() {
      return ErasureUtils.<Class<List<CoreMap>>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) throws ClassNotFoundException {
    // set properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,tokensregex"");
    props.setProperty(""tokensregex.rules"", ""basic_ner.rules"");
    props.setProperty(""tokensregex.matchedExpressionsAnnotationKey"",
        ""edu.stanford.nlp.examples.TokensRegexAnnotatorDemo$MyMatchedExpressionAnnotation"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // annotate
    Annotation ann = new Annotation(""There will be a big announcement by Apple Inc today at 5:00pm.  "" +
        ""She has worked at Miller Corp. for 5 years."");
    pipeline.annotate(ann);
    // show results
    System.out.println(""---"");
    System.out.println(""tokens\n"");
    for (CoreMap sentence : ann.get(CoreAnnotations.SentencesAnnotation.class)) {
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.ner());
      }
      System.out.println("""");
    }
    System.out.println(""---"");
    System.out.println(""matched expressions\n"");
    for (CoreMap me : ann.get(MyMatchedExpressionAnnotation.class)) {
      System.out.println(me);
    }
  }
}",main,"TokensRegexAnnotatorDemo.main,StanfordCoreNLP.main","CoreNLP\src\edu\stanford\nlp\ling\tokensregex\demo\TokensRegexAnnotatorDemo.java
CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;

public class TokensRegexAnnotatorDemo {

  // key for matched expressions
  public static class MyMatchedExpressionAnnotation implements CoreAnnotation<List<CoreMap>> {
    @Override
    public Class<List<CoreMap>> getType() {
      return ErasureUtils.<Class<List<CoreMap>>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) throws ClassNotFoundException {
    // set properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,tokensregex"");
    props.setProperty(""tokensregex.rules"", ""basic_ner.rules"");
    props.setProperty(""tokensregex.matchedExpressionsAnnotationKey"",
        ""edu.stanford.nlp.examples.TokensRegexAnnotatorDemo$MyMatchedExpressionAnnotation"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // annotate
    Annotation ann = new Annotation(""There will be a big announcement by Apple Inc today at 5:00pm.  "" +
        ""She has worked at Miller Corp. for 5 years."");
    pipeline.annotate(ann);
    // show results
    System.out.println(""---"");
    System.out.println(""tokens\n"");
    for (CoreMap sentence : ann.get(CoreAnnotations.SentencesAnnotation.class)) {
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.ner());
      }
      System.out.println("""");
    }
    System.out.println(""---"");
    System.out.println(""matched expressions\n"");
    for (CoreMap me : ann.get(MyMatchedExpressionAnnotation.class)) {
      System.out.println(me);
    }
  }
}","('StanfordCoreNLP', 'StanfordCoreNLP(props)')","('StanfordCoreNLP', ""[['props'], ['props', 'enforceRequirements'], ['propsFileNamePrefix'], ['propsFileNamePrefix', 'enforceRequirements'], ['props', 'enforceRequirements', 'annotatorPool']]"")",CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;

public class TokensRegexAnnotatorDemo {

  // key for matched expressions
  public static class MyMatchedExpressionAnnotation implements CoreAnnotation<List<CoreMap>> {
    @Override
    public Class<List<CoreMap>> getType() {
      return ErasureUtils.<Class<List<CoreMap>>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) throws ClassNotFoundException {
    // set properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,tokensregex"");
    props.setProperty(""tokensregex.rules"", ""basic_ner.rules"");
    props.setProperty(""tokensregex.matchedExpressionsAnnotationKey"",
        ""edu.stanford.nlp.examples.TokensRegexAnnotatorDemo$MyMatchedExpressionAnnotation"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // annotate
    Annotation ann = new Annotation(""There will be a big announcement by Apple Inc today at 5:00pm.  "" +
        ""She has worked at Miller Corp. for 5 years."");
    pipeline.annotate(ann);
    // show results
    System.out.println(""---"");
    System.out.println(""tokens\n"");
    for (CoreMap sentence : ann.get(CoreAnnotations.SentencesAnnotation.class)) {
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.ner());
      }
      System.out.println("""");
    }
    System.out.println(""---"");
    System.out.println(""matched expressions\n"");
    for (CoreMap me : ann.get(MyMatchedExpressionAnnotation.class)) {
      System.out.println(me);
    }
  }
}",pipeline.annotate,"StanfordCoreNLP.annotate,TokensRegexAnnotator.annotate,Util.annotate","CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java
CoreNLP\src\edu\stanford\nlp\pipeline\TokensRegexAnnotator.java
CoreNLP\src\edu\stanford\nlp\naturalli\Util.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;

public class TokensRegexAnnotatorDemo {

  // key for matched expressions
  public static class MyMatchedExpressionAnnotation implements CoreAnnotation<List<CoreMap>> {
    @Override
    public Class<List<CoreMap>> getType() {
      return ErasureUtils.<Class<List<CoreMap>>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) throws ClassNotFoundException {
    // set properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,tokensregex"");
    props.setProperty(""tokensregex.rules"", ""basic_ner.rules"");
    props.setProperty(""tokensregex.matchedExpressionsAnnotationKey"",
        ""edu.stanford.nlp.examples.TokensRegexAnnotatorDemo$MyMatchedExpressionAnnotation"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // annotate
    Annotation ann = new Annotation(""There will be a big announcement by Apple Inc today at 5:00pm.  "" +
        ""She has worked at Miller Corp. for 5 years."");
    pipeline.annotate(ann);
    // show results
    System.out.println(""---"");
    System.out.println(""tokens\n"");
    for (CoreMap sentence : ann.get(CoreAnnotations.SentencesAnnotation.class)) {
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.ner());
      }
      System.out.println("""");
    }
    System.out.println(""---"");
    System.out.println(""matched expressions\n"");
    for (CoreMap me : ann.get(MyMatchedExpressionAnnotation.class)) {
      System.out.println(me);
    }
  }
}",token.word,"CoreLabel.word,Sentence.word,Token.word","CoreNLP\src\edu\stanford\nlp\ling\CoreLabel.java
CoreNLP\src\edu\stanford\nlp\simple\Sentence.java
CoreNLP\src\edu\stanford\nlp\simple\Token.java",False
"package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;
import java.util.List;
import java.util.Properties;

public class TokensRegexAnnotatorDemo {

  // key for matched expressions
  public static class MyMatchedExpressionAnnotation implements CoreAnnotation<List<CoreMap>> {
    @Override
    public Class<List<CoreMap>> getType() {
      return ErasureUtils.<Class<List<CoreMap>>> uncheckedCast(String.class);
    }
  }

  public static void main(String[] args) throws ClassNotFoundException {
    // set properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,tokensregex"");
    props.setProperty(""tokensregex.rules"", ""basic_ner.rules"");
    props.setProperty(""tokensregex.matchedExpressionsAnnotationKey"",
        ""edu.stanford.nlp.examples.TokensRegexAnnotatorDemo$MyMatchedExpressionAnnotation"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // annotate
    Annotation ann = new Annotation(""There will be a big announcement by Apple Inc today at 5:00pm.  "" +
        ""She has worked at Miller Corp. for 5 years."");
    pipeline.annotate(ann);
    // show results
    System.out.println(""---"");
    System.out.println(""tokens\n"");
    for (CoreMap sentence : ann.get(CoreAnnotations.SentencesAnnotation.class)) {
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.ner());
      }
      System.out.println("""");
    }
    System.out.println(""---"");
    System.out.println(""matched expressions\n"");
    for (CoreMap me : ann.get(MyMatchedExpressionAnnotation.class)) {
      System.out.println(me);
    }
  }
}",token.ner,"Token.ner,CoreLabel.ner","CoreNLP\src\edu\stanford\nlp\simple\Token.java
CoreNLP\src\edu\stanford\nlp\ling\CoreLabel.java",False
"{ ruleType: ""tokens"", pattern: ([{word:""I""}] [{word:/like|love/} & {tag:""VBP""}] ([{word:""pizza""}])), action: Annotate($1, ner, ""FOOD""), result: ""PIZZA"" }",Annotate,N/A,N/A,N/A
"# make all patterns case-sensitive
ENV.defaultStringMatchFlags = 0
ENV.defaultStringPatternFlags = 0

# these Java classes will be used by the rules
ner = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$NamedEntityTagAnnotation"" }
tokens = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$TokensAnnotation"" }

# define some regexes over tokens
$COMPANY_BEGINNING = ""/[A-Z][A-Za-z]+/""
$COMPANY_ENDING = ""/(Corp|Inc)\.?/""

# rule for recognizing company names
{ ruleType: ""tokens"", pattern: ([{word:$COMPANY_BEGINNING} & {tag:""NNP""}]+ [{word:$COMPANY_ENDING}]), action: Annotate($0, ner, ""COMPANY""), result: ""COMPANY_RESULT"" }",Annotate,N/A,N/A,N/A
"---
sentence number: 0
sentence text: She has worked at Miller Corp. for 5 years.
She		PRP	null
has		VBZ	null
worked		VBN	null
at		IN	null
Miller		NNP	COMPANY
Corp.		NNP	COMPANY
for		IN	null
5		CD	null
years		NNS	null
.		.	null

matched expression: Miller Corp.
matched expression value: STRING(COMPANY_RESULT)
matched expression char offsets: (18,30)
matched expression tokens:[Miller-5, Corp.-6]
---
sentence number: 1
sentence text: There will be a big announcement by Apple Inc today at 5:00pm.
There		EX	null
will		MD	null
be		VB	null
a		DT	null
big		JJ	null
announcement		NN	null
by		IN	null
Apple		NNP	COMPANY
Inc		NNP	COMPANY
today		NN	null
at		IN	null
5:00		CD	null
pm		NN	null
.		.	null

matched expression: Apple Inc
matched expression value: STRING(COMPANY_RESULT)
matched expression char offsets: (80,89)
matched expression tokens:[Apple-8, Inc-9]
---
sentence number: 2
sentence text: He works for apple inc in cupertino.
He		PRP	null
works		VBZ	null
for		IN	null
apple		NN	null
inc		NN	null
in		IN	null
cupertino		NN	null
.		.	null",STRING,N/A,N/A,N/A
"# uncomment to make all patterns case-insensitive in the rules file
# ENV.defaultStringMatchFlags = 66
# ENV.defaultStringPatternFlags = 66

# these Java classes will be used by the rules
ner = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$NamedEntityTagAnnotation"" }
tokens = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$TokensAnnotation"" }

# variables for complex regexes
$JOB_TITLE_BASES = ""/president|secretary|general/""
$JOB_TITLE_MODIFIERS = ""/vice|assistant|deputy/""

# first phase identifies components of job titles
# a TokensRegex pipeline can run various stages
# to specify a particular stage, set ENV.defaults[""stage""] to the stage number
ENV.defaults[""stage""] = 1

# tokens match phase
{ ruleType: ""tokens"", pattern: ([{word:$JOB_TITLE_MODIFIERS}]+), action: Annotate($0, ner, ""JOB_TITLE_MODIFIER"") }
{ ruleType: ""tokens"", pattern: ([{word:$JOB_TITLE_BASES}]), action: Annotate($0, ner, ""JOB_TITLE_BASE"") }

# second phase identifies complete job titles from components found in first phase
ENV.defaults[""stage""] = 2
{ ruleType: ""tokens"", pattern: ([{ner: ""JOB_TITLE_MODIFIER""}]+ [{ner: ""JOB_TITLE_BASE""}]), 
  action: Annotate($0, ner, ""COMPLETE_JOB_TITLE""), result: ""FOUND_COMPLETE_JOB_TITLE""}

# third phase is a filter phase, and it removes matched expressions that the filter matches
ENV.defaults[""stage""] = 3
# clean up component named entity tags from stage 1
{ ruleType: ""tokens"", pattern: ([{ner:""JOB_TITLE_MODIFIER""} | {ner:""JOB_TITLE_BASE""}]+), action: Annotate($0, ner, ""O"") }
# filter out the matched expression ""deputy vice president""
{ ruleType: ""filter"", pattern: ([{word:""deputy""}] [{word:""vice""}] [{word:""president""}]) }",Annotate,N/A,N/A,N/A
"---
sentence number: 0
sentence text: He is the vice president.
He		PRP	O
is		VBZ	O
the		DT	O
vice		NN	COMPLETE_JOB_TITLE
president		NN	COMPLETE_JOB_TITLE
.		.	O

matched expression: vice president
matched expression value: STRING(FOUND_COMPLETE_JOB_TITLE)
matched expression char offsets: (10,24)
matched expression tokens:[vice-4, president-5]
---
sentence number: 1
sentence text: He is the assistant vice president.
He		PRP	O
is		VBZ	O
the		DT	O
assistant		JJ	COMPLETE_JOB_TITLE
vice		NN	COMPLETE_JOB_TITLE
president		NN	COMPLETE_JOB_TITLE
.		.	O

matched expression: assistant vice president
matched expression value: STRING(FOUND_COMPLETE_JOB_TITLE)
matched expression char offsets: (36,60)
matched expression tokens:[assistant-4, vice-5, president-6]
---
sentence number: 2
sentence text: He is the deputy vice president.
He		PRP	O
is		VBZ	O
the		DT	O
deputy		NN	COMPLETE_JOB_TITLE
vice		NN	COMPLETE_JOB_TITLE
president		NN	COMPLETE_JOB_TITLE
.		.	O
---
sentence number: 3
sentence text: He is the president.
He		PRP	O
is		VBZ	O
the		DT	O
president		NN	O
.		.	O
---
sentence number: 4
sentence text: He is the President.
He		PRP	O
is		VBZ	O
the		DT	O
President		NNP	O
.		.	O",STRING,N/A,N/A,N/A
"---
sentence number: 0
sentence text: John said, ""I thought the pizza was great!""
John		NNP	PERSON
said		VBD	O
,		,	O
``		``	O
I		PRP	O
thought		VBD	O
the		DT	O
pizza		NN	O
was		VBD	O
great		JJ	O
!		.	O
''		''	O

matched expression: ""I thought the pizza was great!""
matched expression value: STRING(QUOTE)
matched expression char offsets: (11,43)
matched expression tokens:[``-4, I-5, thought-6, the-7, pizza-8, was-9, great-10, !-11, ''-12]",STRING,N/A,N/A,N/A
"# rules for finding employment relations
{ ruleType: ""tokens"", pattern: (([{ner:""PERSON""}]+) /works/ /for/ ([{ner:""ORGANIZATION""}]+)), 
  result: Concat(""("", $$1.text, "","", ""works_for"", "","", $$2.text, "")"") } 

{ ruleType: ""tokens"", pattern: (([{ner:""PERSON""}]+) /is/ /employed/ /at|by/ ([{ner:""ORGANIZATION""}]+)), 
  result: Concat(""("", $$1.text, "","", ""works_for"", "","", $$2.text, "")"") }",Concat,N/A,N/A,N/A
"---
sentence number: 0
sentence text: Joe Smith works for Google.
Joe		NNP	PERSON
Smith		NNP	PERSON
works		VBZ	O
for		IN	O
Google		NNP	ORGANIZATION
.		.	O

matched expression: Joe Smith works for Google
matched expression value: STRING((Joe Smith,works_for,Google))
matched expression char offsets: (0,26)
matched expression tokens:[Joe-1, Smith-2, works-3, for-4, Google-5]
---
sentence number: 1
sentence text: Jane Smith is employed by Apple.
Jane		NNP	PERSON
Smith		NNP	PERSON
is		VBZ	O
employed		VBN	O
by		IN	O
Apple		NNP	ORGANIZATION
.		.	O

matched expression: Jane Smith is employed by Apple
matched expression value: STRING((Jane Smith,works_for,Apple))
matched expression char offsets: (28,59)
matched expression tokens:[Jane-1, Smith-2, is-3, employed-4, by-5, Apple-6]",STRING,N/A,N/A,N/A
"orig = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$OriginalTextAnnotation"" }
numtokens = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$NumerizedTokensAnnotation"" }
numcomptype = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$NumericCompositeTypeAnnotation"" }
numcompvalue = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$NumericCompositeValueAnnotation"" }

mytokens = { type: ""CLASS"", value: ""edu.stanford.nlp.examples.TokensRegexDemo$MyTokensAnnotation"" }
type = { type: ""CLASS"", value: ""edu.stanford.nlp.examples.TokensRegexDemo$MyTypeAnnotation"" }
value = { type: ""CLASS"", value: ""edu.stanford.nlp.examples.TokensRegexDemo$MyValueAnnotation"" }

ENV.defaultResultAnnotationKey = ( type, value )
ENV.defaultNestedResultsAnnotationKey = mytokens
ENV.defaults[""stage.limitIters""] = 0

// Numbers
{ ruleType: ""tokens"", pattern: ( [ numcomptype:""NUMBER"" ] ), result: ( ""EXPR"", $0[0].numcompvalue ) }

// Operators
{ pattern: ( ""+"" ),            result: ( ""OP"", ""Add"" ),      priority: 1}
{ pattern: ( /plus/ ),         result: ( ""OP"", ""Add"" ),      priority: 1}
{ pattern: ( ""-"" ),            result: ( ""OP"", ""Subtract"" ), priority: 1}
{ pattern: ( /minus/ ),        result: ( ""OP"", ""Subtract"" ), priority: 1}
{ pattern: ( ""*"" ),            result: ( ""OP"", ""Multiply"" ), priority: 2}
{ pattern: ( /times/ ),        result: ( ""OP"", ""Multiply"" ), priority: 2}
{ pattern: ( ""/"" ),            result: ( ""OP"", ""Divide"" ),   priority: 2}
{ pattern: ( /divided/ /by/ ), result: ( ""OP"", ""Divide"" ),   priority: 2}
{ pattern: ( ""^"" ),            result: ( ""OP"", ""Pow"" ),      priority: 3}

$OP = ( [ type:""OP"" ] )
$EXPR = ( [ type:""EXPR"" ] )

{ ruleType: ""composite"", pattern: ( ($EXPR) ($OP) ($EXPR) ), result: (""EXPR"", Call($2[0].value, $1[0].value, $3[0].value)) }

{ ruleType: ""composite"", pattern: ( [orig:""(""] ($EXPR) [orig:"")""] ), result: (""EXPR"", $1[0].value) }",Call,N/A,N/A,N/A
"---
sentence number: 0
sentence text: (5 + 5) + 5
-LRB-		-LRB-	O
5		CD	NUMBER
+		CC	O
5		CD	NUMBER
-RRB-		-RRB-	O
+		CC	O
5		CD	NUMBER

matched expression: -LRB-5 + 5-RRB- + 5
matched expression value: LIST([STRING(EXPR), NUMBER(15)])
matched expression char offsets: (0,11)
matched expression tokens:[-LRB--1, 5-2, +-3, 5-4, -RRB--5, +-6, 5-7]",LIST,N/A,N/A,N/A
"Exception in thread ""main"" java.lang.NoSuchMethodError: edu.stanford.nlp.tagger.maxent.TaggerConfig.getTaggerDataInputStream(Ljava/lang/String;)Ljava/io/DataInputStream;",TaggerConfig.getTaggerDataInputStream,N/A,N/A,N/A
"Caused by: java.lang.NoSuchMethodError: edu.stanford.nlp.util.Generics.newHashMap()Ljava/util/Map;
    at edu.stanford.nlp.pipeline.AnnotatorPool.(AnnotatorPool.java:27)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.getDefaultAnnotatorPool(StanfordCoreNLP.java:305)","('Generics.newHashMap', 'Generics.newHashMap()')","('Generics.newHashMap', ""[['initialCapacity'], ['m']]"")",CoreNLP\src\edu\stanford\nlp\util\Generics.java,False
"Caused by: java.lang.NoSuchMethodError: edu.stanford.nlp.util.Generics.newHashMap()Ljava/util/Map;
    at edu.stanford.nlp.pipeline.AnnotatorPool.(AnnotatorPool.java:27)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.getDefaultAnnotatorPool(StanfordCoreNLP.java:305)","('StanfordCoreNLP.getDefaultAnnotatorPool', 'StanfordCoreNLP.getDefaultAnnotatorPool(StanfordCoreNLP.java:305)')","('StanfordCoreNLP.getDefaultAnnotatorPool', ""['inputProps', 'annotatorImplementation']"")",CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
new StanfordCoreNLP(props),"('StanfordCoreNLP', 'StanfordCoreNLP(props)')","('StanfordCoreNLP', ""[['props'], ['props', 'enforceRequirements'], ['propsFileNamePrefix'], ['propsFileNamePrefix', 'enforceRequirements'], ['props', 'enforceRequirements', 'annotatorPool']]"")",CoreNLP\src\edu\stanford\nlp\pipeline\StanfordCoreNLP.java,False
"public AnnotationPipeline buildPipeline() {
    AnnotationPipeline pl = new AnnotationPipeline();
    pl.addAnnotator(new TokenizerAnnotator(false));
    pl.addAnnotator(new WordsToSentencesAnnotator(false));
    pl.addAnnotator(new POSTaggerAnnotator(false));
    pl.addAnnotator(new MorphaAnnotator(false));
    pl.addAnnotator(new TimeAnnotator(""sutime"", props));
    pl.addAnnotator(new PhraseAnnotator(phrasesFile, false));
    return pl;
  }",buildPipeline,N/A,N/A,N/A
"public AnnotationPipeline buildPipeline() {
    AnnotationPipeline pl = new AnnotationPipeline();
    pl.addAnnotator(new TokenizerAnnotator(false));
    pl.addAnnotator(new WordsToSentencesAnnotator(false));
    pl.addAnnotator(new POSTaggerAnnotator(false));
    pl.addAnnotator(new MorphaAnnotator(false));
    pl.addAnnotator(new TimeAnnotator(""sutime"", props));
    pl.addAnnotator(new PhraseAnnotator(phrasesFile, false));
    return pl;
  }","('AnnotationPipeline', 'AnnotationPipeline()')","('AnnotationPipeline', ""['annotators', []]"")",CoreNLP\src\edu\stanford\nlp\pipeline\AnnotationPipeline.java,False
"public AnnotationPipeline buildPipeline() {
    AnnotationPipeline pl = new AnnotationPipeline();
    pl.addAnnotator(new TokenizerAnnotator(false));
    pl.addAnnotator(new WordsToSentencesAnnotator(false));
    pl.addAnnotator(new POSTaggerAnnotator(false));
    pl.addAnnotator(new MorphaAnnotator(false));
    pl.addAnnotator(new TimeAnnotator(""sutime"", props));
    pl.addAnnotator(new PhraseAnnotator(phrasesFile, false));
    return pl;
  }","('TokenizerAnnotator', 'TokenizerAnnotator(false)')","('TokenizerAnnotator', ""[['properties'], ['verbose'], ['lang'], ['verbose', 'lang'], ['verbose', 'lang'], ['verbose', 'lang', 'options'], ['verbose', 'props'], ['verbose', 'props', 'options']]"")",CoreNLP\src\edu\stanford\nlp\pipeline\TokenizerAnnotator.java,False
"public AnnotationPipeline buildPipeline() {
    AnnotationPipeline pl = new AnnotationPipeline();
    pl.addAnnotator(new TokenizerAnnotator(false));
    pl.addAnnotator(new WordsToSentencesAnnotator(false));
    pl.addAnnotator(new POSTaggerAnnotator(false));
    pl.addAnnotator(new MorphaAnnotator(false));
    pl.addAnnotator(new TimeAnnotator(""sutime"", props));
    pl.addAnnotator(new PhraseAnnotator(phrasesFile, false));
    return pl;
  }","('WordsToSentencesAnnotator', 'WordsToSentencesAnnotator(false)')","('WordsToSentencesAnnotator', ""[['properties'], ['verbose'], ['verbose', 'boundaryTokenRegex', 'boundaryToDiscard', 'htmlElementsToDiscard', 'newlineIsSentenceBreak', 'boundaryMultiTokenRegex', 'tokenRegexesToDiscard'], ['verbose', 'countLineNumbers', 'wts']]"")",CoreNLP\src\edu\stanford\nlp\pipeline\WordsToSentencesAnnotator.java,False
"public AnnotationPipeline buildPipeline() {
    AnnotationPipeline pl = new AnnotationPipeline();
    pl.addAnnotator(new TokenizerAnnotator(false));
    pl.addAnnotator(new WordsToSentencesAnnotator(false));
    pl.addAnnotator(new POSTaggerAnnotator(false));
    pl.addAnnotator(new MorphaAnnotator(false));
    pl.addAnnotator(new TimeAnnotator(""sutime"", props));
    pl.addAnnotator(new PhraseAnnotator(phrasesFile, false));
    return pl;
  }","('POSTaggerAnnotator', 'POSTaggerAnnotator(false)')","('POSTaggerAnnotator', ""[['verbose'], ['posLoc', 'verbose'], ['posLoc', 'verbose', 'maxSentenceLength', 'numThreads'], ['model'], ['model', 'maxSentenceLength', 'numThreads'], ['annotatorName', 'props']]"")",CoreNLP\src\edu\stanford\nlp\pipeline\POSTaggerAnnotator.java,False
"public AnnotationPipeline buildPipeline() {
    AnnotationPipeline pl = new AnnotationPipeline();
    pl.addAnnotator(new TokenizerAnnotator(false));
    pl.addAnnotator(new WordsToSentencesAnnotator(false));
    pl.addAnnotator(new POSTaggerAnnotator(false));
    pl.addAnnotator(new MorphaAnnotator(false));
    pl.addAnnotator(new TimeAnnotator(""sutime"", props));
    pl.addAnnotator(new PhraseAnnotator(phrasesFile, false));
    return pl;
  }","('MorphaAnnotator', 'MorphaAnnotator(false)')","('MorphaAnnotator', ""[['verbose']]"")",CoreNLP\src\edu\stanford\nlp\pipeline\MorphaAnnotator.java,False
"public AnnotationPipeline buildPipeline() {
    AnnotationPipeline pl = new AnnotationPipeline();
    pl.addAnnotator(new TokenizerAnnotator(false));
    pl.addAnnotator(new WordsToSentencesAnnotator(false));
    pl.addAnnotator(new POSTaggerAnnotator(false));
    pl.addAnnotator(new MorphaAnnotator(false));
    pl.addAnnotator(new TimeAnnotator(""sutime"", props));
    pl.addAnnotator(new PhraseAnnotator(phrasesFile, false));
    return pl;
  }","('TimeAnnotator', 'TimeAnnotator(""sutime"",props)')","('TimeAnnotator', ""[['quiet'], ['name', 'props'], ['name', 'props', 'quiet']]"")",CoreNLP\src\edu\stanford\nlp\time\TimeAnnotator.java,False
"AnnotationPipeline pipeline = buildPipeline();
Annotation annotation = new Annotation(""It's like a topography that is made from cartography of me."");
pipeline.annotate(annotation);",buildPipeline,N/A,N/A,N/A
"AnnotationPipeline pipeline = buildPipeline();
Annotation annotation = new Annotation(""It's like a topography that is made from cartography of me."");
pipeline.annotate(annotation);","('Annotation', 'Annotation(""It\'s like a topography that is made from cartography of me."")')","('Annotation', ""['map', ['text'], ['sentences'], []]"")",CoreNLP\src\edu\stanford\nlp\pipeline\Annotation.java,False
"AnnotationPipeline pipeline = buildPipeline();
Annotation annotation = new Annotation(""It's like a topography that is made from cartography of me."");
pipeline.annotate(annotation);","('pipeline.annotate', 'pipeline.annotate(annotation)')","('AnnotationPipeline.annotate', ""['annotation', ['annotations'], ['annotations', 'callback'], ['annotations', 'numThreads'], ['annotations', 'numThreads', 'callback']]"")",CoreNLP\src\edu\stanford\nlp\pipeline\AnnotationPipeline.java,False
"public void annotate(Annotation annotation);
public Set<Requirement> requirementsSatisfied();
public Set<Requirement> requires();",annotate,N/A,N/A,N/A
