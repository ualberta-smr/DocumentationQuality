Example,Truth functions,Test functions,Linked functions
">>> download('treebank') 
[nltk_data] Downloading package 'treebank'...
[nltk_data]   Unzipping corpora/treebank.zip.",['download'],"[('download', 'N/A')]",[]
">>> download('all-corpora') 
[nltk_data] Downloading package 'abc'...
[nltk_data]   Unzipping corpora/abc.zip.
[nltk_data] Downloading package 'alpino'...
[nltk_data]   Unzipping corpora/alpino.zip.
  ...
[nltk_data] Downloading package 'words'...
[nltk_data]   Unzipping corpora/words.zip.",['download'],"[('download', 'N/A')]",[]
">>> from nltk.featstruct import unify
>>> unify(dict(x=1, y=dict()), dict(a='a', y=dict(b='b')))  
{'y': {'b': 'b'}, 'x': 1, 'a': 'a'}",['unify'],"[('unify', 'N/A'), ('dict', 'N/A')]",[]
"def handler(s, position, reentrances, match): ...",[''],"[('handler', 'N/A')]",[]
">>> from nltk.featstruct import FeatStruct
>>> FeatStruct('[a=?x]').unify(FeatStruct('[b=?x]'))
[a=?x, b=?x2]",['unify'],"['__init__', ('unify', 'N/A')]",[]
">>> from nltk.probability import ConditionalFreqDist
>>> from nltk.tokenize import word_tokenize
>>> sent = ""the the the dog dog some other words that we do not care about""
>>> cfdist = ConditionalFreqDist()
>>> for word in word_tokenize(sent):
...     condition = len(word)
...     cfdist[condition][word] += 1","['word_tokenize', 'word_tokenize']","['__init__', ('word_tokenize', 'N/A'), ('len', 'N/A')]",[]
">>> cfdist = ConditionalFreqDist((len(word), word) for word in word_tokenize(sent))","['__init__', 'word_tokenize']","['__init__', ('len', 'N/A'), ('word_tokenize', 'N/A')]",['__init__']
">>> cfdist[3]
FreqDist({'the': 3, 'dog': 2, 'not': 1})
>>> cfdist[3].freq('the')
0.5
>>> cfdist[3]['dog']
2","['__init__', 'freq']","['__init__', ('freq', 'N/A')]",['__init__']
">>> from nltk.corpus import brown
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.probability import ConditionalProbDist, ELEProbDist
>>> cfdist = ConditionalFreqDist(brown.tagged_words()[:5000])
>>> cpdist = ConditionalProbDist(cfdist, ELEProbDist, 10)
>>> cpdist['passed'].max()
'VBD'
>>> cpdist['passed'].prob('VBD')
0.423...","['ConditionalFreqDist', 'ConditionalProbDist', 'max', 'prob']","['__init__', ('brown.tagged_words', 'N/A'), '__init__', ('max', 'N/A'), ('prob', 'N/A')]",[]
">>> from nltk.tokenize import word_tokenize
>>> from nltk.probability import FreqDist
>>> sent = 'This is an example sentence'
>>> fdist = FreqDist()
>>> for word in word_tokenize(sent):
...    fdist[word.lower()] += 1","['FreqDist', 'word_tokenize']","['__init__', ('word_tokenize', 'N/A'), ('word.lower', 'N/A')]",[]
>>> fdist = FreqDist(word.lower() for word in word_tokenize(sent)),"['', 'word_tokenize']","['__init__', ('word.lower', 'N/A'), ('word_tokenize', 'N/A')]",[]
">>> from nltk.probability import ProbabilisticMixIn
>>> class A:
...     def __init__(self, x, y): self.data = (x,y)
...
>>> class ProbabilisticA(A, ProbabilisticMixIn):
...     def __init__(self, x, y, **prob_kwarg):
...         A.__init__(self, x, y)
...         ProbabilisticMixIn.__init__(self, **prob_kwarg)",['ProbabilisticMixIn'],"[('__init__', 'N/A'), ('ProbabilisticA', 'N/A'), ('A.__init__', 'N/A')]",[]
">>> import nltk.corpus
>>> from nltk.text import Text
>>> moby = Text(nltk.corpus.gutenberg.words('melville-moby_dick.txt'))",['Text'],"['__init__', ('gutenberg.words', 'N/A')]",[]
">>> from nltk.book import text4
>>> text4.collocation_list()[:2]
[('United', 'States'), ('fellow', 'citizens')]",['collocation'],"[('text4.collocation_list', 'N/A')]",[]
">>> from nltk.book import text4
>>> text4.collocations() 
United States; fellow citizens; four years; ...",['collocation'],"[('text4.collocations', 'N/A')]",[]
">>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the",['findall'],"[('print', 'N/A'), ('text5.findall', 'N/A'), ('text1.findall', 'N/A'), ('text9.findall', 'N/A')]",[]
">>> import nltk.corpus
>>> from nltk.text import TextCollection
>>> print('hack'); from nltk.book import text1, text2, text3
hack...
>>> gutenberg = TextCollection(nltk.corpus.gutenberg)
>>> mytexts = TextCollection([text1, text2, text3])",['__init__'],"[('print', 'N/A'), '__init__']",['__init__']
">>> from nltk.text import TokenSearcher
>>> print('hack'); from nltk.book import text1, text5, text9
hack...
>>> text5.findall(""<.*><.*>"")
you rule bro; telling you bro; u twizted bro
>>> text1.findall(""(<.*>)"")
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> text9.findall(""{3,}"")
thread through those; the thought that; that the thing; the thing
that; that that thing; through these than through; them that the;
through the thick; them that they; thought that the",['findall'],"[('print', 'N/A'), ('text5.findall', 'N/A'), ('text1.findall', 'N/A'), ('text9.findall', 'N/A')]",[]
"for parent_index in ptree.parent_indices(parent):
    parent[parent_index] is ptree",['parent_indices'],"[('ptree.parent_indices', 'N/A')]",[]
"for treepos in ptree.treepositions(root):
    root[treepos] is ptree",['treepositions'],"[('ptree.treepositions', 'N/A')]",[]
">>> from nltk.tree import Tree
>>> print(Tree(1, [2, Tree(3, [4]), 5]))
(1 2 (3 4) 5)
>>> vp = Tree('VP', [Tree('V', ['saw']),
...                  Tree('NP', ['him'])])
>>> s = Tree('S', [Tree('NP', ['I']), vp])
>>> print(s)
(S (NP I) (VP (V saw) (NP him)))
>>> print(s[1])
(VP (V saw) (NP him))
>>> print(s[1,1])
(NP him)
>>> t = Tree.fromstring(""(S (NP I) (VP (V saw) (NP him)))"")
>>> s == t
True
>>> t[1][1].set_label('X')
>>> t[1][1].label()
'X'
>>> print(t)
(S (NP I) (VP (V saw) (X him)))
>>> t[0], t[1,1] = t[1,1], t[0]
>>> print(t)
(S (X him) (VP (V saw) (NP I)))","['__init__', 'fromstring']","[('print', 'N/A'), '__init__', 'fromstring', ('set_label', 'N/A'), ('label', 'N/A')]","['__init__', 'fromstring']"
">>> len(t)
2",[''],"[('len', 'N/A')]",[]
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> print(t.flatten())
(S the dog chased the cat)","['fromstring', 'flatten']","['fromstring', ('print', 'N/A'), ('t.flatten', 'N/A')]",['fromstring']
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.height()
5
>>> print(t[0,0])
(D the)
>>> t[0,0].height()
2","['fromstring', 'height']","['fromstring', ('t.height', 'N/A'), ('print', 'N/A'), ('height', 'N/A')]",['fromstring']
">>> t = Tree.fromstring('(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))')
>>> t.label()
'S'","['fromstring', 'label']","['fromstring', ('t.label', 'N/A')]",['fromstring']
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.leaves()
['the', 'dog', 'chased', 'the', 'cat']","['fromstring', 'leaves']","['fromstring', ('t.leaves', 'N/A')]",['fromstring']
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.pos()
[('the', 'D'), ('dog', 'N'), ('chased', 'V'), ('the', 'D'), ('cat', 'N')]","['fromstring', 'pos']","['fromstring', ('t.pos', 'N/A')]",['fromstring']
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.productions()
[S -> NP VP, NP -> D N, D -> 'the', N -> 'dog', VP -> V NP, V -> 'chased',
NP -> D N, D -> 'the', N -> 'cat']","['fromstring', 'productions']","['fromstring', ('t.productions', 'N/A')]",['fromstring']
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.set_label(""T"")
>>> print(t)
(T (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))","['fromstring', 'set_label']","['fromstring', ('t.set_label', 'N/A'), ('print', 'N/A')]",['fromstring']
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> for s in t.subtrees(lambda t: t.height() == 2):
...     print(s)
(D the)
(N dog)
(V chased)
(D the)
(N cat)","['fromstring', 'subtrees']","['fromstring', ('t.subtrees', 'N/A'), ('t.height', 'N/A'), ('print', 'N/A')]",['fromstring']
">>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.treepositions() 
[(), (0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0), (1,), (1, 0), (1, 0, 0), ...]
>>> for pos in t.treepositions('leaves'):
...     t[pos] = t[pos][::-1].upper()
>>> print(t)
(S (NP (D EHT) (N GOD)) (VP (V DESAHC) (NP (D EHT) (N TAC))))","['fromstring', 'treepositions']","['fromstring', ('t.treepositions', 'N/A'), ('upper', 'N/A'), ('print', 'N/A')]",['fromstring']
">>> import nltk
>>> from nltk.util import acyclic_branches_depth_first as tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(tree(wn.synset('certified.a.01'), lambda s:s.also_sees(), cut_mark='...', depth=4))
[Synset('certified.a.01'),
 [Synset('authorized.a.01'),
  [Synset('lawful.a.01'),
   [Synset('legal.a.01'),
    ""Cycle(Synset('lawful.a.01'),0,...)"",
    [Synset('legitimate.a.01'), '...']],
   [Synset('straight.a.06'),
    [Synset('honest.a.01'), '...'],
    ""Cycle(Synset('lawful.a.01'),0,...)""]],
  [Synset('legitimate.a.01'),
   ""Cycle(Synset('authorized.a.01'),1,...)"",
   [Synset('legal.a.01'),
    [Synset('lawful.a.01'), '...'],
    ""Cycle(Synset('legitimate.a.01'),0,...)""],
   [Synset('valid.a.01'),
    ""Cycle(Synset('legitimate.a.01'),0,...)"",
    [Synset('reasonable.a.01'), '...']]],
  [Synset('official.a.01'), ""Cycle(Synset('authorized.a.01'),1,...)""]],
 [Synset('documented.a.01')]]",['acyclic_branches_depth_first'],"[('pprint', 'N/A'), ('tree', 'N/A'), ('wn.synset', 'N/A'), ('s.also_sees', 'N/A'), '__init__', ('Cycle', 'N/A')]",[]
">>> import nltk
>>> from nltk.util import acyclic_depth_first as acyclic_tree
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(acyclic_tree(wn.synset('dog.n.01'), lambda s:s.hypernyms(),cut_mark='...'))
[Synset('dog.n.01'),
 [Synset('canine.n.02'),
  [Synset('carnivore.n.01'),
   [Synset('placental.n.01'),
    [Synset('mammal.n.01'),
     [Synset('vertebrate.n.01'),
      [Synset('chordate.n.01'),
       [Synset('animal.n.01'),
        [Synset('organism.n.01'),
         [Synset('living_thing.n.01'),
          [Synset('whole.n.02'),
           [Synset('object.n.01'),
            [Synset('physical_entity.n.01'),
             [Synset('entity.n.01')]]]]]]]]]]]]],
 [Synset('domestic_animal.n.01'), ""Cycle(Synset('animal.n.01'),-3,...)""]]",['acyclic_branches_depth_first'],"[('pprint', 'N/A'), ('acyclic_tree', 'N/A'), ('wn.synset', 'N/A'), ('s.hypernyms', 'N/A'), '__init__', ('Cycle', 'N/A')]",[]
">>> from nltk.util import bigrams
>>> list(bigrams([1,2,3,4,5]))
[(1, 2), (2, 3), (3, 4), (4, 5)]",['bigrams'],"[('list', 'N/A'), ('bigrams', 'N/A')]",[]
">>> choose(4, 2)
6
>>> choose(6, 2)
15",['choose'],"[('choose', 'N/A')]",[]
>>> sent = 'a b c'.split(),[''],"[('split', 'N/A')]",[]
">>> list(everygrams(sent))
[('a',), ('a', 'b'), ('a', 'b', 'c'), ('b',), ('b', 'c'), ('c',)]",['everygrams'],"[('list', 'N/A'), ('everygrams', 'N/A')]",[]
">>> sorted(everygrams(sent), key=len)
[('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c'), ('a', 'b', 'c')]",['everygrams'],"[('sorted', 'N/A'), ('everygrams', 'N/A')]",[]
">>> list(everygrams(sent, max_len=2))
[('a',), ('a', 'b'), ('b',), ('b', 'c'), ('c',)]",['everygrams'],"[('list', 'N/A'), ('everygrams', 'N/A')]",[]
">>> from nltk.util import flatten
>>> flatten(1, 2, ['b', 'a' , ['c', 'd']], 3)
[1, 2, 'b', 'a', 'c', 'd', 3]",['flatten'],"[('flatten', 'N/A')]",[]
"locale.setlocale(locale.LC_ALL, '')",[''],"[('locale.setlocale', 'N/A')]",[]
">>> from nltk.util import ngrams
>>> list(ngrams([1,2,3,4,5], 3))
[(1, 2, 3), (2, 3, 4), (3, 4, 5)]",['ngrams'],"[('list', 'N/A'), ('ngrams', 'N/A')]",[]
">>> list(ngrams([1,2,3,4,5], 2, pad_right=True))
[(1, 2), (2, 3), (3, 4), (4, 5), (5, None)]
>>> list(ngrams([1,2,3,4,5], 2, pad_right=True, right_pad_symbol=''))
[(1, 2), (2, 3), (3, 4), (4, 5), (5, '')]
>>> list(ngrams([1,2,3,4,5], 2, pad_left=True, left_pad_symbol=''))
[('', 1), (1, 2), (2, 3), (3, 4), (4, 5)]
>>> list(ngrams([1,2,3,4,5], 2, pad_left=True, pad_right=True, left_pad_symbol='', right_pad_symbol=''))
[('', 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, '')]",['ngrams'],"[('list', 'N/A'), ('ngrams', 'N/A')]",[]
">>> list(pad_sequence([1,2,3,4,5], 2, pad_left=True, pad_right=True, left_pad_symbol='', right_pad_symbol=''))
['', 1, 2, 3, 4, 5, '']
>>> list(pad_sequence([1,2,3,4,5], 2, pad_left=True, left_pad_symbol=''))
['', 1, 2, 3, 4, 5]
>>> list(pad_sequence([1,2,3,4,5], 2, pad_right=True, right_pad_symbol=''))
[1, 2, 3, 4, 5, '']",['pad_sequence'],"[('list', 'N/A'), ('pad_sequence', 'N/A')]",[]
">>> sent = ""Insurgents killed in ongoing fighting"".split()
>>> list(skipgrams(sent, 2, 2))
[('Insurgents', 'killed'), ('Insurgents', 'in'), ('Insurgents', 'ongoing'), ('killed', 'in'), ('killed', 'ongoing'), ('killed', 'fighting'), ('in', 'ongoing'), ('in', 'fighting'), ('ongoing', 'fighting')]
>>> list(skipgrams(sent, 3, 2))
[('Insurgents', 'killed', 'in'), ('Insurgents', 'killed', 'ongoing'), ('Insurgents', 'killed', 'fighting'), ('Insurgents', 'in', 'ongoing'), ('Insurgents', 'in', 'fighting'), ('Insurgents', 'ongoing', 'fighting'), ('killed', 'in', 'ongoing'), ('killed', 'in', 'fighting'), ('killed', 'ongoing', 'fighting'), ('in', 'ongoing', 'fighting')]",['skip_grams'],"[('split', 'N/A'), ('list', 'N/A'), ('skipgrams', 'N/A')]",[]
">>> from nltk.util import trigrams
>>> list(trigrams([1,2,3,4,5]))
[(1, 2, 3), (2, 3, 4), (3, 4, 5)]",['trigrams'],"[('list', 'N/A'), ('trigrams', 'N/A')]",[]
">>> import nltk
>>> from nltk.util import unweighted_minimum_spanning_tree as mst
>>> wn=nltk.corpus.wordnet
>>> from pprint import pprint
>>> pprint(mst(wn.synset('bound.a.01'), lambda s:s.also_sees()))
[Synset('bound.a.01'),
 [Synset('unfree.a.02'),
  [Synset('confined.a.02')],
  [Synset('dependent.a.01')],
  [Synset('restricted.a.01'), [Synset('classified.a.02')]]]]",['unweighted_minimum_spanning_tree'],"[('pprint', 'N/A'), ('mst', 'N/A'), ('wn.synset', 'N/A'), ('s.also_sees', 'N/A'), '__init__']",[]
">>> lesk(['I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.'], 'bank', 'n')
Synset('savings_bank.n.02')",['lesk'],"[('lesk', 'N/A'), '__init__']",[]
