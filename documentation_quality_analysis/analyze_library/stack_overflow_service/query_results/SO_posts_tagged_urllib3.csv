post_id,title,body,score,creation_date,tags,is_answered,view_count,answer_count,last_activity_date,link
27440060,How to fix ImportError: No module named packages.urllib3?,"<p>I'm running Python 2.7.6 on an Ubuntu machine. When I run <code>twill-sh</code> (Twill is a browser used for testing websites) in my Terminal, I'm getting the following:</p>

<pre><code>Traceback (most recent call last):
  File ""dep.py"", line 2, in &lt;module&gt;
    import twill.commands
  File ""/usr/local/lib/python2.7/dist-packages/twill/__init__.py"", line 52, in &lt;module&gt;
    from shell import TwillCommandLoop
  File ""/usr/local/lib/python2.7/dist-packages/twill/shell.py"", line 9, in &lt;module&gt;
    from twill import commands, parse, __version__
  File ""/usr/local/lib/python2.7/dist-packages/twill/commands.py"", line 75, in &lt;module&gt;
    browser = TwillBrowser()
  File ""/usr/local/lib/python2.7/dist-packages/twill/browser.py"", line 31, in __init__
    from requests.packages.urllib3 import connectionpool as cpl
ImportError: No module named packages.urllib3
</code></pre>

<p>However, I can import urllib in Python console just fine. What could be the reason?</p>
",16,1418374797,python;urllib2;urllib3;twill,True,75792,8,1711126869,https://stackoverflow.com/questions/27440060/how-to-fix-importerror-no-module-named-packages-urllib3
78204798,urllib3 warning ReadTimeoutError while connecting to Nominatim&#39;s geolocator,"<p>The code below code to extract pincodes, but raises the error below. How to work around this?</p>
<pre><code>def get_zipcode(df, geolocator, lat_field, lon_field):
    location = geolocator.reverse(str(df[lat_field]) + &quot;,&quot; + str(df[lon_field]))
    # print(location)    #-- uncomment this in case of checking the output
    #return location.raw['address']['postcode']
    
    if location == None:
        return &quot;none&quot;
    else:
        return location.raw['address'].get('postcode')

geolocator = geopy.Nominatim(user_agent=&quot;MyGeocodingScript&quot;)


zipcode = df.apply(get_zipcode, axis=1, geolocator=geolocator, lat_field='GPS_Lat', lon_field='GPS_Lon')
</code></pre>
<p>Error:</p>
<pre><code>WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(&quot;HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)&quot;)': /reverse?lat=11.2181948&amp;lon=78.1811652&amp;format=json&amp;addressdetails=1
</code></pre>
",1,1711094546,python;urllib3;httpconnection;nominatim;geolocator,False,35,0,1711108899,https://stackoverflow.com/questions/78204798/urllib3-warning-readtimeouterror-while-connecting-to-nominatims-geolocator
78182205,"Selenium : ValueError: Timeout value connect was &lt;object object at 0x0000020E14D886D0&gt;, but it must be an int, float or None. (python)","<pre><code>from selenium import webdriver

def openDriver():
global driver

    geckodriver_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'geckodriver.exe')
    driver = webdriver.Firefox(executable_path=geckodriver_path)
    
    driver.get('https://google.com/')
    return driver

def main():
driver= openDriver()

if __name__ == '__main__':
main()

</code></pre>
<p>I was using selenium==3.141.0 and urllib3 was 2.2.1 and when executing the script that should recognize the py path, it gave an execution error. It was then that I saw in another question that it could be the version of selenium or urllurllib3
It seems to be a problem with the specific version of Selenium when using executable_path</p>
",0,1710785055,python;selenium-webdriver;urllib3,False,31,1,1710790393,https://stackoverflow.com/questions/78182205/selenium-valueerror-timeout-value-connect-was-object-object-at-0x0000020e14d
78163280,No module named &#39;urllib3.packages.six.moves&#39;,"<p>I'm trying to use the groupme API with GroupyAPI, but I'm having some issues with urllib3.</p>
<pre><code>import urllib3
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;C:\Users\smlac\AppData\Local\Programs\Python\Python312\Lib\site-packages\urllib3\__init__.py&quot;, line 8, in &lt;module&gt;
    from .connectionpool import (
  File &quot;C:\Users\smlac\AppData\Local\Programs\Python\Python312\Lib\site-packages\urllib3\connectionpool.py&quot;, line 11, in &lt;module&gt;
    from .exceptions import (
  File &quot;C:\Users\smlac\AppData\Local\Programs\Python\Python312\Lib\site-packages\urllib3\exceptions.py&quot;, line 2, in &lt;module&gt;
    from .packages.six.moves.http_client import (
ModuleNotFoundError: No module named 'urllib3.packages.six.moves'
</code></pre>
<p>I have tried this on both my laptop and PC, for some reason it works on my PC, but I need to use it on my laptop as well.  I have uninstalled urllib and reinstalled it, I have verified the versions match with what I'm using on my PC and they all match. I made sure that python is in my environmental variables path. I just don't understand why it only works on my PC.</p>
<p>Any help would be greatly appreciated.</p>
",0,1710448057,python;python-3.x;urllib3;six,False,140,0,1710448057,https://stackoverflow.com/questions/78163280/no-module-named-urllib3-packages-six-moves
78162200,How to call an external REST API which authenticates client SSL certificates from AWS Lambda,"<p>We have a lambda in AWS written in python 3.9 which calls a REST API which has authenticates SSL client cert. I am able to call the API on Postman with basic auth and a pfx cert file. However when I try to implement it in the lambda, I get SSL errors.</p>
<p>Here is the sample code which I use:</p>
<pre><code>import urllib3

api_url = &quot;https://...&quot;
headers = urllib3.make_headers(basic_auth=&quot;xxx:yyy&quot;)
http = urllib3.PoolManager(cert_reqs=&quot;CERT_REQUIRED&quot;, cert_file=&quot;/tmp/abcd.pfx&quot;)
response = http.request('GET', api_url, headers=headers)
</code></pre>
<p>I store the ssl cert in S3 bucket and fetch and write it to /tmp before I call the API. I always get SSL error. I tried to pass the PEM as well as PFX files, nothing works</p>
<p>I have two questions:</p>
<ol>
<li>What is the best place to store the ssl client cert in AWS which can be used in the lambda?</li>
<li>Is urllib3 the best way to connect to a REST API on ssl?</li>
</ol>
<p>Would appreciate any pointers on this.</p>
",1,1710435520,python;aws-lambda;urllib3,False,34,0,1710435520,https://stackoverflow.com/questions/78162200/how-to-call-an-external-rest-api-which-authenticates-client-ssl-certificates-fro
78061206,"HTTPConnectionPool(host=&#39;http_proxy=&lt;ip&gt;, port=&lt;port&gt;): Max retries exceeded with url: &lt;url&gt;","<p>When I make requests.post, I get an error like this: &quot;<strong>HTTPConnectionPool(host='http_proxy=, port=): Max retries exceeded with url:  (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x00000246DDDB14B0&gt;: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')))</strong>&quot;.</p>
<p>This problem occurred after adding the proxy. I removed the proxy, but interestingly the problem still persists. Can you help me?</p>
<p>Thanks.</p>
",0,1708953482,python;proxy;http-proxy;urllib3,False,26,0,1708953482,https://stackoverflow.com/questions/78061206/httpconnectionpoolhost-http-proxy-ip-port-port-max-retries-exceeded-wit
78008250,cannot make requests wit urllib3,"<p>Can anyone provide a working urllib3 example that works? I have tried the example on the website <a href=""https://pypi.org/project/urllib3/"" rel=""nofollow noreferrer"">https://pypi.org/project/urllib3/</a>:</p>
<pre><code>import urllib3
resp = urllib3.request(&quot;GET&quot;, &quot;http://httpbin.org/robots.txt&quot;)
</code></pre>
<p>but keep on getting this error (using Python 3.10):</p>
<pre><code>AttributeError: module 'urllib3.request' has no attribute 'Request'
</code></pre>
",0,1708095730,python;python-requests;urllib3,False,40,1,1708096172,https://stackoverflow.com/questions/78008250/cannot-make-requests-wit-urllib3
60590168,"How to view the HTTP headers, response code and html content using urllib3 in Python?","<p>I am interested in retrieving the response code, body and HTTP headers using urllib3. The previous code which I had written was in Python 2 and now I have to rewrite it for Python 3.</p>

<pre><code>import urllib3

http = urllib3.PoolManager();
response = http.request('GET', 'http://192.168.43.131:8000')
print(response)
</code></pre>

<p>I tried different sources, but can someone point me in the right direction of the give a few pointers?</p>
",3,1583688495,python;urllib;urllib3,True,3870,2,1707527455,https://stackoverflow.com/questions/60590168/how-to-view-the-http-headers-response-code-and-html-content-using-urllib3-in-py
77948114,urllib3 not throwing exception on 400 Bad Request response from endpoint,"<p>I'm trying to make a request and handle any errors thrown by an endpoint using <code>urllib3</code></p>
<p>My code and expectations are as follows:</p>
<pre class=""lang-py prettyprint-override""><code>import urllib3
...
try:
  response = http.request('POST', url, headers=headers, body=encoded_data)
  print(response.status) # =&gt; 400

except urllib3.exceptions.HTTPError as e:
  print(e) # Expect this to be reached but isn't

except Exception as e:
  print(e) # This also isn't hit telling me an exception isn't thrown at all
</code></pre>
<p>I've purposefully malformed my request to throw a 400 and expected <code>urllib3</code> to handle the request similarly to <code>urllib</code>:</p>
<pre class=""lang-py prettyprint-override""><code>from urllib import request, error
...
try:
  response = request.urlopen(request, data=encoded_data)
  content = response.read()

except error.HTTPError as e:
  print(e) # This is hit in the case of a 400+ response
</code></pre>
",0,1707226759,python;urllib3,False,67,0,1707261641,https://stackoverflow.com/questions/77948114/urllib3-not-throwing-exception-on-400-bad-request-response-from-endpoint
24628866,Why does my program hang after urllib3 logs Starting new HTTPS connection?,"<p>I am trying to diagnose an issue where some of my celery worker processes appear to hang for several minutes. I have many tasks that make several IO calls (usually to third party APIs). In any given job, I might be making several thousand requests to various APIs. I've looked at the logs and they all have on thing in common: they hang after <code>urllib3</code> makes a connection to a remote url.</p>

<p>At the end of my jobs (takes ~30 minutes), there are usually a few tasks that are hung.</p>

<p>Here is an example of the logs that I use to conclude <code>urllib3</code> is the culprit:</p>

<pre><code>Jul 08 04:46:26 app/worker.1:  [INFO/MainProcess] [???(???)] celery.worker.strategy: Received task: my_celery_task[734a49f6-bf6b-4423-9146-1c48366ba897] 
Jul 08 04:46:28 app/worker.1:  [DEBUG/Worker-11] [my_celery_task(734a49f6-bf6b-4423-9146-1c48366ba897)] src.aggregates.prospect.services.prospect_service: Beginning: Get social account data. provider_name: twitter, account_uid: some_user 
Jul 08 04:46:28 app/worker.1:  [INFO/Worker-11] [my_celery_task(734a49f6-bf6b-4423-9146-1c48366ba897)] requests.packages.urllib3.connectionpool: Starting new HTTPS connection (1): api.some_api.com 
</code></pre>

<p>And then that's it. There is nothing logged after the <code>Starting new HTTPS connection</code> statement.</p>

<p><strong>This is where I restarted the worker:</strong></p>

<pre><code>Jul 08 05:09:18 app/worker.1:  [INFO/MainProcess] [???(???)] celery.worker.strategy: Received task: my_celery_task[734a49f6-bf6b-4423-9146-1c48366ba897] 
Jul 08 05:09:19 app/worker.1:  [DEBUG/Worker-4] [my_celery_task(734a49f6-bf6b-4423-9146-1c48366ba897)] src.aggregates.prospect.services.prospect_service: Beginning: Get social account data. provider_name: twitter, account_uid: some_user 
Jul 08 05:09:19 app/worker.1:  [DEBUG/Worker-4] [my_celery_task(734a49f6-bf6b-4423-9146-1c48366ba897)] requests.packages.urllib3.connectionpool: ""GET /v2/api_call.json?username=some_user HTTP/1.1"" 403 170
Jul 08 05:09:19 app/worker.1:  [INFO/Worker-4] [my_celery_task(734a49f6-bf6b-4423-9146-1c48366ba897)] requests.packages.urllib3.connectionpool: Starting new HTTPS connection (1): api.some_api.com 
Jul 08 05:09:19 app/worker.1:  [INFO/MainProcess] [???(???)] celery.worker.job: Task my_celery_task[734a49f6-bf6b-4423-9146-1c48366ba897] succeeded in 2.265356543008238s: 32345 
</code></pre>

<p>In this case, I received a <code>403</code> status code. So that could be a potential culprit. However, I've seen in the logs that this has happened with status codes of <code>200</code> as well. </p>

<p>FWIW, I also see <code>Resetting dropped connection: api.twitter.com</code> quite often in the logs.</p>

<p>I've made sure to provide a <code>timeout</code> of 10 seconds everywhere I make a request using the <code>requests</code> library.</p>

<p>So it appears that the request is made but then just hangs. It's possible the remote servers are responding at a very slow pace and therefore the timeout never actually occurs, but I find this unlikely as my issue does not occur with just one specific domain.</p>

<p>I am using Rabbit 3.1.3  celery:3.1.11 (Cipater) kombu:3.0.16 py:3.4.0 billiard:3.3.0.17 py-amqp:1.4.5. I am using prefork.</p>

<p>I'm using requests==2.3.0.</p>

<p>So why does it appear that my tasks hang after <code>urllib3</code> logs the <code>Starting new HTTPS connection</code> statement?</p>

<p><strong>EDIT: I've added a lot of logging and provide further context below</strong></p>

<pre><code>2014-07-16T02:43:53.140381+00:00 app[worker.1]: [INFO/MainProcess/2] [???(???)] celery.worker.strategy: Received task: [973c1361-43c3-41a2-9bcd-6ef850c41fcc]
2014-07-16T02:43:56.951211+00:00 app[worker.1]: [1;34m[DEBUG/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] : twitter, account_uid: some_user[0m
2014-07-16T02:43:56.951876+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.sessions: complete: request: about to get request
2014-07-16T02:43:56.952005+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.sessions: begin: request: about to prep request
2014-07-16T02:43:56.952897+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.sessions: complete: request: about to prep request
2014-07-16T02:43:56.954133+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.adapters: begin: send: about to get response not chunked
2014-07-16T02:43:56.954249+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connectionpool: begin: urlopen: about to get a conn from the pool
2014-07-16T02:43:56.954360+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connectionpool: begin: _get_conn: about to get a conn from the pool
2014-07-16T02:43:56.954482+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connectionpool: complete: _get_conn: about to get a conn from the pool
2014-07-16T02:43:56.954587+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connectionpool: begin: returning conn or self new conn
2014-07-16T02:43:56.951725+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.sessions: begin: request: about to get request
2014-07-16T02:43:56.954692+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connectionpool: Starting new HTTPS connection (1): api.fullcontact.com
2014-07-16T02:43:56.954817+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connectionpool: begin: create connection class
2014-07-16T02:43:56.954948+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connectionpool: complete: just about to create connection class
2014-07-16T02:43:56.955062+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connectionpool: complete: returning conn or self new conn
2014-07-16T02:43:56.955166+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connectionpool: complete: urlopen: about to get a conn from the pool
2014-07-16T02:43:56.955290+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connectionpool: begin: _make_request: about to call request
2014-07-16T02:43:56.955396+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: begin: httpconection: send req
2014-07-16T02:43:56.953410+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.sessions: begin: request: about to send
2014-07-16T02:43:56.953554+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.adapters: begin: send: about to get conn
2014-07-16T02:43:56.953986+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.adapters: complete: send: about to get conn
2014-07-16T02:43:56.956014+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: begin: httpconection: _send_output
2014-07-16T02:43:56.955517+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: begin: httpconection: put req
2014-07-16T02:43:56.955715+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: completed: httpconection: put req
2014-07-16T02:43:56.956146+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: begin: httpconection: send
2014-07-16T02:43:56.956309+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: begin: https verified conection: connect
2014-07-16T02:43:56.959157+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: completed: https verified : connect: set socket
2014-07-16T02:43:56.959045+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: begin: https verified : connect: set socket
2014-07-16T02:43:56.959295+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: begin: https verified : connect: resolve cert reqs
2014-07-16T02:43:56.959402+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: completed: https verified : connect: resolve cert reqs
2014-07-16T02:43:56.959508+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: begin: https verified : connect: resolve ssl ver
2014-07-16T02:43:56.959613+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: complete: https verified : connect: resolve ssl ver
2014-07-16T02:43:56.959715+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: begin: https verified : connect: ssl_wrap_socket
2014-07-16T02:43:56.959827+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.util.ssl_: begin: ssl_wrap_socket: about to get ssl context
2014-07-16T02:43:56.960012+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.util.ssl_: complete: ssl_wrap_socket: about to get ssl context
2014-07-16T02:43:56.960119+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.util.ssl_: begin: ssl_wrap_socket: load verify locations


# *******************************************
# *******************************************
# This is when I restarted the job. It processed the same exact task, same parameters, same https url, etc in under 1 second.
# *******************************************
# *******************************************



2014-07-16T03:00:21.885531+00:00 app[worker.1]: [INFO/MainProcess/2] [???(???)] celery.worker.strategy: Received task: [973c1361-43c3-41a2-9bcd-6ef850c41fcc]
2014-07-16T03:00:22.009616+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.sessions: begin: request: about to get request
2014-07-16T03:00:22.010313+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.sessions: begin: request: about to prep request
2014-07-16T03:00:22.016669+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.sessions: begin: request: about to send
2014-07-16T03:00:22.018419+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.adapters: complete: send: about to get conn
2014-07-16T03:00:22.051267+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: completed: httpconection: put req
2014-07-16T03:00:22.051744+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: begin: httpconection: send
2014-07-16T03:00:22.051879+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: begin: https verified conection: connect
2014-07-16T03:00:22.072346+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: begin: https verified : connect: set socket
2014-07-16T03:00:22.103234+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.util.ssl_: begin: ssl_wrap_socket: wrap socket with host name
2014-07-16T03:00:22.007638+00:00 app[worker.1]: [1;34m[DEBUG/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] : : twitter, account_uid: some_user[0m
2014-07-16T03:00:22.009969+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.sessions: complete: request: about to get request
2014-07-16T03:00:22.012954+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.sessions: complete: request: about to prep request
2014-07-16T03:00:22.017254+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.adapters: begin: send: about to get conn
2014-07-16T03:00:22.049101+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.adapters: begin: send: about to get response not chunked
2014-07-16T03:00:22.049241+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connectionpool: begin: urlopen: about to get a conn from the pool
2014-07-16T03:00:22.049361+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connectionpool: begin: _get_conn: about to get a conn from the pool
2014-07-16T03:00:22.049528+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connectionpool: complete: _get_conn: about to get a conn from the pool
2014-07-16T03:00:22.049639+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connectionpool: begin: returning conn or self new conn
2014-07-16T03:00:22.049756+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connectionpool: Starting new HTTPS connection (1): api.fullcontact.com
2014-07-16T03:00:22.049918+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connectionpool: begin: create connection class
2014-07-16T03:00:22.050197+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connectionpool: complete: just about to create connection class
2014-07-16T03:00:22.050318+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connectionpool: complete: returning conn or self new conn
2014-07-16T03:00:22.050425+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connectionpool: complete: urlopen: about to get a conn from the pool
2014-07-16T03:00:22.050587+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connectionpool: begin: _make_request: about to call request
2014-07-16T03:00:22.050777+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: begin: httpconection: send req
2014-07-16T03:00:22.050946+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: begin: httpconection: put req
2014-07-16T03:00:22.051546+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: begin: httpconection: _send_output
2014-07-16T03:00:22.072581+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: completed: https verified : connect: set socket
2014-07-16T03:00:22.072721+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: begin: https verified : connect: resolve cert reqs
2014-07-16T03:00:22.072911+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: completed: https verified : connect: resolve cert reqs
2014-07-16T03:00:22.073044+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: begin: https verified : connect: resolve ssl ver
2014-07-16T03:00:22.073179+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: complete: https verified : connect: resolve ssl ver
2014-07-16T03:00:22.073311+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: begin: https verified : connect: ssl_wrap_socket
2014-07-16T03:00:22.073445+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.util.ssl_: begin: ssl_wrap_socket: about to get ssl context
2014-07-16T03:00:22.073814+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.util.ssl_: complete: ssl_wrap_socket: about to get ssl context
2014-07-16T03:00:22.073979+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.util.ssl_: begin: ssl_wrap_socket: load verify locations
2014-07-16T03:00:22.092134+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.util.ssl_: complete: ssl_wrap_socket: load verify locations
2014-07-16T03:00:22.138197+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.util.ssl_: complete: ssl_wrap_socket: wrap socket with host name
2014-07-16T03:00:22.138207+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: complete: https verified : connect: ssl_wrap_socket
2014-07-16T03:00:22.138209+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: begin: https verified : connect: match hostname
2014-07-16T03:00:22.138211+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: complete: https verified : connect: match host name
2014-07-16T03:00:22.138212+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: completed: https verified conection: connect
2014-07-16T03:00:22.138214+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: begin: httpconection: send: before sock.sendall
2014-07-16T03:00:22.138743+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: completed: httpconection: send: before sock.sendall
2014-07-16T03:00:22.138748+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: completed: httpconection: send
2014-07-16T03:00:22.138796+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: completed: httpconection: _send_output
2014-07-16T03:00:22.139094+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connection: completed: httpconection: send req
2014-07-16T03:00:22.139207+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connectionpool: complete: _make_request: about to call request
2014-07-16T03:00:22.139321+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connectionpool: begin: _make_request: conn.get reponse with buffer
2014-07-16T03:00:22.139925+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connectionpool: begin: _make_request: conn.get reponse with no buffer
2014-07-16T03:00:22.288112+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.sessions: completest: about to send
2014-07-16T03:00:22.286453+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connectionpool: complete: _make_request: conn.get reponse with no buffer
2014-07-16T03:00:22.286596+00:00 app[worker.1]: [1;34m[DEBUG/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connectionpool: ""GET /v2/person.json?twitter=some_user&amp;apiKey=5eeaece3982efd1d&amp;style=dictionary HTTP/1.1"" 403 170[0m
2014-07-16T03:00:22.286720+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connectionpool: begin: urlopen: about to get response from httplib
2014-07-16T03:00:22.287050+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.connectionpool: complete: urlopen: about to get response from httplib
2014-07-16T03:00:22.287170+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.adapters: complete: send: about to get response not chunked
2014-07-16T03:00:22.287279+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.adapters: begin: send: about to build response
2014-07-16T03:00:22.287738+00:00 app[worker.1]: [INFO/Worker-9/23] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.adapters: complete: send: about to build response
2014-07-16T03:00:24.342182+00:00 app[worker.1]: [INFO/MainProcess/2] [???(???)] celery.worker.job: Task [973c1361-43c3-41a2-9bcd-6ef850c41fcc] succeeded in 2.4521394340554252s: 44508
</code></pre>

<p>The last logged entry is </p>

<pre><code>2014-07-16T02:43:56.960119+00:00 app[worker.1]: [INFO/Worker-28/99] [(973c1361-43c3-41a2-9bcd-6ef850c41fcc)] requests.packages.urllib3.util.ssl_: begin: ssl_wrap_socket: load verify locations
</code></pre>

<p>According to my log statement, <a href=""https://github.com/kennethreitz/requests/blob/master/requests/packages/urllib3/util/ssl_.py#L115"" rel=""noreferrer"">the culprit is load_verify_locations</a>.</p>

<p>This is the corresponding code in the <code>requests</code> library:</p>

<pre><code>if ca_certs:
    try:
        context.load_verify_locations(ca_certs)
    # Py32 raises IOError
    # Py33 raises FileNotFoundError
    except Exception as e:  # Reraise as SSLError
        raise SSLError(e)
</code></pre>

<p>So it appears that <code>context.load_verify_locations(ca_certs)</code> is causing this to hang. Why?</p>
",10,1404813379,python;django;celery;python-requests;urllib3,False,11735,1,1707120100,https://stackoverflow.com/questions/24628866/why-does-my-program-hang-after-urllib3-logs-starting-new-https-connection
77752712,"Libressl appears in the local/opt file, but when I run the libressl version, it does not appear on my computer","<p>I installed it with libress homebrew. It appears when I type brew list, but it does not appear in my computer's public bin folder. When I type libressl version it doesn't appear either.</p>
<p>I need to work with selenium in Pycharm. The error I get in the terminal when I run the code:</p>
<p>NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: <a href=""https://github.com/urllib3/urllib3/issues/3020"" rel=""nofollow noreferrer"">https://github.com/urllib3/urllib3/issues/3020</a></p>
<p>I have a libressl file under the /usr/local/opt folder.</p>
<pre><code>echo 'export PATH=&quot;/usr/local/opt/libressl/bin:$PATH&quot;' &gt;&gt; ~/.bashrc
source ~/.bashrc
I tried to export libress but I couldn't. When I try to install it again, it says homebrew is already installed.

UPDATED : Update: There appears to be a libressl arma file in /usr/local/bin. But when I type the libressl version terminal code, I still don't get any results. There is no file about libressl in /usr/bin either
</code></pre>
<p>.</p>
",0,1704293722,python;pycharm;openssl;urllib3;libressl,True,113,1,1706808750,https://stackoverflow.com/questions/77752712/libressl-appears-in-the-local-opt-file-but-when-i-run-the-libressl-version-it
77899300,Stop urllib following HTTP redirects in python3,"<p>I want to prevent urllib to stop follow redirects. Based on an old post, ref : <a href=""https://stackoverflow.com/questions/110498/is-there-an-easy-way-to-request-a-url-in-python-and-not-follow-redirects"">Is there an easy way to request a URL in python and NOT follow redirects?</a></p>
<p>I found the following code snippet that works, but this is in python2</p>
<pre><code>class NoRedirectHandler(urllib2.HTTPRedirectHandler):
    def http_error_302(self, req, fp, code, msg, headers):
        infourl = urllib.addinfourl(fp, headers, req.get_full_url())
        infourl.status = code
        infourl.code = code
        return infourl
    http_error_300 = http_error_302
    http_error_301 = http_error_302
    http_error_303 = http_error_302
    http_error_307 = http_error_302

opener = urllib2.build_opener(NoRedirectHandler())
urllib2.install_opener(opener)
</code></pre>
<p>I tried to convert this code to python3 and got the following error.</p>
<pre><code>class NoRedirectHandler(urllib.request.HTTPRedirectHandler):
    # alternative handler
    def http_error_300(self, req, fp, code, msg, header_list):
        data = urllib.request.addinfourl(fp, header_list, req.get_full_url())
        data.status = code
        data.code = code
        return data

    # setup aliases
    http_error_301 = http_error_300
    http_error_302 = http_error_300
    http_error_303 = http_error_300
    http_error_307 = http_error_300


urllib.request.install_opener(urllib.request.build_opener(NoRedirectHandler()))
</code></pre>
<pre><code>Traceback (most recent call last):
  File &quot;/home/script.py&quot;, in &lt;module&gt;
    res = urllib.request.urlopen(req, timeout=40)
  File &quot;/usr/lib/python3.10/urllib/request.py&quot;, line 216, in urlopen
    return opener.open(url, data, timeout)
  File &quot;/usr/lib/python3.10/urllib/request.py&quot;, line 525, in open
    response = meth(req, response)
  File &quot;/usr/lib/python3.10/urllib/request.py&quot;, line 634, in http_response
    response = self.parent.error(
  File &quot;/usr/lib/python3.10/urllib/request.py&quot;, line 557, in error
    result = self._call_chain(*args)
  File &quot;/usr/lib/python3.10/urllib/request.py&quot;, line 496, in _call_chain
    result = func(*args)
  File &quot;/home/script.py&quot;,i n http_error_300
    data.status = code
AttributeError: can't set attribute 'status'
</code></pre>
<p>How do I make this work in python3?</p>
",0,1706526840,python;urllib2;urllib3,False,60,1,1706606423,https://stackoverflow.com/questions/77899300/stop-urllib-following-http-redirects-in-python3
76476253,Persisting spaCy import error: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+,"<p>I realize this is a recently common error, but none of the solutions I have found online helped me. I am trying to work with spaCy in Jupyter notebook and VScode on a Mac OS , but every time I try to import spacy, I get the following error:</p>
<pre><code>NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
</code></pre>
<p>I tried</p>
<pre><code>brew install openssl@1.1

</code></pre>
<p>This didn't work so I then tried to downgrade urllib3 using</p>
<pre><code>pip install urllib3==1.26.6 

</code></pre>
<p>didn't work either. I tried using a virtual environment, also didn't work. I've been scouring the web trying to find a solution but nothing seems to work.</p>
",3,1686764607,python;spacy;importerror;urllib3,True,6586,2,1705414259,https://stackoverflow.com/questions/76476253/persisting-spacy-import-error-notopensslwarning-urllib3-v2-0-only-supports-ope
62095460,Python: System CA certificates not recognized by urllib3,"<p>When I try to access any HTTP website, even one of the most popular, I get a SSL warning from urllib3 module.</p>

<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; import urllib3
&gt;&gt;&gt; http = urllib3.PoolManager()
&gt;&gt;&gt; http.request(""GET"", ""https://www.google.de"")
/usr/lib/python2.7/site-packages/urllib3/connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
&lt;urllib3.response.HTTPResponse object at 0x7f5251466c90&gt;
&gt;&gt;&gt; 
</code></pre>

<p>Can somebody please help me getting this fixed?</p>

<p>Unfortunately I have to use a API that is apparently using urllib3 under the hood to do the actual REST calls.</p>

<p>So I have to get it fixed w/o avoiding urllib3 module.
I've already checked the ca certificates using <code>ssl.SSLContext.get_ca_certs()</code> which contains the CA certificate.
Doing the same with curl or openssl, works without any verification warnings.</p>

<p>Thanks in advance.</p>
",3,1590791186,python;ssl;urllib3,True,2951,2,1704999743,https://stackoverflow.com/questions/62095460/python-system-ca-certificates-not-recognized-by-urllib3
64284595,obtain the urllib.requests certificate bundle used with urlopen,"<p>I am on windows, and I am seeing a difference between urllib.requests.urlopen and requests package when making calls to the same site.</p>
<p>When I perform the following:</p>
<pre class=""lang-py prettyprint-override""><code>import urllib.request
f = urllib.request.urlopen('https://&lt;domain certificated server&gt; ')


</code></pre>
<p>I can reach my server no problems.</p>
<p>When I do:</p>
<pre class=""lang-py prettyprint-override""><code>import requests
f = requests.get('https://&lt;domain certificated server&gt; ').text
</code></pre>
<p>I am getting an SSL Certificate error.  I know this is caused by <code>certifi</code>.  So my question is this, how can I leverage whatever Python code is doing and use that over <code>certifi</code> in requests?</p>
",2,1602263999,python;python-3.x;python-requests;urllib;urllib3,True,1907,3,1704999565,https://stackoverflow.com/questions/64284595/obtain-the-urllib-requests-certificate-bundle-used-with-urlopen
77784998,Python code to print negotiated tls version at both client and server side during TLS communication,"<p>We can configure supported range of tls versions(at server). During the client to server TLS communication I need to print negotiated TLS version of communication, print this information both at the server and client side logs.</p>
<p>Here I have tried to give sample code(not a running code) from the client side, urllib3 is the client side module used to trigger URL corresponding to server.
Supports both TLSv1_2 and TLSv1_3 versions.</p>
<pre><code>self.__connection = ProxyManager(proxy_url=proxy_url,
                                         proxy_headers=self.__get_proxy_headers(),
                                         proxy_ssl_context=self.__get_proxy_ssl_context(proxy_url),
                                         cert_file=cert_file,
                                         key_file=cert_key,
                                         key_password=key_pwd,
                                         cert_reqs=cert_reqs,
                                         ca_certs=ca_cert,
                                         timeout=self._get_timeout(connect_timeout=self.__connect_timeout,
                                                                   read_timeout=self.__read_timeout),
                                         ssl_context=self.__get_ssl_context(),
                                         retries=False)
                                         
def __get_ssl_context():
    ssl_context = create_urllib3_context(ssl_version=ssl.PROTOCOL_TLS_CLIENT,
                                     
    ciphers=constants.SSL_TLS_CIPHER_SUITE)
    ssl_context.set_ecdh_curve(constants.SSL_TLS_CURVE)
    ssl_context.minimum_version = ssl.TLSVersion.TLSv1_2
    return ssl_context
</code></pre>
<p>Here is the server side code where SSL context is created and assigned to cherrypy server.</p>
<pre><code>cheroot.server.ssl_adapters['ssl-password-adapter'] = SSLPasswordAdapter

class SSLPasswordAdapter(BuiltinSSLAdapter):
   def __init__(self, certificate, private_key, certificate_chain=None, ciphers=None):

    self.certificate = certificate
    self.private_key = private_key
    self.certificate_chain = certificate_chain
    self.ciphers = ciphers

    self.context = ssl.create_default_context(
        purpose=ssl.Purpose.CLIENT_AUTH,
        cafile=certificate_chain,
    )

    self.context.minimum_version = ssl.TLSVersion.TLSv1_2
    self.context.load_cert_chain(certificate, private_key, self._password)
    
    if self.ciphers is not None:
        self.context.set_ciphers(ciphers)
    else:
        self.context.set_ciphers(constants.SSL_TLS_CIPHER_SUITE)
        self.context.set_ecdh_curve(constants.SSL_TLS_CURVE)
</code></pre>
<p>Now, expectation is when server URL is triggered from the client side using urllib3 and it reaches the server side, negotiated TLS version has to be printed on both client and server side.</p>
",0,1704785556,python;ssl;client;cherrypy;urllib3,False,43,0,1704962815,https://stackoverflow.com/questions/77784998/python-code-to-print-negotiated-tls-version-at-both-client-and-server-side-durin
77797152,ImportError: urllib3 v2.0 only supports OpenSSL 1.1.1+,"<p>I am seeing this error when attempting to launch a jupyter notebook from the terminal.</p>
<pre><code>Error loading server extension jupyterlab
    Traceback (most recent call last):
      File &quot;/Users/kevalshah/myvenv/lib/python3.7/site-packages/notebook/notebookapp.py&quot;, line 2047, in init_server_extensions
        mod = importlib.import_module(modulename)
      File &quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
        return _bootstrap._gcd_import(name[level:], package, level)
      File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import
      File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load
      File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked
      File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
      File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
      File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
      File &quot;/Users/kevalshah/myvenv/lib/python3.7/site-packages/jupyterlab/__init__.py&quot;, line 7, in &lt;module&gt;
        from .handlers.announcements import (  # noqa
      File &quot;/Users/kevalshah/myvenv/lib/python3.7/site-packages/jupyterlab/handlers/announcements.py&quot;, line 15, in &lt;module&gt;
        from jupyterlab_server.translation_utils import translator
      File &quot;/Users/kevalshah/myvenv/lib/python3.7/site-packages/jupyterlab_server/__init__.py&quot;, line 5, in &lt;module&gt;
        from .app import LabServerApp
      File &quot;/Users/kevalshah/myvenv/lib/python3.7/site-packages/jupyterlab_server/app.py&quot;, line 14, in &lt;module&gt;
        from .handlers import LabConfig, add_handlers
      File &quot;/Users/kevalshah/myvenv/lib/python3.7/site-packages/jupyterlab_server/handlers.py&quot;, line 18, in &lt;module&gt;
        from .listings_handler import ListingsHandler, fetch_listings
      File &quot;/Users/kevalshah/myvenv/lib/python3.7/site-packages/jupyterlab_server/listings_handler.py&quot;, line 8, in &lt;module&gt;
        import requests
      File &quot;/Users/kevalshah/myvenv/lib/python3.7/site-packages/requests/__init__.py&quot;, line 43, in &lt;module&gt;
        import urllib3
      File &quot;/Users/kevalshah/myvenv/lib/python3.7/site-packages/urllib3/__init__.py&quot;, line 42, in &lt;module&gt;
        &quot;urllib3 v2.0 only supports OpenSSL 1.1.1+, currently &quot;
    ImportError: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'OpenSSL 1.1.0h  27 Mar 2018'. See: https://github.com/urllib3/urllib3/issues/2168
</code></pre>
<p>I have tried to upgrade and installed the following versions:</p>
<p><code>requests==2.31.0</code></p>
<p><code>urllib3==2.0.7</code></p>
<p>how do I resolve this issue? Do I need to upgrade my systems OpenSSL?</p>
",0,1704934984,python;python-requests;openssl;importerror;urllib3,False,593,1,1704936322,https://stackoverflow.com/questions/77797152/importerror-urllib3-v2-0-only-supports-openssl-1-1-1
76187256,"ImportError: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the &#39;ssl&#39; module is compiled with LibreSSL 2.8.3","<p>After <code>pip install openai</code>, when I try to <code>import openai</code>, it shows this error:</p>
<blockquote>
<p>the 'ssl' module of urllib3 is compile with LibreSSL not OpenSSL</p>
</blockquote>
<p>I just followed a tutorial on a project about using API of OpenAI. But when I get to the first step which is the install and import OpenAI, I got stuck. And I tried to find the solution for this error but I found nothing.</p>
<p>Here is the message after I try to import OpenAI:</p>
<pre class=""lang-none prettyprint-override""><code>Python 3.9.6 (default, Mar 10 2023, 20:16:38)
[Clang 14.0.3 (clang-1403.0.22.14.1)] on darwin
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.

&gt;&gt;&gt; import openai

Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/Users/yule/Library/Python/3.9/lib/python/site-packages/openai/__init__.py&quot;, line 19, in &lt;module&gt;
    from openai.api_resources import (
  File &quot;/Users/mic/Library/Python/3.9/lib/python/site-packages/openai/api_resources/__init__.py&quot;, line 1, in &lt;module&gt;
    from openai.api_resources.audio import Audio  # noqa: F401
  File &quot;/Users/mic/Library/Python/3.9/lib/python/site-packages/openai/api_resources/audio.py&quot;, line 4, in &lt;module&gt;
    from openai import api_requestor, util
  File &quot;/Users/mic/Library/Python/3.9/lib/python/site-packages/openai/api_requestor.py&quot;, line 22, in &lt;module&gt;
    import requests
  File &quot;/Users/mic/Library/Python/3.9/lib/python/site-packages/requests/__init__.py&quot;, line 43, in &lt;module&gt;
    import urllib3
  File &quot;/Users/mic/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py&quot;, line 38, in &lt;module&gt;
    raise ImportError(
ImportError: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with LibreSSL 2.8.3. See: https://github.com/urllib3/urllib3/issues/2168
</code></pre>
<p>I tried to <code>--upgrade</code> the <code>urllib3</code>, but it is still not working. The result is:</p>
<pre class=""lang-none prettyprint-override""><code>pip3 install --upgrade urllib3
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: urllib3 in ./Library/Python/3.9/lib/python/site-packages (2.0.2)
</code></pre>
",127,1683349902,python;openai-api;urllib3,True,272477,15,1704842074,https://stackoverflow.com/questions/76187256/importerror-urllib3-v2-0-only-supports-openssl-1-1-1-currently-the-ssl-modu
38794015,Python&#39;s requests &quot;Missing dependencies for SOCKS support&quot; when using SOCKS5 from Terminal,"<p>I'm trying to interact with an API from my Python 2.7 shell using a package that relies on Python's requests. Thing is the remote address is blocked by my network (university library). </p>

<p>So to speak to the API I do the following:</p>

<pre><code>~$ ssh -D 8080 name@myserver.com
</code></pre>

<p>And then, in new terminal, in local computer:</p>

<pre><code>~$ export http_proxy=socks5://127.0.0.1:8080 https_proxy=socks5://127.0.0.1:8080
</code></pre>

<p>Then I run the program in Python console but fails:</p>

<pre><code>~$ python
&gt;&gt;&gt; import myscript
&gt;&gt;&gt; id = '1213'
&gt;&gt;&gt; token = 'jd87jd9'
&gt;&gt;&gt; connect(id,token)

File ""/home/username/.virtualenvs/venv/local/lib/python2.7/site-packages/requests/sessions.py"", line 518, in post
    return self.request('POST', url, data=data, json=json, **kwargs)
  File ""/home/username/.virtualenvs/venv/local/lib/python2.7/site-packages/requests/sessions.py"", line 475, in request
    resp = self.send(prep, **send_kwargs)
  File ""/home/username/.virtualenvs/venv/local/lib/python2.7/site-packages/requests/sessions.py"", line 585, in send
    r = adapter.send(request, **kwargs)
  File ""/home/username/.virtualenvs/venv/local/lib/python2.7/site-packages/requests/adapters.py"", line 370, in send
    conn = self.get_connection(request.url, proxies)
  File ""/home/username/.virtualenvs/venv/local/lib/python2.7/site-packages/requests/adapters.py"", line 273, in get_connection
    proxy_manager = self.proxy_manager_for(proxy)
  File ""/home/username/.virtualenvs/venv/local/lib/python2.7/site-packages/requests/adapters.py"", line 169, in proxy_manager_for
    **proxy_kwargs
  File ""/home/username/.virtualenvs/venv/local/lib/python2.7/site-packages/requests/adapters.py"", line 43, in SOCKSProxyManager
    raise InvalidSchema(""Missing dependencies for SOCKS support."")
requests.exceptions.InvalidSchema: Missing dependencies for SOCKS support.
</code></pre>

<p>This excerpt is from the <a href=""http://docs.python-requests.org/en/master/_modules/requests/adapters/"" rel=""noreferrer"">adapters.py requests module</a>:</p>

<pre><code>&gt; try:
&gt;     from .packages.urllib3.contrib.socks import SOCKSProxyManager except ImportError:
&gt;     def SOCKSProxyManager(*args, **kwargs):
&gt;         raise InvalidSchema(""Missing dependencies for SOCKS support."")
</code></pre>

<p>Now problem seems to be originated in urllib3's SOCKSProxyManager.</p>

<p>So I read you can use SOCKSProxyManager with SOCKS5 if you have install <a href=""https://pypi.python.org/pypi/PySocks"" rel=""noreferrer"">PySocks</a> or you do a <em>pip install urllib3[socks]</em></p>

<p>Alas, I tried both PySocks and urllib3 with Socks without any success.</p>

<p>Any idea of another workaround?</p>

<p><strong>EDIT:</strong></p>

<p>I also tried <em>pip install requests[socks]</em> (that's requests 2.10.0 with Socks support) and I am getting this:</p>

<pre><code>  File ""/home/username/.virtualenvs/venv/local/lib/python2.7/site-packages/requests/adapters.py"", line 467, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: SOCKSHTTPSConnectionPool(host='api-server.com', port=443): Max retries exceeded with url: /auth (Caused by NewConnectionError('&lt;requests.packages.urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x95c7ccc&gt;: Failed to establish a new connection: SOCKS5 proxy server sent invalid data',))
</code></pre>
",93,1470414956,python;python-2.7;python-requests;socks;urllib3,True,168389,17,1704661878,https://stackoverflow.com/questions/38794015/pythons-requests-missing-dependencies-for-socks-support-when-using-socks5-fro
77702434,Python-requests-urllib3-connection timeout error,"<p>Hi Am trying to run one script and receiving the following error</p>
<blockquote>
<p>A request error occurred: HTTPConnectionPool(host='10.88.64.79', port=8080):nError('&lt;urllib3.connection.HTTPConnection object at 0x7f1d653b65b0&gt;: Failed to establish a new connection: [Errno 110] Connection timed out'))</p>
</blockquote>
<p>But when I try to run this request through postman, am getting the output for this url. Not sure why it is not working on the linux server. PFB my code. Could you please any one help me on this issue? Thanks in advance.</p>
<pre><code>import  requests
def findip_ip_api(subnet_ip, subnet_mask):
    try:
        findip_url = &quot;http://x.x.x.x:8080/findip/ip/{}?len={}&quot;.format(subnet_ip, subnet_mask)
        findip_response = requests.request(&quot;GET&quot;, findip_url, verify=False)
        ip_api_data = findip_response.json()['data']
    except requests.exceptions.HTTPError as e:
        raise SystemExit(&quot;An HTTP error occurred: {}&quot;.format(e))
    except requests.exceptions.RequestException as e:
        raise SystemExit(&quot;A request error occurred: {}&quot;.format(e))
    else:
        print(ip_api_data)

findip_ip_api('1.1.1.4','29')
</code></pre>
<p>Am trying to get the IP allocation details from the server, so that I could play with them. The response should like following (The below output received from Postman request)</p>
<pre><code>&quot;data&quot;: [
        {
            &quot;address&quot;: &quot;x.x.x.x&quot;,
            &quot;description&quot;: &quot;interface \&quot;Vlanxx\&quot; (VRF &lt;vrf name&gt;) standby 75&quot;,
            &quot;host&quot;: &quot;&lt;hostname&gt;&quot;,
            &quot;isDecommed&quot;: 1,
            &quot;length&quot;: 29
        },
        
    ],
    &quot;metadata&quot;: {
        &quot;datasize&quot;: 55363258,
        &quot;lastRefresh&quot;: 1702771027,
        &quot;lastRefreshTime&quot;: &quot;Sat 16 Dec 2023 23:57:07 GMT&quot;,
        &quot;lastUpdate&quot;: 1674721862,
        &quot;lastUpdateTime&quot;: &quot;Thu 26 Jan 2023 08:31:02 GMT&quot;,
        &quot;lookup&quot;: &quot;1.1.1.4/29&quot;,
        &quot;recordCount&quot;: 720907,
        &quot;resultCount&quot;: 8
    }
}
</code></pre>
",0,1703232664,python;urllib3,False,84,0,1703242202,https://stackoverflow.com/questions/77702434/python-requests-urllib3-connection-timeout-error
77577631,urlopen error EOF occurred in violation of protocol (_ssl.c:1007),"<p>The code is working fine on localhost but having an issue when deployed over Azure Linux system:</p>
<pre><code>from bs4 import BeautifulSoup
from urllib.request import Request, urlopen
from urllib.parse import quote

url = (f&quot;https://www.wikipedia.org/&quot;)
req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})
webpage = urlopen(req).read()
soup = BeautifulSoup(webpage, &quot;html.parser&quot;)

panel_data = soup.find('div')

print(panel_data)
</code></pre>
<p>Over Azure Linux system I'm receiving this error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.10/site-packages/flask/app.py&quot;, line 2190, in wsgi_app
    response = self.full_dispatch_request()
  File &quot;/usr/local/lib/python3.10/site-packages/flask/app.py&quot;, line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File &quot;/usr/local/lib/python3.10/site-packages/flask/app.py&quot;, line 1484, in full_dispatch_request
    rv = self.dispatch_request()
  File &quot;/usr/local/lib/python3.10/site-packages/flask/app.py&quot;, line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
  File &quot;/flask-doker/app.py&quot;, line 41, in scrap
    content = scrappy()
  File &quot;/flask-doker/scrapper.py&quot;, line 10, in scrappy
    webpage = urlopen(req).read()
  File &quot;/usr/local/lib/python3.10/urllib/request.py&quot;, line 216, in urlopen
    return opener.open(url, data, timeout)
  File &quot;/usr/local/lib/python3.10/urllib/request.py&quot;, line 519, in open
  File &quot;/usr/local/lib/python3.10/urllib/request.py&quot;, line 536, in _open
    result = self._call_chain(self.handle_open, protocol, protocol +
  File &quot;/usr/local/lib/python3.10/urllib/request.py&quot;, line 496, in _call_chain
    result = func(*args)
  File &quot;/usr/local/lib/python3.10/urllib/request.py&quot;, line 1391, in https_open
    return self.do_open(http.client.HTTPSConnection, req,
  File &quot;/usr/local/lib/python3.10/urllib/request.py&quot;, line 1351, in do_open
    raise URLError(err)
urllib.error.URLError: &lt;urlopen error EOF occurred in violation of protocol (_ssl.c:1007)&gt;
</code></pre>
<p>Expecting it to work on Azure Linux as good as it works locally</p>
",0,1701339724,python;urllib;urllib3,False,206,0,1702291666,https://stackoverflow.com/questions/77577631/urlopen-error-eof-occurred-in-violation-of-protocol-ssl-c1007
27981545,Suppress InsecureRequestWarning: Unverified HTTPS request is being made in Python2.6,"<p>I am writing scripts in Python2.6 with use of <a href=""https://github.com/vmware/pyvmomi"">pyVmomi</a> and while using one of the connection methods:</p>

<pre><code>service_instance = connect.SmartConnect(host=args.ip,
                                        user=args.user,
                                        pwd=args.password)
</code></pre>

<p>I get the following warning:</p>

<pre><code>/usr/lib/python2.6/site-packages/requests/packages/urllib3/connectionpool.py:734: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html
  InsecureRequestWarning)
</code></pre>

<p>What's interesting is that I do not have urllib3 installed with pip (but it's there in <em>/usr/lib/python2.6/site-packages/requests/packages/urllib3/</em>).</p>

<p>I have tried as suggested <a href=""https://urllib3.readthedocs.org/en/latest/security.html"">here</a></p>

<pre><code>import urllib3
...
urllib3.disable_warnings()
</code></pre>

<p>but that didn't change anything.</p>
",567,1421402839,python;python-2.6;suppress-warnings;urllib3;pyvmomi,True,860647,15,1701818889,https://stackoverflow.com/questions/27981545/suppress-insecurerequestwarning-unverified-https-request-is-being-made-in-pytho
47489456,Downloading list of urls/files using loop - python,"<p>I need to download approximately 1000 file/url and it will be hard to download them manually.</p>

<p>I tried to put the urls in a list and loop through the list but it I think my code overwrite the previous files and keep only the last item in the list</p>

<p>Here is my code</p>

<pre><code>#!/usr/bin/env python

import urllib3
http = urllib3.PoolManager()

urls = [""http://url1.nt.gz"" , ""http://url2.nt.gz"" , ""http://url3.nt.gz""]
N =1; // counter helps me to rename the downloaded files
print ""downloading with urllib""
for url in urls
 r = http.request('GET',url)
 Name =str(N+1) // each time increment the counter by one 
 with open(""file""+Name+"".nt.gz"", ""wb"") as fcont:
                fcont.write(r.data)
</code></pre>

<p>Any suggestions?</p>
",2,1511635858,python;urllib3,True,5766,2,1700488522,https://stackoverflow.com/questions/47489456/downloading-list-of-urls-files-using-loop-python
77514885,I am trying to pass some json data from my django web app to a server running at localhost. Works fine at localhost but fails in production,"<p>I have a server running at 127.0.0.1:16732.
I am passing json data to this server from my django app: <code>response=requests.post('http://127.0.0.1:16732/api/v2/requests', auth=auth, json=task)</code>
Works fine when I do it from my test python server on localhost.
But when I try to do the same from production server, it fails &amp; give the following error:</p>
<p>&quot;HTTPConnectionPool(host='localhost', port=16732): Max retries exceeded with url: /api/v2/requests (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f3e335006d0&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))&quot;</p>
",0,1700474095,python;django;request;urllib3,False,58,0,1700474095,https://stackoverflow.com/questions/77514885/i-am-trying-to-pass-some-json-data-from-my-django-web-app-to-a-server-running-at
77489504,How to customize retry/redirect logic in urllib3&#39;s Retry class,"<p>I have a python-based library that serves as an enabling layer for processing REST API requests/responses (<a href=""https://github.com/IBM/python-sdk-core"" rel=""nofollow noreferrer"">https://github.com/IBM/python-sdk-core</a>).  Under the covers, it uses the requests and urllib3 libraries but hides these lower-level details from the user for the most part.
The library uses the urllib3 Retry class to enable automatic retries.  The Retry class supports the <code>remove_headers_on_redirect</code> parameter to specify the headers that should be removed from a redirected request.   Unfortunately, this is an all-or-nothing proposition and I'd like to have a bit more control over the removal/inclusion of these &quot;safe&quot; headers.  For example, I would want to include the Authorization header in a redirected request ONLY if the original request URL's host and the redirected URL's host are both in a &quot;trusted domain&quot;.  So, the simple configuration of the <code>remove_headers_on_redirect</code> parameter is not quite sufficient as the decision needs to be conditional in my situation.</p>
<p>Is there any way for me to implement such a conditional inclusion of safe headers other than perhaps subclassing the urllib3 Retry class?  Ideally, the urllib3 Retry class would support some sort of callback hook to allow me to include my custom logic into the process of retrying and/or redirecting a request.</p>
<p>Note: I have a similar question about the Java okhttp library, here: <a href=""https://stackoverflow.com/questions/77483238/okhttp-redirect-with-authorization-header-included"">okhttp redirect with Authorization header included?</a></p>
",0,1700066573,python;python-requests;urllib3,False,83,0,1700066573,https://stackoverflow.com/questions/77489504/how-to-customize-retry-redirect-logic-in-urllib3s-retry-class
77404283,How to debug a &quot;Connection reset by peer&quot; issue in urllib3 when upgrading from v1 to v2,"<p>Does anyone have any pointers or advice on how to get to the bottom of a &quot;Connection reset by peer&quot; issue in Python with urllib3?</p>
<p>I have been using using urllib3 version 1 for year but have a project that when I swap from urllib3==1.26.18 to urllib3&gt;=2.0.0 my requests that were working now fail.</p>
<p>In the real example the requests use the requests package with HTTPBasicAuth but even if I strip that out and use something as basic as the below I hit the issue.</p>
<pre><code>import urllib3
c = urllib3.PoolManager()
url = &quot;HTTPS-endpoint-here&quot;
r = c.request('GET', url)
</code></pre>
<p>The problem appears to be with one site but having run it through SSL labs I've confirmed that the certificates on the site are installed okay and that it supports TLS 1.2 as well as TLS 1.1. It doesn't support TLS1.3 but that's okay as urlib3 v2 supports 1.2 by default.</p>
<p>I've tested using Python 3.11.1 and 3.11.2 on Ubuntu 20.04 and 22.04.</p>
<p>I didn't have pyopenssl installed but I've installed that and no luck</p>
<p>If I run <code>openssl version</code> I can see I'm using <code>OpenSSL 3.0.2 15 Mar 2022 (Library: OpenSSL 3.0.2 15 Mar 2022)</code></p>
<p><code>python -m requests.help</code></p>
<p>currently returns</p>
<pre><code>{
  &quot;chardet&quot;: {
    &quot;version&quot;: null
  },
  &quot;charset_normalizer&quot;: {
    &quot;version&quot;: &quot;2.0.10&quot;
  },
  &quot;cryptography&quot;: {
    &quot;version&quot;: &quot;&quot;
  },
  &quot;idna&quot;: {
    &quot;version&quot;: &quot;3.3&quot;
  },
  &quot;implementation&quot;: {
    &quot;name&quot;: &quot;CPython&quot;,
    &quot;version&quot;: &quot;3.11.1&quot;
  },
  &quot;platform&quot;: {
    &quot;release&quot;: &quot;5.15.0-88-generic&quot;,
    &quot;system&quot;: &quot;Linux&quot;
  },
  &quot;pyOpenSSL&quot;: {
    &quot;openssl_version&quot;: &quot;&quot;,
    &quot;version&quot;: null
  },
  &quot;requests&quot;: {
    &quot;version&quot;: &quot;2.31.0&quot;
  },
  &quot;system_ssl&quot;: {
    &quot;version&quot;: &quot;1010106f&quot;
  },
  &quot;urllib3&quot;: {
    &quot;version&quot;: &quot;2.0.7&quot;
  },
  &quot;using_charset_normalizer&quot;: true,
  &quot;using_pyopenssl&quot;: false
}
</code></pre>
<p>As I can make requests to other HTTPS endpoints include HTTPBin with basicauth the issues must be with the endpoint I'm requesting (a clients) but I'm at a bit of a loss as to what to look at next to try to work out what's wrong.</p>
<p>If anyone has any suggestions I would appreciate it.</p>
<p><strong>Update</strong></p>
<p>Here is an example that can connect with urlib3 version 1.26.18 but if you then upgrade to 2.0.7 you immediately receive the &quot;Connection reset by peer&quot; exception</p>
<pre><code>import urllib3
c = urllib3.PoolManager()
url = &quot;https://api.hicargo.com/BoxDocs/UK/JobDocs/HE630819/99054-77 4500187758.pdf&quot;
r = c.request(&quot;GET&quot;, url)
print(r.status)
</code></pre>
<p>That end-point requires authentication so a &quot;HTTP 401&quot; is the expected response.</p>
",0,1698857578,python;ssl;https;urllib3,False,195,0,1699375561,https://stackoverflow.com/questions/77404283/how-to-debug-a-connection-reset-by-peer-issue-in-urllib3-when-upgrading-from-v
77385976,"Python: urllib3 Library, unable to make requests. Error: urllib3.exceptions.MaxRetryError:","<p>I am new to Python and experiencing issues with the urllib3 library when running on a linux environment. The problem is that the library is unable to make GET requests to any URL's and the requests were tested with other libraries that do similar stuff. The error that I am getting is</p>
<pre><code>urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='google.com', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(\&lt;urllib3.connection.HTTPSConnection object at 0x7f43c27f6cd0\&gt;, 'Connection to google.com timed out. (connect timeout=3)'))
</code></pre>
<p>As mentioned before, using any other package such as requests or wget performs the request well.
Using a PoolManager instead or changing the retry configs will just keep the script running indefinitely.</p>
<p>The code:</p>
<pre><code>import urllib3

try: 
    response = urllib3.request('GET', 'https://google.com')
    if response.status == 200:
        print(response.data)
    else:
        print(response.status)
except Exception as e:
    print(e)
</code></pre>
",0,1698633875,python;request;urllib3,False,241,0,1698783090,https://stackoverflow.com/questions/77385976/python-urllib3-library-unable-to-make-requests-error-urllib3-exceptions-maxr
77311174,urllib.error.URLError: &lt;urlopen error [Errno 11001] getaddrinfo failed issue,"<p>i want to  learn regression problem  related to image processing techniques, therefore i found following tutorial : <a href=""https://github.com/Thundertung/Book-Price-regression-CNNs/blob/main/Judging%20a%20book%20by%20its%20cover.ipynb"" rel=""nofollow noreferrer"">Image regression</a></p>
<p>when i  was  reading following code :</p>
<pre><code>import numpy as np #for numerical computations
import pandas as pd #for dataframe operations
from matplotlib import pyplot as plt #for viewing images and plots
#So that Matplotlib plots don't open in separate windows outside the notebook
import urllib #For fetching data from Web URLs
import cv2   #For image processing
import urllib3
from sklearn.preprocessing import LabelEncoder    #For encoding categorical variables
from sklearn.model_selection import train_test_split #For splitting of data
#All tensorflow utilities for creating, training and working with a CNN
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D, MaxPool2D, BatchNormalization
from keras.layers import Activation, Dropout, Flatten, Dense
from keras.losses import categorical_crossentropy
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint
from keras.models import load_model
def show_image_from_url(image_url):
  response = urllib.request.urlopen(image_url)
  image = np.asarray(bytearray(response.read()), dtype=&quot;uint8&quot;)
  image_bgr = cv2.imdecode(image, cv2.IMREAD_COLOR)
  image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
  plt.imshow(image_rgb), plt.axis('off')
data =pd.read_csv(&quot;https://raw.githubusercontent.com/Thundertung/Book-Price-regression-CNNs/main/main_dataset.csv&quot;)
print(data.head())
# print(data['price'])
# print(data.columns)
plt.figure()
show_image_from_url(data['image'].loc[10])
plt.show()
</code></pre>
<p>i am  getting this error :</p>
<pre><code> File &quot;C:\Users\User\AppData\Local\Programs\Python\Python311\Lib\http\client.py&quot;, line 945, in connect
    self.sock = self._create_connection(
                ^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\User\AppData\Local\Programs\Python\Python311\Lib\socket.py&quot;, line 827, in create_connection
    for res in getaddrinfo(host, port, 0, SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\User\AppData\Local\Programs\Python\Python311\Lib\socket.py&quot;, line 962, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno 11001] getaddrinfo failed
</code></pre>
<p>if i  try to  open link alone, i am also getting error, is this erro related to fact that page  does not exist in reality or i should change some parameters? thanks in advance</p>
",0,1697564720,python;urllib;urllib3,False,130,0,1697564720,https://stackoverflow.com/questions/77311174/urllib-error-urlerror-urlopen-error-errno-11001-getaddrinfo-failed-issue
77303136,Python UrlLib3 - Can&#39;t download file due to SSL Error even when ssl verification is disabled,"<p>I am unable to download a file using this piece of code:</p>
<pre class=""lang-py prettyprint-override""><code>import requests

response = requests.get('https://download.inep.gov.br/informacoes_estatisticas/indicadores_educacionais/taxa_transicao/tx_transicao_municipios_2019_2020.zip', stream=True, verify=False)
with open('tx_transicao_municipios_2019_2020.zip', 'wb') as f:
    for chunk in response.iter_content(chunk_size=1024): 
        if chunk: 
            f.write(chunk)
</code></pre>
<p>I keep getting this error even when <strong>verify=False</strong> is setted:</p>
<blockquote>
<p>urllib3.exceptions.SSLError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1006)</p>
</blockquote>
<p>When using Chrome, I am able to download the file.</p>
<p>Using <strong>verify=certifi.where()</strong> doesn't work also.</p>
<h2>Environment</h2>
<ul>
<li>Windows 10 Enterprise 22H2 (19045.3448);</li>
<li>Python v3.11.5;</li>
<li>OpenSSL v3.0.9;</li>
<li>Urllib3 v2.0.6;</li>
<li>Requests v2.31.0;</li>
<li>Certifi v2023.7.22;</li>
</ul>
<p>Also tried in MacOS Catalina (10.15) and MacOS Big Sur (11.x) with no success.</p>
<p>What am I doing wrong here?</p>
",1,1697469329,python;python-requests;openssl;urllib3;certifi,True,456,2,1697480950,https://stackoverflow.com/questions/77303136/python-urllib3-cant-download-file-due-to-ssl-error-even-when-ssl-verification
50202238,Python (pip) - RequestsDependencyWarning: urllib3 (1.9.1) or chardet (2.3.0) doesn&#39;t match a supported version,"<p>I found several pages about this issue but none of them solved my problem.</p>
<p>Even if I do a :</p>
<pre><code>pip show
</code></pre>
<p>I get :</p>
<pre><code>/usr/local/lib/python2.7/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.9.1) or chardet (2.3.0) doesn't match a supported version!
  RequestsDependencyWarning)
Traceback (most recent call last):
  File &quot;/usr/bin/pip&quot;, line 9, in &lt;module&gt;
    load_entry_point('pip==1.5.6', 'console_scripts', 'pip')()
  File &quot;/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py&quot;, line 480, in load_entry_point
    return get_distribution(dist).load_entry_point(group, name)
  File &quot;/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py&quot;, line 2691, in load_entry_point
    return ep.load()
  File &quot;/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py&quot;, line 2322, in load
    return self.resolve()
  File &quot;/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py&quot;, line 2328, in resolve
    module = __import__(self.module_name, fromlist=['__name__'], level=0)
  File &quot;/usr/lib/python2.7/dist-packages/pip/__init__.py&quot;, line 74, in &lt;module&gt;
    from pip.vcs import git, mercurial, subversion, bazaar  # noqa
  File &quot;/usr/lib/python2.7/dist-packages/pip/vcs/mercurial.py&quot;, line 9, in &lt;module&gt;
    from pip.download import path_to_url
  File &quot;/usr/lib/python2.7/dist-packages/pip/download.py&quot;, line 22, in &lt;module&gt;
    import requests, six
  File &quot;/usr/local/lib/python2.7/dist-packages/requests/__init__.py&quot;, line 90, in &lt;module&gt;
    from urllib3.exceptions import DependencyWarning
ImportError: cannot import name DependencyWarning
</code></pre>
<p>What I did :</p>
<pre><code>pip install --upgrade chardet
</code></pre>
<p>but as explain up, it gaves me the same error.</p>
<p>so I did :</p>
<pre><code>sudo apt remove python-chardet
</code></pre>
<p>and unistalling all his dependecies.
After I reinstall it -&gt; the same :'(</p>
<p>I did the same for <code>python-pip</code>. After reinstalling it -&gt; the same. <br>
Here are the lines about <code>urllib3</code> and <code>chardet</code> versions needed :
extract of <code>/usr/local/lib/python2.7/dist-packages/requests/__init__.py</code> :</p>
<pre><code>    # Check urllib3 for compatibility.
    major, minor, patch = urllib3_version  # noqa: F811
    major, minor, patch = int(major), int(minor), int(patch)
    # urllib3 &gt;= 1.21.1, &lt;= 1.22
    assert major == 1
    assert minor &gt;= 21
    assert minor &lt;= 22

    # Check chardet for compatibility.
    major, minor, patch = chardet_version.split('.')[:3]
    major, minor, patch = int(major), int(minor), int(patch)
    # chardet &gt;= 3.0.2, &lt; 3.1.0
    assert major == 3
    assert minor &lt; 1
    assert patch &gt;= 2


# Check imported dependencies for compatibility.
try:
    check_compatibility(urllib3.__version__, chardet.__version__)
except (AssertionError, ValueError):
    warnings.warn(&quot;urllib3 ({0}) or chardet ({1}) doesn't match a supported &quot;
                  &quot;version!&quot;.format(urllib3.__version__, chardet.__version__),
                  RequestsDependencyWarning)
</code></pre>
<p>My versions are :</p>
<pre><code>ii  python-urllib3 1.9.1-3   all HTTP library with thread-safe connection pooling for Python 
ii  python-chardet  2.3.0-1  all universal character encoding detector for Python2
</code></pre>
<p>I don't have no more ideas...</p>
",114,1525625654,python;pip;python-requests;urllib3;chardet,True,226655,21,1696250789,https://stackoverflow.com/questions/50202238/python-pip-requestsdependencywarning-urllib3-1-9-1-or-chardet-2-3-0-doe
77205007,"urllib throws an exception for non 200 status codes, where as urllib3 and requests gives the proper response","<p>In the below code example <code>xyz.com/apicall</code> rest api call gives below response incase of non 200 Status code error .</p>
<pre><code>{&quot;error&quot;:&quot;&lt;Error Name&gt;&quot;,&quot;errorMessage&quot;:&quot;&lt;Error Description&gt;&quot;,&quot;host&quot;:null}
</code></pre>
<pre><code>import json
import requests
from requests.auth import HTTPBasicAuth
from urllib import request
import urllib3


url = 'xyz.com/apicall'
payload = json.dumps({
  &quot;key1&quot;: &quot;value1&quot;,
}).encode('utf-8')

print (&quot;#######Request By Requests Module###########&quot;)

headers = {'Content-Type': 'application/json',}
response= requests.post(url,headers=headers, data=payload)
print('Response Code: {}'.format(response.status_code))
print('Response Body: {}'.format(response.text))

print (&quot;#######Request By urllib###########&quot;)
req = request.Request(url)  # this will make the method &quot;POST&quot;
req.add_header(&quot;Content-Type&quot;, &quot;application/json&quot;)
try:
  response_url = request.urlopen(req, data=payload)
  print(response_url.read())
except:
  print(&quot;Error Occured in URLLIB&quot;)


print (&quot;#######Request By urllib3###########&quot;)

http = urllib3.PoolManager()
r = http.request('POST', url, headers=headers, body=payload)
resp_body = r.data.decode('utf-8')
resp_dict = json.loads(r.data.decode('utf-8'))
print('Response Code: {}'.format(r.status))
print('Response Body: {}'.format(resp_body))
print (&quot;End&quot;)
</code></pre>
<p>When I run the above code I get o/p like this.</p>
<pre><code>#######Request By Requests Module###########
Response Code: 400
Response Body: {&quot;error&quot;:&quot;&lt;Error Name&gt;&quot;,&quot;errorMessage&quot;:&quot;&lt;Error Description&gt;&quot;,&quot;host&quot;:null}
#######Request By urllib###########
Error Occured in URLLIB
#######Request By urllib3###########
Response Code: 400
Response Body: {&quot;error&quot;:&quot;&lt;Error Name&gt;&quot;,&quot;errorMessage&quot;:&quot;&lt;Error Description&gt;&quot;,&quot;host&quot;:null}
End
</code></pre>
<p>So in case of <strong>urllib</strong> I get an exception where as in urllib3 and Requests I get the actual HTTP Response. How can I replicate the same in urllib.</p>
",0,1696025617,python;rest;python-requests;urllib;urllib3,False,69,0,1696025617,https://stackoverflow.com/questions/77205007/urllib-throws-an-exception-for-non-200-status-codes-where-as-urllib3-and-reques
77094424,"openAI on mac os issue -- NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the &#39;ssl&#39; module is compiled with &#39;LibreSSL 2.8.3&#39;","<p>When I run the following code in pyCharm IDE:</p>
<pre><code>import creds
import os
import openai

os.environ[&quot;OPENAPI_KEY&quot;] = creds.API_KEY
openai.api_key = os.environ[&quot;OPENAPI_KEY&quot;]

keep_prompting = True
while keep_prompting:
    prompt = input(&quot;Please enter in your question or prompt. Type exit if done. &quot;)
    if prompt == &quot;exit&quot;:
        keep_prompting = False
    else:
        response = openai.ChatCompletion.create(
            model=&quot;text-davinci-003&quot;,
            prompt=prompt,
            max_tokens=200
        )
        print(response)
</code></pre>
<p>I get the following run-time error in the terminal window:</p>
<pre><code>/Users/studywithrue/PycharmProjects/chat/venv/bin/python /Users/studywithrue/PycharmProjects/chat/chat.py 
/Users/studywithrue/PycharmProjects/chat/venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
Please enter in your question or prompt. Type exit if done. How does a chicken hatch from an egg?
Traceback (most recent call last):
  File &quot;/Users/studywithrue/PycharmProjects/chat/chat.py&quot;, line 14, in &lt;module&gt;
    response = openai.ChatCompletion.create(
  File &quot;/Users/studywithrue/PycharmProjects/chat/venv/lib/python3.9/site-packages/openai/api_resources/chat_completion.py&quot;, line 25, in create
    return super().create(*args, **kwargs)
  File &quot;/Users/studywithrue/PycharmProjects/chat/venv/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py&quot;, line 153, in create
    response, _, api_key = requestor.request(
  File &quot;/Users/studywithrue/PycharmProjects/chat/venv/lib/python3.9/site-packages/openai/api_requestor.py&quot;, line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
  File &quot;/Users/studywithrue/PycharmProjects/chat/venv/lib/python3.9/site-packages/openai/api_requestor.py&quot;, line 700, in _interpret_response
    self._interpret_response_line(
  File &quot;/Users/studywithrue/PycharmProjects/chat/venv/lib/python3.9/site-packages/openai/api_requestor.py&quot;, line 765, in _interpret_response_line
    raise self.handle_error_response(
openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.

Process finished with exit code 1
</code></pre>
<p>the following are images of my pycharm python packages, and my current homebrew packages and pip packages.</p>
<p><a href=""https://i.stack.imgur.com/0jvvM.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0jvvM.jpg"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/WCIW3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WCIW3.png"" alt=""enter image description here"" /></a></p>
<p>I've been looking at several articles on this for the past hours and nothing works. tried re-installing urllib3 and openssl and nothing works for me on mac os. currently using a mid-2015 macbook pro and i have the latest OS.</p>
",0,1694585226,python;pycharm;openssl;urllib3;libressl,True,326,1,1694759875,https://stackoverflow.com/questions/77094424/openai-on-mac-os-issue-notopensslwarning-urllib3-v2-0-only-supports-openssl
77018145,python3 -m pip: Unverified HTTPS request is being made to host &#39;pypi.org&#39;,"<p>I am running <code>python3</code>:</p>
<pre><code>python3 --version
Python 3.9.2
</code></pre>
<p>I was going to update local <code>pip</code> packages and for that I first checked which were outdated:</p>
<pre><code>python3 -m pip list --outdated
</code></pre>
<p>This produced a warning message for each specific package (they are all the same so I am showing only a sample):</p>
<pre><code>/usr/share/python-wheels/urllib3-1.26.5-py2.py3-none-any.whl/urllib3/connectionpool.py:1015: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
/usr/share/python-wheels/urllib3-1.26.5-py2.py3-none-any.whl/urllib3/connectionpool.py:1015: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
/usr/share/python-wheels/urllib3-1.26.5-py2.py3-none-any.whl/urllib3/connectionpool.py:1015: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
/usr/share/python-wheels/urllib3-1.26.5-py2.py3-none-any.whl/urllib3/connectionpool.py:1015: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
/usr/share/python-wheels/urllib3-1.26.5-py2.py3-none-any.whl/urllib3/connectionpool.py:1015: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
</code></pre>
<p>At the end this produced a list of packages, but I want to understand why <code>pip</code> is showing this behavior.</p>
<p>I checked the <a href=""https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings"" rel=""nofollow noreferrer"">recommended page</a> in the warning, but it does not provide any useful information for my case, as it is focused on programming use cases which use the <code>urllib3</code> and <code>requests</code> packages directly.</p>
<p>There are suggestions to ignore this warning (with all the perils it entails) with a command-line argument passed to <code>pip</code>.</p>
<p>I have no idea why this warning started appearing.</p>
",0,1693503442,python;pip;urllib3,False,306,1,1693503442,https://stackoverflow.com/questions/77018145/python3-m-pip-unverified-https-request-is-being-made-to-host-pypi-org
77004219,SSLError on PUT request to s3 bucket during multi-part upload using Python requests library,"<p>I'm writing a Python script to upload a large file (5GB+) to an s3 bucket using a presigned URL. I have a javascript version of this code working, so I believe the logic and endpoints are all valid.</p>
<p>For each part of the file, I get a presigned multipart upload URL, then I attempt a PUT request to that URL:</p>
<pre><code>offset = 0
part_number = 0
with open(file_path, 'rb') as f:
     while offset &lt; file_size_bytes:
          # Get a presigned URL for this chunk
          get_multipart_upload_url_params = {
               &quot;partNumber&quot;: part_number,
               &quot;uploadId&quot;: upload_id,
               &quot;Key&quot;: file_key,
          }
          get_multipart_upload_url_response = requests.get(GET_MULTIPART_UPLOAD_URL_ENDPOINT, params=get_multipart_upload_url_params)

          if 'uploadURL' not in get_multipart_upload_url_response.json():
               print(&quot;Error: Upload Part URL not found in response&quot;)
               sys.exit(1)

          chunk_upload_url = get_multipart_upload_url_response.json()['uploadURL']

          # Upload the chunk
          remaining_bytes = file_size_bytes - offset
          chunk_size = min(MAX_CHUNK_SIZE, remaining_bytes)
          chunk = f.read(chunk_size)
          if not chunk:
               break

          response = requests.put(chunk_upload_url, data=chunk)
          ...
</code></pre>
<p>When requests.put executes, I see an error that looks like:</p>
<pre><code>urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='bucket-name.s3.amazonaws.com', port=443): Max retries exceeded with url: [PRESIGNED URL REDACTED] (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:2426)')))
</code></pre>
<p>What's extra confusing about this is that when I implement a single-part upload function, it works fine using the same interface:</p>
<pre><code>    # Get presigned upload URL
    upload_response = requests.get(SINGLE_PART_UPLOAD_API_ENDPOINT, params={
        'filename': filename,
    }).json()

    if 'uploadURL' not in upload_response or 'Key' not in upload_response:
        print(&quot;Error: Upload URL or file key not found in response&quot;)
        sys.exit(1)

    upload_url = upload_response['uploadURL']
    file_key = upload_response['Key']

    # Upload the file using requests
    print(f&quot;Uploading: {file_path}&quot;)
    with open(file_path, 'rb') as f:
        response = requests.put(upload_url, data=f, headers={&quot;Content-Type&quot;: &quot;application/octet-stream&quot;})
    ...
</code></pre>
<p>Some of the things I've tried:</p>
<ul>
<li>Print the presigned multipart upload URL to carefully inspect it to make sure they are valid</li>
<li>Confirming that a request to the s3 bucket URL resolves by making the request via a web browser</li>
<li>Switching to a different request library</li>
<li>Running the script on a different computer</li>
<li>Upgrading OpenSSL, requests, and urllib3</li>
</ul>
",0,1693353440,python;amazon-s3;python-requests;openssl;urllib3,True,321,1,1693417932,https://stackoverflow.com/questions/77004219/sslerror-on-put-request-to-s3-bucket-during-multi-part-upload-using-python-reque
76994728,Error with Python + requests: TLS/SSL connection has been closed (EOF) (_ssl.c:1006),"<p>I have been struggling with a simple requests.get(&quot;https://www.boe.es/rss/canal_leg.php?l=l&amp;c=118&quot;). I am getting the following exception:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\user1\AppData\Local\Programs\Python\Python311\Lib\site-packages\urllib3\connectionpool.py&quot;, line 467, in _make_request
    self._validate_conn(conn)
  File &quot;C:\Users\user1\AppData\Local\Programs\Python\Python311\Lib\site-packages\urllib3\connectionpool.py&quot;, line 1092, in _validate_conn
    conn.connect()
  File &quot;C:\Users\user1\AppData\Local\Programs\Python\Python311\Lib\site-packages\urllib3\connection.py&quot;, line 635, in connect
    sock_and_verified = _ssl_wrap_socket_and_match_hostname(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user1\AppData\Local\Programs\Python\Python311\Lib\site-packages\urllib3\connection.py&quot;, line 774, in _ssl_wrap_socket_and_match_hostname
    ssl_sock = ssl_wrap_socket(
               ^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user1\AppData\Local\Programs\Python\Python311\Lib\site-packages\urllib3\util\ssl_.py&quot;, line 459, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user1\AppData\Local\Programs\Python\Python311\Lib\site-packages\urllib3\util\ssl_.py&quot;, line 503, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user1\AppData\Local\Programs\Python\Python311\Lib\ssl.py&quot;, line 517, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user1\AppData\Local\Programs\Python\Python311\Lib\ssl.py&quot;, line 1108, in _create
    self.do_handshake()
  File &quot;C:\Users\user1\AppData\Local\Programs\Python\Python311\Lib\ssl.py&quot;, line 1379, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLZeroReturnError: TLS/SSL connection has been closed (EOF) (_ssl.c:1006)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;C:\Users\user1\AppData\Local\Programs\Python\Python311\Lib\site-packages\urllib3\connectionpool.py&quot;, line 790, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user1\AppData\Local\Programs\Python\Python311\Lib\site-packages\urllib3\connectionpool.py&quot;, line 491, in _make_request
    raise new_e
urllib3.exceptions.SSLError: TLS/SSL connection has been closed (EOF) (_ssl.c:1006)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;C:\Users\user1\AppData\Local\Programs\Python\Python311\Lib\site-packages\requests\adapters.py&quot;, line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File &quot;C:\Users\user1\AppData\Local\Programs\Python\Python311\Lib\site-packages\urllib3\connectionpool.py&quot;, line 844, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user1\AppData\Local\Programs\Python\Python311\Lib\site-packages\urllib3\util\retry.py&quot;, line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.boe.es', port=443): Max retries exceeded with url: /rss/canal_leg.php?l=l&amp;c=118 (Caused by SSLError(SSLZeroReturnError(6, 'TLS/SSL connection has been closed (EOF) (_ssl.c:1006)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;C:\Users\user1\Documents\Propuestas\2022-Regulación\03-Codificación\Regulation\database\updater.py&quot;, line 545, in update_database
    req     = requests.get(source[1], headers={'User-Agent':str(ua.chrome)})
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user1\AppData\Local\Programs\Python\Python311\Lib\site-packages\requests\api.py&quot;, line 73, in get
    return request(&quot;get&quot;, url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user1\AppData\Local\Programs\Python\Python311\Lib\site-packages\requests\api.py&quot;, line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user1\AppData\Local\Programs\Python\Python311\Lib\site-packages\requests\sessions.py&quot;, line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user1\AppData\Local\Programs\Python\Python311\Lib\site-packages\requests\sessions.py&quot;, line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user1\AppData\Local\Programs\Python\Python311\Lib\site-packages\requests\adapters.py&quot;, line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='www.boe.es', port=443): Max retries exceeded with url: /rss/canal_leg.php?l=l&amp;c=118 (Caused by SSLError(SSLZeroReturnError(6, 'TLS/SSL connection has been closed (EOF) (_ssl.c:1006)')))
</code></pre>
<p>I am currently using:</p>
<ul>
<li>requests 2.31.0</li>
<li>urllib3 2.0.4  (tried with 2.0.2, 1.16.16 and 1.24.3)</li>
<li>OpenSSL 3.0.9</li>
<li>Python 3.11.15</li>
</ul>
<p>I have triyed setting &quot;verify=False&quot; in requests and checked the server characteristics in SSLLabs, and they seem correct (I had never had an issue until recently): <a href=""https://www.ssllabs.com/ssltest/analyze.html?d=www.boe.es"" rel=""nofollow noreferrer"">https://www.ssllabs.com/ssltest/analyze.html?d=www.boe.es</a></p>
<p>Also, as already mentioned, I tried with different versions of urllib3 and also requests (I do not remember which of this last one).</p>
<p>Thank you in advance.</p>
",2,1693242179,python;ssl;python-requests;urllib3,False,3240,0,1693242233,https://stackoverflow.com/questions/76994728/error-with-python-requests-tls-ssl-connection-has-been-closed-eof-ssl-c1
76979850,When importing libraries I get ImportError in python3.10.11 (IDE -PyCharm),"<p>This error appears every time I try importing requests and other libraries:</p>
<pre><code>C:\Users\Administrator\AppData\Local\Programs\Python\Python37-32\python.exe C:\Users\Administrator\PycharmProjects\pythonProject11\main.py 
Traceback (most recent call last):
  File &quot;C:\Users\Administrator\PycharmProjects\pythonProject11\main.py&quot;, line 1, in &lt;module&gt;
    import requests
  File &quot;C:\Users\Administrator\AppData\Local\Programs\Python\Python37-32\lib\site-packages\requests\__init__.py&quot;, line 43, in &lt;module&gt;
    import urllib3
  File &quot;C:\Users\Administrator\AppData\Local\Programs\Python\Python37-32\lib\site-packages\urllib3\__init__.py&quot;, line 42, in &lt;module&gt;
    &quot;urllib3 v2.0 only supports OpenSSL 1.1.1+, currently &quot;
ImportError: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'OpenSSL 1.1.0h  27 Mar 2018'. See: https://github.com/urllib3/urllib3/issues/2168
</code></pre>
<p>I am new to coding and went to stackoverflow, but most solutions didn't work!</p>
",-1,1692989358,python;ssl;pycharm;openssl;urllib3,False,210,1,1692990504,https://stackoverflow.com/questions/76979850/when-importing-libraries-i-get-importerror-in-python3-10-11-ide-pycharm
76961355,Code is working on older version of urllib3 package (1.6) but not on latest version (2.0.4),"<p>I have a code, which logging me into server. The code is working on older version of <code>Urllib3</code> (1.6), but is not working on latest version (2.0.4). What do I have to change for making it work on latest version ?</p>
<p>Here is my code that handles ssl:</p>
<pre><code>import requests
import urllib3

requests.packages.urllib3.disable_warnings()
requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += ':HIGH:!DH:!aNULL'
try:
    requests.packages.urllib3.contrib.pyopenssl.util.ssl_.DEFAULT_CIPHERS += ':HIGH:!DH:!aNULL'
except AttributeError:
    # no pyopenssl support used / needed / available
    pass
</code></pre>
<p>Here is the error on latest version:</p>
<pre><code>Traceback (most recent call last):
  File &quot;c:\Users\xxx\xxx\Desktop\request\rqst_01.py&quot;, line 38, in &lt;module&gt;
    requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += ':HIGH:!DH:!aNULL'
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'urllib3.util.ssl_' has no attribute 'DEFAULT_CIPHERS'
</code></pre>
",0,1692793627,python;ssl;python-requests;urllib3,False,83,0,1692864636,https://stackoverflow.com/questions/76961355/code-is-working-on-older-version-of-urllib3-package-1-6-but-not-on-latest-vers
76966914,How to set default ciphers for Python Requests library when using urllib3 ver &gt; 2.0.0,"<p>urllib3 removed the ability to change DEFAULT_CIPHER in the major version 2.0.0. This was the main way to get around the error:</p>
<pre class=""lang-py prettyprint-override""><code>requests.exceptions.SSLError: [SSL: SSL_NEGATIVE_LENGTH] dh key too small (_ssl.c:600) 
</code></pre>
<p>on the client side.</p>
<p>Is there anyway to change the default cipher now in urllib3 ver &gt; 2.0.0. or another way to get around this error?</p>
<p>Previously I had been receiving the error:</p>
<pre class=""lang-py prettyprint-override""><code>requests.exceptions.SSLError: [SSL: SSL_NEGATIVE_LENGTH] dh key too small (_ssl.c:600)
</code></pre>
<p>when using the requests library, which I eventually was able to get around using the line:</p>
<pre class=""lang-py prettyprint-override""><code>requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS = 'ALL:@SECLEVEL=1'
</code></pre>
<p>However, after getting a new laptop and reinstalling my packages, I began to get the original error again. After doing a bunch of research, I believe the issue is that on my new laptop, I've installed urllib3 2.0.3 which removed the &quot;DEFAULT_CIPHERS&quot; as seen in the <a href=""https://stackoverflow.com"">changelog</a> for version 2.0.0: Removed DEFAULT_CIPHERS, HAS_SNI, USE_DEFAULT_SSLCONTEXT_CIPHERS, from the private module urllib3.util.ssl_</p>
<p>Preferably without having to downgrade my urllib3 to an older version, does anyone know how to change the default ciphers again so I can get around the dh key too small error?</p>
<p>Thanks</p>
",0,1692860796,python;security;ssl;python-requests;urllib3,False,626,0,1692860796,https://stackoverflow.com/questions/76966914/how-to-set-default-ciphers-for-python-requests-library-when-using-urllib3-ver
76936758,"How to fix (&#39;No cipher can be selected.&#39;,) when creating context with urllib3?","<p>I am making a proxy server and I am tryng to handle HTTPS requests, but I'm facing a problem with setting the ciphers:</p>
<pre><code># Set up SSL/TLS interception logic
# Wrap sockets with SSLContext
context = urllib3.util.ssl_.create_urllib3_context(ciphers='TLS_AES_256_GCM_SHA384') 
context.options |= ssl.OP_NO_SSLv2 | ssl.OP_NO_SSLv3  # Disable SSLv2 and SSLv3
</code></pre>
<p>I am tryng to connect to a server that uses TLS_AES_256_GCM_SHA384 cipher, but when I run the code i got</p>
<blockquote>
<p>('No cipher can be selected.',)</p>
</blockquote>
<p>How can I fix it?</p>
",0,1692473791,python;python-3.x;ssl;urllib3,False,147,0,1692543289,https://stackoverflow.com/questions/76936758/how-to-fix-no-cipher-can-be-selected-when-creating-context-with-urllib3
62392885,How to use urllib3 to POST x-www-form-urlencoded data,"<p>I am trying to use urllib3 in Python to POST x-www-form-urlencoded data to ServiceNow API. The usual curl command would look like this</p>
<pre><code>curl -d &quot;grant_type=password&amp;client_id=&lt;client_ID&gt;&amp;client_secret=&lt;client_Secret&gt;&amp;username=&lt;username&gt;&amp;password=&lt;password&gt;&quot; https://host.service-now.com/oauth_token.do
</code></pre>
<p>So far, I have tried the following:</p>
<pre><code>import urllib3
import urllib.parse
http = urllib3.PoolManager()
data = {&quot;grant_type&quot;: &quot;password&quot;, &quot;client_id&quot;: &quot;&lt;client_ID&gt;&quot;, &quot;client_secret&quot;: &quot;&lt;client_Secret&gt;&quot;, &quot;username&quot;: &quot;&lt;username&gt;&quot;, &quot;password&quot;: &quot;&lt;password&gt;&quot;}
data = urllib.parse.urlencode(data)

headers = {'Content-Type': 'application/x-www-form-urlencoded'}

accesTokenCreate = http.request('POST', &quot;https://host.service-now.com/outh_token.do&quot;, headers = headers, fields= data)
print(accesTokenCreate.data)
</code></pre>
<p>However, it does not generate the result similar to curl command and gives errors like below:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/VisualStudio/Python/ServiceNow.py&quot;, line 18, in &lt;module&gt;
    accesTokenCreate = http.request('POST', &quot;https://visierdev.service-now.com/outh_token.do&quot;, headers = headers, fields= data)
  File &quot;/usr/local/homebrew/lib/python3.7/site-packages/urllib3/request.py&quot;, line 80, in request
    method, url, fields=fields, headers=headers, **urlopen_kw
  File &quot;/usr/local/homebrew/lib/python3.7/site-packages/urllib3/request.py&quot;, line 157, in request_encode_body
    fields, boundary=multipart_boundary
  File &quot;/usr/local/homebrew/lib/python3.7/site-packages/urllib3/filepost.py&quot;, line 78, in encode_multipart_formdata
    for field in iter_field_objects(fields):
  File &quot;/usr/local/homebrew/lib/python3.7/site-packages/urllib3/filepost.py&quot;, line 42, in iter_field_objects
    yield RequestField.from_tuples(*field)
TypeError: from_tuples() missing 1 required positional argument: 'value'
</code></pre>
<p>Could someone please help me understand how to properly use urllib3 to post such data to the ServiceNow API?</p>
",4,1592239130,python;servicenow;urllib3,True,6016,2,1692220441,https://stackoverflow.com/questions/62392885/how-to-use-urllib3-to-post-x-www-form-urlencoded-data
76887259,Requests and urllib3 exceptions in Python,"<p>Absolutely stumped because I've never had such an issue when using python.
Briefly, I used the code below:</p>
<pre><code>import requests
import json

webhook_url = 'http://127.0.0.1:5000/webhook' #'https://webhook.site/580151cf-1c53-488c-a325-dbc76f12a8c5'

data = {'name': 'Mathieu',
        'Channel URL': 'blablabla'}

r = requests.post(webhook_url, data=json.dumps(data), headers={'Content-Type': 'application/json'})
</code></pre>
<p>However, I received this HUGE error message below:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\User\PycharmProjects\Web\venv\lib\site-packages\urllib3\connection.py&quot;, line 203, in _new_conn
    sock = connection.create_connection(
  File &quot;C:\Users\User\PycharmProjects\Web\venv\lib\site-packages\urllib3\util\connection.py&quot;, line 85, in create_connection
    raise err
  File &quot;C:\Users\User\PycharmProjects\Web\venv\lib\site-packages\urllib3\util\connection.py&quot;, line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;C:\Users\User\PycharmProjects\Web\venv\lib\site-packages\urllib3\connectionpool.py&quot;, line 790, in urlopen
    response = self._make_request(
  File &quot;C:\Users\User\PycharmProjects\Web\venv\lib\site-packages\urllib3\connectionpool.py&quot;, line 496, in _make_request
    conn.request(
  File &quot;C:\Users\User\PycharmProjects\Web\venv\lib\site-packages\urllib3\connection.py&quot;, line 395, in request
    self.endheaders()
  File &quot;C:\Users\User\AppData\Local\Programs\Python\Python39\lib\http\client.py&quot;, line 1280, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File &quot;C:\Users\User\AppData\Local\Programs\Python\Python39\lib\http\client.py&quot;, line 1040, in _send_output
    self.send(msg)
  File &quot;C:\Users\User\AppData\Local\Programs\Python\Python39\libh\http\client.py&quot;, line 980, in send
    self.connect()
  File &quot;C:\Users\User\PycharmProjects\Web\venv\lib\site-packages\urllib3\connection.py&quot;, line 243, in connect
    self.sock = self._new_conn()
  File &quot;C:\Users\User\PycharmProjects\Web\venv\lib\site-packages\urllib3\connection.py&quot;, line 218, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: &lt;urllib3.connection.HTTPConnection object at 0x000001F901B551C0&gt;: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;C:\Users\User\PycharmProjects\Web\venv\lib\site-packages\requests\adapters.py&quot;, line 486, in send
    resp = conn.urlopen(
  File &quot;C:\Users\User\PycharmProjects\Web\venv\lib\site-packages\urllib3\connectionpool.py&quot;, line 844, in urlopen
    retries = retries.increment(
  File &quot;C:\Users\User\PycharmProjects\Web\venv\lib\site-packages\urllib3\util\retry.py&quot;, line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=5000): Max retries exceeded with url: /webhook (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x000001F901B551C0&gt;: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;C:\Users\User\PycharmProjects\Web\main.py&quot;, line 10, in &lt;module&gt;
    r = requests.post(webhook_url, data=json.dumps(data), headers={'Content-Type': 'application/json'})
  File &quot;C:\Users\User\PycharmProjects\Web\venv\lib\site-packages\requests\api.py&quot;, line 115, in post
    return request(&quot;post&quot;, url, data=data, json=json, **kwargs)
  File &quot;C:\Users\User\PycharmProjects\Web\venv\lib\site-packages\requests\api.py&quot;, line 59, in request
    return session.request(method=method, url=url, **kwargs)
  File &quot;C:\Users\User\PycharmProjects\Web\venv\lib\site-packages\requests\sessions.py&quot;, line 589, in request
    resp = self.send(prep, **send_kwargs)
  File &quot;C:\Users\User\PycharmProjects\Web\venv\lib\site-packages\requests\sessions.py&quot;, line 703, in send
    r = adapter.send(request, **kwargs)
  File &quot;C:\Users\User\PycharmProjects\Web\venv\lib\site-packages\requests\adapters.py&quot;, line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=5000): Max retries exceeded with url: /webhook (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x000001F901B551C0&gt;: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
</code></pre>
",0,1691798915,python;python-requests;urllib3,False,620,0,1691799443,https://stackoverflow.com/questions/76887259/requests-and-urllib3-exceptions-in-python
58968264,Using urllib3 with python 2.7 to download .txt file from url?,"<p>I'm using Python 2.7, and I have urllib3.  I'm trying to download each of the .txt files in this link: <a href=""http://web.mta.info/developers/turnstile.html"" rel=""nofollow noreferrer"">http://web.mta.info/developers/turnstile.html</a></p>
<p>Here's my code:</p>
<pre><code>#!/usr/bin/env python
# -*- coding: utf-8 -*-

from bs4 import BeautifulSoup
import requests
import urllib3, shutil


http = urllib3.PoolManager()

MTA_url = requests.get(&quot;http://web.mta.info/developers/turnstile.html&quot;).text
MTA_soup = BeautifulSoup(MTA_url)
#Find each link to be downloaded
MTA_soup.findAll('a')
#Let's test it with the 36th link
one_a_tag = MTA_soup.findAll(&quot;a&quot;)[36]
MTA_link = one_a_tag[&quot;href&quot;]

download_url = 'http://web.mta.info/developers/'+ MTA_link
print download_url #valid url, will take you to download
</code></pre>
<p>This is where I'm stuck. I can't seem to figure out how to download the .txt file at <code>download_url</code>, let alone iterate through the list. I've tried this:</p>
<pre><code>open('/Users/me/Documents/test_output_download.csv', 'wb').write(download_url.content)
</code></pre>
<p>But that gives me the error:</p>
<pre><code>AttributeError: 'unicode' object has no attribute 'content'
</code></pre>
<p>After reading further, I also tried:</p>
<pre><code>out_file = '/Users/me/Documents/test_output_download.csv'
http.request('GET', download_url, preload_content=False) as res, open(out_file, 'wb') as out_file:
   shutil.copyfileobj(res, out_file)
</code></pre>
<p>But I get past this syntax error:</p>
<pre><code>    http.request('GET', download_url, preload_content=False) as res, open(out_file, 'wb') as out_file:
                                                              ^
SyntaxError: invalid syntax
</code></pre>
<p>How can I just download the .txt file that is located at <code>download_url</code> and save it to my local drive, using urllib3?</p>
",1,1574316243,python;beautifulsoup;python-requests;shutil;urllib3,True,191,1,1691363547,https://stackoverflow.com/questions/58968264/using-urllib3-with-python-2-7-to-download-txt-file-from-url
53765366,"urllib3 connectionpool - Connection pool is full, discarding connection","<p>Does seeing the </p>

<pre><code>urllib3.connectionpool WARNING - Connection pool is full, discarding connection
</code></pre>

<p>mean that I am effectively loosing data (because of lost connection)<br>
OR <br>
Does it mean that connection is dropped (because pool is full); however, the same connection will be re-tried later on when connection pool becomes available?</p>
",70,1544715681,python;connection;pool;urllib3,True,85281,2,1690359122,https://stackoverflow.com/questions/53765366/urllib3-connectionpool-connection-pool-is-full-discarding-connection
76703154,zlib.error: Error -3 while decompressing data: invalid block type while decompressing .gzip with python,"<p>I am trying to decompress <code>.gzip</code> file into <code>.xlsx</code> with NumPy.</p>
<p>I am using Python3.10.9, Numpy 1.23.5, and running the lines on VS Code.</p>
<p>My system is Win64.</p>
<p>However, I got these error code. I'm not sure how to solve this.</p>
<pre><code>  File &quot;c:\Users\USER\Folder\file.py&quot;, line 219, in &lt;module&gt;
    _, _, _, _, _, _, _, wdir = read_windsat_averaged_v8(filename) ###Error Occured
  File &quot;c:\Users\USER\Folder\file.py&quot;, line 47, in read_windsat_averaged_v8
    data = np.frombuffer(f.read(), dtype=np.uint8) #Exception has occurred: error
  File &quot;C:\Users\USER\anaconda3\lib\gzip.py&quot;, line 301, in read
    return self._buffer.read(size)
  File &quot;C:\Users\USER\anaconda3\lib\_compression.py&quot;, line 118, in readall
    while data := self.read(sys.maxsize):
  File &quot;C:\Users\USER\anaconda3\lib\gzip.py&quot;, line 496, in read
    uncompress = self._decompressor.decompress(buf, size)
zlib.error: Error -3 while decompressing data: invalid stored block lengths
</code></pre>
<p>These are my code, where the error occured:</p>
<pre><code>import numpy as np
import os
import openpyxl
import gzip
import shutil

xscale = [0.15, 0.2, 0.2, 0.3, 0.01, 0.1, 0.2, 1.5] # A list of scaling factors for each variable.
offset = [-3.0, 0.0, 0.0, 0.0, -0.05, 0.0, 0.0, 0.0] # A list of offsets for each variable.
xdim = 1440 # The x-dimension of the data.
ydim = 720 # The y-dimension of the data.
numvar = 8 # The number of variables in the data.\

...

if data_file.endswith('.gz'):
        with gzip.open(data_file, 'rb') as f:
            data = np.frombuffer(f.read(), dtype=np.uint8) ### Error occured.
</code></pre>
<p>I use the function <code>print(data.size)</code> to get the size of the ndarray. It's 8294400.
I tried to reshape it into (8, 1440, 720), but it still did not work. And I got the same error code again.</p>
<p>I have read the <a href=""https://numpy.org/doc/stable/reference/generated/numpy.frombuffer.html"" rel=""nofollow noreferrer"">Official Manuals of Numpy</a>, and I am not sure how should I do to solve it.</p>
<p>Actually I have used the same code to decompress the other chunk of data successfully, and I did not sure how to solve this. <a href=""https://github.com/tensorflow/tensorflow/issues/1533"" rel=""nofollow noreferrer"">One of the articles online</a> suggests this might happened because the <code>.gzip</code> file is broken. However, even though I redownloaded the .gzip file, it still did not work. Is there any solution?</p>
<p>This is my first time asking question on StackOverflow, and I hope I am describing it clearly. If not, I'd like to know how I can improve it.</p>
<p>Here is a link to the file: <a href=""https://data.remss.com/windsat/bmaps_v07.0.1/y2016/m08/wsat_20160801v7.0.1_d3d.gz"" rel=""nofollow noreferrer"">https://data.remss.com/windsat/bmaps_v07.0.1/y2016/m08/wsat_20160801v7.0.1_d3d.gz</a></p>
<p>From the starting, I download it with <code>urllib3.request()</code>, and I found that this problem may cause by the default setting of urllib3.
<a href=""https://stackoverflow.com/questions/58189324/invalid-stored-block-lengths-jython-and-requests-library"">See this post.</a>
However, even though I change the line <code>Accept-Encoding: gzip, deflate</code> to <code>Accept-Encoding: None</code> manually, it still does not work.</p>
",0,1689586528,python;numpy;http;gzip;urllib3,False,526,0,1689645835,https://stackoverflow.com/questions/76703154/zlib-error-error-3-while-decompressing-data-invalid-block-type-while-decompre
67872404,How to get log of every retry attempt made by Python Requests Library?,"<p>I'm using following code to implement retry mechanism in python requests.</p>
<pre><code>from requests import Session
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

s = Session()
retries = Retry(total=5,
                raise_on_redirect=True, 
                raise_on_status=True,
                backoff_factor=1,
                status_forcelist=[ 500, 502, 503, 504 ])
s.mount('http://', HTTPAdapter(max_retries=retries))
s.mount('https://', HTTPAdapter(max_retries=retries))
response=s.get('https://example.com')
</code></pre>
<p>It is working perfectly, however all this happens silently.
What I want is to get informed whenever a retry attempt is made and if possible why that retry was happened. I want something like this for every retry attempt.</p>
<pre><code>print(Exception.__class__)
print('Retrying after' + x + 'seconds')
</code></pre>
<p>Is it possible?</p>
",8,1623071968,python;python-requests;urllib3,True,4694,3,1689525658,https://stackoverflow.com/questions/67872404/how-to-get-log-of-every-retry-attempt-made-by-python-requests-library
76682151,"NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the &#39;ssl&#39; module is compiled with &#39;LibreSSL 2.8.3&#39; error using Selenium on MAC","<p>I am trying to open a website with selenium, currently trying with youtube. However when I run my program it takes ages for it load the website. I ran the same code on my windows pc but there I had no problems.</p>
<p>I have a good and stable internet connection. Still I receive a warning which follows as:</p>
<pre><code>NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
</code></pre>
<p>however I am not certain if this is what's causing it to be so slow.</p>
<p>Any ideas what's causing it to be so slow?</p>
<p>I tried fixing the ssl warning with doing things written on reddit, but to no avail.</p>
<p>This is my current code:</p>
<pre><code>from selenium import webdriver
from selenium.webdriver.chrome.options import Options

options = Options()
options.add_experimental_option(&quot;detach&quot;, True)
driver = webdriver.Chrome(options=options)
driver.get(&quot;https://www.youtube.com&quot;)
</code></pre>
",0,1689271617,python;macos;selenium-webdriver;urllib3;libressl,False,1441,1,1689373301,https://stackoverflow.com/questions/76682151/notopensslwarning-urllib3-v2-0-only-supports-openssl-1-1-1-currently-the-ssl
76680804,Error connecting to site when webscraping?,"<p>I have the following code that has been working beautifully up until recently:</p>
<pre><code>import time
from datetime import datetime
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# Create a session with retries
session = requests.Session()
retry = Retry(connect=3, backoff_factor=0.5)
adapter = HTTPAdapter(max_retries=retry)
session.mount('http://', adapter)
session.mount('https://', adapter)


url = &quot;https://api.smws.com/api/v1/bottles&quot;

#https://api.smws.com/api/v1/bottles?store_id=uk&amp;parent_id=61&amp;page=1&amp;sortBy=pricedesc&amp;minPrice=0&amp;maxPrice=0&amp;perPage=256
params = {
    &quot;store_id&quot;: &quot;uk&quot;,
    &quot;parent_id&quot;: 61,
    &quot;page&quot;: 1,
    &quot;sortBy&quot;: &quot;pricedesc&quot;,
    &quot;minPrice&quot;: 0,
    &quot;maxPrice&quot;: 0,
    &quot;perPage&quot;: 256,
}

# Initialize store and new_store lists
store = []
new_store = []

# current dateTime
now = datetime.now()

date = now.strftime(&quot;%d/%m %H:%M&quot;)
print('Time started', date)

# Make initial API request and parse response
response = session.get(url, params=params)  # Use the session for the request
data = response.json()

print(data)
</code></pre>
<p>However now I'm getting the following error:</p>
<pre><code>
Traceback (most recent call last):
  File &quot;main.py&quot;, line 39, in &lt;module&gt;
    response = session.get(url, params=params)  # Use the session for the request
  File &quot;/Users/xxxxxx/Library/Python/3.8/lib/python/site-packages/requests/sessions.py&quot;, line 555, in get
    return self.request('GET', url, **kwargs)
  File &quot;/Users/xxxxxx/Library/Python/3.8/lib/python/site-packages/requests/sessions.py&quot;, line 542, in request
    resp = self.send(prep, **send_kwargs)
  File &quot;/Users/xxxxxx/Library/Python/3.8/lib/python/site-packages/requests/sessions.py&quot;, line 677, in send
    history = [resp for resp in gen]
  File &quot;/Users/xxxxxx/Library/Python/3.8/lib/python/site-packages/requests/sessions.py&quot;, line 677, in &lt;listcomp&gt;
    history = [resp for resp in gen]
  File &quot;/Users/xxxxxx/Library/Python/3.8/lib/python/site-packages/requests/sessions.py&quot;, line 237, in resolve_redirects
    resp = self.send(
  File &quot;/Users/xxxxxx/Library/Python/3.8/lib/python/site-packages/requests/sessions.py&quot;, line 655, in send
    r = adapter.send(request, **kwargs)
  File &quot;/Users/xxxxxx/Library/Python/3.8/lib/python/site-packages/requests/adapters.py&quot;, line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7XXXXXXXXXXX&gt;: Failed to establish a new connection: [Errno 61] Connection refused'))
</code></pre>
<p>This error occurs 3 times due to connect=3. Any ideas how to get around this? The code was working absolutely fine until of recent. Thanks in advance!</p>
",0,1689260111,python;web-scraping;python-requests;urllib3,False,45,1,1689261759,https://stackoverflow.com/questions/76680804/error-connecting-to-site-when-webscraping
36087637,How often does python-requests perform dns queries,"<p>We are using <a href=""http://locust.io"" rel=""noreferrer"">Locust</a> to for load testing rest api services behind elastic load balancing. I came across <a href=""http://aws.amazon.com/articles/1636185810492479"" rel=""noreferrer"">this article</a> regarding load balancing and auto scaling, which is something we are testing. </p>

<p>Locust is using <strong>python-requests</strong> which is using <strong>urllib3</strong>, so my question is if python-requests does a dns query for every connect, and if not, is it configurable? </p>
",13,1458313130,python;dns;python-requests;urllib3;locust,True,9508,3,1688649314,https://stackoverflow.com/questions/36087637/how-often-does-python-requests-perform-dns-queries
76617236,UTF8 encoding error while posting request after adding boundaries to original file content,"<p>I am trying to read a file, add boundaries around it and post it to my url.
I tried to read file, decode (using <code>latin1</code> as <code>utf-8</code> was not working) and concatenate boundary strings, save this file with <code>utf-8</code> encoding.
All these work fine.</p>
<p>But when i tried to post this file it gave me an error <em><strong>&quot;'utf-8' codec can't decode byte 0x94 in position 23: invalid start byte&quot;</strong></em> - How do I solve this problem? Is there a way to post other encodings than <code>utf-8</code> in requests?</p>
<pre><code>gen_data = open(fw_file,'rb').read()
boundary = '------WebKitFormBoundaryV2I0Y3IdM8aQPLv3\r\nContent-Disposition: form-data; name=&quot;file&quot;; filename=&quot;'\
           + file_name + '&quot;\r\nContent-Type: application/octet-stream\r\n\r\n'
boundary_end = '\r\n\r\n------WebKitFormBoundaryV2I0Y3IdM8aQPLv3--\r\n'

def create_data(boundary, data, end):
    return boundary + data.decode('latin1') + end

data = create_data(boundary, gen_data, boundary_end)
export_file = rf'c:\temp\{file_name}'
open(export_file, 'w', encoding=&quot;utf-8&quot;).write(data)
resp = requests.post(target, files=export_file, headers=headers, verify=False)
</code></pre>
<p>Following is the exception log:</p>
<pre><code>Traceback (most recent call last):
      File &quot;C:\Users\username\AppData\Local\Programs\Python\Python39\lib\site-packages\requests\api.py&quot;, line 61, in request
        return session.request(method=method, url=url, **kwargs)
      File &quot;C:\Users\username\AppData\Local\Programs\Python\Python39\lib\site-packages\requests\sessions.py&quot;, line 515, in request
        prep = self.prepare_request(req)
      File &quot;C:\Users\username\AppData\Local\Programs\Python\Python39\lib\site-packages\requests\sessions.py&quot;, line 443, in prepare_request
        p.prepare(
      File &quot;C:\Users\username\AppData\Local\Programs\Python\Python39\lib\site-packages\requests\models.py&quot;, line 321, in prepare
        self.prepare_body(data, files, json)
      File &quot;C:\Users\username\AppData\Local\Programs\Python\Python39\lib\site-packages\requests\models.py&quot;, line 514, in prepare_body
        (body, content_type) = self._encode_files(files, data)
      File &quot;C:\Users\username\AppData\Local\Programs\Python\Python39\lib\site-packages\requests\models.py&quot;, line 128, in _encode_files
        files = to_key_val_list(files or {})
      File &quot;C:\Users\username\AppData\Local\Programs\Python\Python39\lib\site-packages\requests\utils.py&quot;, line 343, in to_key_val_list
        raise ValueError('cannot encode objects that are not 2-tuples')
    ValueError: cannot encode objects that are not 2-tuples
    python-BaseException
</code></pre>
<p><em><strong>EDIT</strong></em>
I tried to upload data directly using following code:
<code>resp = requests.post(target, data=data, headers=headers, verify=False)</code></p>
<p>But got following exception:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\username\AppData\Local\Programs\Python\Python39\lib\site-packages\urllib3\connectionpool.py&quot;, line 699, in urlopen
    httplib_response = self._make_request(
  File &quot;C:\Users\username\AppData\Local\Programs\Python\Python39\lib\site-packages\urllib3\connectionpool.py&quot;, line 445, in _make_request
    six.raise_from(e, None)
  File &quot;&lt;string&gt;&quot;, line 3, in raise_from
  File &quot;C:\Users\username\AppData\Local\Programs\Python\Python39\lib\site-packages\urllib3\connectionpool.py&quot;, line 440, in _make_request
    httplib_response = conn.getresponse()
  File &quot;C:\Users\username\AppData\Local\Programs\Python\Python39\lib\http\client.py&quot;, line 1372, in getresponse
    response.begin()
  File &quot;C:\Users\username\AppData\Local\Programs\Python\Python39\lib\http\client.py&quot;, line 320, in begin
    version, status, reason = self._read_status()
  File &quot;C:\Users\username\AppData\Local\Programs\Python\Python39\lib\http\client.py&quot;, line 289, in _read_status
    raise RemoteDisconnected(&quot;Remote end closed connection without&quot;
http.client.RemoteDisconnected: Remote end closed connection without response
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File &quot;C:\Users\username\AppData\Local\Programs\Python\Python39\lib\site-packages\requests\adapters.py&quot;, line 440, in send
    resp = conn.urlopen(
  File &quot;C:\Users\username\AppData\Local\Programs\Python\Python39\lib\site-packages\urllib3\connectionpool.py&quot;, line 755, in urlopen
    retries = retries.increment(
  File &quot;C:\Users\username\AppData\Local\Programs\Python\Python39\lib\site-packages\urllib3\util\retry.py&quot;, line 532, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File &quot;C:\Users\username\AppData\Local\Programs\Python\Python39\lib\site-packages\urllib3\packages\six.py&quot;, line 769, in reraise
    raise value.with_traceback(tb)
  File &quot;C:\Users\username\AppData\Local\Programs\Python\Python39\lib\site-packages\urllib3\connectionpool.py&quot;, line 699, in urlopen
    httplib_response = self._make_request(
  File &quot;C:\Users\username\AppData\Local\Programs\Python\Python39\lib\site-packages\urllib3\connectionpool.py&quot;, line 445, in _make_request
    six.raise_from(e, None)
  File &quot;&lt;string&gt;&quot;, line 3, in raise_from
  File &quot;C:\Users\username\AppData\Local\Programs\Python\Python39\lib\site-packages\urllib3\connectionpool.py&quot;, line 440, in _make_request
    httplib_response = conn.getresponse()
  File &quot;C:\Users\username\AppData\Local\Programs\Python\Python39\lib\http\client.py&quot;, line 1372, in getresponse
    response.begin()
  File &quot;C:\Users\username\AppData\Local\Programs\Python\Python39\lib\http\client.py&quot;, line 320, in begin
    version, status, reason = self._read_status()
  File &quot;C:\Users\username\AppData\Local\Programs\Python\Python39\lib\http\client.py&quot;, line 289, in _read_status
    raise RemoteDisconnected(&quot;Remote end closed connection without&quot;
urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File &quot;C:\Users\username\AppData\Local\Programs\Python\Python39\lib\site-packages\requests\api.py&quot;, line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File &quot;C:\Users\username\AppData\Local\Programs\Python\Python39\lib\site-packages\requests\sessions.py&quot;, line 533, in request
    resp = self.send(prep, **send_kwargs)
  File &quot;C:\Users\username\AppData\Local\Programs\Python\Python39\lib\site-packages\requests\sessions.py&quot;, line 649, in send
    r = adapter.send(request, **kwargs)
  File &quot;C:\Users\username\AppData\Local\Programs\Python\Python39\lib\site-packages\requests\adapters.py&quot;, line 501, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
python-BaseException
</code></pre>
",0,1688533342,python;python-3.x;python-requests;urllib3,False,46,0,1688533958,https://stackoverflow.com/questions/76617236/utf8-encoding-error-while-posting-request-after-adding-boundaries-to-original-fi
76607228,i have a problem about pytube and tkinter,"<p>I'm getting this error and I couldn't find the solution</p>
<pre><code>Exception in Tkinter callback
Traceback (most recent call last):
  File &quot;C:\Users\fetta\AppData\Local\Programs\Python\Python311\Lib\tkinter\__init__.py&quot;, line 1948, in __call__
    return self.func(*args)
           ^^^^^^^^^^^^^^^^
  File &quot;C:\Users\fetta\PycharmProjects\KofteTubeV2\main.py&quot;, line 16, in Downloader
    video = url.streams.get_highest_resolution()
            ^^^^^^^^^^^
  File &quot;C:\Users\fetta\PycharmProjects\KofteTubeV2\venv\Lib\site-packages\pytube\__main__.py&quot;, line 295, in streams
    self.check_availability()
  File &quot;C:\Users\fetta\PycharmProjects\KofteTubeV2\venv\Lib\site-packages\pytube\__main__.py&quot;, line 210, in check_availability
    status, messages = extract.playability_status(self.watch_html)
                                                  ^^^^^^^^^^^^^^^
  File &quot;C:\Users\fetta\PycharmProjects\KofteTubeV2\venv\Lib\site-packages\pytube\__main__.py&quot;, line 102, in watch_html
    self._watch_html = request.get(url=self.watch_url)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\fetta\PycharmProjects\KofteTubeV2\venv\Lib\site-packages\pytube\request.py&quot;, line 53, in get
    response = _execute_request(url, headers=extra_headers, timeout=timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\fetta\PycharmProjects\KofteTubeV2\venv\Lib\site-packages\pytube\request.py&quot;, line 37, in _execute_request
    return urlopen(request, timeout=timeout)  # nosec
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\fetta\AppData\Local\Programs\Python\Python311\Lib\urllib\request.py&quot;, line 212, in urlopen
    _opener = opener = build_opener()
                       ^^^^^^^^^^^^^^
  File &quot;C:\Users\fetta\AppData\Local\Programs\Python\Python311\Lib\urllib\request.py&quot;, line 593, in build_opener
    opener.add_handler(klass())
                       ^^^^^^^
  File &quot;C:\Users\fetta\AppData\Local\Programs\Python\Python311\Lib\urllib\request.py&quot;, line 1386, in __init__
    context = http.client._create_https_context(http_version)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'http.client' has no attribute '_create_https_context'
</code></pre>
<p>I'm waiting for my code to download video, audio and playlist from youtube but it didn't work</p>
<pre><code>from tkinter import *
import pytube

kof = Tk()
kof.geometry('600x400')
kof.title(&quot;Kofte Youtube | Video &amp; Sound &amp; Playlist | Downloader&quot;)

Label(kof, text='Youtube Video Downloader', font='arial 20 bold').pack()

link = StringVar()
Label(kof, text='Link Here:', font='arial 15 bold').pack()
Entry(kof, width=70, textvariable=link).pack()

def Downloader():
    url = pytube.YouTube(str(link.get()))
    video = url.streams.get_highest_resolution()
    video.download()
    Label(kof, text='Video Downloaded', font=&quot;arial 15&quot;).place(x=180, y=210)


def Sounder():
    url = pytube.YouTube(str(link.get()))
    video = url.streams.filter(only_audio=True).first()
    video.download()
    Label(kof, text='Video Downloaded', font=&quot;arial 15&quot;).place(x=180, y=210)


def Playlister():
    url = pytube.YouTube(str(link.get()))
    video = url.streams.first()
    video.download()
    Label(kof, text='Playlist Downloaded', font=&quot;arial 15&quot;).place(x=180, y=210)


Button(kof, text=&quot;Downloader&quot;, font=&quot;arial 10 bold&quot;, padx=2, command=Downloader).pack()
Button(kof, text=&quot;Sounder&quot;, font=&quot;arial 10 bold&quot;, padx=2, command=Sounder).pack()
Button(kof, text=&quot;Playlister&quot;, font=&quot;arial 10 bold&quot;, padx=2, command=Playlister).pack()
kof.mainloop()
</code></pre>
",0,1688407707,python;python-3.x;tkinter;urllib3;pytube,False,54,0,1688407756,https://stackoverflow.com/questions/76607228/i-have-a-problem-about-pytube-and-tkinter
76570434,k8s core_v1_api list_pod_for_all_namespaces failed with exception: urllib3.exceptions.ProtocolError: Connection broken: InvalidChunkLength,"<p>I am using kubernetes/client/api/core_v1_api (<a href=""https://github.com/kubernetes-client/python/blob/master/kubernetes/client/api/core_v1_api.py"" rel=""nofollow noreferrer"">https://github.com/kubernetes-client/python/blob/master/kubernetes/client/api/core_v1_api.py</a>)</p>
<p>Sometimes code failed on:</p>
<pre><code>for pod_event in self._watcher.stream(func=self._core_api.list_pod_for_all_namespaces, **watch_kwargs):
</code></pre>
<p>getting this exception:</p>
<pre><code>urllib3.exceptions.ProtocolError: (&quot;Connection broken: InvalidChunkLength(got length b'', 0 bytes read)&quot;, InvalidChunkLength(got length b'', 0 bytes read))
</code></pre>
<p>I have some suggestion:</p>
<ul>
<li><p>I think to adding timeout_seconds=0 while watch.stream loop
<a href=""https://medium.com/programming-kubernetes/building-stuff-with-the-kubernetes-api-part-3-using-python-aea5ab16f627"" rel=""nofollow noreferrer"">https://medium.com/programming-kubernetes/building-stuff-with-the-kubernetes-api-part-3-using-python-aea5ab16f627</a>
I am not sure if it will help</p>
</li>
<li><p>try to catch this specific exception and ignore it. because it mean trying to chunk empty string (I am not sure if its mean there is no more data)</p>
</li>
<li><p>upgrade urlib3/kubernetes packages on my project or open issue for this</p>
</li>
</ul>
",1,1687932254,python;kubernetes;urllib3,False,218,0,1687932254,https://stackoverflow.com/questions/76570434/k8s-core-v1-api-list-pod-for-all-namespaces-failed-with-exception-urllib3-excep
51188661,Adding callback function on each retry attempt using requests/urllib3,"<p>I've implemented a retry mechanism to <code>requests</code> session using <a href=""http://urllib3.readthedocs.io/en/latest/reference/urllib3.util.html#module-urllib3.util.retry"" rel=""noreferrer""><code>urllib3.util.retry</code></a> as suggested both <a href=""https://stackoverflow.com/questions/15431044/can-i-set-max-retries-for-requests-request"">here</a> and <a href=""https://stackoverflow.com/questions/49121365/implementing-retry-for-requests-in-python"">here</a>.</p>
<p>Now, I am trying to figure out what is the best way to add a callback function that will be called on every retry attempt.</p>
<p>To explain myself even more, if either the <code>Retry</code> object or the requests <code>get</code> method had a way to add a callback function, it would have been great. Maybe something like:</p>
<pre><code>import requests
from requests.packages.urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

def retry_callback(url):
    print url   

s = requests.Session()
retries = Retry(total=5, status_forcelist=[ 500, 502, 503, 504 ])
s.mount('http://', HTTPAdapter(max_retries=retries))

url = 'http://httpstat.us/500'
s.get(url, callback=retry_callback, callback_params=[url])
</code></pre>
<p>I know that for printing url I can use the logging, but this is only a simple example for a more complex use.</p>
",18,1530785495,python;callback;python-requests;urllib3,True,6213,1,1687049381,https://stackoverflow.com/questions/51188661/adding-callback-function-on-each-retry-attempt-using-requests-urllib3
65289720,AttributeError: &#39;Retry&#39; object has no attribute &#39;method_whitelist&#39;,"<p>The following error occurred while trying to run code:</p>
<pre class=""lang-none prettyprint-override""><code>Traceback (most recent call last):

    response = session.post(base_url, params={'query': filename_query})
  File &quot;/usr/local/lib/python3.7/site-packages/requests/sessions.py&quot;, line 578, in post
    return self.request('POST', url, data=data, json=json, **kwargs)
  File &quot;/usr/local/lib/python3.7/site-packages/requests/sessions.py&quot;, line 530, in request
    resp = self.send(prep, **send_kwargs)
  File &quot;/usr/local/lib/python3.7/site-packages/requests/sessions.py&quot;, line 643, in send
    r = adapter.send(request, **kwargs)
  File &quot;/usr/local/lib/python3.7/site-packages/requests/adapters.py&quot;, line 449, in send
    timeout=timeout
  File &quot;/usr/local/lib/python3.7/site-packages/urllib3/connectionpool.py&quot;, line 805, in urlopen
    if retries.is_retry(method, response.status, has_retry_after):
  File &quot;/usr/local/lib/python3.7/site-packages/urllib3/util/retry.py&quot;, line 343, in is_retry
    if not self._is_method_retryable(method):
  File &quot;/usr/local/lib/python3.7/site-packages/urllib3/util/retry.py&quot;, line 331, in _is_method_retryable
    if self.method_whitelist and method.upper() not in self.method_whitelist:
AttributeError: 'Retry' object has no attribute 'method_whitelist'
</code></pre>
<p>Could someone help me with this?</p>
",1,1607952040,python;python-3.x;request;urllib3,True,12877,5,1686915765,https://stackoverflow.com/questions/65289720/attributeerror-retry-object-has-no-attribute-method-whitelist
76384373,Python 3.10 -sslv3 alert handhsake failure 997,"<p>Python 3.10 increased default security setting of the TLS. the same below code is working in python 3.9 but throwing [SSL:SSLV3_ALERT_HANDSHAKE_FAILURE] (_ssl.c:997)</p>
<pre><code>import requests
import urllib3
requests.package.urllib3.util.ssl_.DEFAULT_CIPHERS = 'ALL:@SECLEVEL=1'

headers = {'Content-Type':'application/json'}
data = {
        'selct1':sel,
        'selct2':der
}
res = requests.get(url,data=data,headers=headers,verify=False)
</code></pre>
<p>I tried changing SecLevel to 1 , but still its failing.</p>
",1,1685640860,python;python-3.x;python-requests;urllib3,True,249,1,1685722714,https://stackoverflow.com/questions/76384373/python-3-10-sslv3-alert-handhsake-failure-997
76348554,How to solve a version problem in urllib3?,"<p>So, i'm trying to make a web scraping using python like this:</p>
<pre><code>import requests
from bs4 import BeautifulSoup

url = 'http://127.0.0.1:3744/'
response = requests.get(url)
raw_html = response.text 

parsed_html = BeautifulSoup(raw_html, 'html.parser')
print(raw_html)
</code></pre>
<p>and here's the error:</p>
<p><code>line 57, in &lt;module&gt;     raise RuntimeError('Requests dependency \'urllib3\' must be version &gt;= 1.21.1, &lt; 1.22!') RuntimeError: Requests dependency 'urllib3' must be version &gt;= 1.21.1, &lt; 1.22!</code></p>
<p>I tried to do everything, downloading different versions of urllib3, but there is a major problem because Selenium and Requests require different versions of urllib3.</p>
<p>checkout this:</p>
<p><em>after pip install urllib=1.23 for selenium</em></p>
<p>Collecting urllib3==1.23
Downloading urllib3-1.23-py2.py3-none-any.whl (133 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.3/133.3 kB 2.6 MB/s eta 0:00:00
Installing collected packages: urllib3
Attempting uninstall: urllib3
Found existing installation: urllib3 2.0.2
Uninstalling urllib3-2.0.2:
Successfully uninstalled urllib3-2.0.2
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
requests2 2.16.0 requires urllib3&lt;1.22,&gt;=1.21.1, but you have urllib3 1.23 which is incompatible.
selenium 4.9.1 requires urllib3[socks]&lt;3,&gt;=1.26, but you have urllib3 1.23 which is incompatible.</p>
<p>and after trying to update urrlib3...</p>
<p>Requirement already satisfied: urllib3 in &quot;dir&quot;
Collecting urllib3
Using cached urllib3-2.0.2-py3-none-any.whl (123 kB)
Installing collected packages: urllib3
Attempting uninstall: urllib3
Found existing installation: urllib3 1.23
Uninstalling urllib3-1.23:
Successfully uninstalled urllib3-1.23
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
requests2 2.16.0 requires urllib3&lt;1.22,&gt;=1.21.1, but you have urllib3 2.0.2 which is incompatible.</p>
",-1,1685214063,python;http;web-scraping;python-requests;urllib3,True,3322,2,1685215198,https://stackoverflow.com/questions/76348554/how-to-solve-a-version-problem-in-urllib3
76293394,InsecureRequestWarning: Unverified HTTPS request is being made to host error,"<p>I have installed Google Cloud CLI on my windows computer and When i am running the gcloud command from my windows computer through PowerShell I am getting below error message;</p>
<pre><code>python.exe : C:\Program Files (x86)\Google\Cloud SDK\google-cloud-sdk\lib\third_party\urllib3\connectionpool.py:1063: InsecureRequestWarning: Unverified HTTPS request is being made to host 
'iam.googleapis.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
At C:\Program Files (x86)\Google\Cloud SDK\google-cloud-sdk\bin\gcloud.ps1:114 char:3
+   &amp; &quot;$cloudsdk_python&quot; $run_args_array
+   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : NotSpecified: (C:\Program File...ml#ssl-warnings:String) [], RemoteException
    + FullyQualifiedErrorId : NativeCommandError 
</code></pre>
<p>I read in many posts that we can disable this error message by adding this below lines</p>
<p>import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)</p>
<p>Or</p>
<p>import requests
requests.packages.urllib3.disable_warnings()</p>
<p>But I am not sure on which file and path i need to add above lines. I tried to add that lines on file “C:\Program Files (x86)\Google\Cloud SDK\google-cloud-sdk\lib\third_party\urllib3\connectionpool.py” by editing it in notepad and saved it but still i am getting the same error. Can someone please guide me how to stop this error from the PowerShell script.</p>
<p>Regards.</p>
",0,1684552819,python;powershell;google-cloud-platform;gcloud;urllib3,False,1020,0,1684552819,https://stackoverflow.com/questions/76293394/insecurerequestwarning-unverified-https-request-is-being-made-to-host-error
76183443,Azure DevOps release pipeline AttributeError: type object &#39;Retry&#39; has no attribute &#39;DEFAULT_METHOD_WHITELIST&#39;,"<p>I have a release pipeline that has been working fine until 5/4/2023, when it started throwing this error and getting hung up in a retry loop upon trying to start a Databricks cluster. The log looks like this, and it does not exit until a user manually cancels it:</p>
<pre><code>2023-05-04T15:31:48.9504235Z ##[section]Starting: Start cluster
2023-05-04T15:31:48.9507476Z ==============================================================================
2023-05-04T15:31:48.9507600Z Task         : Start a Databricks Cluster
2023-05-04T15:31:48.9507679Z Description  : Make sure a Databricks Cluster is started
2023-05-04T15:31:48.9507786Z Version      : 0.5.6
2023-05-04T15:31:48.9507851Z Author       : Microsoft DevLabs
2023-05-04T15:31:48.9507957Z Help         : 
2023-05-04T15:31:48.9508027Z ==============================================================================
2023-05-04T15:31:49.5839599Z parse error: Invalid numeric literal at line 1, column 6
2023-05-04T15:31:49.6142380Z Cluster *** not running, turning on...
2023-05-04T15:31:49.7846916Z Error: AttributeError: type object 'Retry' has no attribute 'DEFAULT_METHOD_WHITELIST'
2023-05-04T15:31:49.9842231Z parse error: Invalid numeric literal at line 1, column 6
2023-05-04T15:32:20.0262096Z Starting...
2023-05-04T15:32:20.2317560Z parse error: Invalid numeric literal at line 1, column 6
2023-05-04T15:32:50.2656819Z Starting...
2023-05-04T15:32:50.4489482Z parse error: Invalid numeric literal at line 1, column 6
2023-05-04T15:33:20.4791525Z Starting...
2023-05-04T15:33:20.6468387Z parse error: Invalid numeric literal at line 1, column 6
2023-05-04T15:33:50.6758000Z Starting...
2023-05-04T15:33:50.9257125Z parse error: Invalid numeric literal at line 1, column 6
2023-05-04T15:34:20.9617006Z Starting...
2023-05-04T15:34:21.1491705Z parse error: Invalid numeric literal at line 1, column 6
2023-05-04T15:34:51.1782850Z Starting...
2023-05-04T15:34:51.3835540Z parse error: Invalid numeric literal at line 1, column 6
2023-05-04T15:35:21.4213881Z Starting...
2023-05-04T15:35:21.6628904Z parse error: Invalid numeric literal at line 1, column 6
2023-05-04T15:35:51.6999474Z Starting...
2023-05-04T15:35:51.8763360Z parse error: Invalid numeric literal at line 1, column 6
</code></pre>
<p>This is happening for release pipelines to several different environments.</p>
<p>I tried restarting the Databricks cluster, but the same thing happens once the cluster starts again.</p>
<p>If the Start Cluster step is removed, the same happens in the next step, where it tries to deploy notebooks to a workspace.</p>
",3,1683298074,python;azure-devops;azure-databricks;azure-pipelines-release-pipeline;urllib3,True,1069,3,1683760405,https://stackoverflow.com/questions/76183443/azure-devops-release-pipeline-attributeerror-type-object-retry-has-no-attribu
76218020,Python &quot;requests&quot; library targeting multiple redundant hosts,"<p>We have a Python library that uses the &quot;requests&quot; package and acts as a client to a HW device that provides a REST API. In the setup of the data centers, there are multiple redundant such HW devices. Any one of them can be used by the client library because they synchronize their data.</p>
<p>We would like to avoid setting up a load balancer that provides a single IP address for those redundant HW devices, and instead want to configure the Python client library with a list of target IP addresses or host names of these redundant HW devices, so that it uses anyone of them that currently works.</p>
<p>Is there a way to configure that in the Python &quot;requests&quot; library (or its underlying urllib3 library)?</p>
",0,1683719291,python;python-requests;urllib3,True,154,1,1683721457,https://stackoverflow.com/questions/76218020/python-requests-library-targeting-multiple-redundant-hosts
76180798,"Jenkins Job Failing for urllib3: ValueError: Timeout value connect was &lt;object object at 0x7efe5adb9aa0&gt;, but it must be an int, float or None","<p>As of May 4, 2023, at 16:00, I started seeing one of our Jenkins job failing with the following error:</p>
<pre class=""lang-py prettyprint-override""><code>Traceback (most recent call last):
   File &quot;/home/jenkins/agent/workspace/my-jenkins-job/.tox/appdev-ci-staging/lib/python3.9/site-packages/jenkins/__init__.py&quot;, line 822, in get_info
     return json.loads(self.jenkins_open(
   File &quot;/home/jenkins/agent/workspace/my-jenkins-job/.tox/appdev-ci-staging/lib/python3.9/site-packages/jenkins/__init__.py&quot;, line 560, in jenkins_open
     return self.jenkins_request(req, add_crumb, resolve_auth).text
   File &quot;/home/jenkins/agent/workspace/my-jenkins-job/.tox/appdev-ci-staging/lib/python3.9/site-packages/jenkins/__init__.py&quot;, line 576, in jenkins_request
     self.maybe_add_crumb(req)
   File &quot;/home/jenkins/agent/workspace/my-jenkins-job/.tox/appdev-ci-staging/lib/python3.9/site-packages/jenkins/__init__.py&quot;, line 373, in maybe_add_crumb
     response = self.jenkins_open(requests.Request(
   File &quot;/home/jenkins/agent/workspace/my-jenkins-job/.tox/appdev-ci-staging/lib/python3.9/site-packages/jenkins/__init__.py&quot;, line 560, in jenkins_open
     return self.jenkins_request(req, add_crumb, resolve_auth).text
   File &quot;/home/jenkins/agent/workspace/my-jenkins-job/.tox/appdev-ci-staging/lib/python3.9/site-packages/jenkins/__init__.py&quot;, line 579, in jenkins_request
     self._request(req))
   File &quot;/home/jenkins/agent/workspace/my-jenkins-job/.tox/appdev-ci-staging/lib/python3.9/site-packages/jenkins/__init__.py&quot;, line 553, in _request
     return self._session.send(r, **_settings)
   File &quot;/home/jenkins/agent/workspace/my-jenkins-job/.tox/appdev-ci-staging/lib/python3.9/site-packages/requests/sessions.py&quot;, line 701, in send
     r = adapter.send(request, **kwargs)
   File &quot;/home/jenkins/agent/workspace/my-jenkins-job/.tox/appdev-ci-staging/lib/python3.9/site-packages/requests/adapters.py&quot;, line 483, in send
     timeout = TimeoutSauce(connect=timeout, read=timeout)
   File &quot;/home/jenkins/agent/workspace/my-jenkins-job/.tox/appdev-ci-staging/lib/python3.9/site-packages/urllib3/util/timeout.py&quot;, line 119, in __init__
     self._connect = self._validate_timeout(connect, &quot;connect&quot;)
   File &quot;/home/jenkins/agent/workspace/my-jenkins-job/.tox/appdev-ci-staging/lib/python3.9/site-packages/urllib3/util/timeout.py&quot;, line 156, in _validate_timeout
     raise ValueError(
 ValueError: Timeout value connect was &lt;object object at 0x7efe5adb9aa0&gt;, but it must be an int, float or None.
</code></pre>
<p>As nothing had changed on my side in my configuration, it looks like an upstream issue.</p>
<p>I was using <code>requests</code> Python library in my job and <code>requests</code> uses <code>urllib3</code>.</p>
<p>How can we fix this?</p>
",4,1683279331,python;jenkins;python-requests;jenkins-pipeline;urllib3,True,4614,1,1683717711,https://stackoverflow.com/questions/76180798/jenkins-job-failing-for-urllib3-valueerror-timeout-value-connect-was-object-o
75926017,os.environ[&#39;SSL_CERT_FILE&#39;] stores a path to a non-existing file in visual studio code while debugging,"<p>This question is directly connected to <a href=""https://stackoverflow.com/q/75904821/7745011"">my last question</a>, however tackles a different topic so I am opening a new one.</p>
<p>As mentioned there I am getting an error relating to a missing SSL cert. The error does not appear when the script is started from Terminal, using PyCharm or running from VSCode, but without the debugger. Only when the script is run with the debugger, the exception is thrown.</p>
<p><strong>After debugging a while I have found that the reason for the problem is the environment variable <code>os.environ['SSL_CERT_FILE']</code> which in this case leads to a non-existing <code>C:\\Users\\MYUSER~1\\AppData\\Local\\Temp\\_MEI97082\\certifi\\cacert.pem</code></strong></p>
<ol>
<li>Starting the script without the debugger or in PyCharm, this variable is not set (debugging the imported minio package showed me that the result of <code>certifi.where()</code> is used if the variable is empty.</li>
<li>With the debugger on, it is set before any of my script is executed (import os and print out all environment variables in the first line)</li>
<li>If I manually delete the variable with <code>del os.environ['SSL_CERT_FILE']</code> the rest of the script works fine, but the variable is again set in the next run</li>
<li>I am using python 3.11, MiniConda and Windows 10, Visual Studio Code is updated to the last version 1.77.0</li>
<li>Setting the environment variable in <code>launch.json</code> with <code>&quot;env&quot;: {&quot;SSL_CERT_FILE&quot;: &quot;foo&quot;}</code> will override the varible as expected, however as soon as I remove this line the wrong value appears again.</li>
<li>The part &quot;<code>..\\_MEI247522\\...</code>&quot; in the value will change from run to run</li>
<li>Creating a completely new folder/project the problem still exists</li>
<li>I also tested with another python environment (Python 3.9.7) and the problem still is the same.</li>
<li>From user @Horsing's suggestion: I have also removed all the code from my script, except for <code>import os</code>. As soon as <code>os</code> is imported and I can inspect <code>os.environ</code>, the environment variable is already set.</li>
</ol>
<p>I honestly have no idea, where and why this variable is set when the script is run in the debugger and what triggers it. Any help would be much appreciated, since manually deleting it is not really a good solution!</p>
<p><strong>Addition</strong>
Here is the Python Debug Console output in VS Code (with my username changed). For this I have removed the launch.json and started the debugger with <code>Python:File</code></p>
<p><em>complete code:</em></p>
<pre><code>import os
print(os.environ.get('SSL_CERT_FILE'))
</code></pre>
<p><em>console output:</em></p>
<pre><code>(minio) PS C:\Users\myuser\Documents\source\Python\minio-project&gt;  c:; cd 'c:\Users\myuser\Documents\source\Python\minio-project'; &amp; 'C:\Users\myuser\Miniconda3\envs\minio\python.exe' 'c:\Users\myuser\.vscode\extensions\ms-python.python-2023.6.0\pythonFiles\lib\python\debugpy\adapter/../..\debugpy\launcher' '60007' '--' 'C:\Users\myuser\Documents\source\Python\minio-project\main.py' 
C:\Users\MYUSER~1\AppData\Local\Temp\_MEI223042\certifi\cacert.pem
</code></pre>
<p>Again, the printed path does not exist on my computer</p>
",2,1680588744,python;visual-studio-code;environment-variables;urllib3,True,808,2,1683181833,https://stackoverflow.com/questions/75926017/os-environssl-cert-file-stores-a-path-to-a-non-existing-file-in-visual-studi
63958960,"parallel azure blob upload get warning &#39;urllib3.connectionpool WARNING - Connection pool is full, discarding connection&#39;","<p>Since I need to upload a large number of files over 100000 to azure blob storage, I wrote a program to upload by multi-thread processing like this.</p>
<pre><code>from azure.storage.blob import BlobServiceClient, BlobClient
from itertools import repeat
from concurrent.futures import ThreadPoolExecutor
import os

def upload_single_blob(blob_service_client, blob_path):
    # Create a blob client using the local file name as the name for the blob
    blob_client = blob_service_client.get_blob_client(container='MyContainer', 
    blob=blob_path)

    # Upload the file
    with open(blob_path, &quot;rb&quot;) as data:
        blob_client.upload_blob(data)

# make blob service client from connect str
blob_service_client = BlobServiceClient.from_connection_string(connect_str)
# make file path list to upload
blob_path_list = os.listdir(&quot;./blob_files/&quot;)
blob_path_list = map(lambda x: &quot;./blob_files/&quot;+x, blob_path_list)
blob_path_list = list(blob_path_list)

# multi threading upload to blob
with ThreadPoolExecutor(max_workers=100) as executor:
    executor.map(upload_single_blob, repeat(blob_service_client), blob_path_list)
</code></pre>
<p>However, when I ran this program at azure VM (OS is ubuntu18.04), I got the warning a lot.</p>
<pre><code>urllib3.connectionpool WARNING --Connection pool is full, discarding connection: myblobaccount.blob.core.windows.net
</code></pre>
<p>I didn't measure it accurately, but it seemed that there were only about 10 connections at the same time, even though uploading in parallel with 100 threads.</p>
<p>How can I increase the number of connections any more?</p>
",4,1600444165,python;multithreading;azure;azure-blob-storage;urllib3,False,544,1,1683146937,https://stackoverflow.com/questions/63958960/parallel-azure-blob-upload-get-warning-urllib3-connectionpool-warning-connect
76122752,"SSL certification error with some Python libraries, but not others","<p>I'm trying to make HTTP requests from a Python application but I'm encountering some odd responses surrounding SSL certification. I set up the following simple experiment:</p>
<p>Test scenario: Send <code>GET</code> request to <code>https://example.com</code></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th></th>
<th>Within Docker container</th>
<th>Outside Docker container</th>
</tr>
</thead>
<tbody>
<tr>
<td>requests</td>
<td>SSL cert error</td>
<td>SSL cert error</td>
</tr>
<tr>
<td>httpx</td>
<td>SSL cert error</td>
<td>SSL cert error</td>
</tr>
<tr>
<td>urllib3</td>
<td>SSL cert error</td>
<td>No error</td>
</tr>
<tr>
<td>Postman</td>
<td>N/A</td>
<td>No error</td>
</tr>
<tr>
<td>cURL</td>
<td>N/A</td>
<td>No error</td>
</tr>
</tbody>
</table>
</div>
<p>The <code>SSL cert error</code> is always a variation of this:</p>
<pre class=""lang-py prettyprint-override""><code>httpx.ConnectError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)
</code></pre>
<p>I sent all these requests while on my company's VPN. If I disable the VPN I get an error relating to a proxy:</p>
<pre class=""lang-py prettyprint-override""><code>requests.exceptions.ProxyError: HTTPSConnectionPool(host='example.com', port=443): Max retries exceeded with url: / (Caused by ProxyError('Unable to connect to proxy', NameResolutionError(&quot;&lt;urllib3.connection.HTTPSConnection object at 0x105bb03a0&gt;: Failed to resolve &lt;company_proxy_url_here&gt; ([Errno 8] nodename nor servname provided, or not known)&quot;)))
</code></pre>
<p>Is anyone able see if the issue is VPN related, library related, or Docker related? Or some combination of them all?</p>
",0,1682613231,python;docker;ssl;https;urllib3,False,571,0,1682613231,https://stackoverflow.com/questions/76122752/ssl-certification-error-with-some-python-libraries-but-not-others
66109512,Segmentation fault on requests.request,"<p>I am getting a segmentation fault after using python requests library to connect to Twitter's v2 API endpoint.</p>
<p>The request is created on a separate thread in a generator object</p>
<pre><code>class TwitterStream:
    def __init__():
        self.response = self.build_response()    

    def build_response():
        response = requests.request(&quot;GET&quot;, url, headers, stream=True)
        if response.status_code != 200:
            raise Exception(&quot;Exception&quot;)
        for record in response.iter_lines():
            yield record

    def __next__():
        return next(self.response)

</code></pre>
<p>Then on the execution thread, I use a ThreadPoolExecutor to get each response line:</p>
<pre><code>record_future = threadpoolexecutor.submit(next, TwitterStreamObjectInstance)
if record_future.done():
   # do stuff  with record_future.result()
</code></pre>
<p>I keep getting a segmentation fault at the <code>response = requests.request(&quot;GET&quot;, url, headers, stream=True)</code> line, I'm pretty sure, based on a lot of <code>print()</code> debugging.</p>
<p>I tried the gdb debugging method, and got back this stacktrace -- I am assuming libssl.so.1 is causing this problem, but I don't really know how to investigate this further.</p>
<pre><code>#0  0x000000000001d85e in ?? ()
#1  0x00007ffff406ffa0 in OPENSSL_init_ssl () from /mnt/d/Projects/EBKA/edna_env/lib/python3.7/site-packages/mysql/vendor/libssl.so.1.1
#2  0x00007ffff4074be3 in SSL_CTX_new () from /mnt/d/Projects/EBKA/edna_env/lib/python3.7/site-packages/mysql/vendor/libssl.so.1.1
#3  0x00007ffff43c855d in ?? () from /usr/lib/python3.7/lib-dynload/_ssl.cpython-37m-x86_64-linux-gnu.so
#4  0x00005555556c9ff2 in ?? ()
#5  0x00005555556857d0 in _PyMethodDef_RawFastCallKeywords ()
#6  0x0000555555685560 in _PyCFunction_FastCallKeywords ()
#7  0x00005555556fa4e4 in _PyEval_EvalFrameDefault ()
#8  0x00005555556f4e2f in _PyEval_EvalCodeWithName ()
#9  0x0000555555687591 in _PyObject_Call_Prepend ()
#10 0x00005555556c9e3c in ?? ()
#11 0x00005555556c5ce5 in ?? ()
#12 0x0000555555685f72 in _PyObject_FastCallKeywords ()
#13 0x00005555556fa00d in _PyEval_EvalFrameDefault ()
#14 0x00005555556f4e2f in _PyEval_EvalCodeWithName ()
#15 0x0000555555686efa in _PyFunction_FastCallKeywords ()
#16 0x00005555556f6b0a in _PyEval_EvalFrameDefault ()
#17 0x0000555555686e1a in _PyFunction_FastCallKeywords ()
.
.
.
#58 0x00005555555c5d24 in _PyFunction_FastCallDict ()
#59 0x00005555556f73d0 in _PyEval_EvalFrameDefault ()
#60 0x0000555555686e1a in _PyFunction_FastCallKeywords ()
#61 0x00005555556f5fde in _PyEval_EvalFrameDefault ()
#62 0x0000555555686e1a in _PyFunction_FastCallKeywords ()
#63 0x00005555556f5fde in _PyEval_EvalFrameDefault ()
#64 0x0000555555687359 in _PyObject_Call_Prepend ()
#65 0x00005555556879b8 in PyObject_Call ()
#66 0x0000555555800193 in ?? ()
#67 0x00005555557c30d7 in ?? ()
#68 0x00007ffff7bbb6db in start_thread (arg=0x7fffd0f92700) at pthread_create.c:463
#69 0x00007ffff6cf071f in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95
</code></pre>
<hr />
<p>A side note -- I have been having a problem with closing threads, where I get the following</p>
<pre><code>libgcc_s.so.1 must be installed for pthread_cancel to work
</code></pre>
<p>A suggestion I got was to do the following in the beginning of execution:</p>
<pre><code>import ctypes
libgcc_s = ctypes.CDLL('libgcc_s.so.1')
</code></pre>
<p>but this didn't work. The reason I mention this is because after I tried this, I kept getting a &quot;cannot find libssl&quot; error, and I am not sure if it would be related.</p>
",2,1612819126,python;gdb;python-multithreading;threadpoolexecutor;urllib3,True,959,2,1682541406,https://stackoverflow.com/questions/66109512/segmentation-fault-on-requests-request
51778764,Obnoxious CryptographyDeprecationWarning because of missing hmac.compare_time function everywhere,"<p>Things were running along fine until one of my projects started printing this everywhere, at the top of every execution, at least once:</p>

<pre><code>local/lib/python2.7/site-packages/cryptography/hazmat/primitives/constant_time.py:26: CryptographyDeprecationWarning: Support for your Python version is deprecated. The next version of cryptography will remove support. Please upgrade to a 2.7.x release that supports hmac.compare_digest as soon as possible.
</code></pre>

<p>I have no idea why it started and it's disrupting the applications'/tools' output, especially when it's being captured and consumed by other tools. Like many difficulties throughout time, I'm fairly certain it is related to <code>urllib</code> and, by association, <code>requests</code>. Worse, I have so many projects and cross-dependencies that I can't possibly update all of the imports and branches with the call to <code>warnings.filterwarnings()</code> to suppress the warning.</p>

<p>I have Python 2.7.6 . Apparently this goes away in 2.7.7 . Only, I have some systems that have 2.7.6 where I <em>do not</em> see the warnings. So, something may or may not be disabling them in one version and I might've inadvertently replaced it with another version.</p>

<p>My Ubuntu, Python, urllib, requests (with the security option), cryptography, and hmac are all identical versions/builds on systems that do print the warning and systems that do not.</p>

<p>There appears to be no relevant warnings or announcements online and it seems like any related project is static/stable by this point (even though 'hmac' can be installed via PIP, it hasn't changed in eight years).</p>
",20,1533873573,python;python-requests;urllib;urllib3,True,53586,7,1682445621,https://stackoverflow.com/questions/51778764/obnoxious-cryptographydeprecationwarning-because-of-missing-hmac-compare-time-fu
76097342,Can PythonAnywhere Free Account code call outside https Apis?,"<p>I'm writing a telegram bot to serve a set of relational card games, and used a blend of ChatGPT output with what I found at <a href=""https://blog.pythonanywhere.com/148/"" rel=""nofollow noreferrer"">https://blog.pythonanywhere.com/148/</a>.</p>
<p>I can get the bot to start, which is great.
The issue is that when I call my website <a href=""http://www.relationalgames.com/cards/deck?Papo"" rel=""nofollow noreferrer"">www.relationalgames.com/cards/deck?Papo</a> a Papo, I get the error:</p>
<p>File &quot;/usr/local/lib/python3.10/site-packages/urllib3/util/retry.py&quot;, line 592, in increment
raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.relationalgames.com', port=443): Max retries exceeded with url: /cards?deck=Papo%20a%20Papo (Caused by NewCo
nnectionError('&lt;urllib3.connection.HTTPSConnection object at 0x7f46da0873a0&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))</p>
<p>I wonder if this is some limitation of PythonAnywhere free account, or if I am doing something wrong...Any ideas on how to overcome this?</p>
<p>Here is my current code</p>
<pre><code>import telepot
import json
import random
import urllib3
import time

# You can leave this bit out if you're using a paid PythonAnywhere account
proxy_url = &quot;http://proxy.server:3128&quot;
telepot.api._pools = {
    'default': urllib3.ProxyManager(proxy_url=proxy_url, num_pools=3, maxsize=10, retries=False, timeout=30),
}
telepot.api._onetime_pool_spec = (urllib3.ProxyManager, dict(proxy_url=proxy_url, num_pools=1, maxsize=1, retries=False, timeout=30))
# end of the stuff that's only needed for free accounts

http = urllib3.PoolManager(timeout=60)

def handle(msg):
    content_type, chat_type, chat_id = telepot.glance(msg)
    if content_type == 'text':
        if msg['text'] == '/start':
            bot.sendMessage(chat_id, &quot;Enter the name of a deck:&quot;)
        else:
            #try:
                deck_name = msg['text']
                response = http.request('GET', f'https://www.relationalgames.com/cards?deck={deck_name}') # requests.get(f'https://www.relationalgames.com/cards?deck={deck_name}')
                print(response)
                playable_deck = json.loads(response.text)
                random.shuffle(playable_deck)

                while len(playable_deck) &gt; 0:
                    card = playable_deck.pop()
                    if 'url' in card:
                        bot.sendMessage(chat_id, card['url'])
                    else:
                        bot.sendMessage(chat_id, card['cardText'])

                    if len(playable_deck) == 0:
                        bot.sendMessage(chat_id, &quot;Do you want to restart the game? (yes or no)&quot;)
                        break

                    bot.sendMessage(chat_id, &quot;Press any key for next card&quot;)
                    bot.getUpdates()
            #except:
             #   bot.sendMessage(chat_id, &quot;Error processing your request&quot;)

TOKEN = 'My Token'
bot = telepot.Bot(TOKEN)
bot.message_loop(handle)

print ('Listening ...')

# Keep the program running.
while 1:
    time.sleep(10)
</code></pre>
",0,1682390910,python;telegram-bot;pythonanywhere;urllib3,True,171,1,1682415900,https://stackoverflow.com/questions/76097342/can-pythonanywhere-free-account-code-call-outside-https-apis
76094220,SSL problem when making requests in threaded gunicorn app,"<p>I have a python app hosted in APP engine with the gunicorn gthread worker:
<code>gunicorn -w 4 --threads 16 src.main:app</code></p>
<p>In my app i make requests to a GCP cloud function in this way:</p>
<p><code>response = requests.post(url, json=payload)</code></p>
<p>I have bugsnag as error reporting and i keep getting the following error:
<code>HTTPSConnectionPool(host='GCP CLOUD FUNCTION', port=443): Max retries exceeded with url: /path (Caused by SSLError(SSLZeroReturnError(6, 'TLS/SSL connection has been closed (EOF) (_ssl.c:997)')))</code></p>
<p><a href=""https://i.stack.imgur.com/eCYOF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eCYOF.png"" alt=""Bugsnag error"" /></a></p>
<p>I'm using python 3.11 with <code>requests 2.28.2</code> <code>urllib3 1.26.15</code>
How can i avoid this problem in a threaded gunicorn app?</p>
<p>At first i thought it was a problem with certificates, i tried setting <code>verify=False</code> in the requests.post call, and the error stopped, however the problem also occurs in other libraries i use, like bugsnag and google cloud storage, and i can't disable that parameter there.</p>
<p>Maybe i was calling the functions too much, i reduced calls per minute of my app, but it kept occurring.</p>
<p>Tried updating python and dependiencies, it also failed.</p>
<p>Then i tried a last approach, removing <code>--threads</code> from the gunicorn entrypoint, and then the error was gone.</p>
<p>Does python requests work in a multi threaded environment? I searched around and found it was, as long as i don't reuse a requests.Session in many threads, but that's not the case, unless... requests uses a global session under the hood?</p>
",0,1682354864,python;python-requests;thread-safety;gunicorn;urllib3,False,332,0,1682354864,https://stackoverflow.com/questions/76094220/ssl-problem-when-making-requests-in-threaded-gunicorn-app
76081685,Post list field In urllib3,"<p>I'm creating a simple AWS lambda function to post some data to an API but the API expects a list however I can't get it to work. I've got the following code that I'm testing with</p>
<pre class=""lang-py prettyprint-override""><code>import urllib3
import json

http = urllib3.PoolManager()
file_path = 'file.png'
with open(file_path, 'rb') as fp:
  file_data = fp.read()

payload = {
  'from': ['a', 'b', 'c'],
  'file': ('file.png', file_data),
}

r = http.request(
  'POST',
  'http://httpbin.org/post',
  fields=payload
)

json.loads(r.data.decode('utf-8'))

</code></pre>
<p>But this returns an error about expecting the field to be a certain type and basically not being allowed to be a list which is fine, but how can I make this work.</p>
<p>I'm connecting to a Rails based server, and in order for a form-encoded form submission to be a list, I need to send multiple params with a name containing <code>[]</code> on the end, but of course if I try creating a dict with multiple keys the same, it'll just ignore them all so I'm not really sure how to handle this.</p>
<p>I'm pretty new to python so sorry if this is a stupid question</p>
",0,1682193607,python;urllib3,True,216,1,1682240114,https://stackoverflow.com/questions/76081685/post-list-field-in-urllib3
75985331,Python OpenAI API TypeError,"<p>I'm trying to use OpenAI API and run into a problem.
I used the standard example code from documentation:</p>
<pre><code>import openai

API_KEY = 'MY_API_KEY'
openai.api_key = API_KEY

response = openai.ChatCompletion.create(
  model=&quot;gpt-3.5-turbo&quot;,
  messages=[
        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who won the world series in 2020?&quot;},
        {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;The Los Angeles Dodgers won the World Series in 2020.&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Where was it played?&quot;}
    ]
)

print(response)
</code></pre>
<p>The output is:</p>
<blockquote>
<p>TypeError: Queue.<strong>init</strong>() takes 1 positional argument but 2 were given'</p>
</blockquote>
<p>What is the problem?</p>
<p>I tried to update requirements (<em>urrlib3</em>, <a href=""https://requests.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">Requests</a>, and <em>openai</em>) and Python.
But all them have the actual version.</p>
",0,1681212350,python;typeerror;urllib3;openai-api;chatgpt-api,False,312,1,1681654923,https://stackoverflow.com/questions/75985331/python-openai-api-typeerror
75904821,"Max retries exceeded with url: / (Caused by SSLError(FileNotFoundError(2, &#39;No such file or directory&#39;))) only during debug","<p>I am using minio as follows (example):</p>
<pre><code>minio_client = Minio(
endpoint=&quot;my.minio.com:10092&quot;,
access_key=&quot;minio&quot;,
secret_key=&quot;minio123!&quot;
)

buckets = minio_client.list_buckets()
for bucket in buckets:
    print(bucket.name)
</code></pre>
<p>Debugging the code above with VSCode (Python 3.11 / Miniconda - a new environment with only minio installed) I get the following error:</p>
<blockquote>
<p>File &quot;C:\Path\to\Miniconda3\envs\minio\Lib\site-packages\urllib3\util\retry.py&quot;, line 592, in increment
raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='my.minio.com', port=10092): Max retries exceeded with url: / (Caused by SSLError(FileNotFoundError(2, 'No such file or directory')))</p>
</blockquote>
<p>I have double checked address and credentials several times, the minio is accessible from other SDKs (I have a different application using the C# SDK) and the WebUI. Additionally if I run the program from the terminal with the same Miniconda environment activated, it works flawlessly. What could be the reason for this issue?</p>
<p><strong>EDIT</strong></p>
<p>Just tested the same code with PyCharm and there it works as well. So the issue definitely has something to do with VSCode. Additionally running the script in VSCode without the debugger works as well.</p>
",0,1680334567,python;visual-studio-code;vscode-debugger;minio;urllib3,True,648,1,1681373289,https://stackoverflow.com/questions/75904821/max-retries-exceeded-with-url-caused-by-sslerrorfilenotfounderror2-no-su
75986583,How does python access the apiClient of k8s through socks5 proxy,"<p>Accessing k8s apiclient is successful without using a proxy, as follows:</p>
<pre class=""lang-py prettyprint-override""><code>configuration = client. Configuration()
configuration.verify_ssl = False
configuration.host = &quot;xxx&quot;
configuration.api_key = {&quot;authorization&quot;: &quot;Bearer &quot; + self.token}
c = api_client. ApiClient(configuration=configuration)
api = core_v1_api.CoreV1Api(c)
# Query the namespace, the step is successful
result = api.list_namespace()
</code></pre>
<p>However, since k8s api is automatically generated, the socks proxy cannot be directly set:<a href=""https://github.com/kubernetes-client/python/issues/1064"" rel=""nofollow noreferrer"">https://github.com/kubernetes-client/python/issues/1064</a></p>
<p>Since k8s uses restClient, there is no way to pass the socks5 proxy. Currently, this method is used to connect, but it is invalid:</p>
<pre class=""lang-py prettyprint-override""><code>configuration = client. Configuration()
configuration.verify_ssl = False
configuration.api_key = {&quot;authorization&quot;: &quot;Bearer &quot; + self.token}
configuration.host = &quot;xxx&quot;
c = api_client. ApiClient(configuration=configuration)
proxy = &quot;socks5://xxx:1080&quot;
c.rest_client.pool_manager = self.build_socks_proxy_manager(configuration)
api = core_v1_api.CoreV1Api(c)
# Query namespace, this step timed out, unable to connect
result = api.list_namespace()

def build_socks_proxy_manager(self, configuration, pools_size=4, maxsize=None):
  # skip some initialization steps
  return SOCKSProxyManager(num_pools=pools_size,
                    maxsize=maxsize,
                    cert_reqs = cert_reqs,
                    ca_certs=ca_certs,
                    cert_file=configuration.cert_file,
                    key_file=configuration.key_file,
                    proxy_url=configuration.proxy,
                    **addition_pool_args)
</code></pre>
<p>Updated based on larsks answer:</p>
<pre class=""lang-py prettyprint-override""><code>configuration = client. Configuration()
configuration.verify_ssl = False
configuration.api_key = {&quot;authorization&quot;: &quot;Bearer &quot; + self.token}
configuration.host = &quot;xxx&quot;
c = api_client. ApiClient(configuration=configuration)
proxy = &quot;socks5://xxx:1080&quot;
c.rest_client.pool_manager = SOCKSProxyManager(proxy_url=proxy)
api = core_v1_api.CoreV1Api(c)
# Query namespace, this step still timed out, unable to connect
result = api.list_namespace()
</code></pre>
",-1,1681220449,python;kubernetes;urllib3;socks5,True,160,1,1681271695,https://stackoverflow.com/questions/75986583/how-does-python-access-the-apiclient-of-k8s-through-socks5-proxy
75957723,Requests &#39;Connection refused&#39; when run on a local machine but works on google colab,"<p>It may may be a very simple question but I'm stuck here.
I tried to scrap a very well known website using the following command:</p>
<pre><code>search_url = 'http://www.p***hub.com/'
response = requests.get(search_url)
</code></pre>
<p>But I getting the following response when I try to run it localy on my computer:</p>
<blockquote>
<p>HTTPConnectionPool(host='www.p***hub.com', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f61969838d0&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))</p>
</blockquote>
<p>However, it does work when the same script is run on google colab. So I tried to use a proxy with requests (several different ones), but I still get the same error.</p>
<p>If someone knows how to solve this, I would really appreciate that.</p>
<p>Thank you and have a nice day :)</p>
",0,1680864291,python;python-requests;proxy;google-colaboratory;urllib3,False,171,0,1680864291,https://stackoverflow.com/questions/75957723/requests-connection-refused-when-run-on-a-local-machine-but-works-on-google-co
75929920,Redact sensitive info from urllib3 logger,"<p>I would like to apply a Filter to the urllib3 Loggers used in the requests module, so that the sensible info from all log strings would be redacted. For some reason, my filter is not applied to urllib3.connectionpool Logger when it's called by <code>requests.get()</code>.</p>
<h3>Reproducible example</h3>
<pre><code>import logging
import re
import requests


class Redactor(logging.Filter):
    &quot;&quot;&quot;Filter subclass to redact patterns from logs.&quot;&quot;&quot;
    redact_replacement_string = &quot;&lt;REDACTED_INFO&gt;&quot;

    def __init__(self, patterns: list[re.Pattern] = None):
        super().__init__()
        self.patterns = patterns or list()

    def filter(self, record: logging.LogRecord) -&gt; bool:
        &quot;&quot;&quot;
        Overriding the original filter method to redact, rather than filter.
        :return: Always true - i.e. always apply filter
        &quot;&quot;&quot;
        for pattern in self.patterns:
            record.msg = pattern.sub(self.redact_replacement_string, record.msg)
        return True

# Set log level
urllib_logger = logging.getLogger(&quot;urllib3.connectionpool&quot;)
urllib_logger.setLevel(&quot;DEBUG&quot;)

# Add handler
handler = logging.StreamHandler()
handler.setFormatter(logging.Formatter(&quot;logger name: {name} | message: {message}&quot;, style=&quot;{&quot;))
urllib_logger.addHandler(handler)

# Add filter
urllib_logger.info(&quot;Sensitive string before applying filter: www.google.com&quot;)
sensitive_patterns = [re.compile(r&quot;google&quot;)]
redact_filter = Redactor(sensitive_patterns)
urllib_logger.addFilter(redact_filter)
urllib_logger.info(&quot;Sensitive string after applying filter: www.google.com&quot;)

# Perform a request that's supposed to use the filtered logger
requests.get(&quot;https://www.google.com&quot;)

# Check if the logger has been reconfigured
urllib_logger.info(&quot;Sensitive string after request: www.google.com&quot;)

</code></pre>
<p>The result of this code is that the Handler is applied to all log strings, but the Filter is not applied to log strings emitted by the <code>requests.get()</code> function:</p>
<pre><code>logger name: urllib3.connectionpool | message: Sensitive string before applying filter: www.google.com
logger name: urllib3.connectionpool | message: Sensitive string after applying filter: www.&lt;REDACTED_INFO&gt;.com
logger name: urllib3.connectionpool | message: Starting new HTTPS connection (1): www.google.com:443
logger name: urllib3.connectionpool | message: https://www.google.com:443 &quot;GET / HTTP/1.1&quot; 200 None
logger name: urllib3.connectionpool | message: Sensitive string after request: www.&lt;REDACTED_INFO&gt;.com
</code></pre>
<h3>What I'm expecting</h3>
<p>I would like the sensitive pattern (&quot;google&quot;) to be redacted everywhere:</p>
<pre><code>logger name: urllib3.connectionpool | message: Sensitive string before applying filter: www.google.com
logger name: urllib3.connectionpool | message: Sensitive string after applying filter: www.&lt;REDACTED_INFO&gt;.com
logger name: urllib3.connectionpool | message: Starting new HTTPS connection (1): www.&lt;REDACTED_INFO&gt;.com:443
logger name: urllib3.connectionpool | message: https://www.&lt;REDACTED_INFO&gt;.com:443 &quot;GET / HTTP/1.1&quot; 200 None
logger name: urllib3.connectionpool | message: Sensitive string after request: www.&lt;REDACTED_INFO&gt;.com
</code></pre>
<h3>What I tried</h3>
<ol>
<li>I tried applying the same Filter to &quot;root&quot; Logger, to &quot;urllib3&quot; Logger and to all existing Loggers and get the same result (like this):</li>
</ol>
<pre><code>all_loggers = [logger for logger in logging.root.manager.loggerDict.values()
               if not isinstance(logger, logging.PlaceHolder)]

for logger in all_loggers:
    logger.addFilter(redact_filter)
</code></pre>
<ol start=""2"">
<li><p>I tried applying the Filter to the Handler, not to the Logger, since it seems that the Handler is applied to all log strings. Still no luck.</p>
</li>
<li><p>I know that I could subclass a Formatter and do the redactions in there, but I think formatting and redacting are two different functions and I would like to keep them separately. Also, it would be nice to understand the logic in the logging module that produces the results that I'm getting.</p>
</li>
</ol>
",2,1680614738,python;python-requests;urllib3;python-logging;redaction,False,405,2,1680707231,https://stackoverflow.com/questions/75929920/redact-sensitive-info-from-urllib3-logger
75926525,Python urllib2 -&gt; urllib3 request.urlopen,"<p>I need to switch from urllib2 to urllib3. There is a problem with a request.</p>
<p>python2 code:</p>
<pre><code>reqdata='{&quot;PM1OBJ1&quot;:{&quot;FREQ&quot;:&quot;&quot;,&quot;U_AC&quot;:&quot;&quot;,&quot;I_AC&quot;:&quot;&quot;,&quot;P_AC&quot;:&quot;&quot;,&quot;P_TOTAL&quot;:&quot;&quot;}}'
response = urllib2.urlopen('http://'+ ipaddress +'/lala.cgi' ,data=reqdata)
</code></pre>
<p>python3 code:</p>
<pre><code>reqdata={&quot;PM1OBJ1&quot;:{&quot;FREQ&quot;:&quot;&quot;,&quot;U_AC&quot;:&quot;&quot;,&quot;I_AC&quot;:&quot;&quot;,&quot;P_AC&quot;:&quot;&quot;,&quot;P_TOTAL&quot;:&quot;&quot;}}
mydata = urllib.parse.urlencode(reqdata)
mydata = mydata.encode('ascii') # data should be bytes
response = urllib.request.urlopen('http://'+ ipaddress +'/lala.cgi', mydata)
</code></pre>
<p>When I use Wireshare for debugging I see that the reqdata under pythen2 is transmittet as string.
<a href=""https://i.stack.imgur.com/E9ePd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/E9ePd.png"" alt=""enter image description here"" /></a></p>
<p>With python3 it looks like:</p>
<p><a href=""https://i.stack.imgur.com/Y7van.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Y7van.png"" alt=""enter image description here"" /></a></p>
<p>So how can I user urllib3 with the same output as urllib2?</p>
",1,1680592763,python;urllib2;urllib3,True,81,1,1680593878,https://stackoverflow.com/questions/75926525/python-urllib2-urllib3-request-urlopen
75922950,Connection Lifetime with urllib3 Library,"<p>I’m using Locust for load testing and I’m trying to implement a connection lifetime such that connections in a connection pool are closed on the client side after a certain amount of time has passed since the connection’s creation. This functionality exists in C# with <a href=""https://learn.microsoft.com/en-us/dotnet/api/system.net.http.socketshttphandler.pooledconnectionlifetime?view=net-7.0"" rel=""nofollow noreferrer"">PooledConnectionLifetime</a>.</p>
<p>I've been unable to replicate the connection lifetime functionality with the connection library that Locust uses, urllib3. I have a client script that uses HttpUser. I’ve not had success after getting the HttpUser to use a PoolManager subclass. The subclass uses HttpConnection and HttpsConnection subclasses that have overridden methods which attempt to implement the connection lifetime functionality. It seems like the subclasses are not even used, as they don't produce any logs when the Locust client is running.</p>
<p>Is there a way to add the connection lifetime functionality to urllib3, or another solution that can be used?</p>
<p>Edit: My attempt to implement the functionality leveraged the <code>conn.timeout.get_connect_duration()</code> <a href=""https://urllib3.readthedocs.io/en/stable/reference/urllib3.util.html#urllib3.util.Timeout.get_connect_duration"" rel=""nofollow noreferrer"">method</a>. I'm not sure if <code>start_connect()</code> is called only on the first time <code>_make_request</code> is used, or every time it's taken out of the pool.</p>
",2,1680548732,python;locust;urllib3,True,193,1,1680553668,https://stackoverflow.com/questions/75922950/connection-lifetime-with-urllib3-library
67502234,snowflake python connector throws 403 forbidden after 6 hours,"<p>I have a piece of code that downloads large amounts of data(&gt;200M) from snowflake and writes it into my local database. I am using <code>Python Snowflake connector v2.1.3</code> to pull my data. It runs properly for ~6 hours after which it starts to throw 403 error from snowflake and eventually shuts down. I have tried using the recommended params such as <code>insecure_mode=True</code> and <code>client_session_keep_alive=True</code>, but nothing work:</p>
<pre><code>2021-05-12 09:45:40,061 [INFO/ForkPoolWorker-2/pid(26)][/app/.venv/lib/python3.7/site-packages/snowflake/connector/ssl_wrap_socket.py:427] THIS CONNECTION IS IN INSECURE MODE. IT MEANS THE CERTIFICATE WILL BE VALIDATED BUT THE CERTIFICATE REVOCATION STATUS WILL NOT BE CHECKED.
2021-05-12 09:45:40,279 [DEBUG/ForkPoolWorker-2/pid(26)][/app/.venv/lib/python3.7/site-packages/urllib3/connectionpool.py:393] https://sfc-va-ds1-2-customer-stage.s3.amazonaws.com:443 &quot;GET /9jz1-s-vass3666/results/019c2ffa-0502-21c7-0000-39b10525bbee_0/main/data_0_0_18?x-amz-server-side-encryption-customer-algorithm=AES256&amp;response-content-encoding=gzip&amp;AWSAccessKeyId=&lt;key_id&gt;&amp;Expires=1620808614&amp;Signature=&lt;signature&gt; HTTP/1.1&quot; 403 None
2021-05-12 09:45:40,280 [DEBUG/ForkPoolWorker-2/pid(26)][/app/.venv/lib/python3.7/site-packages/snowflake/connector/network.py:822] HTTP 403: Forbidden. Retrying...
2021-05-12 09:45:40,281 [ERROR/ForkPoolWorker-2/pid(26)][/app/.venv/lib/python3.7/site-packages/snowflake/connector/network.py:678] HTTP 403: Forbidden
Traceback (most recent call last):
  File &quot;/app/.venv/lib/python3.7/site-packages/snowflake/connector/network.py&quot;, line 652, in _request_exec_wrapper
    **kwargs)
  File &quot;/app/.venv/lib/python3.7/site-packages/snowflake/connector/network.py&quot;, line 913, in _request_exec
    raise err
  File &quot;/app/.venv/lib/python3.7/site-packages/snowflake/connector/network.py&quot;, line 824, in _request_exec
    raise RetryRequest(exi)
snowflake.connector.network.RetryRequest: HTTP 403: Forbidden
2021-05-12 09:45:40,282 [DEBUG/ForkPoolWorker-2/pid(26)][/app/.venv/lib/python3.7/site-packages/snowflake/connector/network.py:953] Active requests sessions: 1, idle: 3
2021-05-12 09:45:40,282 [ERROR/ForkPoolWorker-2/pid(26)][/app/.venv/lib/python3.7/site-packages/snowflake/connector/chunk_downloader.py:148] Failed to fetch the large result set chunk 18/2035
Traceback (most recent call last):
  File &quot;/app/.venv/lib/python3.7/site-packages/snowflake/connector/network.py&quot;, line 652, in _request_exec_wrapper
    **kwargs)
  File &quot;/app/.venv/lib/python3.7/site-packages/snowflake/connector/network.py&quot;, line 913, in _request_exec
    raise err
  File &quot;/app/.venv/lib/python3.7/site-packages/snowflake/connector/network.py&quot;, line 824, in _request_exec
    raise RetryRequest(exi)
snowflake.connector.network.RetryRequest: HTTP 403: Forbidden

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/app/.venv/lib/python3.7/site-packages/snowflake/connector/chunk_downloader.py&quot;, line 125, in _download_chunk
    result_data = self._fetch_chunk(self._chunks[idx].url, headers)
  File &quot;/app/.venv/lib/python3.7/site-packages/snowflake/connector/chunk_downloader.py&quot;, line 258, in _fetch_chunk
    binary_data_handler=handler)
  File &quot;/app/.venv/lib/python3.7/site-packages/snowflake/connector/network.py&quot;, line 612, in fetch
    **kwargs)
  File &quot;/app/.venv/lib/python3.7/site-packages/snowflake/connector/network.py&quot;, line 692, in _request_exec_wrapper
    Error.errorhandler_wrapper(conn, None, cause)
  File &quot;/app/.venv/lib/python3.7/site-packages/snowflake/connector/errors.py&quot;, line 100, in errorhandler_wrapper
    connection.errorhandler(connection, cursor, errorclass, errorvalue)
  File &quot;/app/.venv/lib/python3.7/site-packages/snowflake/connector/errors.py&quot;, line 73, in default_errorhandler
    done_format_msg=errorvalue.get(u'done_format_msg'))
snowflake.connector.errors.ForbiddenError: HTTP 403: Forbidden
</code></pre>
<p>and all that you know after this is</p>
<pre><code>2021-05-12 09:45:46,405 [DEBUG/ForkPoolWorker-2/pid(26)][/app/.venv/lib/python3.7/site-packages/urllib3/connectionpool.py:393] https://sfc-va-ds1-2-customer-stage.s3.amazonaws.com:443 &quot;GET /9jz1-s-vass3666/results/019c2ffa-0502-21c7-0000-39b10525bbee_0/main/data_0_0_19?x-amz-server-side-encryption-customer-algorithm=AES256&amp;response-content-encoding=gzip&amp;AWSAccessKeyId=&lt;key&gt;&amp;Expires=1620808614&amp;Signature=&lt;signature&gt; HTTP/1.1&quot; 403 None
2021-05-12 09:45:46,406 [DEBUG/ForkPoolWorker-2/pid(26)][/app/.venv/lib/python3.7/site-packages/snowflake/connector/network.py:822] HTTP 403: Forbidden. Retrying...
2021-05-12 09:45:46,407 [DEBUG/ForkPoolWorker-2/pid(26)][/app/.venv/lib/python3.7/site-packages/snowflake/connector/network.py:706] retrying: errorclass=&lt;class 'snowflake.connector.errors.ForbiddenError'&gt;, error=HTTP 403: Forbidden, counter=136, sleeping=16(s)
</code></pre>
<p>it looks to me that the connection to snowflake is active but the connection to the stage is expired, is there any way that I can solve this ?</p>
",2,1620816842,python;amazon-s3;snowflake-cloud-data-platform;urllib3,True,2378,1,1680539024,https://stackoverflow.com/questions/67502234/snowflake-python-connector-throws-403-forbidden-after-6-hours
75800935,urllib: TypeError: expected string or bytes-like object,"<p>I am trying to make an API call where I pass a file in the header</p>
<pre><code>import urllib.request

headers = {'files[]': open('File.sql','rb')}
datagen ={}

request = urllib.request.Request('https://www.rebasedata.com/api/v1/convert', datagen, headers)

response1 = urllib.request.urlopen(request) 
# Error : TypeError: expected string or bytes-like object

response2 = urllib.request.urlopen('https://www.rebasedata.com/api/v1/convert', datagen, headers)
#Error: 'dict' object cannot be interpreted as an integer
</code></pre>
<p>The error I am retrieving is:</p>
<blockquote>
<p>TypeError: expected string or bytes-like object</p>
</blockquote>
<p>Any help will be appreciated</p>
",0,1679400128,python;urllib;urllib3,True,648,1,1679412225,https://stackoverflow.com/questions/75800935/urllib-typeerror-expected-string-or-bytes-like-object
75664208,WinError 10060 - urllib3,"<p>Used this code for automated open Chrome:</p>
<pre><code>from urllib.request import urlopen
html = urlopen(&quot;http://www.google.com/&quot;).read()
print(html)
</code></pre>
<p>Getting this error:</p>
<pre><code>URLError: &lt;urlopen error [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond&gt;
</code></pre>
<p>Not sure if it's proxy or timer issue?</p>
",1,1678204142,python;selenium-webdriver;selenium-chromedriver;urllib3,False,464,0,1678307742,https://stackoverflow.com/questions/75664208/winerror-10060-urllib3
75025022,urllib.error.HTTPError: HTTP Error 404: Not Found Issue when Trying to read url from Yahoo Finance given Stock Symbol,"<p>I am trying to write a program that will help me with accounting work. I am trying to pull information by using stock prices for companies over time. I am utilizing <code>urllib.request</code> to read and open URLs, but I can't get around this 404 issue, despite the URL existing. Any help would be appreciated.</p>
<p>I have tried changing the <code>import</code> statement to:</p>
<pre class=""lang-py prettyprint-override""><code>import urllib.request as ur
</code></pre>
<p>so that I can also try:</p>
<pre class=""lang-py prettyprint-override""><code>read_data = ur.urlopen(url_is).read() 
soup_is = BeautifulSoup(read_data, 'lxml')
</code></pre>
<p>but that came up with <a href=""https://i.stack.imgur.com/0HU6U.png"" rel=""nofollow noreferrer"">the same error</a>.</p>
",0,1672957669,python;web-scraping;urllib;urllib2;urllib3,False,271,1,1678269041,https://stackoverflow.com/questions/75025022/urllib-error-httperror-http-error-404-not-found-issue-when-trying-to-read-url
2422922,Python urllib3 and how to handle cookie support?,"<p>So I'm looking into <a href=""http://code.google.com/p/urllib3/"" rel=""noreferrer"">urllib3</a> because it has connection pooling and is thread safe (so performance is better, especially for crawling), but the documentation is... minimal to say the least. urllib2 has build_opener so something like:</p>

<pre><code>#!/usr/bin/python
import cookielib, urllib2
cj = cookielib.CookieJar()
opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))
r = opener.open(""http://example.com/"")
</code></pre>

<p>But urllib3 has no build_opener method, so the only way I have figured out so far is to manually put it in the header:</p>

<pre><code>#!/usr/bin/python
import urllib3
http_pool = urllib3.connection_from_url(""http://example.com"")
myheaders = {'Cookie':'some cookie data'}
r = http_pool.get_url(""http://example.org/"", headers=myheaders)
</code></pre>

<p>But I am hoping there is a better way and that one of you can tell me what it is. Also can someone tag this with ""urllib3"" please.</p>
",16,1268287390,python;urllib3,True,25095,6,1678036819,https://stackoverflow.com/questions/2422922/python-urllib3-and-how-to-handle-cookie-support
54524448,How do you use cookiejar with urllib3?,"<p>How do you use <a href=""https://docs.python.org/3/library/http.cookiejar.html"" rel=""noreferrer"">cookiejar</a> with urllib3?</p>

<p>According to <a href=""https://stackoverflow.com/q/2422922/369450"">Python urllib3 and how to handle cookie support?</a> from 2010 it wasn't supported then. However, in the change log for <a href=""https://pypi.org/project/urllib3/"" rel=""noreferrer"">urllib3</a> 1.22 (2017-07-20) there's mention of a compatibility fix for cookiejar.</p>

<blockquote>
  <p>Fixed compatibility for cookiejar. (Issue #1229)</p>
</blockquote>

<p>So it sounds like some support has been added. But I have been unable to find any mention about cookiejar or even cookies in urllib3's documentation.</p>
",5,1549315111,python;urllib3,True,459,1,1678021033,https://stackoverflow.com/questions/54524448/how-do-you-use-cookiejar-with-urllib3
10667960,Python Requests throwing SSLError,"<p>I'm working on a simple script that involves CAS, jspring security check, redirection, etc.  I would like to use Kenneth Reitz's python requests because it's a great piece of work!  However, CAS requires getting validated via SSL so I have to get past that step first.  I don't know what Python requests is wanting?  Where is this SSL certificate supposed to reside?</p>

<pre><code>Traceback (most recent call last):
  File ""./test.py"", line 24, in &lt;module&gt;
  response = requests.get(url1, headers=headers)
  File ""build/bdist.linux-x86_64/egg/requests/api.py"", line 52, in get
  File ""build/bdist.linux-x86_64/egg/requests/api.py"", line 40, in request
  File ""build/bdist.linux-x86_64/egg/requests/sessions.py"", line 209, in request 
  File ""build/bdist.linux-x86_64/egg/requests/models.py"", line 624, in send
  File ""build/bdist.linux-x86_64/egg/requests/models.py"", line 300, in _build_response
  File ""build/bdist.linux-x86_64/egg/requests/models.py"", line 611, in send
requests.exceptions.SSLError: [Errno 1] _ssl.c:503: error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed
</code></pre>
",527,1337453120,python;ssl;python-requests;urllib3,True,1181293,29,1677712145,https://stackoverflow.com/questions/10667960/python-requests-throwing-sslerror
52686968,"MaxRetryError: HTTPConnectionPool: Max retries exceeded (Caused by ProtocolError(&#39;Connection aborted.&#39;, error(111, &#39;Connection refused&#39;)))","<p>I have one question:I want to test &quot;select&quot; and &quot;input&quot;.can I write it like the code below:
original code:</p>
<pre><code> 12 class Sinaselecttest(unittest.TestCase):
 13 
 14     def setUp(self):
 15         binary = FirefoxBinary('/usr/local/firefox/firefox')
 16         self.driver = webdriver.Firefox(firefox_binary=binary)
 17 
 18     def test_select_in_sina(self):
 19         driver = self.driver
 20         driver.get(&quot;https://www.sina.com.cn/&quot;)
 21         try:
 22             WebDriverWait(driver,30).until(
 23                 ec.visibility_of_element_located((By.XPATH,&quot;/html/body/div[9]/div/div[1]/form/div[3]/input&quot;))
 24             )
 25         finally:
 26             driver.quit()
 # #测试select功能
 27         select=Select(driver.find_element_by_xpath(&quot;//*[@id='slt_01']&quot;)).select_by_value(&quot;微博&quot;)
 28         element=driver.find_element_by_xpath(&quot;/html/body/div[9]/div/div[1]/form/div[3]/input&quot;)
 29         element.send_keys(&quot;杨幂&quot;)
 30         driver.find_element_by_xpath(&quot;/html/body/div[9]/div/div[1]/form/input&quot;).click()
 31         driver.implicitly_wait(5)

 32    def tearDown(self):
 33        self.driver.close()
</code></pre>
<p>I want to test Selenium &quot;select&quot; function.so I choose sina website to select one option and input text in textarea.then search it .but when I run this test,it has error:</p>
<pre><code> Traceback (most recent call last):
      File &quot;test_sina_select.py&quot;, line 32, in tearDown
        self.driver.close()
      File &quot;/usr/lib/python2.7/site-packages/selenium/webdriver/remote/webdriver.py&quot;, line 688, in close
        self.execute(Command.CLOSE)
      File &quot;/usr/lib/python2.7/site-packages/selenium/webdriver/remote/webdriver.py&quot;, line 319, in execute
        response = self.command_executor.execute(driver_command, params)
      File &quot;/usr/lib/python2.7/site-packages/selenium/webdriver/remote/remote_connection.py&quot;, line 376, in execute
        return self._request(command_info[0], url, body=data)
      File &quot;/usr/lib/python2.7/site-packages/selenium/webdriver/remote/remote_connection.py&quot;, line 399, in _request
        resp = self._conn.request(method, url, body=body, headers=headers)
      File &quot;/usr/lib/python2.7/site-packages/urllib3/request.py&quot;, line 68, in request
        **urlopen_kw)
      File &quot;/usr/lib/python2.7/site-packages/urllib3/request.py&quot;, line 81, in request_encode_url
        return self.urlopen(method, url, **urlopen_kw)
      File &quot;/usr/lib/python2.7/site-packages/urllib3/poolmanager.py&quot;, line 247, in urlopen
        response = conn.urlopen(method, u.request_uri, **kw)
      File &quot;/usr/lib/python2.7/site-packages/urllib3/connectionpool.py&quot;, line 617, in urlopen
        release_conn=release_conn, **response_kw)
      File &quot;/usr/lib/python2.7/site-packages/urllib3/connectionpool.py&quot;, line 617, in urlopen
        release_conn=release_conn, **response_kw)
      File &quot;/usr/lib/python2.7/site-packages/urllib3/connectionpool.py&quot;, line 617, in urlopen
        release_conn=release_conn, **response_kw)
      File &quot;/usr/lib/python2.7/site-packages/urllib3/connectionpool.py&quot;, line 597, in urlopen
        _stacktrace=sys.exc_info()[2])
      File &quot;/usr/lib/python2.7/site-packages/urllib3/util/retry.py&quot;, line 271, in increment
        raise MaxRetryError(_pool, url, error or ResponseError(cause))
    MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=51379): Max retries exceeded with url: /session/2e64d2a1-3c7f-4221-96fe-9d0b1c102195/window (Caused by ProtocolError('Connection aborted.', error(111, 'Connection refused')))
    
    ----------------------------------------------------------------------
    Ran 1 test in 72.106s
</code></pre>
<p>who can tell me why?thanks</p>
",11,1538902897,python;selenium;selenium-webdriver;python-requests;urllib3,True,51079,3,1677498471,https://stackoverflow.com/questions/52686968/maxretryerror-httpconnectionpool-max-retries-exceeded-caused-by-protocolerror
61592735,urllib3 HTTPResponse.read() returns empty bytes,"<p>I'm trying to read a website's content but I get an empty bytes object, <code>b''</code>.</p>

<pre><code>import urllib3
from urllib3 import PoolManager
urllib3.disable_warnings()
https = PoolManager()

r = https.request('GET', 'https://minemen.club/leaderboards/practice/')

print(r.status)
print(r.read())
</code></pre>

<p>When I open the URL in a web browser I see the website, and <code>r.status</code> is 200 (success).</p>

<p>Why does <code>r.read()</code> not return the content?</p>
",5,1588597083,python;urllib3,True,9366,2,1677139494,https://stackoverflow.com/questions/61592735/urllib3-httpresponse-read-returns-empty-bytes
75507449,using a varible in &quot;urllib.request.urlopen&quot; throws an error,"<p>I want to get data from NCBI website using python3. When I use</p>
<pre><code>fp = urllib.request.urlopen(&quot;https://www.ncbi.nlm.nih.gov/gene/?term=50964&quot;)     
mybytes = fp.read()
mystr = mybytes.decode(&quot;utf8&quot;)
fp.close()
print(mystr) #### executes without any error
</code></pre>
<p>but when I pass the <strong>id</strong> as a variable in the url, it throws an error.</p>
<pre><code>id_pool=[50964, 4552,7845,987,796]

for  id in id_pool:
   id=str(id) 

   url=f'&quot;https://www.ncbi.nlm.nih.gov/gene/?term={id}&quot;'

   print(url) ## &quot;https://www.ncbi.nlm.nih.gov/gene/?term=50964&quot; ### same as above

   fp = urllib.request.urlopen(url)

   mybytes = fp.read()

   mystr = mybytes.decode(&quot;utf8&quot;)

   fp.close()

   print(mystr) #### shows the following error

   break


&quot;    raise URLError('unknown url type: %s' % type)
urllib.error.URLError: &lt;urlopen error unknown url type: &quot;https&gt;&quot;
</code></pre>
",0,1676885394,python;url;urllib;urllib3,True,59,2,1676885757,https://stackoverflow.com/questions/75507449/using-a-varible-in-urllib-request-urlopen-throws-an-error
9446387,How to retry urllib2.request when fails?,"<p>When <code>urllib2.request</code> reaches timeout, a <code>urllib2.URLError</code> exception is raised.
What is the pythonic way to retry establishing a connection?</p>
",40,1330191976,python;decorator;urllib2;urllib3,True,35133,4,1676193954,https://stackoverflow.com/questions/9446387/how-to-retry-urllib2-request-when-fails
75382508,DeprecationWarning: HTTPResponse.getheader() is deprecated and will be removed in urllib3 v2.1.0. Instead use HTTPResponse.headers.get() with Selenium,"<p>Every time when i run my selenium script it shows some warnings. Can anyone explain me the reason of the warnings and how to solve it.</p>
<pre><code>C:\Users\1154-Talha\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\remote\remote_connection.py:418: DeprecationWarning: HTTPResponse.getheader() is deprecated and will be removed in urllib3 v2.1.0. Instead use HTTPResponse.headers.get(name, default).
  if resp.getheader('Content-Type') is not None:
C:\Users\1154-Talha\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\remote\remote_connection.py:419: DeprecationWarning: HTTPResponse.getheader() is deprecated and will be removed in urllib3 v2.1.0. Instead use HTTPResponse.headers.get(name, default).
  content_type = resp.getheader('Content-Type').split(';')
</code></pre>
",0,1675841038,python;selenium;httpresponse;urllib3;deprecation-warning,True,1894,2,1675846571,https://stackoverflow.com/questions/75382508/deprecationwarning-httpresponse-getheader-is-deprecated-and-will-be-removed-i
56517820,Pull CME price data into Python 3.6.8,"<p>I am relatively new to Python so I apologize if this is a 'bush league' question.</p>

<p>I am trying to retrieve the WTI futures prices from this website:
<a href=""https://www.cmegroup.com/trading/energy/crude-oil/west-texas-intermediate-wti-crude-oil-calendar-swap-futures_quotes_globex.html"" rel=""nofollow noreferrer"">https://www.cmegroup.com/trading/energy/crude-oil/west-texas-intermediate-wti-crude-oil-calendar-swap-futures_quotes_globex.html</a></p>

<p>Which libraries should I be using?  How will I need to adjust the output when it is pulled from the website? </p>

<p>Currently operating in Python 3.6.8 with the pandas, numpy, requests, urllib3, BeautifulSoup, and json libraries. I am not exactly sure if these are the correct libraries and if they are which functions I should be using.</p>

<p>Here is a basic version of the code:</p>

<pre><code>wtiFutC = 'https://www.cmegroup.com/trading/energy/crude-oil/west-texas-intermediate-wti-crude-oil-calendar-swap-futures_quotes_globex.html'
http = urllib3.PoolManager()
response2 = http.request('GET', wtiFutC)
print(type(response2.data)) #check the type of the data produced - bytes
print(response2.data) #prints out the data

soup2 = BeautifulSoup(response2.data.decode('utf-8'), features='html.parser')
print(type(soup2)) #check the type of the data produced - 'bs4.BeautifulSoup'
print(soup2) #prints out the BeautifulSoup version of the data
</code></pre>

<p>I want a way to see the 'Last' price for the WTI future for the whole curve.  Instead I am seeing something like this:</p>

<pre><code>&lt;!DOCTYPE html PUBLIC ""-//W3C//DTD XHTML 1.0 Transitional//EN"" 
""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd""&gt;

&lt;!--[if (gt IE 9) |!(IE)]&gt;&lt;!--&gt;
&lt;html class=""cmePineapple no-js"" lang=""en"" xml:lang=""en"" 
xmlns=""http://www.w3.org/1999/xhtml""&gt;
&lt;!--&lt;![endif]--&gt;
</code></pre>

<p></p>

<p>Any help or direction would be greatly appreciated.  Thank you so much! :)</p>
",2,1560108979,python;web-scraping;beautifulsoup;python-requests;urllib3,True,3536,3,1675304500,https://stackoverflow.com/questions/56517820/pull-cme-price-data-into-python-3-6-8
71858905,Does urllib3 support HTTP/2 Requests? Will it?,"<p>I know the following about various python HTTP libraries:</p>
<ul>
<li><a href=""https://docs.python-requests.org/en/latest/"" rel=""noreferrer"">Requests</a> does <a href=""https://stackoverflow.com/q/44931070/1473320"">not support HTTP/2 requests</a>.</li>
<li><a href=""https://github.com/python-hyper/hyper"" rel=""noreferrer"">Hyper</a> does support HTTP/2 requests, but is archived as of <a href=""https://github.com/python-hyper/hyper/commit/b77e758f472f00b098481e3aa8651b0808524d84"" rel=""noreferrer"">early 2021</a> and wouldn't be a good choice for new projects.</li>
<li><a href=""https://www.python-httpx.org/"" rel=""noreferrer"">HTTPX</a> does support HTTP/2, but this support is <a href=""https://www.python-httpx.org/http2/"" rel=""noreferrer"">optional, requires installing extra dependencies, and comes with some caveats about rough edges</a>.</li>
<li><a href=""https://docs.aiohttp.org/en/stable/index.html"" rel=""noreferrer"">AIOHTTP</a> does <a href=""https://docs.aiohttp.org/en/stable/changes.html?highlight=http2#id532"" rel=""noreferrer"">not support HTTP2 yet</a> (as of mid April 2022).
<ul>
<li>The focus of this project is also not solely on being a client -- this package also includes a server.</li>
</ul>
</li>
</ul>
<p>The other major HTTP request library I'm aware of is <a href=""https://urllib3.readthedocs.io/en/stable/"" rel=""noreferrer"">urllib3</a>. This is what <a href=""https://openapi-generator.tech/"" rel=""noreferrer"">OpenAPI Generator</a> uses by default when generating python client libraries.</p>
<p>My Questions are:</p>
<p><strong>Can urrlib3 be configured to make HTTP/2 requests?</strong></p>
<p>I cannot find any information on http2 support in <a href=""https://urllib3.readthedocs.io/en/stable/reference/index.html#"" rel=""noreferrer"">the documentation</a>, and through my testing of a generated OpenAPI client, all requests are HTTP/1.1. If the answer is no currently, <strong>are the maintainers planning HTTP/2 support</strong>? I cannot find any evidence of this in the project's <a href=""https://github.com/urllib3/urllib3/issues?q=is%3Aissue+%22http%2F2%22"" rel=""noreferrer"">open issues</a>.</p>
",9,1649858805,python;http2;urllib3;openapi-generator,True,3713,2,1674129757,https://stackoverflow.com/questions/71858905/does-urllib3-support-http-2-requests-will-it
75099226,urllib3 self-signed certificate: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate,"<p>I am attempting to connect to a third party system and this will be using self-signed certificates from both sides as this will not be public internet facing.</p>
<p>I am using Python 3 with urllib3 Pool Manager. I have a copy of the certificate, the private key and the verification certificate from the third party. Utilising these in curl confirms the connection works:</p>
<pre><code>curl https://third_party_url.com/hello -iv --cert ./cert.cert --cacert ./verify.cert   --key ./key.key
</code></pre>
<p>However, when I try to utilise this in code:</p>
<pre><code>client = urllib3.PoolManager(
    cert_file = &quot;./cert.cert&quot;,
    key_file=&quot;./key.key&quot;,
    ca_certs=&quot;./verify.cert&quot;,
    cert_reqs=&quot;CERT_REQUIRED&quot;
)
resp = client.request(&quot;GET&quot;, &quot;https://third_party_url.com/hello&quot;)
</code></pre>
<p>An exception occurs:</p>
<pre><code>    Exception has occurred: SSLError
   [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)
</code></pre>
<p>A lot of the answers to similar questions is about disabling the verification, which is definitely not an option. Any input would be greatly appreciated.</p>
<p><strong>EDIT</strong></p>
<p>Answers to the questions raised by @Steffen</p>
<p>1)
When running without the <code>-cacert</code> argument, curl provides this output:</p>
<pre><code>*   Trying 10.10.10.10:443...
* Connected to third_party_url.com (10.10.10.10) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*  CAfile: /etc/ssl/certs/ca-certificates.crt
*  CApath: /etc/ssl/certs
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (OUT), TLS alert, unknown CA (560):
* SSL certificate problem: unable to get local issuer certificate
* Closing connection 0
curl: (60) SSL certificate problem: unable to get local issuer certificate
More details here: https://curl.se/docs/sslcerts.html

curl failed to verify the legitimacy of the server and therefore could not
establish a secure connection to it. To learn more about this situation and
how to fix it, please visit the web page mentioned above.
</code></pre>
<p>So it's the same issue. It's like urllib3 is not accepting the argument to load a provided one.</p>
<p>2)
On running <code>openssl x509 -in verify.cert -text</code>
The output does not have basic constraints set to CA:true.</p>
<p>The output is:</p>
<pre><code>Certificate:
    Data:
        Version: 1 (0x0)
        Serial Number:
            &lt;SerialNumber&gt;
        Signature Algorithm: sha256WithRSAEncryption
        Issuer: C = GB, O = me.com, OU = third-party
        Validity
            Not Before: Valid Date
            Not After : Expiry Date
        Subject: C = GB, O = Third Party Company Name, OU = third-party, CN = *third-party.com
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                Public-Key: (2048 bit)
                Modulus:
            &lt;Modulus&gt;
                Exponent: &lt;Redacted Value&gt;
    Signature Algorithm: sha256WithRSAEncryption
    Signature Value:
    &lt;Signature Value&gt;
-----BEGIN CERTIFICATE-----
&lt;Certificate Details&gt;
-----END CERTIFICATE-----
</code></pre>
",0,1673539853,python;ssl;urllib3,True,780,1,1673548162,https://stackoverflow.com/questions/75099226/urllib3-self-signed-certificate-ssl-certificate-verify-failed-certificate-ve
75089175,Accessing Minio with a self signed certificate and the Python client library,"<p>We have an instance of minio running with a certificate that is signed by our corporate CA. Accessing it with S3 Browser works perfect. Now I try to write a python script to upload files. I try to use the windows cert store to get my CA certs</p>
<pre><code>myssl = ssl.create_default_context()

myhttpclient = urllib3.PoolManager(
      cert_reqs='CERT_REQUIRED',
      ca_certs=myssl.get_ca_certs()
)

s3dev = Minio(&quot;s3dev.mycorp.com:9000,
     access_key=&quot;myAccessKey&quot;,
     secret_key=&quot;mySecretKey&quot;
     secure=True,
     http_client=myhttpclient
)
</code></pre>
<p>I get an error &quot;TypeError: unhashable type: list&quot;</p>
<p>Getting the CA Certs from Windows cert store with ssl.get_ca_certs() returns a list with all the certs in it which seems logic to me, what am I missing here to get something this simple to work ?</p>
",0,1673472447,python;python-3.x;minio;urllib3,False,443,0,1673472447,https://stackoverflow.com/questions/75089175/accessing-minio-with-a-self-signed-certificate-and-the-python-client-library
75025480,How to get a style value inside a span tag with a python scraper?,"<p>I downloaded the code of this website: <a href=""https://www.ilmakiage.co.il/mineral-lip-pencil-4040"" rel=""nofollow noreferrer"">https://www.ilmakiage.co.il/mineral-lip-pencil-4040</a> with the response.get method. In the inspectors mode I could see that there was a style value &quot;display: block;&quot; or &quot;display: none;&quot; in a unique span class=&quot;qtyValidate_color&quot; tag I wanted to download. But when I opened my soup or response.text I couldn't find that value in that span tag. It's empty. Please let me know what library or method I can use to get this style value in the span tag.</p>
<p>My Python code</p>
<pre><code>from bs4 import BeautifulSoup
import requests

response = requests.get('https://www.ilmakiage.co.il/mineral-lip-pencil-4043')

soup = BeautifulSoup(response.text, 'lxml')

no_quantity = soup.find('span', class_='qtyValidate_color').contents[0]

if no_quantity == 'הכמות המבוקשת אינה קיימת':
    print(&quot;Ooops, 'הכמות המבוקשת אינה קיימת' the stock is empty. Open the app later. You will see the 'Shopping time' phrase when the lipstick be in stock&quot;)
else:
    print('Shopping time')
</code></pre>
<p>HTML code on the website in Chrome inspector mode</p>
<pre><code>&lt;span class=&quot;qtyValidate_color&quot; style=&quot;display: block;&quot;&gt;הכמות המבוקשת אינה קיימת&lt;/span&gt;
&lt;span class=&quot;qtyValidate_color&quot; style=&quot;display: none;&quot;&gt;הכמות המבוקשת אינה קיימת&lt;/span&gt;
</code></pre>
<p>Screenshot:
<a href=""https://i.stack.imgur.com/Eh9u6.jpg"" rel=""nofollow noreferrer"">Website code in Inspector mode Chrome</a></p>
<p>lXML code in my soup</p>
<pre><code>&lt;span class=&quot;qtyValidate_color&quot;&gt;הכמות המבוקשת אינה קיימת&lt;/span&gt;
</code></pre>
<p>Screenshot 2:
<a href=""https://i.stack.imgur.com/y9v2L.jpg"" rel=""nofollow noreferrer"">Website code lxml in my soup</a></p>
<p>I tried reading stackoverflow and replacing request.get method with urllib3 methods</p>
<p>ps. i'm doing data analysis course now and created this tool for my girlfriend as a part of my training.</p>
",0,1672961520,python;web-scraping;python-requests;urllib;urllib3,False,201,1,1673003829,https://stackoverflow.com/questions/75025480/how-to-get-a-style-value-inside-a-span-tag-with-a-python-scraper
68084166,python prepared requests - removing unwanted header,"<p>I'm having troubles with this code chunk:</p>
<pre><code>with requests.Session() as s:
    _hs = s.headers
    
    req = requests.Request('POST', url, data=json.dumps(data), headers=headers)
    prepared_req = req.prepare()
    if 'Content-Length' in prepared_req.headers:
        prepared_req.headers.pop('Content-Length')
    rsp = s.send(prepared_req, timeout=self._TIMEOUT)
    try:
        rsp.raise_for_status()
    except requests.HTTPError:
        self._logger.exception(&quot;error in retrieving response from %s -- response content: %s&quot;,
                                        url, rsp.content)
        raise
    return rsp.json()
</code></pre>
<p><code>Content-Length</code> is correctly removed from the PreparedRequest headers, however during <code>send</code> something goes wrong:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/opt/projects/MyProj/my-http-client/my_http_client/http_client.py&quot;, line 297, in _http_post
    rsp = s.send(prepared_req, timeout=self._TIMEOUT)
  File &quot;/home/user/venvs/my-http-client-venv/lib/python3.8/site-packages/requests/sessions.py&quot;, line 655, in send
    r = adapter.send(request, **kwargs)
  File &quot;/home/user/venvs/my-http-client-venv/lib/python3.8/site-packages/requests/adapters.py&quot;, line 472, in send
    low_conn.send(i)
  File &quot;/usr/lib/python3.8/http/client.py&quot;, line 975, in send
    self.sock.sendall(d)
TypeError: a bytes-like object is required, not 'str'
</code></pre>
<p>same thing happens if I remove the header with <code>del</code>:</p>
<pre><code>del prepared_req.headers['Content-Length']
</code></pre>
<p>anyone does know what's wrong? without the <code>headers.pop</code>, everything runs fine.</p>
",0,1624367352,python;python-requests;http-headers;urllib3,False,1240,3,1672758595,https://stackoverflow.com/questions/68084166/python-prepared-requests-removing-unwanted-header
74914825,error when post file using urllib3 in python,"<p>I'm getting this error:
<code>b'{&quot;status&quot;:&quot;error&quot;,&quot;message&quot;:&quot;Upload failed.&quot;}'</code></p>
<p>I don't know why the file not getting uploaded. here is my code:</p>
<pre><code>import urllib3

http = urllib3.PoolManager()
url = &quot;https://tmpfiles.org/api/v1/upload&quot;

dfile = open(&quot;image&quot;, &quot;rb&quot;)
dfile=dfile.read()


response = http.request(
    &quot;POST&quot;,
    url,
    fields={ &quot;file&quot;: dfile }
)

print(response.data)

</code></pre>
<p>I tried using <code>requests</code>, and it works fine, but I want to use <code>urllib3</code> because it is faster</p>
",0,1671989059,python;post;python-requests;urllib3,False,155,0,1671989366,https://stackoverflow.com/questions/74914825/error-when-post-file-using-urllib3-in-python
66642705,Why requests raise this exception &quot;check_hostname requires server_hostname&quot;?,"<pre><code>p={
    'http':'http://my correct proxy here',
    'https':'https://my correct proxy here'
    }
self.response=requests.get(url=url,headers=self.headers,timeout=(6,15),proxies=p)
</code></pre>
<p>And then it raise the exception:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\xyl13509876955\Desktop\Monitor\dicks.py&quot;, line 61, in send_request
    self.response=requests.get(url=url,headers=self.headers,timeout=(6,15),proxies=p)
  File &quot;C:\Users\xyl13509876955\AppData\Local\Programs\Python\Python37\lib\site-packages\requests\api.py&quot;, line 76, in get
    return request('get', url, params=params, **kwargs)
  File &quot;C:\Users\xyl13509876955\AppData\Local\Programs\Python\Python37\lib\site-packages\requests\api.py&quot;, line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File &quot;C:\Users\xyl13509876955\AppData\Local\Programs\Python\Python37\lib\site-packages\requests\sessions.py&quot;, line 542, in request
    resp = self.send(prep, **send_kwargs)
  File &quot;C:\Users\xyl13509876955\AppData\Local\Programs\Python\Python37\lib\site-packages\requests\sessions.py&quot;, line 655, in send
    r = adapter.send(request, **kwargs)
  File &quot;C:\Users\xyl13509876955\AppData\Local\Programs\Python\Python37\lib\site-packages\requests\adapters.py&quot;, line 449, in send
    timeout=timeout
  File &quot;C:\Users\xyl13509876955\AppData\Local\Programs\Python\Python37\lib\site-packages\urllib3\connectionpool.py&quot;, line 696, in urlopen
    self._prepare_proxy(conn)
  File &quot;C:\Users\xyl13509876955\AppData\Local\Programs\Python\Python37\lib\site-packages\urllib3\connectionpool.py&quot;, line 964, in _prepare_proxy
    conn.connect()
  File &quot;C:\Users\xyl13509876955\AppData\Local\Programs\Python\Python37\lib\site-packages\urllib3\connection.py&quot;, line 359, in connect
    conn = self._connect_tls_proxy(hostname, conn)
  File &quot;C:\Users\xyl13509876955\AppData\Local\Programs\Python\Python37\lib\site-packages\urllib3\connection.py&quot;, line 506, in _connect_tls_proxy
    ssl_context=ssl_context,
  File &quot;C:\Users\xyl13509876955\AppData\Local\Programs\Python\Python37\lib\site-packages\urllib3\util\ssl_.py&quot;, line 432, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls)
  File &quot;C:\Users\xyl13509876955\AppData\Local\Programs\Python\Python37\lib\site-packages\urllib3\util\ssl_.py&quot;, line 474, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock)
  File &quot;C:\Users\xyl13509876955\AppData\Local\Programs\Python\Python37\lib\ssl.py&quot;, line 423, in wrap_socket
    session=session
  File &quot;C:\Users\xyl13509876955\AppData\Local\Programs\Python\Python37\lib\ssl.py&quot;, line 827, in _create
    raise ValueError(&quot;check_hostname requires server_hostname&quot;)
ValueError: check_hostname requires server_hostname
</code></pre>
<p>Please help me solve the problem and the best way is to show me the right code. I am very confused and frustrated for the problem!!</p>
",49,1615829131,python;python-requests;urllib3,True,109724,15,1671674149,https://stackoverflow.com/questions/66642705/why-requests-raise-this-exception-check-hostname-requires-server-hostname
74861794,Universal proxy configuration for Python urllib3,"<p>I am trying to use and install a python library according to the instructions at: <a href=""https://pypi.org/project/ai4bharat-transliteration/"" rel=""nofollow noreferrer"">https://pypi.org/project/ai4bharat-transliteration/</a>. My system is behind a corporate proxy and I am able to use pip and other libraries including urllib3 to access the internet when I am writing code from scratch.</p>
<p>However, in this case, the library wants to access some files over internet when running:</p>
<pre><code>`e = XlitEngine(&quot;hi&quot;, beam_width=10, rescore=True)`
</code></pre>
<p>And that results in a wall of urllib3 proxy related errors, the first of which is:</p>
<pre><code>Downloading Multilingual model for transliteration
SSL certificate not verified...
/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host '172.xx.yy.zzz'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py&quot;, line 700, in urlopen
    self._prepare_proxy(conn)
  File &quot;/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py&quot;, line 996, in _prepare_proxy
    conn.connect()
  File &quot;/usr/local/lib/python3.8/dist-packages/urllib3/connection.py&quot;, line 369, in connect
    self._tunnel()
  File &quot;/usr/lib/python3.8/http/client.py&quot;, line 901, in _tunnel
    (version, code, message) = response._read_status()
  File &quot;/usr/lib/python3.8/http/client.py&quot;, line 277, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), &quot;iso-8859-1&quot;)
  File &quot;/usr/lib/python3.8/socket.py&quot;, line 669, in readinto
    return self._sock.recv_into(b)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/usr/lib/python3/dist-packages/requests/adapters.py&quot;, line 439, in send
    resp = conn.urlopen(
  File &quot;/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py&quot;, line 787, in urlopen
    retries = retries.increment(
  File &quot;/usr/local/lib/python3.8/dist-packages/urllib3/util/retry.py&quot;, line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='objects.githubusercontent.com', port=443): Max retries exceeded with url: /github-production-release-asset-2e65be/487173539/4ef3b62d-385b-4a3a-9ab1-a3cc55764ef3?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221220%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20221220T101158Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=d24db49d92188df3dbf8a0f1a05126bdaae8bf42289befe734331a41b336f11c&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;key_id=0&amp;repo_id=487173539&amp;response-content-disposition=attachment%3B%20filename%3Dindicxlit-en-indic-v1.0.zip&amp;response-content-type=application%2Foctet-stream (Caused by ProxyError('Cannot connect to proxy.', timeout('timed out')))

</code></pre>
<p>Where 172.xx.yy.zzz is the url to my proxy and I am guessing that the warning regarding the SSL certificate is just a warning and the problem is the proxy configuration of urllib3.</p>
<p>If so, is there a way to set a universal proxy which will be honored by urllib3 before the library is called by the XlitEngine package installed above. I am reluctant to attempt any changes to the XlitEngine package installed above. I tried posting the issue on Github for XlitEngine but have not received any response so far.</p>
<p>If it is of any consequence, I am using Python 3.8.10 on a headless Ubuntu 20.04 Server Virtual Machine.
Cheers!</p>
",1,1671532822,python;urllib3,False,194,0,1671532822,https://stackoverflow.com/questions/74861794/universal-proxy-configuration-for-python-urllib3
48132036,Python module googletrans throwing &quot;Max retries exceeded with url: /&quot; error,"<p>My Editor is PyCharm 2017.3 and my Python version is 3.4. <br>
I trying to translate the word clicked on my application from english to hindi.<br>
This application scraps words from a site and lists it.Everything is working fine with this application except the translation feature.</p>

<p>I have just installed googletrans library but it is giving me error that goes like max no of retries exceeded.</p>

<pre><code>   def translate(self):
    translate=Translator()
    translate.translate(self._word, dest=""hi"",src=""en"")
</code></pre>

<p>Here is my translate function and here is the <a href=""http://py-googletrans.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">documentation</a> I am referring to.</p>

<p>This is my Error showing in console.</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Python34\lib\site-packages\urllib3\connectionpool.py"", line 601, in urlopen
chunked=chunked)
  File ""C:\Python34\lib\site-packages\urllib3\connectionpool.py"", line 346, in _make_request
self._validate_conn(conn)
  File ""C:\Python34\lib\site-packages\urllib3\connectionpool.py"", line 850, in _validate_conn
conn.connect()
  File ""C:\Python34\lib\site-packages\urllib3\connection.py"", line 326, in connect
ssl_context=context)
  File ""C:\Python34\lib\site-packages\urllib3\util\ssl_.py"", line 329, in ssl_wrap_socket
return context.wrap_socket(sock, server_hostname=server_hostname)
  File ""C:\Python34\Lib\ssl.py"", line 344, in wrap_socket
_context=self)
  File ""C:\Python34\Lib\ssl.py"", line 540, in __init__
self.do_handshake()
  File ""C:\Python34\Lib\ssl.py"", line 767, in do_handshake
self._sslobj.do_handshake()
ssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:598)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""C:\Python34\lib\site-packages\requests\adapters.py"", line 440, in send
timeout=timeout
File ""C:\Python34\lib\site-packages\urllib3\connectionpool.py"", line 639, in urlopen
_stacktrace=sys.exc_info()[2])
File ""C:\Python34\lib\site-packages\urllib3\util\retry.py"", line 388, in increment
raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='translate.google.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:598)'),))


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""C:/Users/ROCKSTAR/PycharmProjects/Dictionary/execute.py"", line 56, in getPerWordDisplay
self.query.value(4), self.query.value(5))
File ""C:\Users\ROCKSTAR\PycharmProjects\Dictionary\PerWordWindow.py"", line 23, in __init__
self.translate()
 File ""C:\Users\ROCKSTAR\PycharmProjects\Dictionary\PerWordWindow.py"", line 37, in translate
translate.translate(self._word, dest=""hi"",src=""en"")
 File ""C:\Python34\lib\site-packages\googletrans\client.py"", line 132, in translate
data = self._translate(text, dest, src)
 File ""C:\Python34\lib\site-packages\googletrans\client.py"", line 57, in _translate
token = self.token_acquirer.do(text)
 File ""C:\Python34\lib\site-packages\googletrans\gtoken.py"", line 180, in do
self._update()
 File ""C:\Python34\lib\site-packages\googletrans\gtoken.py"", line 57, in _update
r = self.session.get(self.host)
 File ""C:\Python34\lib\site-packages\requests\sessions.py"", line 521, in get
return self.request('GET', url, **kwargs)
 File ""C:\Python34\lib\site-packages\requests\sessions.py"", line 508, in request
resp = self.send(prep, **send_kwargs)
 File ""C:\Python34\lib\site-packages\requests\sessions.py"", line 618, in send
r = adapter.send(request, **kwargs)
 File ""C:\Python34\lib\site-packages\requests\adapters.py"", line 506, in send
raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='translate.google.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:598)'),))

Process finished with exit code 0
</code></pre>
",4,1515273784,python;python-3.x;urllib;google-translate;urllib3,False,1315,1,1671446840,https://stackoverflow.com/questions/48132036/python-module-googletrans-throwing-max-retries-exceeded-with-url-error
66689696,Urllib3 error &quot;SSL: wrong signature type&quot;,"<p>When I run the following code in python 3.8.5 from an Ubuntu Server:</p>
<pre><code>import urllib3
import certifi
url = &quot;https://www.website.com&quot;
http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED', ca_certs=certifi.where())
content = http.request(&quot;GET&quot;, url, preload_content=False).read()
</code></pre>
<p>The following error occurr:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/usr/lib/python3/dist-packages/urllib3/connectionpool.py&quot;, line 665, in urlopen
    httplib_response = self._make_request(
  File &quot;/usr/lib/python3/dist-packages/urllib3/connectionpool.py&quot;, line 376, in _make_request
    self._validate_conn(conn)
  File &quot;/usr/lib/python3/dist-packages/urllib3/connectionpool.py&quot;, line 996, in _validate_conn
    conn.connect()
  File &quot;/usr/lib/python3/dist-packages/urllib3/connection.py&quot;, line 366, in connect
    self.sock = ssl_wrap_socket(
  File &quot;/usr/lib/python3/dist-packages/urllib3/util/ssl_.py&quot;, line 370, in ssl_wrap_socket
    return context.wrap_socket(sock, server_hostname=server_hostname)
  File &quot;/usr/lib/python3.8/ssl.py&quot;, line 500, in wrap_socket
    return self.sslsocket_class._create(
  File &quot;/usr/lib/python3.8/ssl.py&quot;, line 1040, in _create
    self.do_handshake()
  File &quot;/usr/lib/python3.8/ssl.py&quot;, line 1309, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: WRONG_SIGNATURE_TYPE] wrong signature type (_ssl.c:1123)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;MyFile.py&quot;, line 46, in &lt;module&gt;
    content = urllib3.PoolManager().request(&quot;GET&quot;, url)
  File &quot;/usr/lib/python3/dist-packages/urllib3/request.py&quot;, line 75, in request
    return self.request_encode_url(
  File &quot;/usr/lib/python3/dist-packages/urllib3/request.py&quot;, line 97, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
  File &quot;/usr/lib/python3/dist-packages/urllib3/poolmanager.py&quot;, line 330, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File &quot;/usr/lib/python3/dist-packages/urllib3/connectionpool.py&quot;, line 747, in urlopen
    return self.urlopen(
  File &quot;/usr/lib/python3/dist-packages/urllib3/connectionpool.py&quot;, line 747, in urlopen
    return self.urlopen(
  File &quot;/usr/lib/python3/dist-packages/urllib3/connectionpool.py&quot;, line 747, in urlopen
    return self.urlopen(
  File &quot;/usr/lib/python3/dist-packages/urllib3/connectionpool.py&quot;, line 719, in urlopen
    retries = retries.increment(
  File &quot;/usr/lib/python3/dist-packages/urllib3/util/retry.py&quot;, line 436, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.website.com', port=443): Max retries exceeded with url: /path/to/web/page.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_SIGNATURE_TYPE] wrong signature type (_ssl.c:1123)')))
</code></pre>
<p>The system is updated as all the libraries I am using</p>
<p>Running the same code on my Windows 10 pc results in no errors</p>
<p>What am I missing?</p>
",1,1616065091,python;ssl;ssl-certificate;urllib3,True,5428,4,1671282406,https://stackoverflow.com/questions/66689696/urllib3-error-ssl-wrong-signature-type
74824088,"Python proxy with authentification (request,pytrends...)","<p>Hello I was previously using older packages and in our corporate I was able to reach outside with proxy with Python in a way:</p>
<pre class=""lang-py prettyprint-override""><code>username = &quot;Platform\\PersonalID&quot;

proxies = {
           &quot;http&quot; : &quot;http://username:password@adress:port&quot;, 
           &quot;https&quot; : &quot;http://username:password@adress:port&quot;,
          }
</code></pre>
<p>with for example:</p>
<pre class=""lang-py prettyprint-override""><code>requests.get(url, proxies=proxies)
</code></pre>
<p>However, when I try this on new packages I get a problem with the connection to the proxy and get a timeout error from the urllib3:</p>
<p>&quot;Connection to Platform timed out&quot;</p>
<p>I was able to track it to some changes in urllib3 when I was comparing the old and new packages, but can't figure out how to feed the proxy settings to the requests, pytrends and other packeges (in fact to the new urllib3 which the requests and other libraries are build on)</p>
<p>I have tried to search throught web was looking for several ways how to get the proxy working but failed in one way or another.</p>
<p>for example</p>
<p>If I use the proxies without the password and try to pass the password as an auth=(&quot;username&quot;,&quot;password&quot;) then I get error 407 authentication required. Even though the password and ID is correct.</p>
<p>Example:</p>
<pre class=""lang-py prettyprint-override""><code>import requests


proxies = {&quot;http&quot;:&quot;http://Platform\\userID:password@adress:port&quot;,
           &quot;https&quot;: &quot;http://Platform\\userID:password@adress:port&quot;
           }

url=&quot;https://www.google.com/&quot;
requests.get(url, proxies=proxies).content

</code></pre>
<p>works with:
urllib3 up to 1.24</p>
<p>doesnt work with:</p>
<p>urllib3 1.25 (Unable to parse the proxyAdress)</p>
<p>urllib3 1.26.13 (TimeoutError, trying to connect to badly parsed proxyAdress is my tip?)</p>
",0,1671191577,python;python-requests;proxy;urllib3,False,301,0,1671198897,https://stackoverflow.com/questions/74824088/python-proxy-with-authentification-request-pytrends
74822499,Import Error when importing requests in Python 3,"<p>I'm trying to run a python program where I run</p>
<pre><code>from pycocotools.coco import COCO        
import requests 
</code></pre>
<p>I use Python 3.11.1. It throws ImportError</p>
<pre><code>C:\path&gt;python utils.py --img_out obj/ --label_out obj/
Traceback (most recent call last):
  File &quot;C:\path\utils.py&quot;, line 6, in &lt;module&gt;
    import requests
  File &quot;C:\Users\windows\AppData\Local\Programs\Python\Python311\Lib\site-packages\requests\__init__.py&quot;, line 43, in &lt;module&gt;
    import urllib3
  File &quot;C:\Users\windows\AppData\Local\Programs\Python\Python311\Lib\site-packages\urllib3\__init__.py&quot;, line 8, in &lt;module&gt;
    from .connectionpool import (
  File &quot;C:\Users\windows\AppData\Local\Programs\Python\Python311\Lib\site-packages\urllib3\connectionpool.py&quot;, line 29, in &lt;module&gt;
    from .connection import (
  File &quot;C:\Users\windows\AppData\Local\Programs\Python\Python311\Lib\site-packages\urllib3\connection.py&quot;, line 39, in &lt;module&gt;
    from .util.ssl_ import (
  File &quot;C:\Users\windows\AppData\Local\Programs\Python\Python311\Lib\site-packages\urllib3\util\__init__.py&quot;, line 3, in &lt;module&gt;
    from .connection import is_connection_dropped
  File &quot;C:\Users\windows\AppData\Local\Programs\Python\Python311\Lib\site-packages\urllib3\util\connection.py&quot;, line 3, in &lt;module&gt;
    from .wait import wait_for_read
  File &quot;C:\Users\windows\AppData\Local\Programs\Python\Python311\Lib\site-packages\urllib3\util\wait.py&quot;, line 1, in &lt;module&gt;
    from .selectors import (
  File &quot;C:\Users\windows\AppData\Local\Programs\Python\Python311\Lib\site-packages\urllib3\util\selectors.py&quot;, line 14, in &lt;module&gt;
    from collections import namedtuple, Mapping

ImportError: cannot import name 'Mapping' from 'collections' (C:\Users\windows\AppData\Local\Programs\Python\Python311\Lib\collections\__init__.py)
</code></pre>
<p>I installed the requests module via pip</p>
",0,1671182814,python;python-3.x;python-requests;importerror;urllib3,False,1098,1,1671183781,https://stackoverflow.com/questions/74822499/import-error-when-importing-requests-in-python-3
51799251,"urllib3.exceptions.ProtocolError: (&#39;Connection aborted.&#39;, error(10054, &#39;An existing connection was forcibly closed by the remote host&#39;))","<p>I am trying to open a website on chrome using Python Selenium chromedriver. Chrome browser is opening (with warnings) and the url is not opening. </p>

<p>Version details : Chrome : 68.0.3440.106 
                  selenium : 3.14.0
                  chromedriver : 2.20
                  python : 2.7</p>

<p>I am using below code :</p>

<pre><code>import time
from selenium import webdriver
import selenium
driver = webdriver.Chrome(""C:/Python27/chromedriver.exe"")
driver.get(""https://vancouver.craigslist.ca/"")
print(driver.title)
time.sleep(8)
driver.quit()
</code></pre>

<p>I am getting below error:</p>

<pre><code>C:\Users\sohil7777\PycharmProjects\temp.py\venv\Scripts\python.exe C:/Users/sohil7777/.PyCharmCE2018.2/config/scratches/scratch.py
Traceback (most recent call last):
  File ""C:/Users/sohil7777/.PyCharmCE2018.2/config/scratches/scratch.py"", line 6, in &lt;module&gt;
    driver = webdriver.Chrome(""C:/Python27/chromedriver.exe"")
  File ""C:\Python27\lib\site-packages\selenium\webdriver\chrome\webdriver.py"", line 75, in __init__
    desired_capabilities=desired_capabilities)
  File ""C:\Python27\lib\site-packages\selenium\webdriver\remote\webdriver.py"", line 156, in __init__
    self.start_session(capabilities, browser_profile)
  File ""C:\Python27\lib\site-packages\selenium\webdriver\remote\webdriver.py"", line 251, in start_session
    response = self.execute(Command.NEW_SESSION, parameters)
  File ""C:\Python27\lib\site-packages\selenium\webdriver\remote\webdriver.py"", line 318, in execute
    response = self.command_executor.execute(driver_command, params)
  File ""C:\Python27\lib\site-packages\selenium\webdriver\remote\remote_connection.py"", line 375, in execute
    return self._request(command_info[0], url, body=data)
  File ""C:\Python27\lib\site-packages\selenium\webdriver\remote\remote_connection.py"", line 397, in _request
    resp = self._conn.request(method, url, body=body, headers=headers)
  File ""C:\Python27\lib\site-packages\urllib3\request.py"", line 72, in request
    **urlopen_kw)
  File ""C:\Python27\lib\site-packages\urllib3\request.py"", line 150, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File ""C:\Python27\lib\site-packages\urllib3\poolmanager.py"", line 322, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File ""C:\Python27\lib\site-packages\urllib3\connectionpool.py"", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File ""C:\Python27\lib\site-packages\urllib3\util\retry.py"", line 367, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File ""C:\Python27\lib\site-packages\urllib3\connectionpool.py"", line 600, in urlopen
    chunked=chunked)
  File ""C:\Python27\lib\site-packages\urllib3\connectionpool.py"", line 377, in _make_request
    httplib_response = conn.getresponse(buffering=True)
  File ""C:\Python27\Lib\httplib.py"", line 1121, in getresponse
    response.begin()
  File ""C:\Python27\Lib\httplib.py"", line 438, in begin
    version, status, reason = self._read_status()
  File ""C:\Python27\Lib\httplib.py"", line 394, in _read_status
    line = self.fp.readline(_MAXLINE + 1)
  File ""C:\Python27\Lib\socket.py"", line 480, in readline
    data = self._sock.recv(self._rbufsize)
urllib3.exceptions.ProtocolError: ('Connection aborted.', error(10054, 'An existing connection was forcibly closed by the remote host'))
</code></pre>

<p>Am i missing something? Really appreciate your help</p>
",6,1533987361,python;selenium;google-chrome;selenium-chromedriver;urllib3,True,27509,1,1670818660,https://stackoverflow.com/questions/51799251/urllib3-exceptions-protocolerror-connection-aborted-error10054-an-exist
74748421,"urllib3 and requests, fix status code 403 in urllib3, requests work good as well","<p>When I run the code only requests code work and it's return status code <code>200</code>. and urllib3 fail, it return status code <code>403</code>. I want urllib3 will return the same as requests results, here are the following codes:</p>
<p>requests code:</p>
<pre class=""lang-py prettyprint-override""><code>import requests

proxies = {&quot;http&quot;:&quot;http://username:password@host:port&quot;} #proxy protocol


r = requests.get('https://www.examples.com', proxies=proxies) #make the request

print(r.status_code) #return status code 200, successful request
</code></pre>
<p>urllib3 code:</p>
<pre class=""lang-py prettyprint-override""><code>import urllib3

auth_headers = urllib3.make_headers(proxy_basic_auth='username:password') #proxy authorization build

Proxy = urllib3.ProxyManager(proxy_url=&quot;http://host:port&quot;, proxy_headers=auth_headers)

url = 'https://www.example.com/'

r = Proxy.request('GET', url) #make the request

print(r.status) #return status code 403, I want it to return status code 200 like the same as the requests code.
</code></pre>
<p>Edited: It's also work good with PoolManager()</p>
",0,1670620999,python;python-requests;proxy;authorization;urllib3,False,269,0,1670625178,https://stackoverflow.com/questions/74748421/urllib3-and-requests-fix-status-code-403-in-urllib3-requests-work-good-as-well
74736943,How can I set a timeout for urllib3 request?,"<p>How can I get urllib3 to try to reach a site for 10 seconds and return an error or data if it can't?</p>
<pre class=""lang-py prettyprint-override""><code>http = urllib3.PoolManager()

page = http.request('get', link)
</code></pre>
",1,1670537933,python;urllib3,True,5636,2,1670539266,https://stackoverflow.com/questions/74736943/how-can-i-set-a-timeout-for-urllib3-request
74642461,How do I pass a user-agent to panda&#39;s pd.read_html()?,"<p>some websites automatically decline requests due to lack of user-agent, and it's a hassle using bs4 to scrape many different types of tables.</p>
<p>This issue was resolved before through this code:</p>
<pre><code>url = 'http://finance.yahoo.com/quote/A/key-statistics?p=A'
opener = urllib2.build_opener()
opener.addheaders = [('User-agent', 'Mozilla/5.0')]
response = opener.open(url)
tables = pd.read_html(response.read()
</code></pre>
<p>However urllib2 has been depreciated and urllib3 doesn't have a build_opener() attribute, and I could not find an equivalent attribute either even though I'm sure it has one.</p>
",0,1669900454,python;urllib2;user-agent;urllib3,True,265,1,1669904613,https://stackoverflow.com/questions/74642461/how-do-i-pass-a-user-agent-to-pandas-pd-read-html
73358402,How to disable SSL for a method,"<p>I have a problem. I am using <a href=""https://github.com/EasyPost/easypost-python"" rel=""nofollow noreferrer""><code>easypost</code></a>. The problem is that I got the following error</p>
<pre class=""lang-py prettyprint-override""><code>WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1129)'))': /v2/shipments.
</code></pre>
<p>Is there any option to deactive the ssl for the method ?</p>
<pre class=""lang-py prettyprint-override""><code>!pip install easypost
import os
import easypost
easypost.api_key = &lt;app_key&gt;
shipment = easypost.Shipment.create(
    from_address = {
        &quot;name&quot;: &quot;EasyPost&quot;,
        &quot;street1&quot;: &quot;118 2nd Street&quot;,
        &quot;street2&quot;: &quot;4th Floor&quot;,
        &quot;city&quot;: &quot;San Francisco&quot;,
        &quot;state&quot;: &quot;CA&quot;,
        &quot;zip&quot;: &quot;94105&quot;,
        &quot;country&quot;: &quot;US&quot;,
        &quot;phone&quot;: &quot;415-456-7890&quot;,
    },
    to_address = {
        &quot;name&quot;: &quot;Dr. Steve Brule&quot;,
        &quot;street1&quot;: &quot;179 N Harbor Dr&quot;,
        &quot;city&quot;: &quot;Redondo Beach&quot;,
        &quot;state&quot;: &quot;CA&quot;,
        &quot;zip&quot;: &quot;90277&quot;,
        &quot;country&quot;: &quot;US&quot;,
        &quot;phone&quot;: &quot;310-808-5243&quot;,
    },
    parcel = {
        &quot;length&quot;: 10.2,
        &quot;width&quot;: 7.8,
        &quot;height&quot;: 4.3,
        &quot;weight&quot;: 21.2,
    },
)
shipment.buy(rate=shipment.lowest_rate())
print(shipment)
</code></pre>
<p>Complete Log</p>
<pre class=""lang-py prettyprint-override""><code>WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1129)'))': /v2/shipments
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1129)'))': /v2/shipments
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1129)'))': /v2/shipments
---------------------------------------------------------------------------
SSLCertVerificationError                  Traceback (most recent call last)
File ~\Anaconda3\lib\site-packages\urllib3\connectionpool.py:703, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    702 # Make the request on the httplib connection object.
--&gt; 703 httplib_response = self._make_request(
    704     conn,
    705     method,
    706     url,
    707     timeout=timeout_obj,
    708     body=body,
    709     headers=headers,
    710     chunked=chunked,
    711 )
    713 # If we're going to release the connection in ``finally:``, then
    714 # the response doesn't need to know about the connection. Otherwise
    715 # it will also try to release it and we'll have a double-release
    716 # mess.

File ~\Anaconda3\lib\site-packages\urllib3\connectionpool.py:386, in HTTPConnectionPool._make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
    385 try:
--&gt; 386     self._validate_conn(conn)
    387 except (SocketTimeout, BaseSSLError) as e:
    388     # Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.

File ~\Anaconda3\lib\site-packages\urllib3\connectionpool.py:1040, in HTTPSConnectionPool._validate_conn(self, conn)
   1039 if not getattr(conn, &quot;sock&quot;, None):  # AppEngine might not have  `.sock`
-&gt; 1040     conn.connect()
   1042 if not conn.is_verified:

File ~\Anaconda3\lib\site-packages\urllib3\connection.py:414, in HTTPSConnection.connect(self)
    412     context.load_default_certs()
--&gt; 414 self.sock = ssl_wrap_socket(
    415     sock=conn,
    416     keyfile=self.key_file,
    417     certfile=self.cert_file,
    418     key_password=self.key_password,
    419     ca_certs=self.ca_certs,
    420     ca_cert_dir=self.ca_cert_dir,
    421     ca_cert_data=self.ca_cert_data,
    422     server_hostname=server_hostname,
    423     ssl_context=context,
    424     tls_in_tls=tls_in_tls,
    425 )
    427 # If we're using all defaults and the connection
    428 # is TLSv1 or TLSv1.1 we throw a DeprecationWarning
    429 # for the host.

File ~\Anaconda3\lib\site-packages\urllib3\util\ssl_.py:449, in ssl_wrap_socket(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)
    448 if send_sni:
--&gt; 449     ssl_sock = _ssl_wrap_socket_impl(
    450         sock, context, tls_in_tls, server_hostname=server_hostname
    451     )
    452 else:

File ~\Anaconda3\lib\site-packages\urllib3\util\ssl_.py:493, in _ssl_wrap_socket_impl(sock, ssl_context, tls_in_tls, server_hostname)
    492 if server_hostname:
--&gt; 493     return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
    494 else:

File ~\Anaconda3\lib\ssl.py:500, in SSLContext.wrap_socket(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)
    494 def wrap_socket(self, sock, server_side=False,
    495                 do_handshake_on_connect=True,
    496                 suppress_ragged_eofs=True,
    497                 server_hostname=None, session=None):
    498     # SSLSocket class handles server_hostname encoding before it calls
    499     # ctx._wrap_socket()
--&gt; 500     return self.sslsocket_class._create(
    501         sock=sock,
    502         server_side=server_side,
    503         do_handshake_on_connect=do_handshake_on_connect,
    504         suppress_ragged_eofs=suppress_ragged_eofs,
    505         server_hostname=server_hostname,
    506         context=self,
    507         session=session
    508     )

File ~\Anaconda3\lib\ssl.py:1040, in SSLSocket._create(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)
   1039             raise ValueError(&quot;do_handshake_on_connect should not be specified for non-blocking sockets&quot;)
-&gt; 1040         self.do_handshake()
   1041 except (OSError, ValueError):

File ~\Anaconda3\lib\ssl.py:1309, in SSLSocket.do_handshake(self, block)
   1308         self.settimeout(None)
-&gt; 1309     self._sslobj.do_handshake()
   1310 finally:

SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1129)

During handling of the above exception, another exception occurred:

MaxRetryError                             Traceback (most recent call last)
File ~\Anaconda3\lib\site-packages\requests\adapters.py:440, in HTTPAdapter.send(self, request, stream, timeout, verify, cert, proxies)
    439 if not chunked:
--&gt; 440     resp = conn.urlopen(
    441         method=request.method,
    442         url=url,
    443         body=request.body,
    444         headers=request.headers,
    445         redirect=False,
    446         assert_same_host=False,
    447         preload_content=False,
    448         decode_content=False,
    449         retries=self.max_retries,
    450         timeout=timeout
    451     )
    453 # Send the request.
    454 else:

File ~\Anaconda3\lib\site-packages\urllib3\connectionpool.py:813, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    810     log.warning(
    811         &quot;Retrying (%r) after connection broken by '%r': %s&quot;, retries, err, url
    812     )
--&gt; 813     return self.urlopen(
    814         method,
    815         url,
    816         body,
    817         headers,
    818         retries,
    819         redirect,
    820         assert_same_host,
    821         timeout=timeout,
    822         pool_timeout=pool_timeout,
    823         release_conn=release_conn,
    824         chunked=chunked,
    825         body_pos=body_pos,
    826         **response_kw
    827     )
    829 # Handle redirect?

File ~\Anaconda3\lib\site-packages\urllib3\connectionpool.py:813, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    810     log.warning(
    811         &quot;Retrying (%r) after connection broken by '%r': %s&quot;, retries, err, url
    812     )
--&gt; 813     return self.urlopen(
    814         method,
    815         url,
    816         body,
    817         headers,
    818         retries,
    819         redirect,
    820         assert_same_host,
    821         timeout=timeout,
    822         pool_timeout=pool_timeout,
    823         release_conn=release_conn,
    824         chunked=chunked,
    825         body_pos=body_pos,
    826         **response_kw
    827     )
    829 # Handle redirect?

File ~\Anaconda3\lib\site-packages\urllib3\connectionpool.py:813, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    810     log.warning(
    811         &quot;Retrying (%r) after connection broken by '%r': %s&quot;, retries, err, url
    812     )
--&gt; 813     return self.urlopen(
    814         method,
    815         url,
    816         body,
    817         headers,
    818         retries,
    819         redirect,
    820         assert_same_host,
    821         timeout=timeout,
    822         pool_timeout=pool_timeout,
    823         release_conn=release_conn,
    824         chunked=chunked,
    825         body_pos=body_pos,
    826         **response_kw
    827     )
    829 # Handle redirect?

File ~\Anaconda3\lib\site-packages\urllib3\connectionpool.py:785, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    783     e = ProtocolError(&quot;Connection aborted.&quot;, e)
--&gt; 785 retries = retries.increment(
    786     method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]
    787 )
    788 retries.sleep()

File ~\Anaconda3\lib\site-packages\urllib3\util\retry.py:592, in Retry.increment(self, method, url, response, error, _pool, _stacktrace)
    591 if new_retry.is_exhausted():
--&gt; 592     raise MaxRetryError(_pool, url, error or ResponseError(cause))
    594 log.debug(&quot;Incremented Retry for (url='%s'): %r&quot;, url, new_retry)

MaxRetryError: HTTPSConnectionPool(host='api.easypost.com', port=443): Max retries exceeded with url: /v2/shipments (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1129)')))

During handling of the above exception, another exception occurred:

SSLError                                  Traceback (most recent call last)
File ~\Anaconda3\lib\site-packages\easypost\requestor.py:222, in Requestor.requests_request(self, method, abs_url, headers, params)
    221 try:
--&gt; 222     result = requests_session.request(
    223         method=method.value,
    224         url=abs_url,
    225         params=url_params,
    226         headers=headers,
    227         json=body,
    228         timeout=timeout,
    229         verify=True,
    230     )
    231     http_body = result.text

File ~\Anaconda3\lib\site-packages\requests\sessions.py:529, in Session.request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)
    528 send_kwargs.update(settings)
--&gt; 529 resp = self.send(prep, **send_kwargs)
    531 return resp

File ~\Anaconda3\lib\site-packages\requests\sessions.py:645, in Session.send(self, request, **kwargs)
    644 # Send the request
--&gt; 645 r = adapter.send(request, **kwargs)
    647 # Total elapsed time of the request (approximately)

File ~\Anaconda3\lib\site-packages\requests\adapters.py:517, in HTTPAdapter.send(self, request, stream, timeout, verify, cert, proxies)
    515 if isinstance(e.reason, _SSLError):
    516     # This branch is for urllib3 v1.22 and later.
--&gt; 517     raise SSLError(e, request=request)
    519 raise ConnectionError(e, request=request)

SSLError: HTTPSConnectionPool(host='api.easypost.com', port=443): Max retries exceeded with url: /v2/shipments (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1129)')))

During handling of the above exception, another exception occurred:

Error                                     Traceback (most recent call last)
Input In [52], in &lt;cell line: 6&gt;()
      2 import easypost
      4 easypost.api_key = &lt;apikey&gt;
----&gt; 6 shipment = easypost.Shipment.create(
      7     from_address = {
      8         &quot;name&quot;: &quot;EasyPost&quot;,
      9         &quot;street1&quot;: &quot;118 2nd Street&quot;,
     10         &quot;street2&quot;: &quot;4th Floor&quot;,
     11         &quot;city&quot;: &quot;San Francisco&quot;,
     12         &quot;state&quot;: &quot;CA&quot;,
     13         &quot;zip&quot;: &quot;94105&quot;,
     14         &quot;country&quot;: &quot;US&quot;,
     15         &quot;phone&quot;: &quot;415-456-7890&quot;,
     16     },
     17     to_address = {
     18         &quot;name&quot;: &quot;Dr. Steve Brule&quot;,
     19         &quot;street1&quot;: &quot;179 N Harbor Dr&quot;,
     20         &quot;city&quot;: &quot;Redondo Beach&quot;,
     21         &quot;state&quot;: &quot;CA&quot;,
     22         &quot;zip&quot;: &quot;90277&quot;,
     23         &quot;country&quot;: &quot;US&quot;,
     24         &quot;phone&quot;: &quot;310-808-5243&quot;,
     25     },
     26     parcel = {
     27         &quot;length&quot;: 10.2,
     28         &quot;width&quot;: 7.8,
     29         &quot;height&quot;: 4.3,
     30         &quot;weight&quot;: 21.2,
     31     },
     32 )
     34 shipment.buy(rate=shipment.lowest_rate())
     36 print(shipment)

File ~\Anaconda3\lib\site-packages\easypost\shipment.py:32, in Shipment.create(cls, api_key, with_carbon_offset, **params)
     27 url = cls.class_url()
     28 wrapped_params = {
     29     cls.snakecase_name(): params,
     30     &quot;carbon_offset&quot;: with_carbon_offset,
     31 }
---&gt; 32 response, api_key = requestor.request(method=RequestMethod.POST, url=url, params=wrapped_params)
     33 return convert_to_easypost_object(response=response, api_key=api_key)

File ~\Anaconda3\lib\site-packages\easypost\requestor.py:91, in Requestor.request(self, method, url, params, api_key_required, beta)
     89 if params is None:
     90     params = {}
---&gt; 91 http_body, http_status, my_api_key = self.request_raw(
     92     method=method,
     93     url=url,
     94     params=params,
     95     api_key_required=api_key_required,
     96     beta=beta,
     97 )
     98 response = self.interpret_response(http_body=http_body, http_status=http_status)
     99 return response, my_api_key

File ~\Anaconda3\lib\site-packages\easypost\requestor.py:177, in Requestor.request_raw(self, method, url, params, api_key_required, beta)
    173     http_body, http_status = self.urlfetch_request(
    174         method=method, abs_url=abs_url, headers=headers, params=params
    175     )
    176 elif request_lib == &quot;requests&quot;:
--&gt; 177     http_body, http_status = self.requests_request(
    178         method=method, abs_url=abs_url, headers=headers, params=params
    179     )
    180 else:
    181     raise Error(f&quot;Bug discovered: invalid request_lib: {request_lib}. Please report to {SUPPORT_EMAIL}.&quot;)

File ~\Anaconda3\lib\site-packages\easypost\requestor.py:234, in Requestor.requests_request(self, method, abs_url, headers, params)
    232     http_status = result.status_code
    233 except Exception as e:
--&gt; 234     raise Error(
    235         &quot;Unexpected error communicating with EasyPost. If this &quot;
    236         f&quot;problem persists please let us know at {SUPPORT_EMAIL}.&quot;,
    237         original_exception=e,
    238     )
    239 return http_body, http_status

Error: Unexpected error communicating with EasyPost. If this problem persists please let us know at support@easypost.com.
</code></pre>
<hr />
<p>For Postman, if I I am <code>deactivating SSL certificate verification</code> it works.
What I got with:</p>
<p><a href=""https://i.stack.imgur.com/kCuy6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kCuy6.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/X51E8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/X51E8.png"" alt=""Enable SSL certificate verification"" /></a></p>
<hr />
<p>If am calling directly the api and using <code>verify=false</code> that works. See <a href=""https://stackoverflow.com/questions/10667960/python-requests-throwing-sslerror"">Python Requests throwing SSLError</a>.</p>
<pre class=""lang-py prettyprint-override""><code>import requests
url = &quot;https://api.easypost.com/something&quot;
returnResponse = requests.get(url, verify=False)
</code></pre>
<hr />
<hr />
<p>Edit</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
File ~\Anaconda3\lib\site-packages\easypost\requestor.py:222, in Requestor.requests_request(self, method, abs_url, headers, params)
    221 try:
--&gt; 222     result = requests_session.request(
    223         method=method.value,
    224         url=abs_url,
    225         params=url_params,
    226         headers=headers,
    227         json=body,
    228         timeout=timeout,
    229         verify=False,
    230     )
    231     http_body = result.text

File ~\Anaconda3\lib\site-packages\requests\sessions.py:587, in Session.request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)
    586 send_kwargs.update(settings)
--&gt; 587 resp = self.send(prep, **send_kwargs)
    589 return resp

File ~\Anaconda3\lib\site-packages\requests\sessions.py:701, in Session.send(self, request, **kwargs)
    700 # Send the request
--&gt; 701 r = adapter.send(request, **kwargs)
    703 # Total elapsed time of the request (approximately)

File ~\Anaconda3\lib\site-packages\requests\adapters.py:489, in HTTPAdapter.send(self, request, stream, timeout, verify, cert, proxies)
    488 if not chunked:
--&gt; 489     resp = conn.urlopen(
    490         method=request.method,
    491         url=url,
    492         body=request.body,
    493         headers=request.headers,
    494         redirect=False,
    495         assert_same_host=False,
    496         preload_content=False,
    497         decode_content=False,
    498         retries=self.max_retries,
    499         timeout=timeout,
    500     )
    502 # Send the request.
    503 else:

File ~\Anaconda3\lib\site-packages\urllib3\connectionpool.py:703, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    702 # Make the request on the httplib connection object.
--&gt; 703 httplib_response = self._make_request(
    704     conn,
    705     method,
    706     url,
    707     timeout=timeout_obj,
    708     body=body,
    709     headers=headers,
    710     chunked=chunked,
    711 )
    713 # If we're going to release the connection in ``finally:``, then
    714 # the response doesn't need to know about the connection. Otherwise
    715 # it will also try to release it and we'll have a double-release
    716 # mess.

File ~\Anaconda3\lib\site-packages\urllib3\connectionpool.py:386, in HTTPConnectionPool._make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
    385 try:
--&gt; 386     self._validate_conn(conn)
    387 except (SocketTimeout, BaseSSLError) as e:
    388     # Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.

File ~\Anaconda3\lib\site-packages\urllib3\connectionpool.py:1040, in HTTPSConnectionPool._validate_conn(self, conn)
   1039 if not getattr(conn, &quot;sock&quot;, None):  # AppEngine might not have  `.sock`
-&gt; 1040     conn.connect()
   1042 if not conn.is_verified:

File ~\Anaconda3\lib\site-packages\urllib3\connection.py:401, in HTTPSConnection.connect(self)
    400 context = self.ssl_context
--&gt; 401 context.verify_mode = resolve_cert_reqs(self.cert_reqs)
    403 # Try to load OS default certs if none are given.
    404 # Works well on Windows (requires Python3.4+)

File ~\Anaconda3\lib\ssl.py:720, in SSLContext.verify_mode(self, value)
    718 @verify_mode.setter
    719 def verify_mode(self, value):
--&gt; 720     super(SSLContext, SSLContext).verify_mode.__set__(self, value)

ValueError: Cannot set verify_mode to CERT_NONE when check_hostname is enabled.

During handling of the above exception, another exception occurred:

Error                                     Traceback (most recent call last)
Input In [5], in &lt;cell line: 6&gt;()
      2 import easypost
      4 easypost.api_key = &lt;apikey&gt;
----&gt; 6 shipment = easypost.Shipment.create(
      7     from_address = {
      8         &quot;name&quot;: &quot;EasyPost&quot;,
      9         &quot;street1&quot;: &quot;118 2nd Street&quot;,
     10         &quot;street2&quot;: &quot;4th Floor&quot;,
     11         &quot;city&quot;: &quot;San Francisco&quot;,
     12         &quot;state&quot;: &quot;CA&quot;,
     13         &quot;zip&quot;: &quot;94105&quot;,
     14         &quot;country&quot;: &quot;US&quot;,
     15         &quot;phone&quot;: &quot;415-456-7890&quot;,
     16     },
     17     to_address = {
     18         &quot;name&quot;: &quot;Dr. Steve Brule&quot;,
     19         &quot;street1&quot;: &quot;179 N Harbor Dr&quot;,
     20         &quot;city&quot;: &quot;Redondo Beach&quot;,
     21         &quot;state&quot;: &quot;CA&quot;,
     22         &quot;zip&quot;: &quot;90277&quot;,
     23         &quot;country&quot;: &quot;US&quot;,
     24         &quot;phone&quot;: &quot;310-808-5243&quot;,
     25     },
     26     parcel = {
     27         &quot;length&quot;: 10.2,
     28         &quot;width&quot;: 7.8,
     29         &quot;height&quot;: 4.3,
     30         &quot;weight&quot;: 21.2,
     31     },
     32 )
     34 shipment.buy(rate=shipment.lowest_rate())
     36 print(shipment)

File ~\Anaconda3\lib\site-packages\easypost\shipment.py:32, in Shipment.create(cls, api_key, with_carbon_offset, **params)
     27 url = cls.class_url()
     28 wrapped_params = {
     29     cls.snakecase_name(): params,
     30     &quot;carbon_offset&quot;: with_carbon_offset,
     31 }
---&gt; 32 response, api_key = requestor.request(method=RequestMethod.POST, url=url, params=wrapped_params)
     33 return convert_to_easypost_object(response=response, api_key=api_key)

File ~\Anaconda3\lib\site-packages\easypost\requestor.py:91, in Requestor.request(self, method, url, params, api_key_required, beta)
     89 if params is None:
     90     params = {}
---&gt; 91 http_body, http_status, my_api_key = self.request_raw(
     92     method=method,
     93     url=url,
     94     params=params,
     95     api_key_required=api_key_required,
     96     beta=beta,
     97 )
     98 response = self.interpret_response(http_body=http_body, http_status=http_status)
     99 return response, my_api_key

File ~\Anaconda3\lib\site-packages\easypost\requestor.py:177, in Requestor.request_raw(self, method, url, params, api_key_required, beta)
    173     http_body, http_status = self.urlfetch_request(
    174         method=method, abs_url=abs_url, headers=headers, params=params
    175     )
    176 elif request_lib == &quot;requests&quot;:
--&gt; 177     http_body, http_status = self.requests_request(
    178         method=method, abs_url=abs_url, headers=headers, params=params
    179     )
    180 else:
    181     raise Error(f&quot;Bug discovered: invalid request_lib: {request_lib}. Please report to {SUPPORT_EMAIL}.&quot;)

File ~\Anaconda3\lib\site-packages\easypost\requestor.py:234, in Requestor.requests_request(self, method, abs_url, headers, params)
    232     http_status = result.status_code
    233 except Exception as e:
--&gt; 234     raise Error(
    235         &quot;Unexpected error communicating with EasyPost. If this &quot;
    236         f&quot;problem persists please let us know at {SUPPORT_EMAIL}.&quot;,
    237         original_exception=e,
    238     )
    239 return http_body, http_status

Error: Unexpected error communicating with EasyPost. If this problem persists please let us know at support@easypost.com.
</code></pre>
",0,1660551433,python;ssl;urllib3;easypost,False,4255,1,1669839839,https://stackoverflow.com/questions/73358402/how-to-disable-ssl-for-a-method
74621849,Why does requests.get raise a TimeoutError when running in Databricks but not locally?,"<p>I am trying to do some basic web scraping using a Bureau of Transportation Statistics website (<a href=""https://www.transtats.bts.gov/HomeDrillChart.asp"" rel=""nofollow noreferrer"">https://www.transtats.bts.gov/HomeDrillChart.asp</a>). I initially wrote and ran the code locally in VSCode, which worked fine, but I need this to work in a Databricks notebook as part of a larger data pipeline and when I try running the code in my notebook, I keep running into a TimeoutError.</p>
<p>The following lines that pull in the website data, which I can then parse with Beautiful Soup, work perfectly when I run them locally in VSCode:</p>
<pre><code># importing necessary libraries
import requests
import pandas as pd
from bs4 import BeautifulSoup
import ssl

ssl._create_default_https_context = ssl._create_unverified_context

# Downloading contents of the summary web page
base_url = 'https://www.transtats.bts.gov/HomeDrillChart.asp'
years_data = requests.get(base_url, verify=False).text
</code></pre>
<p>When I copy the above code into a Databricks notebook and try to run it, I get <code>ConnectionError: HTTPSConnectionPool(host='www.transtats.bts.gov', port=443): Max retries exceeded with url: /HomeDrillChart.asp (Caused by NewConnectionError('&lt;urllib3.connection.HTTPSConnection object at 0x7fce182a0d90&gt;: Failed to establish a new connection: [Errno 110] Connection timed out'))</code></p>
<p>I tried changing my proxy settings using two random proxies I found from Google (<a href=""https://www.us-proxy.org/"" rel=""nofollow noreferrer"">https://www.us-proxy.org/</a>) but I get the same <code>TimeoutError: [Errno 110] Connection timed out</code>:</p>
<pre><code>base_url = 'https://www.transtats.bts.gov/HomeDrillChart.asp'
proxies: {
    'https': 'https://75.72.55.108:8118',
    'http': 'http://198.49.68.80:80'
}
try:
    r = requests.get(url=base_url,verify=False,proxies=proxies)
    print(r.status_code)
    print(r.content)
except requests.exceptions.RequestException as e:
    print(e.message)
</code></pre>
<p>I've also tried passing custom headers with User-Agent and cookie information, including removing the cookie expiration from the headers, but nothing is working. Any help would be appreciated.</p>
<p>Full original error message:</p>
<pre><code>---------------------------------------------------------------------------
TimeoutError                              Traceback (most recent call last)
/databricks/python/lib/python3.8/site-packages/urllib3/connection.py in _new_conn(self)
    158         try:
--&gt; 159             conn = connection.create_connection(
    160                 (self._dns_host, self.port), self.timeout, **extra_kw

/databricks/python/lib/python3.8/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options)
     83     if err is not None:
---&gt; 84         raise err
     85 

/databricks/python/lib/python3.8/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options)
     73                 sock.bind(source_address)
---&gt; 74             sock.connect(sa)
     75             return sock

TimeoutError: [Errno 110] Connection timed out

During handling of the above exception, another exception occurred:

NewConnectionError                        Traceback (most recent call last)
/databricks/python/lib/python3.8/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    669             # Make the request on the httplib connection object.
--&gt; 670             httplib_response = self._make_request(
    671                 conn,

/databricks/python/lib/python3.8/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
    380         try:
--&gt; 381             self._validate_conn(conn)
    382         except (SocketTimeout, BaseSSLError) as e:

/databricks/python/lib/python3.8/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn)
    977         if not getattr(conn, &quot;sock&quot;, None):  # AppEngine might not have  `.sock`
--&gt; 978             conn.connect()
    979 

/databricks/python/lib/python3.8/site-packages/urllib3/connection.py in connect(self)
    308         # Add certificate verification
--&gt; 309         conn = self._new_conn()
    310         hostname = self.host

/databricks/python/lib/python3.8/site-packages/urllib3/connection.py in _new_conn(self)
    170         except SocketError as e:
--&gt; 171             raise NewConnectionError(
    172                 self, &quot;Failed to establish a new connection: %s&quot; % e

NewConnectionError: &lt;urllib3.connection.HTTPSConnection object at 0x7fce182a0d90&gt;: Failed to establish a new connection: [Errno 110] Connection timed out

During handling of the above exception, another exception occurred:

MaxRetryError                             Traceback (most recent call last)
/databricks/python/lib/python3.8/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    438             if not chunked:
--&gt; 439                 resp = conn.urlopen(
    440                     method=request.method,

/databricks/python/lib/python3.8/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    725 
--&gt; 726             retries = retries.increment(
    727                 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]

/databricks/python/lib/python3.8/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace)
    445         if new_retry.is_exhausted():
--&gt; 446             raise MaxRetryError(_pool, url, error or ResponseError(cause))
    447 

MaxRetryError: HTTPSConnectionPool(host='www.transtats.bts.gov', port=443): Max retries exceeded with url: /HomeDrillChart.asp (Caused by NewConnectionError('&lt;urllib3.connection.HTTPSConnection object at 0x7fce182a0d90&gt;: Failed to establish a new connection: [Errno 110] Connection timed out'))

During handling of the above exception, another exception occurred:

ConnectionError                           Traceback (most recent call last)
&lt;command-4350135934483206&gt; in &lt;module&gt;
      1 # Downloading contents of the summary web page
      2 base_url = 'https://www.transtats.bts.gov/HomeDrillChart.asp'
----&gt; 3 years_data = requests.get(base_url, verify=False).text
      4 
      5 # Creating BeautifulSoup object, getting a dict of years and links to each year page

/databricks/python/lib/python3.8/site-packages/requests/api.py in get(url, params, **kwargs)
     74 
     75     kwargs.setdefault('allow_redirects', True)
---&gt; 76     return request('get', url, params=params, **kwargs)
     77 
     78 

/databricks/python/lib/python3.8/site-packages/requests/api.py in request(method, url, **kwargs)
     59     # cases, and look like a memory leak in others.
     60     with sessions.Session() as session:
---&gt; 61         return session.request(method=method, url=url, **kwargs)
     62 
     63 

/databricks/python/lib/python3.8/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)
    540         }
    541         send_kwargs.update(settings)
--&gt; 542         resp = self.send(prep, **send_kwargs)
    543 
    544         return resp

/databricks/python/lib/python3.8/site-packages/requests/sessions.py in send(self, request, **kwargs)
    653 
    654         # Send the request
--&gt; 655         r = adapter.send(request, **kwargs)
    656 
    657         # Total elapsed time of the request (approximately)

/databricks/python/lib/python3.8/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    514                 raise SSLError(e, request=request)
    515 
--&gt; 516             raise ConnectionError(e, request=request)
    517 
    518         except ClosedPoolError as e:

ConnectionError: HTTPSConnectionPool(host='www.transtats.bts.gov', port=443): Max retries exceeded with url: /HomeDrillChart.asp (Caused by NewConnectionError('&lt;urllib3.connection.HTTPSConnection object at 0x7fce182a0d90&gt;: Failed to establish a new connection: [Errno 110] Connection timed out'))
</code></pre>
",0,1669770392,python;python-requests;databricks;urllib3,False,683,0,1669770392,https://stackoverflow.com/questions/74621849/why-does-requests-get-raise-a-timeouterror-when-running-in-databricks-but-not-lo
55442205,"Python requests [Errno 111] Connection refused when running on server, but not on local PC","<p>I have a web scraper script which runs fine on my (Windows) PC, but I'm trying to get it to run from a (Linux) web server. I have a number of other scripts which run fine on the server (connecting to different websites than this one), but when I run this script, I get a <code>[Errno 111] Connection refused</code> error.</p>

<p>Here is a minimal version of the script to demonstrate the problem:</p>

<pre><code>import time
import requests
import urllib.request
from bs4 import BeautifulSoup

s = requests.Session()

target = ""http://taxsearch.co.grayson.tx.us:8443/""
headers = {""Accept"": ""text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8"",
           ""Accept-Encoding"": ""gzip, deflate"",
           ""Accept-Language"": ""en"",
           ""Cache-Control"": ""no-cache"",
           ""Connection"": ""keep-alive"",
           ""Host"": ""taxsearch.co.grayson.tx.us:8443"",
           ""Pragma"": ""no-cache"",
           ""Upgrade-Insecure-Requests"": ""1"",
           ""User-Agent"": ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36""
}


time.sleep(1)
response = s.get(target, headers=headers)

if response.status_code == requests.codes.ok:
    results = BeautifulSoup(response.text, 'html.parser')

    # Do something with output
else:
    response.raise_for_status()
</code></pre>

<p>This runs fine on my PC, but when running on the server, I get the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""/home/jken/virtualenv/web-scraper/3.6/lib/python3.6/site-packages/urllib3/connection.py"", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File ""/home/jken/virtualenv/web-scraper/3.6/lib/python3.6/site-packages/urllib3/util/connection.py"", line 80, in create_connection
    raise err
  File ""/home/jken/virtualenv/web-scraper/3.6/lib/python3.6/site-packages/urllib3/util/connection.py"", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/jken/virtualenv/web-scraper/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen
    chunked=chunked)
  File ""/home/jken/virtualenv/web-scraper/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File ""/opt/alt/python36/lib64/python3.6/http/client.py"", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File ""/opt/alt/python36/lib64/python3.6/http/client.py"", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File ""/opt/alt/python36/lib64/python3.6/http/client.py"", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File ""/opt/alt/python36/lib64/python3.6/http/client.py"", line 1026, in _send_output
    self.send(msg)
  File ""/opt/alt/python36/lib64/python3.6/http/client.py"", line 964, in send
    self.connect()
  File ""/home/jken/virtualenv/web-scraper/3.6/lib/python3.6/site-packages/urllib3/connection.py"", line 181, in connect
    conn = self._new_conn()
  File ""/home/jken/virtualenv/web-scraper/3.6/lib/python3.6/site-packages/urllib3/connection.py"", line 168, in _new_conn
    self, ""Failed to establish a new connection: %s"" % e)
urllib3.exceptions.NewConnectionError: &lt;urllib3.connection.HTTPConnection object at 0x2af700598c18&gt;: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/jken/virtualenv/web-scraper/3.6/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send
    timeout=timeout
  File ""/home/jken/virtualenv/web-scraper/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File ""/home/jken/virtualenv/web-scraper/3.6/lib/python3.6/site-packages/urllib3/util/retry.py"", line 398, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='taxsearch.co.grayson.tx.us', port=8443): Max retries exceeded with url: / (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x2af700598c18&gt;: Failed to establish a new connection: [Errno 111] Connection refused',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""../python/grayson-2year.py"", line 22, in &lt;module&gt;
    response = s.get(target, headers=headers)
  File ""/home/jken/virtualenv/web-scraper/3.6/lib/python3.6/site-packages/requests/sessions.py"", line 546, in get
    return self.request('GET', url, **kwargs)
  File ""/home/jken/virtualenv/web-scraper/3.6/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request
    resp = self.send(prep, **send_kwargs)
  File ""/home/jken/virtualenv/web-scraper/3.6/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send
    r = adapter.send(request, **kwargs)
  File ""/home/jken/virtualenv/web-scraper/3.6/lib/python3.6/site-packages/requests/adapters.py"", line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='taxsearch.co.grayson.tx.us', port=8443): Max retries exceeded with url: / (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x2af700598c18&gt;: Failed to establish a new connection: [Errno 111] Connection refused',))
</code></pre>

<p>My guess would be that the issue here is down to some firewall issue on the webserver or something, but I'm really not sure. Is there something I'm missing?</p>
",8,1554044329,python;python-requests;urllib3,False,12100,0,1669705744,https://stackoverflow.com/questions/55442205/python-requests-errno-111-connection-refused-when-running-on-server-but-not-o
42651145,No module named urllib3,"<p>I wrote a script to call an API and ran it successfully last week. This week, it won't run. I get back the following error message:</p>

<pre><code>Traceback (most recent call last):
  File ""user_audit.py"", line 2, in &lt;module&gt;
    import requests
  File ""c:\Python27\lib\site-packages\requests\__init__.py"", line 60, in &lt;module&gt;
    from .packages.urllib3.exceptions import DependencyWarning
  File ""c:\Python27\lib\site-packages\requests\packages\__init__.py"", line 29, in &lt;module&gt;
    import urllib3
ImportError: No module named urllib3
</code></pre>

<p>I've confirmed that packages is up to date, tried uninstalling and reinstalling it, but nothing has worked so far. Can someone help?</p>

<p><strong>ADDENDUM</strong></p>

<p>I installed urllib3 as suggested by @MSHossain, but then got another error message. The new message referenced another file that I'd written, which had created a Python compiled file. The other file was using smptlib to attempt to send an email. I don't understand how this would happen, but I deleted the other file and my script ran without any problems. I've accepted the answer below as I was able to pip install urllib3, but it should have already been included in the requests module.</p>
",25,1488897767,python;xml;python-2.7;urllib3,True,141459,9,1669193230,https://stackoverflow.com/questions/42651145/no-module-named-urllib3
74533122,Deactivate SSL verification globally in Python&#39;s requests or urllib package,"<p>I know that I can <a href=""https://requests.readthedocs.io/en/latest/user/advanced/#ssl-cert-verification"" rel=""nofollow noreferrer"">disable SSL verification in Python's request package</a> as follows:</p>
<pre><code>import requests


response = requests.put(&quot;some.host/RESTfulService/My/Endpoint/&quot;, verify=False)
</code></pre>
<p>The problem is that I use a package in which <code>requests</code> is used to make requests with it's default setting <code>verify=True</code> and I cannot access this keyword argument to set it to <code>False</code> which throws the obvious error:</p>
<pre><code>requests.exceptions.SSLError: HTTPSConnectionPool(host='some.host', port=443): Max retries exceeded with url: /RESTfulService/My/Endpoint/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate (_ssl.c:1129)')))
</code></pre>
<p>Is there any way to deactivate SSL verification globally in <code>requests</code> e. g. by setting an environment variable?</p>
",1,1669122434,python;python-requests;urllib;urllib3,False,1303,0,1669122434,https://stackoverflow.com/questions/74533122/deactivate-ssl-verification-globally-in-pythons-requests-or-urllib-package
74445374,Python requests equivalent of urllib.request.HTTPPasswordMgrWithDefaultRealm,"<p>I'm trying to authenticate and download some images from a NASA Modis product.
As an example, let's take an hdf file from <a href=""https://e4ftl01.cr.usgs.gov/MOLT/MOD09GQ.061/2000.05.14"" rel=""nofollow noreferrer"">this folder</a> (not jpg, which is visible without authentication).</p>
<p>I was able to authenticate and download using urllib (code follows); still, I would like to use requests for this task.<br />
I can't find any useful result online. I know I can just use urllib, but I usually rely on requests and this problem is bugging my head. I think curiosity is a good enough reason. ;)</p>
<p>I have this <strong>working</strong> python script to download a file, based on urllib:</p>
<pre><code>url = &quot;https://e4ftl01.cr.usgs.gov/MOLT/MOD09GQ.061/2000.05.14/MOD09GQ.A2000135.h20v02.061.2020042041314.hdf&quot;

password_manager = urllib.request.HTTPPasswordMgrWithDefaultRealm()
password_manager.add_password(None, &quot;https://urs.earthdata.nasa.gov&quot;, username, password)
CJ = cookielib.CookieJar()

opener = urllib.request.build_opener(
    urllib.request.HTTPBasicAuthHandler(password_manager),
    urllib.request.HTTPCookieProcessor(CJ))
urllib.request.install_opener(opener)

req = urllib.request.Request(url)
with urllib.request.urlopen(req) as resp:
    with open(&quot;im.hdf&quot;, &quot;wb&quot;) as _fb:
        _fb.write(resp.read())
</code></pre>
<p>For the sake of the example, I read the response and write it in a local file, which is perfectly readable.</p>
<p>I would like to perform the same task using requests.
I tried:</p>
<pre><code>with requests.Session() as s:
    _auth = (username, password)
    with s.get(url, auth=_auth) as resp:
        print(resp.status_code)
        print(resp.headers)
</code></pre>
<p>I also tried using HTTPBasicAuth from requests.auth:</p>
<pre><code>with requests.Session() as s:
    _auth = HTTPBasicAuth(username, password)
    with s.get(url, auth=_auth) as resp:
        print(resp.status_code)
        print(resp.headers)
</code></pre>
<p>And authenticating the session:</p>
<pre><code>with requests.Session() as s:
    s.auth = (username, password)
    with s.get(url) as resp:
        print(resp.status_code)
        print(resp.headers)
</code></pre>
<p>I always get <strong>error 401</strong>, and the headers dict contains:</p>
<pre><code>'WWW-Authenticate': 'Basic realm=&quot;Please enter your Earthdata Login credentials. If you do not have a Earthdata Login, create one at https://urs.earthdata.nasa.gov//users/new&quot;'
</code></pre>
<p>The content of the response (<code>resp.content</code>) is:</p>
<pre><code>b'HTTP Basic: Access denied.\n'
</code></pre>
<p>I'm using:</p>
<ul>
<li><p><strong>Python</strong> 3.8.10 - Linux (WSL)</p>
</li>
<li><p><strong>urllib</strong> doesn't show up in pip freeze and doesn't have a __version __ attribute, I'm not sure how to check</p>
</li>
<li><p><strong>requests</strong>==2.28.1</p>
</li>
<li><p><strong>requests-oauthlib</strong>==1.3.1 (not sure if it is relevant)</p>
</li>
</ul>
<p>If I can provide any other detail let me know.</p>
",1,1668514174,python;python-requests;urllib3,False,140,0,1668514174,https://stackoverflow.com/questions/74445374/python-requests-equivalent-of-urllib-request-httppasswordmgrwithdefaultrealm
71364759,"requests.exceptions.ConnectionError: HTTPConnectionPool(host=&#39;localhost&#39;, port=5000): Max retries exceeded with url: /blockchain (Caused by NewConnect","<p><a href=""https://i.stack.imgur.com/a4KbL.png"" rel=""nofollow noreferrer"">enter image description here</a><a href=""https://i.stack.imgur.com/EfqJo.png"" rel=""nofollow noreferrer"">this is my code where i get stuck</a></p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\annun\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\urllib3\connection.py&quot;, line 159, in _new_conn
    conn = connection.create_connection(
  File &quot;C:\Users\annun\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\urllib3\util\connection.py&quot;, line 84, in create_connection
    raise err
  File &quot;C:\Users\annun\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\urllib3\util\connection.py&quot;, line 74, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;C:\Users\annun\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\urllib3\connectionpool.py&quot;, line 670, in urlopen
    httplib_response = self._make_request(
  File &quot;C:\Users\annun\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\urllib3\connectionpool.py&quot;, line 392, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File &quot;C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.752.0_x64__qbz5n2kfra8p0\lib\http\client.py&quot;, line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File &quot;C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.752.0_x64__qbz5n2kfra8p0\lib\http\client.py&quot;, line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File &quot;C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.752.0_x64__qbz5n2kfra8p0\lib\http\client.py&quot;, line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File &quot;C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.752.0_x64__qbz5n2kfra8p0\lib\http\client.py&quot;, line 1037, in _send_output
    self.send(msg)
  File &quot;C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.752.0_x64__qbz5n2kfra8p0\lib\http\client.py&quot;, line 975, in send
    self.connect()
  File &quot;C:\Users\annun\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\urllib3\connection.py&quot;, line 187, in connect
    conn = self._new_conn()
  File &quot;C:\Users\annun\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\urllib3\connection.py&quot;, line 171, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: &lt;urllib3.connection.HTTPConnection object at 0x000001AD66840430&gt;: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;C:\Users\annun\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\requests\adapters.py&quot;, line 439, in send
    resp = conn.urlopen(
  File &quot;C:\Users\annun\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\urllib3\connectionpool.py&quot;, line 726, in urlopen
    retries = retries.increment(
  File &quot;C:\Users\annun\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\urllib3\util\retry.py&quot;, line 446, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /blockchain (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x000001AD66840430&gt;: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.752.0_x64__qbz5n2kfra8p0\lib\runpy.py&quot;, line 187, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File &quot;C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.752.0_x64__qbz5n2kfra8p0\lib\runpy.py&quot;, line 146, in _get_module_details
    return _get_module_details(pkg_main_name, error)
  File &quot;C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.752.0_x64__qbz5n2kfra8p0\lib\runpy.py&quot;, line 110, in _get_module_details
    __import__(pkg_name)
  File &quot;C:\Users\annun\python-blockchain\backend\app\__init__.py&quot;, line 82, in &lt;module&gt;
    result = requests.get(f'http://localhost:{ROOT_PORT}/blockchain')
  File &quot;C:\Users\annun\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\requests\api.py&quot;, line 75, in get
    return request('get', url, params=params, **kwargs)
  File &quot;C:\Users\annun\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\requests\api.py&quot;, line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File &quot;C:\Users\annun\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\requests\sessions.py&quot;, line 533, in request
    resp = self.send(prep, **send_kwargs)
  File &quot;C:\Users\annun\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\requests\sessions.py&quot;, line 646, in send
    r = adapter.send(request, **kwargs)
  File &quot;C:\Users\annun\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\requests\adapters.py&quot;, line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /blockchain (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x000001AD66840430&gt;: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
</code></pre>
",1,1646504167,python;debugging;backend;connection-pooling;urllib3,True,2466,1,1667799221,https://stackoverflow.com/questions/71364759/requests-exceptions-connectionerror-httpconnectionpoolhost-localhost-port-5
73348411,Why is the request ignoring my proxy parameter and returning my IP to me?,"<p>It all started with the fact that I reinstalled paycharm on my computer, reinstalled python
For example, I write normal code, it always worked:</p>
<pre><code>import os
import requests
proxies = {'https': 'https://181.232.190.130:999'}
s = requests.Session()
s.proxies = proxies
r = s.get(url = 'http://wtfismyip.com/text', verify=False)
ip = r.text
print ('Your IP is ' + ip)
os.system(&quot;pause&quot;)
</code></pre>
<p>Of course, the proxies are up-to-date and work.
The problem is that the request returns me my real IP. As if it just ignores this parameter.
I am sure that the problem is not in the code, but in something else! But I have no idea where to look! Spent a whole day, but could not achieve anything!</p>
",0,1660436766,python;python-requests;proxy;urllib3,False,215,1,1667566049,https://stackoverflow.com/questions/73348411/why-is-the-request-ignoring-my-proxy-parameter-and-returning-my-ip-to-me
74186169,proxy max retries exceeded with url,"<p>am following a tutorial on Youtube where we learn SQL injection on portswigger platform
and we write the exploit as script with python, am using kali linux as virtual machine, am suffering from proxy error and i didn't know what to search for other than stackoverflow with no benefit answers. now here is my python code first :</p>
<pre><code>import requests
import sys
import urllib3
from bs4 import BeautifulSoup
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# the purpose of proxy is to proxy to myself in order to pass my request through burpsuit
proxies = {'http': 'http://127.0.0.1:8080', 'https': 'https://127.0.0.1:8080'}

def get_csrf_token(s, url):
    r = s.get(url, verify=False, proxies=proxies)
    soup = BeautifulSoup(r.text, 'html.parser')
    csrf = soup.find(&quot;input&quot;)['value']
    return csrf


def exploit_sqli(s, url, payload):
    csrf = get_csrf_token(s, url)
    data = {&quot;csrf&quot;: csrf,
        &quot;username&quot;: payload,
        &quot;password&quot;: &quot;randomtext&quot;}

    r = s.post(url, data=data, verify=False, proxies=proxies)
    res = r.text
    if &quot;Log out&quot; in res:
        return True
    else:
        return False

if __name__ == &quot;__main__&quot;:
    try:
        url = sys.argv[1].strip()
        sqli_payload = sys.argv[2].strip()
    except IndexError:
        print('[-] Usage: %s &lt;url&gt; &lt;sql-payload&gt;' % sys.argv[0])
        print('[-] Example: %s www.example.com &quot;1=1&quot;' % sys.argv[0])

    s = requests.Session()

    if exploit_sqli(s, url, sqli_payload):
        print('[+] SQL injection successful! We have logged in as the administrator user.')
    else:
        print('[-] SQL injection unsuccessful.')
</code></pre>
<p>and am suffering of runtime error and here is my track trace:</p>
<blockquote>
<p>Traceback (most recent call last):   File
&quot;/home/kali/.local/lib/python3.10/site-packages/urllib3/connectionpool.py&quot;,
line 700, in urlopen
self._prepare_proxy(conn)   File &quot;/home/kali/.local/lib/python3.10/site-packages/urllib3/connectionpool.py&quot;,
line 996, in _prepare_proxy
conn.connect()   File &quot;/home/kali/.local/lib/python3.10/site-packages/urllib3/connection.py&quot;,
line 364, in connect
self.sock = conn = self._connect_tls_proxy(hostname, conn)   File &quot;/home/kali/.local/lib/python3.10/site-packages/urllib3/connection.py&quot;,
line 499, in <em>connect_tls_proxy
socket = ssl_wrap_socket(   File &quot;/home/kali/.local/lib/python3.10/site-packages/urllib3/util/ssl</em>.py&quot;,
line 453, in ssl_wrap_socket
ssl_sock = <em>ssl_wrap_socket_impl(sock, context, tls_in_tls)   File &quot;/home/kali/.local/lib/python3.10/site-packages/urllib3/util/ssl</em>.py&quot;,
line 495, in _ssl_wrap_socket_impl
return ssl_context.wrap_socket(sock)   File &quot;/usr/lib/python3.10/ssl.py&quot;, line 513, in wrap_socket
return self.sslsocket_class._create(   File &quot;/usr/lib/python3.10/ssl.py&quot;, line 1071, in _create
self.do_handshake()   File &quot;/usr/lib/python3.10/ssl.py&quot;, line 1342, in do_handshake
self._sslobj.do_handshake() ssl.SSLError: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:997)</p>
<p>During handling of the above exception, another exception occurred:</p>
<p>Traceback (most recent call last):   File
&quot;/home/kali/.local/lib/python3.10/site-packages/requests/adapters.py&quot;,
line 489, in send
resp = conn.urlopen(   File &quot;/home/kali/.local/lib/python3.10/site-packages/urllib3/connectionpool.py&quot;,
line 787, in urlopen
retries = retries.increment(   File &quot;/home/kali/.local/lib/python3.10/site-packages/urllib3/util/retry.py&quot;,
line 592, in increment
raise MaxRetryError(_pool, url, error or ResponseError(cause)) urllib3.exceptions.MaxRetryError:
HTTPSConnectionPool(host='0a0000f503d3bb1ac1b8273000b600cb.web-security-academy.net',
port=443): Max retries exceeded with url: /login (Caused by
ProxyError('Your proxy appears to only use HTTP and not HTTPS, try
changing your proxy URL to be HTTP. See:
<a href=""https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#https-proxy-error-http-proxy%27"" rel=""nofollow noreferrer"">https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#https-proxy-error-http-proxy'</a>,
SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number
(_ssl.c:997)'))))</p>
<p>During handling of the above exception, another exception occurred:</p>
<p>Traceback (most recent call last):   File
&quot;/home/kali/Desktop/machines/portSwiggerLabs/sqlInjection/lab-02/sql-lab-02.py&quot;,
line 43, in 
if exploit_sqli(s, url, sqli_payload):   File &quot;/home/kali/Desktop/machines/portSwiggerLabs/sqlInjection/lab-02/sql-lab-02.py&quot;,
line 21, in exploit_sqli
csrf = get_csrf_token(s, url)   File &quot;/home/kali/Desktop/machines/portSwiggerLabs/sqlInjection/lab-02/sql-lab-02.py&quot;,
line 12, in get_csrf_token
r = s.get(url, verify=False, proxies=proxies)   File &quot;/home/kali/.local/lib/python3.10/site-packages/requests/sessions.py&quot;,
line 600, in get
return self.request(&quot;GET&quot;, url, **kwargs)   File &quot;/home/kali/.local/lib/python3.10/site-packages/requests/sessions.py&quot;,
line 587, in request
resp = self.send(prep, **send_kwargs)   File &quot;/home/kali/.local/lib/python3.10/site-packages/requests/sessions.py&quot;,
line 701, in send
r = adapter.send(request, **kwargs)   File &quot;/home/kali/.local/lib/python3.10/site-packages/requests/adapters.py&quot;,
line 559, in send
raise ProxyError(e, request=request) requests.exceptions.ProxyError:
HTTPSConnectionPool(host='0a0000f503d3bb1ac1b8273000b600cb.web-security-academy.net',
port=443): Max retries exceeded with url: /login (Caused by
ProxyError('Your proxy appears to only use HTTP and not HTTPS, try
changing your proxy URL to be HTTP. See:
<a href=""https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#https-proxy-error-http-proxy%27"" rel=""nofollow noreferrer"">https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#https-proxy-error-http-proxy'</a>,
SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number
(_ssl.c:997)'))))</p>
</blockquote>
<p>i tried to check my localhost(127.0.0.1) ip if it's another one or maybe not configured or anything related and I didn't reach any result</p>
",0,1666641429,python;python-3.x;proxy;urllib3,True,2450,2,1666978393,https://stackoverflow.com/questions/74186169/proxy-max-retries-exceeded-with-url
2018026,"What are the differences between the urllib, urllib2, urllib3 and requests module?","<p>In Python, what are the differences between the <a href=""https://docs.python.org/library/urllib.html"" rel=""noreferrer""><code>urllib</code></a>, <a href=""https://docs.python.org/2.7/library/urllib2.html"" rel=""noreferrer""><code>urllib2</code></a>, <a href=""https://urllib3.readthedocs.io/en/latest/"" rel=""noreferrer""><code>urllib3</code></a> and <a href=""https://requests.readthedocs.io"" rel=""noreferrer""><code>requests</code></a> modules? Why are there three? They seem to do the same thing...</p>
",1041,1262834795,python;python-requests;urllib;urllib2;urllib3,True,486147,11,1666881342,https://stackoverflow.com/questions/2018026/what-are-the-differences-between-the-urllib-urllib2-urllib3-and-requests-modul
43901079,TypeError: can&#39;t concat bytes to str python3,"<p>I'm trying to send HTTP request in <code>python3</code> using <code>urllib3</code>.</p>

<p>Here is code snippet</p>

<pre><code>request_body = {'grant_type':'password','username': username,'password': password}
request_headers = {'Content-Type' : 'application/x-www-form-urlencoded','Authorization': ""hash string""}
http = urllib3.PoolManager()
response = http.request('POST', 'https://api/url/endpoint', headers=request_headers, body=request_body)
</code></pre>

<p>But when I try to execute it, it throws following error.</p>

<blockquote>
  <p>TypeError: can't concat bytes to str</p>
</blockquote>

<p>Full traceback</p>

<pre><code>Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
File ""/Users/Mubin/anaconda/lib/python3.6/site-
packages/urllib3/request.py"", line 70, in request
**urlopen_kw)
File ""/Users/Mubin/anaconda/lib/python3.6/site-
packages/urllib3/request.py"", line 148, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
File ""/Users/Mubin/anaconda/lib/python3.6/site-
  packages/urllib3/poolmanager.py"", line 321, in urlopen
response = conn.urlopen(method, u.request_uri, **kw)
File ""/Users/Mubin/anaconda/lib/python3.6/site-
  packages/urllib3/connectionpool.py"", line 600, in urlopen
chunked=chunked)
File ""/Users/Mubin/anaconda/lib/python3.6/site-
  packages/urllib3/connectionpool.py"", line 356, in _make_request
conn.request(method, url, **httplib_request_kw)
File ""/Users/Mubin/anaconda/lib/python3.6/http/client.py"", line 1239, in request
self._send_request(method, url, body, headers, encode_chunked)
File ""/Users/Mubin/anaconda/lib/python3.6/http/client.py"", line 1285, in _send_request
self.endheaders(body, encode_chunked=encode_chunked)
File ""/Users/Mubin/anaconda/lib/python3.6/http/client.py"", line 1234, in endheaders
self._send_output(message_body, encode_chunked=encode_chunked)
File ""/Users/Mubin/anaconda/lib/python3.6/http/client.py"", line 1064, in _send_output
+ b'\r\n'
</code></pre>

<p>Can anybody point out what I'm doing wrong?</p>

<p>EDIT</p>

<p><code>request_body</code> type check</p>

<pre><code>&gt;&gt;&gt; type(request_body)
    &lt;class 'dict'&gt;
&gt;&gt;&gt; type(request_body['username'])
    &lt;class 'str'&gt;
&gt;&gt;&gt; type(request_body['password'])
    &lt;class 'str'&gt;
&gt;&gt;&gt; type(request_body['grant_type'])
    &lt;class 'str'&gt;
</code></pre>
",2,1494444065,python;python-3.x;urllib3,True,2483,3,1665594453,https://stackoverflow.com/questions/43901079/typeerror-cant-concat-bytes-to-str-python3
61293989,Is there a way to stop or cancel a urlretrieve in python?,"<p>So I basically have written a program <strong>in python using tkinter and urllib.request</strong> which is supposed to work as a downloader, but each downloader has to have a pause or cancel button but I can't seem to find anyway to do this! Recently I bumped into the same question in stackoverflow( the link: <a href=""https://stackoverflow.com/questions/50419057/is-it-possible-to-stop-cancel-urlretrieve-process"">Is it possible to stop (cancel) urlretrieve process?</a>) and it seems like that I have to use <strong>threads or multi-processing</strong> but I don't have a single idea how to do this! By the way how is threading or multi-processing going to help with the canceling pausing the download? Can someone explain to me what should I do? Is there anyway to do this without threading or multi-processing? If there is not, can you explain how to use threading or multi-processing in this program because I don't have a single clue what to do! Please help me out on this.
My code:</p>

<pre><code>from tkinter import *
from tkinter import font as tkFont
import random
import urllib.request
import requests


def printsth():
    print(""Yay it works! "")


def main_menu():
    root = Tk()
    root.title('8-bit downloader ')
    root.iconbitmap(r""C:\Users\rayanravesh\PycharmProjects\GUI_Calculator\icon.ico"")
    root.geometry(""600x280"")
    # the top menu
    num = IntVar()
    chum = IntVar()
    # var = IntVar()
    menu = Menu(root)
    root.config(menu=menu)
    submenu = Menu(menu)
    menu.add_cascade(label=""Settings"", menu=submenu)

    def custom_op():
        custom = Toplevel()
        custom.iconbitmap(r""C:\Users\rayanravesh\PycharmProjects\GUI_Calculator\icon.ico"")
    submenu.add_command(label=""Customization "", command=custom_op)

    def settings_op():
        global gps
        set_win = Toplevel()
        set_win.iconbitmap(r""C:\Users\rayanravesh\PycharmProjects\GUI_Calculator\icon.ico"")
        path_label = Label(set_win, text=""Current default download path: "")
        path_entry = Entry(set_win, width=30)
        file_read = open('Data.txt', 'r')
        data_base = file_read.read()
        path_entry.insert(0, data_base)
        file_read.close()

        def default_output():
            global location
            file_read2 = open('Data.txt', 'r+')
            file_read2.truncate(0)
            file_read2.close()
            write_file2 = open('Data.txt', 'w')
            write_file2.write(path_entry.get())
            write_file2.close()
            location = path_entry.get() + ""\\""
            default_location = location.replace(""\\"", ""\\\\"")
        path_btn = Button(set_win, text=""Submit "", command=default_output)
        path_label.pack(anchor=CENTER, expand=1)
        path_entry.pack(anchor=CENTER, expand=1)
        path_btn.pack(anchor=CENTER, expand=1)
    submenu.add_command(label=""Settings "", command=settings_op)
    submenu.add_separator()
    submenu.add_command(label=""Exit"", command=root.destroy)

    # the section menu
    editmenu = Menu(menu)
    menu.add_cascade(label=""Sections(soon)"", menu=editmenu)
    editmenu.add_command(label=""Downloader"", command=printsth)
    editmenu.add_command(label=""Converter"", command=printsth)
    editmenu.add_command(label=""Media Player"", command=printsth)
    editmenu.add_command(label=""Editor"", command=printsth)
    # the tool bar
    toolbar = Frame(root, bg=""light gray"")
    insert_button = Button(toolbar, text=""Insert an image"", command=printsth)
    insert_button.pack(side=LEFT, padx=2, pady=2)
    print_button = Button(toolbar, text=""Print"", command=printsth)
    print_button.pack(side=LEFT, padx=2, pady=2)
    toolbar.pack(side=TOP, fill=X)

    # the download function
    def download_image():
        global formatname
        if num.get() == 1:
            name = random.randrange(1, 1000000)
        else:
            name = str(name_entry.get())
        formatname = str(format_entry.get())
        '''if var.get() == 1:
            operator = str(url_entry.get())
            formatname = '.' + operator[-3] + operator[-2] + operator[-1]
        else:
            pass'''
        fullname = str(name) + formatname
        url = str(url_entry.get())
        fw = open('file-size.txt', 'w')
        file_size = int(requests.head(url, headers={'accept-encoding': ''}).headers['Content-Length'])
        fw.write(str(file_size))
        fw.close()
        if chum.get() == 1:
            filee = open('Data.txt', 'r')
            destination = filee.read()
            path = destination
            output_entry.insert(0, destination)
            filee.close()
        else:
            output_entry.delete(0, END)
            path = str(output_entry.get()) + ""\\""
        urllib.request.urlretrieve(url, path.replace(""\\"", ""\\\\"") + fullname)

    # the status bar
    status_bar = Label(root, text=""Downloading..."", bd=1, relief=SUNKEN, anchor=W)
    status_bar.pack(side=BOTTOM, fill=X)

    # the download frame
    body_frame = Frame(root, bg=""light blue"")
    download_button = Button(body_frame, text=""Download! "", command=download_image, border=3, width=20, height=5)
    download_design = tkFont.Font(size=12, slant='italic')
    download_button['font'] = download_design
    download_button.pack(side=LEFT, pady=5, padx=5)
    body_frame.pack(side=LEFT, fill=Y)
    # the main interaction menu
    inter_frame = Frame(root)
    url_entry = Entry(inter_frame, width=30)
    label = Label(inter_frame, text=""Enter the image URL: "")
    file_format = Label(inter_frame, text=""Choose your file format: "")
    format_entry = Entry(inter_frame, width=30)
    file_name = Label(inter_frame, text=""File's name: "")
    name_entry = Entry(inter_frame, width=30)
    check_name = Checkbutton(inter_frame, text=""Give a random name"", variable=num)
    # check_format = Checkbutton(inter_frame, text=""Download with default format"", variable=var)
    check_default = Checkbutton(inter_frame, text=""Download to default path"", variable=chum)
    output_path = Label(inter_frame, text=""Choose output path: "")
    output_entry = Entry(inter_frame, width=30)
    file_name.pack(anchor=CENTER, expand=1)
    name_entry.pack(anchor=CENTER, expand=1)
    check_name.pack(anchor=CENTER, expand=1)
    label.pack(anchor=CENTER, expand=1)
    url_entry.pack(anchor=CENTER, expand=1)
    file_format.pack(anchor=CENTER, expand=1)
    format_entry.pack(anchor=CENTER, expand=1)
    format_entry.insert(0, '.')
    # check_format.pack(anchor=CENTER)
    output_path.pack(anchor=CENTER, expand=1)
    output_entry.pack(anchor=CENTER, expand=1)
    check_default.pack(anchor=CENTER, expand=1)
    inter_frame.pack(expand=1)
    root.mainloop()

    # the end!


main_menu()
</code></pre>
",0,1587233642,python;tkinter;urllib;urllib3;cancel-button,True,925,2,1664748199,https://stackoverflow.com/questions/61293989/is-there-a-way-to-stop-or-cancel-a-urlretrieve-in-python
73776133,How To Submit GET Request using urllib-3 in python,"<p>I'm trying to submit a get request using the following code in AWS Lambda.</p>
<pre><code>hed = {'Authorization': f&quot;bearer {token}&quot;}
URL = &quot;https://integrate.elluciancloud.com/api/academic-periods&quot;
r = http.request('GET', URL, headers=hed)
print(r.data)
</code></pre>
<p>The Token which I'm passing is : <code>iLCJ0b2ciOlsiRVhDSEFOR0VfREV.TSUdORVJfRU5BQkxFRCIsIlBBQ0tBR0VTX0ZFQVRVUkVTX0VOQUJMRUQiXSwicm9sZXMiOlsiYWZlZWYyMDQtZTM3MS00MTlhLWE4OTAtOTNlNjYyMWE0MjM4Il0sInRlbmFudCI6eyJpZCI6IjcyMjJhZmEwLWYzNGYtNDRhNy1hZWVmLTEzZDE5NWJkNTQ4YSIsImFjY291bnRJZCI6IkludGVybmFsRWxsdWNpYW5CYW5uZXJNb2Rlcm5pemF</code></p>
<p>But I'm the following error message in <code>r.data</code></p>
<pre><code>b'{&quot;errors&quot;:[{&quot;code&quot;:&quot;General.error&quot;,&quot;description&quot;:&quot;Application error&quot;,&quot;message&quot;:&quot;java.lang.IllegalArgumentException&quot;}]}'
</code></pre>
<p>I tried to encode the token (<code>token = token.encode('ascii', 'ignore')</code>) but it's not working</p>
",0,1663602790,python;urllib3,False,53,0,1663602790,https://stackoverflow.com/questions/73776133/how-to-submit-get-request-using-urllib-3-in-python
73450442,Download with python requests asks captcha but browser does,"<p>My code is using a simple python request (or pandas, urllib3, and others) to download a xlsx file.
There are similar questions <a href=""https://stackoverflow.com/questions/31279624/python-urllib-requests-fails-to-download-file-but-browser-does"">here</a> and <a href=""https://stackoverflow.com/questions/59205601/how-to-download-a-file-using-web-url-in-python-download-through-browser-works-b"">here</a> but the answers do not apply.</p>
<p>I run this code:</p>
<pre><code>import requests
headers= {}
headers['User-Agent']=  &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:65.0) Gecko/20100101 Firefox/65.0&quot;
r = requests.get(url, headers=headers)
bts = BytesIO(r.content)
</code></pre>
<p>And the r.content is an html with the following message</p>
<pre>
<h1>We apologize for the inconvenience...</h1>\n            <p>...but your activity and behavior on this site made us think that you are a bot.</p>\n            <p><b>Note:</b> A number of things could be going on here.</p>\n            \n                <li>If you are attempting to access this site using an anonymous Private/Proxy network, please disable that and try accessing site again.</li>\n                <li>Due to previously detected malicious behavior which originated from the network you\'re using, please request unblock to site.</li>\n 
</pre>
<p>It seems the website is able to find out this is not a browser. I naturally do not want to solve captchas and am just thinking of masking the code as if it were my browser. Is it possible and how? Does the inspect option of this url give any solution?</p>
<p>My best attempts have downloaded corrupted files with a size between 1 and 3KB instead of the actual 800kb file.</p>
",0,1661198540,python;python-requests;urllib;urllib3;python-requests-html,False,334,0,1661198540,https://stackoverflow.com/questions/73450442/download-with-python-requests-asks-captcha-but-browser-does
73398239,Selenium closes browser after Python code is finished filling out form,"<p>I'm using Selenium to auto-fill forms using <code>forminfo.py</code> for the inputted information. (Right now I'm testing it out on Instagram just to figure everything out.) What I have done is, I have it check to see if the google chrome driver is already installed in a particular location and if not it installs it and if so it just continues and runs the code to fill out the form.</p>
<p>The problem I'm having is, everything runs fine IF the driver isn't installed and it installs it manually. BUT If the driver is already installed, after the form is filled out and it hits the submit button it closes the browser immediately. Ive only been learning Python for about a few weeks now and I cant figure out whats wrong. Also every where I've look I can find a solution to the problem. Also, I'm not getting any errors in my terminal.</p>
<pre><code>################## IMPORTS ##################

import forminfo
import os
import urllib.request as urllib
import webbrowser
import zipfile
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.proxy import Proxy, ProxyType
from time import sleep

################## CHROME DRIVER DOWNLOAD ##################

path = &quot;C:/Users/&quot;+os.getlogin()+&quot;/Documents/chromedriver.exe&quot;
isFile = os.path.isfile(path)
if not os.path.isfile(path): 
    webbrowser.get('windows-default').open(&quot;http://www.google.com&quot;, new=0)
    zip_path, _ = urllib.urlretrieve(forminfo.downloadurl)
    urllib.urlopen = zip_path
    extract_dir = &quot;C:/Users/&quot;+os.getlogin()+&quot;/Documents&quot;
    zipfile.ZipFile(zip_path, &quot;r&quot;).extractall(extract_dir)
else:
    print(&quot;Starting now....&quot;)

################## PROXY INFO ##################

prox = Proxy()
prox.proxy_type = ProxyType.MANUAL
prox.socks_version = 5
prox.socks_proxy = forminfo.socks5ip
#prox.http_proxy = &quot;ip_addr:port&quot;
#prox.ssl_proxy = &quot;ip_addr:port&quot;

################## BROWSER ##################


capabilities = webdriver.DesiredCapabilities.CHROME
prox.add_to_capabilities(capabilities)

#
capabilities['acceptInsecureCerts'] = True
capabilities['acceptSslCerts'] = True
#

driver_service = Service(executable_path=&quot;C:/Users/&quot;+os.getlogin()+&quot;/Documents/chromedriver&quot;)
chrome_options = Options()
chrome_options.add_argument(&quot;--incognito&quot;)
chrome_options.add_argument(&quot;start-maximized&quot;)
chrome_options.add_experimental_option(&quot;useAutomationExtension&quot;, False)
chrome_options.add_experimental_option(&quot;excludeSwitches&quot;,[&quot;enable-automation&quot;])

#
chrome_options.add_argument(&quot;--ignore-certificate-error&quot;)
chrome_options.add_argument(&quot;--ignore-ssl-errors&quot;)
#

driver = webdriver.Chrome(service=driver_service, options=chrome_options, desired_capabilities=capabilities)
driver.get('https://www.instagram.com')


################## BOT ##################


sleep(3)
elem = driver.find_element(By.NAME, &quot;username&quot;)
elem.send_keys(forminfo.username)

elem = driver.find_element(By.NAME, &quot;password&quot;)
elem.send_keys(forminfo.password)

sleep(3)
elem = driver.find_element(By.XPATH, &quot;//*[@id=\&quot;loginForm\&quot;]/div/div[3]/button/div&quot;)
elem.click()
</code></pre>
<p>The only solution Ive found to the problem is <a href=""https://stackoverflow.com/a/38085207/19710148"">here</a> by adding a <code>breakpoint()</code> at the very end of my code. But I feel like this would mess some things up down the road if I decided to expand on this code and eventually turn it into a .exe file with pyIntsaller or Nuitak. So I wanted to see If there was a solution before just moving forward with that resolution.</p>
<p>I know it's a problem with the <code>################## CHROME DRIVER DOWNLOAD ##################</code> code because I wasn't having this problem until I decided to add that code. This Is the very most recent part I added and everything was fine until then.</p>
",1,1660803268,python;python-3.x;selenium;selenium-webdriver;urllib3,True,1443,1,1660808851,https://stackoverflow.com/questions/73398239/selenium-closes-browser-after-python-code-is-finished-filling-out-form
73370126,how to clear cache of urllib3.PoolManager?,"<p>I am using <code>urllib3.PoolManager</code> for creating a request and sending it. this is my code:</p>
<pre><code>import urllib3
http = urllib3.PoolManager(
            timeout=10,
        )
response=http.request('GET','example.com')
print(response.headers)
</code></pre>
<p>The output is as follows:</p>
<pre><code>HTTPHeaderDict({'Cache-Control': 'private', 'Content-Type': 'text/html; charset=utf-8', 'Vary': 'Accept-Encoding', 'Server': 'Microsoft-IIS/10.0', 'X-AspNet-Version': '4.0.30319', 'X-Powered-By': 'ASP.NET', 'X-Powered-By-Plesk': 'PleskWin', 'Date': 'Tue, 16 Aug 2022 06:57:53 GMT', 'Content-Length': '5807'})
</code></pre>
<p>After that, I used python <code>requests</code> instead of <code>urllib3</code> as follows:</p>
<pre><code>import requests
response = requests.get('example.com')
print(response.headers)
</code></pre>
<p>The output is as follows:</p>
<pre><code>{'Cache-Control': 'private', 'Content-Type': 'text/html; charset=utf-8', 'Content-Encoding': 'gzip', 'Vary': 'Accept-Encoding,Accept-Encoding', 'Server': 'Microsoft-IIS/10.0', 'Set-Cookie': 'ASP.NET_SessionId=2tygffzylqwnw3spoavb4lbe; path=/; HttpOnly; SameSite=Lax', 'X-AspNet-Version': '4.0.30319', 'X-Powered-By': 'ASP.NET', 'X-Powered-By-Plesk': 'PleskWin', 'Date': 'Tue, 16 Aug 2022 06:57:55 GMT', 'Content-Length': '40682'}
</code></pre>
<p>As can be seen, when I use python requests, the response header is including <code>Set-Cookie</code>, but, when I used urllib3, the response header does not have <code>Set-Cookie</code>.</p>
<p>What is the problem? I need the <code>Set-Cookie</code> varible in the <code>urllib3</code> response header.</p>
",0,1660634261,python;python-requests;get;urllib3,False,272,0,1660634261,https://stackoverflow.com/questions/73370126/how-to-clear-cache-of-urllib3-poolmanager
73254104,Twilio - Phone Lookup requests.exceptions.SSLError,"<p>I'm using Twilio's <a href=""https://www.twilio.com/docs/lookup/v2-api"" rel=""nofollow noreferrer"">phone number lookup API</a> (<code>client.lookups.v2.phone_numbers</code>) for Python. I will <em>only</em> be using this on my local machine to validate phone numbers.</p>
<p>Here's my code:</p>
<pre><code>from twilio.rest import Client

account_sid = &quot;••••••••••••••••••••••••••&quot;
auth_token = &quot;••••••••••••••••••••••••••&quot;

client = Client(account_sid, auth_token)

phone_number = client.lookups.v2.phone_numbers(&quot;+14159929960&quot;).fetch()
print(phone_number)
</code></pre>
<p>I put the full error stack at the bottom because it's a huge wall of text.</p>
<p>In the <a href=""https://support.twilio.com/hc/en-us/articles/360007853433-Troubleshooting-Certificate-Errors-from-the-Twilio-REST-API"" rel=""nofollow noreferrer"">Troubleshooting Certificate Errors from the Twilio REST API</a> page, they suggest that I:</p>
<blockquote>
<p>download the latest Mozilla-provided CA certificate bundle in PEM format from curl's website, which already includes our new root certificate. The specific certificate required from the bundle is DigiCert Global Root CA, which is directly available for download in CRT format from DigiCert.</p>
</blockquote>
<p>so I downloaded and installed the <a href=""https://curl.haxx.se/docs/caextract.html"" rel=""nofollow noreferrer"">CA certificate bundle</a> as well as the <a href=""https://www.digicert.com/digicert-root-certificates.htm"" rel=""nofollow noreferrer"">DigiCert Global Root CA</a>, but it threw the same errors.</p>
<p>I'm pretty sure that the problem exists in <code>python3.9/site-packages/OpenSSL/SSL.py</code> with <code>result = _lib.SSL_do_handshake(self._ssl)</code>. I looked around for a few hours and I could <em>not</em> find the <code>SSL_do_handshake</code> def to see where the holdup is.</p>
<ul>
<li><a href=""https://stackoverflow.com/a/31297747/7668146"">This answer</a> didn't work on my macbook, I just couldn't import the files into my keychain.</li>
<li>I would like to try <a href=""https://stackoverflow.com/q/36600583/7668146"">this</a> solution, but I don't know where I would put <code>ssl.CERT_NONE</code>. I believe Twilio uses <code>urllib3</code>, but I can't find a place to put it.</li>
<li>This answer involves using <code>requests</code>, which I am not using.</li>
<li>Most answers are addressing issues with ```requests``, so there isn't much that helps there.</li>
<li>Looking at <a href=""https://stackoverflow.com/questions/22027418/openssl-python-requests-error-certificate-verify-failed"">this</a> question inspired me to try this bash script:</li>
</ul>
<pre><code>curl -X GET 'https://lookups.twilio.com/v2/PhoneNumbers/+14159929960' \
-u $TWILIO_ACCOUNT_SID:$TWILIO_AUTH_TOKEN
</code></pre>
<p>And it worked! <strong>So this seems to be a Python issue and not a certificate issue.</strong></p>
<pre><code>Traceback (most recent call last):
  File &quot;/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/urllib3/contrib/pyopenssl.py&quot;, line 437, in wrap_socket
    cnx.do_handshake()
  File &quot;/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/OpenSSL/SSL.py&quot;, line 1835, in do_handshake
    self._raise_ssl_error(self._ssl, result)
  File &quot;/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/OpenSSL/SSL.py&quot;, line 1570, in _raise_ssl_error
    _raise_current_error()
  File &quot;/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/OpenSSL/_util.py&quot;, line 51, in exception_from_error_queue
    raise exception_type(errors)
OpenSSL.SSL.Error: [('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/urllib3/connectionpool.py&quot;, line 597, in urlopen
    httplib_response = self._make_request(conn, method, url,
  File &quot;/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/urllib3/connectionpool.py&quot;, line 343, in _make_request
    self._validate_conn(conn)
  File &quot;/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/urllib3/connectionpool.py&quot;, line 839, in _validate_conn
    conn.connect()
  File &quot;/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/urllib3/connection.py&quot;, line 337, in connect
    self.sock = ssl_wrap_socket(
  File &quot;/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/urllib3/util/ssl_.py&quot;, line 345, in ssl_wrap_socket
    return context.wrap_socket(sock, server_hostname=server_hostname)
  File &quot;/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/urllib3/contrib/pyopenssl.py&quot;, line 443, in wrap_socket
    raise ssl.SSLError(&quot;bad handshake: %r&quot; % e)
ssl.SSLError: (&quot;bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')])&quot;,)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/requests/adapters.py&quot;, line 439, in send
    resp = conn.urlopen(
  File &quot;/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/urllib3/connectionpool.py&quot;, line 637, in urlopen
    retries = retries.increment(method, url, error=e, _pool=self,
  File &quot;/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/urllib3/util/retry.py&quot;, line 399, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='lookups.twilio.com', port=443): Max retries exceeded with url: /v2/PhoneNumbers/+14159929960 (Caused by SSLError(SSLError(&quot;bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')])&quot;)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/path/to/twiliophonelookup.py&quot;, line 16, in &lt;module&gt;
    phone_number = client.lookups.v2.phone_numbers(&quot;+14159929960&quot;).fetch()
  File &quot;/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/twilio/rest/lookups/v2/phone_number.py&quot;, line 153, in fetch
    payload = self._version.fetch(
  File &quot;/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/twilio/base/version.py&quot;, line 72, in fetch
    response = self.request(
  File &quot;/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/twilio/base/version.py&quot;, line 39, in request
    return self.domain.request(
  File &quot;/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/twilio/base/domain.py&quot;, line 38, in request
    return self.twilio.request(
  File &quot;/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/twilio/rest/__init__.py&quot;, line 142, in request
    return self.http_client.request(
  File &quot;/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/twilio/http/http_client.py&quot;, line 89, in request
    response = session.send(prepped_request, **settings)
  File &quot;/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/requests/sessions.py&quot;, line 646, in send
    r = adapter.send(request, **kwargs)
  File &quot;/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/requests/adapters.py&quot;, line 514, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='lookups.twilio.com', port=443): Max retries exceeded with url: /v2/PhoneNumbers/+14159929960 (Caused by SSLError(SSLError(&quot;bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')])&quot;)))
</code></pre>
",1,1659724973,python;ssl;ssl-certificate;twilio;urllib3,True,330,1,1660248293,https://stackoverflow.com/questions/73254104/twilio-phone-lookup-requests-exceptions-sslerror
34837026,What&#39;s the meaning of pool_connections in requests.adapters.HTTPAdapter?,"<p>When initializing a requests' <code>Session</code>, two <a href=""http://docs.python-requests.org/en/latest/api/#requests.adapters.HTTPAdapter"" rel=""noreferrer""><code>HTTPAdapter</code></a> will be created and <a href=""https://github.com/kennethreitz/requests/blob/master/requests/sessions.py#L340-L341"" rel=""noreferrer"">mount to <code>http</code> and <code>https</code></a>.</p>

<p>This is how <code>HTTPAdapter</code> is defined:</p>

<pre><code>class requests.adapters.HTTPAdapter(pool_connections=10, pool_maxsize=10,
                                    max_retries=0, pool_block=False)
</code></pre>

<p>While I understand the meaning of <code>pool_maxsize</code>(which is the number of session a pool can save), I don't understand what <code>pool_connections</code> means or what it does. Doc says:</p>

<pre><code>Parameters: 
pool_connections – The number of urllib3 connection pools to cache.
</code></pre>

<p>But what does it mean ""to cache""? And what's the point using multiple connection pools?</p>
",33,1453024249,python;python-requests;urllib3,True,25097,4,1659367536,https://stackoverflow.com/questions/34837026/whats-the-meaning-of-pool-connections-in-requests-adapters-httpadapter
73145785,urllib.error.HTTPError: HTTP Error 403: Forbidden undetected-chromedriver,"<p>When I try to use Selenium, it gives an error in the first lines of the code</p>
<p>code:</p>
<pre><code>driverLocation = &quot;/usr/bin/chromedriver&quot;
binaryLocation = &quot;/usr/bin/google-chrome-stable&quot;

chrome_options = Options()
dir = r&quot;/home/ubuntu/.config/google-chrome/Default&quot;
chrome_options.add_argument(f&quot;--user-data-dir={dir}&quot;)
chrome_options.binary_location = binaryLocation
chrome_options.add_argument('--headless')


driver = UC.Chrome(executable_path=driverLocation, options=chrome_options)
</code></pre>
<p>error:</p>
<pre><code>&gt; Traceback (most recent call last):
  File &quot;main.py&quot;, line 34, in &lt;module&gt;
    driver = UC.Chrome(executable_path=driverLocation, options=chrome_options)
  File &quot;/home/ubuntu/.local/lib/python3.8/site-packages/undetected_chromedriver/__init__.py&quot;, line 233, in __init__
    patcher.auto()
  File &quot;/home/ubuntu/.local/lib/python3.8/site-packages/undetected_chromedriver/patcher.py&quot;, line 130, in auto
    self.unzip_package(self.fetch_package())
  File &quot;/home/ubuntu/.local/lib/python3.8/site-packages/undetected_chromedriver/patcher.py&quot;, line 166, in fetch_package
    return urlretrieve(u)[0]
  File &quot;/usr/lib/python3.8/urllib/request.py&quot;, line 247, in urlretrieve  
     with contextlib.closing(urlopen(url, data)) as fp:
  File &quot;/usr/lib/python3.8/urllib/request.py&quot;, line 222, in urlopen
    return opener.open(url, data, timeout)
  File &quot;/usr/lib/python3.8/urllib/request.py&quot;, line 531, in open
    response = meth(req, response)
  File &quot;/usr/lib/python3.8/urllib/request.py&quot;, line 640, in http_response
    response = self.parent.error(
  File &quot;/usr/lib/python3.8/urllib/request.py&quot;, line 569, in error
    return self._call_chain(*args)
  File &quot;/usr/lib/python3.8/urllib/request.py&quot;, line 502, in _call_chain
    result = func(*args)
  File &quot;/usr/lib/python3.8/urllib/request.py&quot;, line 649, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
</code></pre>
<p>And when I use Selenium web driver, it doesn't give this error (it only happens when I use undetected-chromedriver)</p>
<p>I run this code on Ubuntu 20.04 server with Chrome 103<br />
And some of my servers do not give this error<br />
And I set up all the servers in the same way</p>
",0,1658966073,python;selenium;python-requests;urllib3;undetected-chromedriver,False,791,1,1658967412,https://stackoverflow.com/questions/73145785/urllib-error-httperror-http-error-403-forbidden-undetected-chromedriver
73138593,Module Not found on some runs of python script woocommerce api to get orders,"<p>I currently have a program that I wrote to work with the woocommerce API to get orders daily from our websites. The program runs(most of the time) without issue. However, randomly, I will get errors like the image attached. I have the proper packages to make the program work but it gets hung up and fails around 5% of the time. This is a big issue as I am going to automate the process. Does anyone have any idea why this error would only happen a small amount of the time? See code below for context. Thank you.</p>
<pre><code>import csv
import time
from datetime import datetime, timedelta, date
import os
from woocommerce import API

# Define woocommerce api credentials
wcapi = API(
    url=&quot;https://mysite/&quot;,
    consumer_key=&quot;ck_123456789&quot;,  # Your consumer key
    consumer_secret=&quot;cs_secret&quot;,  # Your consumer secret
    wp_api=True,  # Enable the WP REST API integration
    version=&quot;wc/v3&quot;)

# set a time query for orders over the passed 24 hours
yesterday = datetime.now() - timedelta(hours=24)
today = date.today()
timeNow = datetime.now()
current_time = timeNow.strftime(&quot;%H:%M:%S&quot;)
file_date = datetime.now().strftime(&quot;%m-%d-%Y-%H%M&quot;)

r = wcapi.get(f&quot;orders?after={yesterday}&quot;)

meta = []
# If the get request returns a valid status
# get email error reporting set up if file sets up
if r.status_code in range(200, 299):

    time.sleep(1)

    # Fill the empty list with the json from the get request
    orders = r.json()
    # set the path that you want to save the file
    dir_path = &quot;I:\store\orders&quot;
    # set the name of the file with todays date
    file_name = f&quot;store_orders-{file_date}.csv&quot;
    # create the full file path
    file_path = os.path.join(dir_path, file_name)

    with open(file_path, 'w', newline='', encoding='utf_8') as csv_file:

        writer = csv.writer(csv_file, delimiter=',')
        # delcare all of the column headers in the csv file

        header = ['Order ID', 'Order Number', 'Order Created at', 'Status', 'Currency', 'Total', 'Total Shipping',
                  'Total Discount',
                  'Shipping Tax', 'Total Tax', 'Total Items', 'Cart Tax', 'Discount Tax', 'Total Fees', 'Total Refunds',
                  'Net',
                  'Billing Fist Name', 'Billing Last Name', 'Billing Company', 'Billing address1', 'Billing address2',
                  'Billing city', 'Billing state', 'Billing postcode', 'Billing Country', 'Email', 'Phone',
                  'Shipping Fist Name', 'Shipping Last Name', 'Shipping Company', 'Shipping address1',
                  'Shipping address2',
                  'Shipping city', 'Shipping state', 'Shipping postcode', 'Shipping Country', 'Order updated at',
                  'order completed at', 'order paid at', 'payment method', 'payment method title',
                  'shipping method title',
                  'Transaction ID', 'Order key', 'customer IP', 'Customer browser', 'Customer note', 'customer id',
                  'customer role', 'created via', 'referring site', 'coupon code', 'coupon amount', 'fees', 'cogs',
                  'gross profit', 'line item id', 'line item product id', 'line item variation id', 'line item name',
                  'line item sku', 'line itme qty', 'line item meta', 'line item subtotal', 'line item subtotal tax',
                  'line item total', 'line item total tax', 'line item price', 'line item cogs']
        # write headers to csv file
        writer.writerow(header)

        for order in orders:

            # create dictionary reference variables for nested dictionaries to clean up keys below
            shipping = order['shipping']
            billing = order['billing']
            shipping_lines = order['shipping_lines']

            # this for look goes through the list of shipping_line items for shipping information and matches the pairs
            # in dictionary format to be accessed below
            for ship_info in shipping_lines:
                pass

            for line_item in order['line_items']:
                # create the new row of data to send to csv
                row = []

                # create a dictionary out of a list of meta data and assign the meta data list to the meta var
                mt = line_item['meta_data']
                for m in mt:
                    if m['key'] == 'vpc-cart-data':
                        meta = m
                #build csv
                row.append(order['id'])
                row.append('#'+ order['number'])
                row.append(order['date_created'])
                row.append(order['status'])
                row.append(order['currency'])
                row.append(order['total'])
                row.append(shipping_lines[0]['total'])
                row.append(order['discount_total'])
                row.append(order['shipping_tax'])
                row.append(order['total_tax'])
                row.append(line_item['quantity'])
                row.append(order['cart_tax'])
                row.append(order['discount_tax'])
                row.append(order['fee_lines'])
                row.append(order['refunds'])
                row.append(order['total'])
                row.append(billing['first_name'])
                row.append(billing['last_name'])
                row.append(billing['company'])
                row.append(billing['address_1'])
                row.append(billing['address_2'])
                row.append(billing['city'])
                row.append(billing['state'])
                row.append(billing['postcode'])
                row.append(billing['country'])
                row.append(billing['email'])
                row.append(billing['phone'])
                row.append(shipping['first_name'])
                row.append(shipping['last_name'])
                row.append(shipping['company'])
                row.append(shipping['address_1'])
                row.append(shipping['address_2'])
                row.append(shipping['city'])
                row.append(shipping['state'])
                row.append(shipping['postcode'])
                row.append(shipping['country'])
                row.append(order['date_modified'])
                row.append(order['date_completed'])
                row.append(order['date_paid'])
                row.append(order['payment_method'])
                row.append(order['payment_method_title'])
                row.append(order['payment_method_title'])
                row.append(order['transaction_id'])
                row.append(order['order_key'])
                row.append(order['customer_ip_address'])
                row.append(order['customer_user_agent'])
                row.append(order['customer_note'])
                row.append(order['customer_id'])
                row.append(order['customer_id'])
                row.append(order['created_via'])
                row.append(order['payment_url'])
                row.append(order['discount_total'])
                row.append(order['discount_total'])
                row.append(order['customer_note'])
                row.append(order['discount_tax'])
                row.append(order['discount_tax'])
                row.append(line_item['id'])
                row.append(line_item['product_id'])
                row.append(line_item['variation_id'])
                row.append(line_item['name'])
                row.append(line_item['sku'])
                row.append(line_item['quantity'])
                row.append(line_item['parent_name'])
                row.append(line_item['subtotal'])
                row.append(line_item['subtotal_tax'])
                row.append(line_item['total'])
                row.append(line_item['total_tax'])
                row.append(line_item['price'])
                row.append(line_item['price'])

                # write each row to the csv file
                writer.writerow(row)

                # clear accumulated meta data after row is written
                meta.clear()

                csv_file.close()
</code></pre>
",0,1658927497,python;api;woocommerce;pip;urllib3,False,81,0,1658927497,https://stackoverflow.com/questions/73138593/module-not-found-on-some-runs-of-python-script-woocommerce-api-to-get-orders
73080122,Image file type is not supported when downloading using urllib3,"<p>hi I am downloading image like this</p>
<pre><code>import urllib3     

http = urllib3.PoolManager()
r = http.request('GET', 'https://i.picsum.photos/id/192/536/354.png?hmac=a22QkdSZ7zXUHpV4-gnB48PPYaLlcvaTMeDXxcPRxs8')
print(r.data)
</code></pre>
<p>then uploading it to s3 using this</p>
<pre><code> s3 = boto3.resource(s3')
    key = 'file_name + '.png'    
    bucket = s3.Bucket(bucket_name)
    bucket.upload_fileobj(io.BytesIO(r.data), key) 
</code></pre>
<p>but when I open I get error on image opening &quot;File type is not supported &quot;  when I open using photo opener</p>
<pre><code>**EDIT:** I did as suggested by passing
 ContentType='text/png' 
</code></pre>
<p>and when I opening image by url getting this on aws : I opened this using presinged url</p>
<p><a href=""https://i.stack.imgur.com/0bvmr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0bvmr.png"" alt=""enter image description here"" /></a></p>
",0,1658491694,python;python-3.x;boto3;urllib3,True,434,1,1658888790,https://stackoverflow.com/questions/73080122/image-file-type-is-not-supported-when-downloading-using-urllib3
50823753,concurrent connections in urllib3,"<p>Using a loop to make multiple requests to various websites, how is it possible to do this with a proxy in urllib3?</p>

<p>The code will read in a tuple of URLs, and use a for loop to connect to each site, however, currently it does not connect past the first url in the tuple. There is a proxy in place as well.</p>

<pre><code>list = ['https://URL1.com', 'http://URL2.com', 'http://URL3.com']
for i in list:
    http = ProxyManager(""PROXY-PROXY"")
    http_get = http.request('GET', i, preload_content=False).read().decode()
</code></pre>

<p>I have removed the urls and proxy information from the above code. The first URL in the tuple will run fine, but after this, nothing else occurs, just waiting. I have tried the <code>clear()</code> method to reset the connection for each time in the loop.</p>
",-1,1528828757,python;python-3.x;urllib;urllib3,False,3898,2,1656519803,https://stackoverflow.com/questions/50823753/concurrent-connections-in-urllib3
72783674,Python: urllib3 errors where requests works,"<p>I have the following code</p>
<pre><code>http = urllib3.PoolManager()
resp = http.request(
    &quot;GET&quot;,
    &quot;https://api.nhs.uk/medicines/aciclovir&quot;,
    headers={
        &quot;subscription-key&quot;:&quot;XXX&quot;
    }
)
print(resp.data)
</code></pre>
<p>The response is:</p>
<pre><code>You don't have permission to access &quot;http://api.nhs.uk/medicines/aciclovir&quot; on this server
</code></pre>
<p>Why has it gone to a <code>http://</code> address?</p>
<p>However when I use requests as below, it's a success</p>
<pre><code>url = &quot;https://api.nhs.uk/medicines/aciclovir&quot;

payload={}
headers = {
  'subscription-key': 'XXX'
}

response = requests.request(&quot;GET&quot;, url, headers=headers, data=payload)

print(response.text)
</code></pre>
<p>Any suggestions?</p>
<p><strong>UPDATE 1</strong></p>
<p>This is the exact code that is being executed for the error to happen which yes, it does show a <code>http://</code> error</p>
<p><a href=""https://i.stack.imgur.com/x79g9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/x79g9.png"" alt=""enter image description here"" /></a></p>
<p><strong>UPDATE 2</strong></p>
<p><strong>urllib3</strong></p>
<pre><code>Listening on 0.0.0.0 7777
Connection received on *** 50054
GET / HTTP/1.1
Host: ***:7777
Accept-Encoding: identity
subscription-key: xxx
User-Agent: python-urllib3/1.26.9
</code></pre>
<p><strong>requests</strong></p>
<pre><code>Listening on 0.0.0.0 7777
Connection received on *** 50095
GET / HTTP/1.1
Host: ***:7777
User-Agent: python-requests/2.28.0
Accept-Encoding: gzip, deflate
Accept: */*
Connection: keep-alive
subscription-key: xxx
</code></pre>
",0,1656407332,python;python-3.x;python-requests;urllib3,False,840,1,1656409114,https://stackoverflow.com/questions/72783674/python-urllib3-errors-where-requests-works
72629471,Python requests error - SSL UNSAFE_LEGACY_RENEGOTIATION_DISABLED,"<p>I am running Python 3.10 in Ubuntu 22.04 and I am getting an error when I try to access a https server using requests.get.</p>
<p>Things I have varied, with no luck</p>
<ul>
<li><a href=""https://www.openssl.org/"" rel=""nofollow noreferrer"">OpenSSL</a> versions: 3.0.0 and 1.1.1l</li>
<li>Different Python <code>requests</code> package versions</li>
<li>Different Python <code>cryptography</code> package versions</li>
<li>Different Python <code>urllib3</code> package versions</li>
</ul>
<p>The most striking thing is that the Python code <strong>works perfectly well</strong> if I access all other https servers I tested, apart from <code>&quot;https://nomads.ncep.noaa.gov/&quot;</code>. For example, <code>requests.get(&quot;https://github.com&quot;)</code> returns a 200 status code.</p>
<p>Do you have any idea of what could be happening here? Could it be due to a black-listed IP?</p>
<p><strong>Python call</strong></p>
<pre><code>import requests
r = requests.get(&quot;https://nomads.ncep.noaa.gov/&quot;)
</code></pre>
<p><strong>Error</strong></p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/ubuntu/Desktop/test/venv/lib/python3.10/site-packages/urllib3/connectionpool.py&quot;, line 703, in urlopen
    httplib_response = self._make_request(
  File &quot;/home/ubuntu/Desktop/test/venv/lib/python3.10/site-packages/urllib3/connectionpool.py&quot;, line 386, in _make_request
    self._validate_conn(conn)
  File &quot;/home/ubuntu/Desktop/test/venv/lib/python3.10/site-packages/urllib3/connectionpool.py&quot;, line 1040, in _validate_conn
    conn.connect()
  File &quot;/home/ubuntu/Desktop/test/venv/lib/python3.10/site-packages/urllib3/connection.py&quot;, line 414, in connect
    self.sock = ssl_wrap_socket(
  File &quot;/home/ubuntu/Desktop/test/venv/lib/python3.10/site-packages/urllib3/util/ssl_.py&quot;, line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
  File &quot;/home/ubuntu/Desktop/test/venv/lib/python3.10/site-packages/urllib3/util/ssl_.py&quot;, line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File &quot;/usr/lib/python3.10/ssl.py&quot;, line 512, in wrap_socket
    return self.sslsocket_class._create(
  File &quot;/usr/lib/python3.10/ssl.py&quot;, line 1070, in _create
    self.do_handshake()
  File &quot;/usr/lib/python3.10/ssl.py&quot;, line 1341, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:997)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/home/ubuntu/Desktop/test/venv/lib/python3.10/site-packages/requests/adapters.py&quot;, line 489, in send
    resp = conn.urlopen(
  File &quot;/home/ubuntu/Desktop/test/venv/lib/python3.10/site-packages/urllib3/connectionpool.py&quot;, line 785, in urlopen
    retries = retries.increment(
  File &quot;/home/ubuntu/Desktop/test/venv/lib/python3.10/site-packages/urllib3/util/retry.py&quot;, line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nomads.ncep.noaa.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:997)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/home/ubuntu/Desktop/test/venv/lib/python3.10/site-packages/requests/api.py&quot;, line 73, in get
    return request(&quot;get&quot;, url, params=params, **kwargs)
  File &quot;/home/ubuntu/Desktop/test/venv/lib/python3.10/site-packages/requests/api.py&quot;, line 59, in request
    return session.request(method=method, url=url, **kwargs)
  File &quot;/home/ubuntu/Desktop/test/venv/lib/python3.10/site-packages/requests/sessions.py&quot;, line 587, in request
    resp = self.send(prep, **send_kwargs)
  File &quot;/home/ubuntu/Desktop/test/venv/lib/python3.10/site-packages/requests/sessions.py&quot;, line 701, in send
    r = adapter.send(request, **kwargs)
  File &quot;/home/ubuntu/Desktop/test/venv/lib/python3.10/site-packages/requests/adapters.py&quot;, line 563, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='nomads.ncep.noaa.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:997)')))
</code></pre>
",1,1655287595,python;ssl;https;openssl;urllib3,False,213,0,1655287595,https://stackoverflow.com/questions/72629471/python-requests-error-ssl-unsafe-legacy-renegotiation-disabled
72534059,cannot resolve urllib3 version issue,"<p>I am trying to use Adobe's pdfservices-sdk and am getting the following error</p>
<pre><code>ERROR: pip's dependency resolver does not currently take into account all the packages 
that are installed. This behaviour is the source of the following dependency conflicts.
pdfservices-extract-sdk 1.0.0b1 requires urllib3==1.26.3, but you have urllib3 1.26.8 
which is incompatible.
Successfully installed urllib3-1.26.8
</code></pre>
<p>When I instead install version 1.26.3, it says that it is incompatible and requires 1.26.8.</p>
<p>I keep going back and forth, it is not working. Any thoughts? Thanks!</p>
",1,1654616375,python;urllib3;adobe-pdfservices,True,2812,2,1655010246,https://stackoverflow.com/questions/72534059/cannot-resolve-urllib3-version-issue
72486680,pass parameters to a string json within a function,"<p>If I run a post request like this, it works:</p>
<pre><code>def myFunction():
  contentTypeHeader = {
    'Content-type': 'application/json',
  }
 
  data = &quot;&quot;&quot;
  {
  &quot;target&quot;: {
    &quot;ref_type&quot;: &quot;branch&quot;,
    &quot;type&quot;: &quot;pipeline_ref_target&quot;,
    &quot;ref_name&quot;: &quot;master&quot;,
  &quot;selector&quot;: {
    &quot;type&quot;: &quot;custom&quot;,
    &quot;pattern&quot; : &quot;mypipeline&quot;
    }
  },
  &quot;variables&quot;: []
  }
  &quot;&quot;&quot;

  http = urllib3.PoolManager()
  headers = urllib3.make_headers(basic_auth='{}:{}'.format(&quot;username&quot;, &quot;password&quot;))
  headers = dict(list(contentTypeHeader .items()) + list(headers.items()))
  try:
    resp = http.urlopen('POST', 'https://api.bitbucket.org/2.0/repositories/owner/slugg/pipelines/', headers=headers, body=data)
    print('Response', str(resp.data))
  except Exception as e:
    print('Error', e)


myFunction()
</code></pre>
<p>However, if instead of hardcoding the values, I try to pass them on as a function:</p>
<pre><code>def myFunction(bitbucket_branch, pipeline_name):
  contentTypeHeader = {
    'Content-type': 'application/json',
  }
  
  data = &quot;&quot;&quot;
  {
  &quot;target&quot;: {
    &quot;ref_type&quot;: &quot;branch&quot;,
    &quot;type&quot;: &quot;pipeline_ref_target&quot;,
    &quot;ref_name&quot;: &quot;${bitbucket_branch}&quot;,
  &quot;selector&quot;: {
    &quot;type&quot;: &quot;custom&quot;,
    &quot;pattern&quot; : &quot;${pipeline_name}&quot;
    }
  },
  &quot;variables&quot;: []
  }
  &quot;&quot;&quot;
...


 myFunction(&quot;master&quot;,&quot;mypipeline&quot;)
</code></pre>
<p>I get this error:</p>
<pre><code>Response b'{&quot;error&quot;: {&quot;message&quot;: &quot;Not found&quot;, &quot;detail&quot;: &quot;Could not find last reference for branch ${bitbucket_branch}&quot;, &quot;data&quot;: {&quot;key&quot;: &quot;result-service.pipeline.reference-not-found&quot;, &quot;arguments&quot;: {&quot;uuid&quot;: &quot;${bitbucket_branch}&quot;}}}}'
</code></pre>
<p>Additionally, in my function :</p>
<pre><code>def myFunction(bitbucket_branch, pipeline_name):
</code></pre>
<p>the parameters are still in a light color in my VSCode, which indicates that the parameters aren't actually being used in the function.</p>
<p>Perhaps I am doing something wrong with encoding strings but can't figure out what exactly.</p>
",0,1654244305,python;python-3.x;curl;urllib;urllib3,True,645,1,1654244640,https://stackoverflow.com/questions/72486680/pass-parameters-to-a-string-json-within-a-function
71932852,exchangelib - No Body element in SOAP response,"<p>I am using exchangelib to connect to Exchange Server 2013 and reply to emails, But this week my company moved to another City.
All my Script was working fine before we move. Outlook working fine for me in the new location</p>
<p><strong>May be someone know how to resolve this probleme. Thanks a lot</strong></p>
<p><strong>My Python code is :</strong></p>
<pre><code>from exchangelib import Account, Credentials, Configuration, Message, DELEGATE, Mailbox, FileAttachment, HTMLBody, FolderCollection
from exchangelib.protocol import BaseProtocol, NoVerifyHTTPAdapter
from exchangelib.folders import Folder
import warnings

warnings.filterwarnings(&quot;ignore&quot;)
BaseProtocol.HTTP_ADAPTER_CLS = NoVerifyHTTPAdapter

credentials = Credentials(username='DOMAIN\\USERNAME', password='XXXXXXXXX')

config = Configuration(server='XXXXXXXX', credentials=credentials)
account = Account(primary_smtp_address='ADRESSE_EMAIL', config=config, autodiscover=False, access_type=DELEGATE)

for email in account.inbox.all().order_by('-datetime_received')[:3]:
    print(str(email.subject))
</code></pre>
<p><strong>When I trie to execute My script I get this errors :</strong></p>
<pre><code>Z:\AUTO_SCRIPTS&gt;TEST.py
Account None: Exception in _get_elements: Traceback (most recent call last):
  File &quot;C:\Users\AutoJob\AppData\Local\Programs\Python\Python37-32\lib\site-packages\exchangelib\services\common.py&quot;, line 330, in _get_elements
    yield from self._response_generator(payload=payload)
  File &quot;C:\Users\AutoJob\AppData\Local\Programs\Python\Python37-32\lib\site-packages\exchangelib\services\common.py&quot;, line 292, in _response_generator
    response = self._get_response_xml(payload=payload)
  File &quot;C:\Users\AutoJob\AppData\Local\Programs\Python\Python37-32\lib\site-packages\exchangelib\services\common.py&quot;, line 421, in _get_response_xml
    header, body = self._get_soap_parts(response=r, **parse_opts)
  File &quot;C:\Users\AutoJob\AppData\Local\Programs\Python\Python37-32\lib\site-packages\exchangelib\services\common.py&quot;, line 518, in _get_soap_parts
    raise MalformedResponseError(&quot;No Body element in SOAP response&quot;)
exchangelib.errors.MalformedResponseError: No Body element in SOAP response

Traceback (most recent call last):
  File &quot;Z:\AUTO_SCRIPTS\TEST.py&quot;, line 14, in &lt;module&gt;
    account = Account(primary_smtp_address='XXXXX_MYADRESSEEMAIL_XXXXXXX', config=config, autodiscover=False, access_type=DELEGATE)
  File &quot;C:\Users\AutoJob\AppData\Local\Programs\Python\Python37-32\lib\site-packages\exchangelib\account.py&quot;, line 204, in __init__
    self.version = self.protocol.version.copy()
  File &quot;C:\Users\AutoJob\AppData\Local\Programs\Python\Python37-32\lib\site-packages\exchangelib\protocol.py&quot;, line 492, in version
    self.config.version = Version.guess(self, api_version_hint=self._api_version_hint)
  File &quot;C:\Users\AutoJob\AppData\Local\Programs\Python\Python37-32\lib\site-packages\exchangelib\version.py&quot;, line 233, in guess
    list(ResolveNames(protocol=protocol).call(unresolved_entries=[name]))
  File &quot;C:\Users\AutoJob\AppData\Local\Programs\Python\Python37-32\lib\site-packages\exchangelib\services\common.py&quot;, line 252, in _elems_to_objs
    for elem in elems:
  File &quot;C:\Users\AutoJob\AppData\Local\Programs\Python\Python37-32\lib\site-packages\exchangelib\services\common.py&quot;, line 310, in _chunked_get_elements
    yield from self._get_elements(payload=payload_func(chunk, **kwargs))
  File &quot;C:\Users\AutoJob\AppData\Local\Programs\Python\Python37-32\lib\site-packages\exchangelib\services\common.py&quot;, line 330, in _get_elements
    yield from self._response_generator(payload=payload)
  File &quot;C:\Users\AutoJob\AppData\Local\Programs\Python\Python37-32\lib\site-packages\exchangelib\services\common.py&quot;, line 292, in _response_generator
    response = self._get_response_xml(payload=payload)
  File &quot;C:\Users\AutoJob\AppData\Local\Programs\Python\Python37-32\lib\site-packages\exchangelib\services\common.py&quot;, line 421, in _get_response_xml
    header, body = self._get_soap_parts(response=r, **parse_opts)
  File &quot;C:\Users\AutoJob\AppData\Local\Programs\Python\Python37-32\lib\site-packages\exchangelib\services\common.py&quot;, line 518, in _get_soap_parts
    raise MalformedResponseError(&quot;No Body element in SOAP response&quot;)
exchangelib.errors.MalformedResponseError: No Body element in SOAP response
</code></pre>
<p><strong>This is the output after I enable logging :</strong></p>
<pre><code>Z:\AUTO_SCRIPTS&gt;TEST.PY
WARNING:root:This will get logged to a file
DEBUG:exchangelib.protocol:Waiting for _protocol_cache_lock
DEBUG:exchangelib.protocol:Protocol __call__ cache miss. Adding key '('https://MyServer.CO/EWS/Exchange.asmx', Credentials('DOMAIN\\USERNAME', '********'))'
DEBUG:exchangelib.version:Asking server for version info using API version Exchange2019
DEBUG:exchangelib.services.common:Processing chunk 1 containing 1 items
DEBUG:exchangelib.services.common:Calling service ResolveNames
DEBUG:exchangelib.services.common:Trying API version Exchange2019
DEBUG:exchangelib.protocol:Server MyServer.CO: Increasing session pool size from 0 to 1
DEBUG:exchangelib.transport:Requesting b'&lt;?xml version=\'1.0\' encoding=\'utf-8\'?&gt;\n&lt;s:Envelope xmlns:m=&quot;http://schemas.microsoft.com/exchange/services/2006/messages&quot; xmlns:s=&quot;http://schemas.xmlsoap.org/soap/envelope/&quot; xmlns:t=&quot;http://schemas.microsoft.com/exchange/services/2006/types&quot;&gt;&lt;s:Header&gt;&lt;t:RequestServerVersion Version=&quot;Exchange2019&quot;/&gt;&lt;/s:Header&gt;&lt;s:Body&gt;&lt;m:ResolveNames ReturnFullContactData=&quot;false&quot;&gt;&lt;m:UnresolvedEntry&gt;DOMAIN\\USERNAME&lt;/m:UnresolvedEntry&gt;&lt;/m:ResolveNames&gt;&lt;/s:Body&gt;&lt;/s:Envelope&gt;' from https://MyServer.CO/EWS/Exchange.asmx
DEBUG:exchangelib.transport:Trying to get service auth type for https://MyServer.CO/EWS/Exchange.asmx
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): MyServer.CO:443
DEBUG:urllib3.connectionpool:https://MyServer.CO:443 &quot;POST /EWS/Exchange.asmx HTTP/1.1&quot; 401 0
DEBUG:exchangelib.transport:Request headers: {'User-Agent': 'exchangelib/4.7.2 (python-requests/2.21.0)', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive', 'Content-Type': 'text/xml; charset=utf-8', 'Content-Length': '460'}
DEBUG:exchangelib.transport:Response headers: {'request-id': '8bafd18d-0fe8-46be-b06f-56da0d19f49e', 'X-OWA-Version': '15.1.2176.14', 'WWW-Authenticate': 'Negotiate, NTLM', 'X-FEServer': 'DCSMSG02', 'Date': 'Tue, 19 Apr 2022 23:12:48 GMT', 'Content-Length': '0', 'Set-Cookie': 'TS010bba1d=01c63b1ddfb6be746dd7856a2fb8b268acf7e960c087d44fd6a6167e4a122947e9a18669aaa1727cad6495b9fb5b9338897d30a80e; Path=/'}
DEBUG:exchangelib.transport:Auth type is NTLM
DEBUG:exchangelib.protocol:Server MyServer.CO: Created session 29156
DEBUG:exchangelib.protocol:Server MyServer.CO: Waiting for session
DEBUG:exchangelib.protocol:Server MyServer.CO: Got session 29156
DEBUG:exchangelib.util:Session 29156 thread 6584: retry 0 timeout 120 POST'ing to https://MyServer.CO/EWS/Exchange.asmx after 10s wait
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): MyServer.CO:443
DEBUG:urllib3.connectionpool:https://MyServer.CO:443 &quot;POST /EWS/Exchange.asmx HTTP/1.1&quot; 401 0
DEBUG:urllib3.connectionpool:https://MyServer.CO:443 &quot;POST /EWS/Exchange.asmx HTTP/1.1&quot; 401 0
DEBUG:urllib3.connectionpool:https://MyServer.CO:443 &quot;POST /EWS/Exchange.asmx HTTP/1.1&quot; 500 None
DEBUG:exchangelib.util:Retry: 0
Waited: 10
Timeout: 120
Session: 29156
Thread: 6584
Auth type: &lt;requests_ntlm.requests_ntlm.HttpNtlmAuth object at 0x041EDFD0&gt;
URL: https://MyServer.CO/EWS/Exchange.asmx
HTTP adapter: &lt;exchangelib.protocol.NoVerifyHTTPAdapter object at 0x041EDF70&gt;
Allow redirects: False
Streaming: False
Response time: 0.15700000000651926
Status code: 500
Request headers: {'User-Agent': 'exchangelib/4.7.2 (python-requests/2.21.0)', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'Keep-Alive', 'Content-Type': 'text/xml; charset=utf-8', 'Content-Length': '460', 'Authorization': 'NTLM TlRMTVNTUAADAAAAGAAYAGgAAAD2APYAgAAAAAcABwBYAAAACQAJAF8AAAAAAAAAaAAAABAAEAB2AQAANoKJ4gYBsR0AAAAPyNMB8KYPBxfU0wqCe2C4k0JBUklEQURlZGl0aXF1ZW4AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABR8LOgphZIm/EAeDYtJE50AQEAAAAAAACmmnv8QlTYAVM52uSDOhWSAAAAAAIADgBCAEEAUgBJAEQAQQBEAAEAEABEAEMAUwBNAFMARwAwADIABAAiAGIAYQByAGkAZABhAGQALgBiAGEAbQBuAGUAdAAuAG0AYQADADQARABDAFMATQBTAEcAMAAyAC4AYgBhAHIAaQBkAGEAZAAuAGIAYQBtAG4AZQB0AC4AbQBhAAUAEgBiAGEAbQBuAGUAdAAuAG0AYQAHAAgAppp7/EJU2AEGAAQAAgAAAAoAEACeNHLkFPXbs0xQaxoIbUbvAAAAAAAAAAA9VKKIFFe6RMuugXH+iNwp', 'Cookie': 'TS010bba1d=01c63b1ddf2aa69892928ea66f2646b2e49bf49a9833e65a8fb482f25bddf77c5a77f13c5229273fc883601143b57fecd64088296c; Path=/'}
Response headers: {'Cache-Control': 'private', 'Content-Type': 'text/xml; charset=utf-8', 'request-id': 'c1730b03-7fd2-4719-8566-bafabc9675e9', 'X-CalculatedBETarget': 'dcsmsg02.DOMAIN.co', 'X-DiagInfo': 'DCSMSG02', 'X-BEServer': 'DCSMSG02', 'Set-Cookie': 'exchangecookie=82163548ce1145e5bb345d3641a2f2fe; expires=Wed, 19-Apr-2023 23:12:48 GMT; path=/; HttpOnly, X-BackEndCookie=S-1-5-21-1454471165-1292428093-839522115-21152742=u56Lnp2ejJqByMzPyc+cnprSzsvNntLLz83O0sfPysfSxpzOyZnHzZnMz53IgYHNz83N0s/K0s7Gq83Mxc7NxcvHgZ2ekpGai9GSnoHP; expires=Thu, 19-May-2022 23:12:48 GMT; path=/EWS; secure; HttpOnly, TS010bba1d=01c63b1ddf2aa69892928ea66f2646b2e49bf49a9833e65a8fb482f25bddf77c5a77f13c5229273fc883601143b57fecd64088296c; Path=/, TS01aa8c4e=01c63b1ddf2aa69892928ea66f2646b2e49bf49a9833e65a8fb482f25bddf77c5a77f13c5229273fc883601143b57fecd64088296c; path=/EWS', 'Persistent-Auth': 'true', 'X-FEServer': 'DCSMSG02', 'Date': 'Tue, 19 Apr 2022 23:12:48 GMT', 'Transfer-Encoding': 'chunked'}
DEBUG:exchangelib.util.xml:Request XML: &lt;?xml version='1.0' encoding='utf-8'?&gt;
&lt;s:Envelope
    xmlns:m=&quot;http://schemas.microsoft.com/exchange/services/2006/messages&quot;
    xmlns:s=&quot;http://schemas.xmlsoap.org/soap/envelope/&quot;
    xmlns:t=&quot;http://schemas.microsoft.com/exchange/services/2006/types&quot;&gt;
  &lt;s:Header&gt;
    &lt;t:RequestServerVersion Version=&quot;Exchange2019&quot;/&gt;
  &lt;/s:Header&gt;
  &lt;s:Body&gt;
    &lt;m:ResolveNames ReturnFullContactData=&quot;false&quot;&gt;
      &lt;m:UnresolvedEntry&gt;DOMAIN\USERNAME&lt;/m:UnresolvedEntry&gt;
    &lt;/m:ResolveNames&gt;
  &lt;/s:Body&gt;
&lt;/s:Envelope&gt;
Response XML: &lt;?xml version='1.0' encoding='utf-8'?&gt;
&lt;s:Envelope
    xmlns:s=&quot;https://schemas.xmlsoap.org/soap/envelope/&quot;&gt;
  &lt;s:Body&gt;
    &lt;s:Fault&gt;
      &lt;faultcode
    xmlns:a=&quot;https://schemas.microsoft.com/exchange/services/2006/types&quot;&gt;a:ErrorInvalidRequest&lt;/faultcode&gt;
      &lt;faultstring xml:lang=&quot;en-US&quot;&gt;The request is invalid.&lt;/faultstring&gt;
      &lt;detail&gt;
        &lt;e:ResponseCode
    xmlns:e=&quot;https://schemas.microsoft.com/exchange/services/2006/errors&quot;&gt;ErrorInvalidRequest&lt;/e:ResponseCode&gt;
        &lt;e:Message
    xmlns:e=&quot;https://schemas.microsoft.com/exchange/services/2006/errors&quot;&gt;The request is invalid.&lt;/e:Message&gt;
      &lt;/detail&gt;
    &lt;/s:Fault&gt;
  &lt;/s:Body&gt;
&lt;/s:Envelope&gt;
DEBUG:exchangelib.protocol:No retry: no fail-fast policy
DEBUG:exchangelib.util:Got status code 500 but trying to parse content anyway
DEBUG:exchangelib.util:Session 29156 thread 6584: Useful response from https://MyServer.CO/EWS/Exchange.asmx
DEBUG:exchangelib.protocol:Server MyServer.CO: Releasing session 29156
DEBUG:exchangelib.services.common:No header in XML response
WARNING:exchangelib.services.common:Account None: Exception in _get_elements: Traceback (most recent call last):
  File &quot;C:\Users\AutoJob\AppData\Local\Programs\Python\Python37-32\lib\site-packages\exchangelib\services\common.py&quot;, line 330, in _get_elements
    yield from self._response_generator(payload=payload)
  File &quot;C:\Users\AutoJob\AppData\Local\Programs\Python\Python37-32\lib\site-packages\exchangelib\services\common.py&quot;, line 292, in _response_generator
    response = self._get_response_xml(payload=payload)
  File &quot;C:\Users\AutoJob\AppData\Local\Programs\Python\Python37-32\lib\site-packages\exchangelib\services\common.py&quot;, line 421, in _get_response_xml
    header, body = self._get_soap_parts(response=r, **parse_opts)
  File &quot;C:\Users\AutoJob\AppData\Local\Programs\Python\Python37-32\lib\site-packages\exchangelib\services\common.py&quot;, line 518, in _get_soap_parts
    raise MalformedResponseError(&quot;No Body element in SOAP response&quot;)
exchangelib.errors.MalformedResponseError: No Body element in SOAP response
</code></pre>
",0,1650415191,python;exchange-server;exchangewebservices;urllib3;exchangelib,False,828,2,1653978239,https://stackoverflow.com/questions/71932852/exchangelib-no-body-element-in-soap-response
72377625,Airflow/Buildkite ValueError: Invalid header value b&#39;Bearer xxxx\n&#39;,"<p>We are using Airflow to run some ML training jobs, and this creates a buildkite build that generates a GitHub PR to use a new ML model. It had been working before, but now we are getting a strange error:</p>
<p><code>ValueError: Invalid header value b'Bearer xxxx\n'</code></p>
<p>I replaced the bearer token/secret with xxxx, but it's a longer string of letters/numbers. We haven't changed anything that I think should be effecting this, but it looks like Airflow has released a new version in the past few days that may be the root cause here. We are doing something like this:</p>
<pre class=""lang-py prettyprint-override""><code>from pybuildkite.buildkite import Buildkite

buildkite = Buildkite()
buildkite.set_access_token(os.getenv(&quot;BUILDKITE_API_KEY&quot;))

resp = buildkite.builds().create_build(...)
</code></pre>
<p>We are using the latest versions of pybuildkite, requests, and urllib3. The full traceback looks like this:</p>
<pre class=""lang-py prettyprint-override""><code>[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -     resp = buildkite.builds().create_build(
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -   File &quot;/usr/local/lib/python3.8/site-packages/pybuildkite/builds.py&quot;, line 305, in create_build
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -     return self.client.post(
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -   File &quot;/usr/local/lib/python3.8/site-packages/pybuildkite/client.py&quot;, line 147, in post
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -     return self.request(
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -   File &quot;/usr/local/lib/python3.8/site-packages/pybuildkite/client.py&quot;, line 73, in request
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -     response = requests.request(
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -   File &quot;/usr/local/lib/python3.8/site-packages/requests/api.py&quot;, line 61, in request
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -     return session.request(method=method, url=url, **kwargs)
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -   File &quot;/usr/local/lib/python3.8/site-packages/requests/sessions.py&quot;, line 529, in request
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -     resp = self.send(prep, **send_kwargs)
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -   File &quot;/usr/local/lib/python3.8/site-packages/requests/sessions.py&quot;, line 645, in send
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -     r = adapter.send(request, **kwargs)
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -   File &quot;/usr/local/lib/python3.8/site-packages/requests/adapters.py&quot;, line 440, in send
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -     resp = conn.urlopen(
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -   File &quot;/usr/local/lib/python3.8/site-packages/urllib3/connectionpool.py&quot;, line 703, in urlopen
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -     httplib_response = self._make_request(
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -   File &quot;/usr/local/lib/python3.8/site-packages/urllib3/connectionpool.py&quot;, line 398, in _make_request
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -     conn.request(method, url, **httplib_request_kw)
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -   File &quot;/usr/local/lib/python3.8/site-packages/urllib3/connection.py&quot;, line 239, in request
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -     super(HTTPConnection, self).request(method, url, body=body, headers=headers)
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -   File &quot;/usr/local/lib/python3.8/http/client.py&quot;, line 1230, in request
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -     self._send_request(method, url, body, headers, encode_chunked)
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -   File &quot;/usr/local/lib/python3.8/http/client.py&quot;, line 1271, in _send_request
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -     self.putheader(hdr, value)
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -   File &quot;/usr/local/lib/python3.8/site-packages/urllib3/connection.py&quot;, line 224, in putheader
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -     _HTTPConnection.putheader(self, header, *values)
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -   File &quot;/usr/local/lib/python3.8/http/client.py&quot;, line 1208, in putheader
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO -     raise ValueError('Invalid header value %r' % (values[i],))
[2022-05-25, 12:16:58 UTC] {pod_manager.py:197} INFO - ValueError: Invalid header value b'Bearer xxxx\n'
</code></pre>
<p>I'm finding it hard to trace down though the Airflow code what's happening, but we are using a secret and passing this to a kubernetes pod in our airflow code:</p>
<pre class=""lang-py prettyprint-override""><code>from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import (
    KubernetesPodOperator,
)
from kubernetes.client import models as k8s
from airflow.kubernetes.secret import Secret

secret_buildkite = Secret(
    deploy_type=&quot;env&quot;,
    deploy_target=&quot;BUILDKITE_API_KEY&quot;,
    secret=&quot;enrichment-ml-env-buildkite-api-key&quot;,
    key=&quot;buildkite-api-key&quot;,
)

model_retraining = KubernetesPodOperator(
            namespace=&quot;airflow&quot;,
            image=&quot;gcr.io/...&quot;,  # container name changed to ... for SO post
            image_pull_policy=&quot;Always&quot;,
            arguments=[&quot;--environment&quot;, target_cluster, &quot;--market&quot;, market, &quot;--income&quot;],
            secrets=secrets,
            ...) # there are more args but I'm not sure they're relevant

</code></pre>
<p>Anyone have ideas what could be going wrong or how to fix it?</p>
",1,1653481562,python;python-requests;airflow;urllib3;buildkite,False,2015,0,1653481562,https://stackoverflow.com/questions/72377625/airflow-buildkite-valueerror-invalid-header-value-bbearer-xxxx-n
72344206,What package and function to use to get a response that can be formatted in JSON after sending a query string,"<p>I am currently using AWS Lambda which does not support the &quot;requests&quot; package. Hence I am trying to look for alternatives using urllib3</p>
<p>My code using the &quot;requests&quot; package is as such:</p>
<pre><code>site_dict = {
'1':'JSDOKAJDISADK',
'2':'IOASJD8917238ASDW',
'3':'UIO2NKDNAK123'
for sId in site_dict:
params = {
    'api_key': site_dict[sId]
}
r = requests.get(url = id_url, params = params)
data = r.json()
</code></pre>
<p>Params is a dictionary where the
Using urllib3:</p>
<pre><code>http = urllib3.PoolManager()
r = http.request('GET', url=id_url, headers=params)
data = r.json()
</code></pre>
<p>However, I get the error:</p>
<pre class=""lang-py prettyprint-override""><code>Traceback (most recent call last):
  File &quot;C:\Users\Owner\PycharmProjects\pythonProject\API Test.py&quot;, line 37, in &lt;module&gt;
    data = r.json()
AttributeError: 'HTTPResponse' object has no attribute 'json'
</code></pre>
<p>How do I get my data which I can then format into JSON format using urllib3?</p>
",0,1653288149,python;aws-lambda;urllib3;python-requests-html,False,292,1,1653299859,https://stackoverflow.com/questions/72344206/what-package-and-function-to-use-to-get-a-response-that-can-be-formatted-in-json
72320100,get status_code for http.urlopen,"<p>How can I print the status code for this response? (eg 200/401 etc)</p>
<pre><code>resp = http.urlopen('POST', 'https://api.bitbucket.org/2.0/repositories/6789oh', headers=headers, body=json.dumps(data))
print(str(resp.data))
</code></pre>
<p>I tried:</p>
<pre><code>resp.code
resp.get_code()
</code></pre>
<p>etc but none of them work for <code>http.urlopen</code>.</p>
",-1,1653054816,python;python-3.x;http;urllib2;urllib3,False,37,1,1653056959,https://stackoverflow.com/questions/72320100/get-status-code-for-http-urlopen
47862835,how do i catch SSL: CERTIFICATE_VERIFY_FAILED error python?,"<p>I am using urllib3.PoolManager to make http requests. And in some part of my code I use this code to make a request</p>

<pre><code>resp = h.request(self.method, self.url, body=body, headers=headers, timeout=TIMEOUT, retries=retries)
</code></pre>

<p>and I get the error SSL: CERTIFICATE_VERIFY_FAILED. Below is the full stack trace.</p>

<pre><code>  File ""/lib/python2.7/site-packages/urllib3/request.py"", line 69, in request
    **urlopen_kw)

  File ""/lib/python2.7/site-packages/urllib3/request.py"", line 90, in request_encode_url
    return self.urlopen(method, url, **extra_kw)

  File ""/lib/python2.7/site-packages/urllib3/poolmanager.py"", line 248, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)

  File ""/lib/python2.7/site-packages/urllib3/connectionpool.py"", line 621, in urlopen
    raise SSLError(e)

[SSL: CERTIFICATE_VERIFY_FAILED]
</code></pre>

<p>The error is expected. But the problem is I cannot catch the error in try except block.</p>

<p>I tried to use</p>

<pre><code> except ssl.SSLError:
</code></pre>

<p>but that does not catch this error.
I also tried ssl.CertificateError but no results.
I can catch it by using the Exception class but I need to catch the specific errors and handle them differently. Can someone please find me a solution to this?</p>
",3,1513576629,python;ssl;exception;ssl-certificate;urllib3,True,4556,3,1652959322,https://stackoverflow.com/questions/47862835/how-do-i-catch-ssl-certificate-verify-failed-error-python
67474641,Python requests &amp; urllib3 Retry - How may retries were made?,"<p>Given following example usage:</p>
<pre class=""lang-py prettyprint-override""><code>adapter = HTTPAdapter(max_retries=Retry(
    total=5,
    backoff_factor=0.1,
    status_forcelist=[429, 500, 502, 503, 504],
    method_whitelist=[&quot;HEAD&quot;, &quot;GET&quot;, &quot;OPTIONS&quot;]
))
session = requests.Session()
session.mount(&quot;http://&quot;, adapter)
session.mount(&quot;https://&quot;, adapter)
rsp = session.post(url, json=my_json, params=my_params)
</code></pre>
<p>How do I tell how many retries were made? I'm trying to debug/diagnose/resolve an issue posted in <a href=""https://stackoverflow.com/questions/67474679/python-requests-urllib3-retry-how-to-simulate-a-connectionerror-from-inside"">this related question</a></p>
<p>Alternatively, is there a different usage of these libs that provides this?</p>
",2,1620665839,python;python-requests;urllib3,True,2695,2,1652953907,https://stackoverflow.com/questions/67474641/python-requests-urllib3-retry-how-may-retries-were-made
72238460,"Python ImportError: sys.meta_path is None, Python is likely shutting down","<p>When using <code>__del__</code>
datetime.date.today() throws ImportError: sys.meta_path is None, Python is likely shutting down</p>
<pre><code>import datetime
import time
import sys


class Bug(object):

    def __init__(self):
        print_meta_path()

    def __del__(self):
        print_meta_path()
        try_date('time')
        try_date('datetime')


def print_meta_path():
    print(f'meta_path: {sys.meta_path}')


def try_date(date_type):
    try:
        print('----------------------------------------------')
        print(date_type)
        if date_type == 'time':
            print(datetime.date.fromtimestamp(time.time()))
        if date_type == 'datetime':
            print(datetime.date.today())
    except Exception as ex:
        print(ex)


if __name__ == '__main__':
    print(sys.version)
    bug = Bug()
</code></pre>
<p>output with different envs (3.10, 3.9, 3.7):</p>
<pre><code>3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:39:04) [GCC 10.3.0]
meta_path: [&lt;_distutils_hack.DistutilsMetaFinder object at 0x7ff8731f6860&gt;, &lt;class '_frozen_importlib.BuiltinImporter'&gt;, &lt;class '_frozen_importlib.FrozenImporter'&gt;, &lt;class '_frozen_importlib_external.PathFinder'&gt;]
meta_path: None
----------------------------------------------
time
2022-05-17
----------------------------------------------
datetime
sys.meta_path is None, Python is likely shutting down
</code></pre>
<pre><code>3.9.12 | packaged by conda-forge | (main, Mar 24 2022, 23:22:55)
[GCC 10.3.0]
meta_path: [&lt;_distutils_hack.DistutilsMetaFinder object at 0x7fb01126e490&gt;, &lt;class '_frozen_importlib.BuiltinImporter'&gt;, &lt;class '_frozen_importlib.FrozenImporter'&gt;, &lt;class '_frozen_importlib_external.PathFinder'&gt;]
meta_path: None
----------------------------------------------
time
2022-05-17
----------------------------------------------
datetime
sys.meta_path is None, Python is likely shutting down
</code></pre>
<pre><code>3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53)
[GCC 9.4.0]
meta_path: [&lt;class '_frozen_importlib.BuiltinImporter'&gt;, &lt;class '_frozen_importlib.FrozenImporter'&gt;, &lt;class '_frozen_importlib_external.PathFinder'&gt;]
meta_path: None
----------------------------------------------
time
2022-05-17
----------------------------------------------
datetime
sys.meta_path is None, Python is likely shutting down
</code></pre>
<pre><code>3.8.10 (default, Mar 15 2022, 12:22:08) 
[GCC 9.4.0]
meta_path: [&lt;class '_frozen_importlib.BuiltinImporter'&gt;, &lt;class '_frozen_importlib.FrozenImporter'&gt;, &lt;class '_frozen_importlib_external.PathFinder'&gt;]
meta_path: None
----------------------------------------------
time
2022-05-17
----------------------------------------------
datetime
sys.meta_path is None, Python is likely shutting down
</code></pre>
<p>Why is that happening?
I need to use requests which use urllib3 connection.py</p>
<p><code>380:  is_time_off = datetime.date.today() &lt; RECENT_DATE</code></p>
<pre><code>  File &quot;/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/requests/api.py&quot;, line 117, in post
  File &quot;/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/requests/api.py&quot;, line 61, in request
  File &quot;/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/requests/sessions.py&quot;, line 529, in request
  File &quot;/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/requests/sessions.py&quot;, line 645, in send
  File &quot;/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/requests/adapters.py&quot;, line 440, in send
  File &quot;/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/urllib3/connectionpool.py&quot;, line 703, in urlopen
  File &quot;/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/urllib3/connectionpool.py&quot;, line 386, in _make_request
  File &quot;/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/urllib3/connectionpool.py&quot;, line 1040, in _validate_conn
  File &quot;/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/urllib3/connection.py&quot;, line 380, in connect
ImportError: sys.meta_path is None, Python is likely shutting down
</code></pre>
<p>switching the line to
<code>380:  is_time_off = datetime.date.fromtimestamp(time.time()) &lt; RECENT_DATE</code> solve it.</p>
<pre><code>OS Linux-5.13.0-41-generic-x86_64-with-glibc2.31
urllib3 1.26.9
</code></pre>
<p>I already tried to rebind <code>__del__</code> arguments default</p>
<p><code>def __del__(self, datetime=datetime):....</code></p>
<p>Does anyone have an idea? thanks</p>
",8,1652516302,python;python-requests;garbage-collection;python-datetime;urllib3,True,3430,1,1652796827,https://stackoverflow.com/questions/72238460/python-importerror-sys-meta-path-is-none-python-is-likely-shutting-down
72271239,Basic Authentication does not work with urllib3,"<p>I'm trying to access a website which requires username and password
code snippet
...
import urllib3
http = urllib3.PoolManager()
url = 'http://192.168.1.1'
headers = urllib3.make_headers(basic_auth='root:admin')
r = http.request('GET', url, headers=headers)
...
I get response 200 OK , even if I pass wrong credentials</p>
<p>please advice</p>
",0,1652778580,python;authentication;urllib3,True,310,1,1652790285,https://stackoverflow.com/questions/72271239/basic-authentication-does-not-work-with-urllib3
35466918,Python Requests: IOError: [Errno 22] Invalid argument,"<p>I am new to Python Requests and am encountering an <code>IOError:[Errno 22] Invalid argument</code> when I attempt a <code>requests.get()</code>. In short, I am attempting to connect to an internal web application using SSL and so I pass a cert/key combination per the Requests documentation.</p>

<p>I've spent quite a bit of time researching potential issues and saw some mention of potential SNI issues but am not savvy enough to know how I might go about remedying the issue (again, new to Requests). Appreciate any nudge in the right direction/where to dig in further (I'm guessing the urllib3 piece?) </p>

<h2>My Code</h2>

<pre><code>import requests

cert_file_path = ""/Users/me/Documents/cert.pem""
key_file_path = ""/Users/me/Documents/key.pem""

url = ""https://mydomain/path/to/something""
cert = (cert_file_path, key_file_path)
r = requests.get(url, cert=cert) 
</code></pre>

<h2>My Error</h2>

<pre><code>IOError                               Traceback (most recent call last)
    &lt;ipython-input-48-1ee4a7f23d00&gt; in &lt;module&gt;()
          4 url = ""https://mydomain/path/to/something""
          5 cert = (cert_file_path, key_file_path)
    ----&gt; 6 r = requests.get(url, cert=cert)

    /Users/me/anaconda/lib/python2.7/site-packages/requests/api.pyc in get(url, **kwargs)
         66 
         67     kwargs.setdefault('allow_redirects', True)
    ---&gt; 68     return request('get', url, **kwargs)
         69 
         70 

    /Users/me/anaconda/lib/python2.7/site-packages/requests/api.pyc in request(method, url, **kwargs)
         48 
         49     session = sessions.Session()
    ---&gt; 50     response = session.request(method=method, url=url, **kwargs)
         51     # By explicitly closing the session, we avoid leaving sockets open which
         52     # can trigger a ResourceWarning in some cases, and look like a memory leak

    /Users/me/anaconda/lib/python2.7/site-packages/requests/sessions.pyc in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)
        462         }
        463         send_kwargs.update(settings)
    --&gt; 464         resp = self.send(prep, **send_kwargs)
        465 
        466         return resp

    /Users/me/anaconda/lib/python2.7/site-packages/requests/sessions.pyc in send(self, request, **kwargs)
        574 
        575         # Send the request
    --&gt; 576         r = adapter.send(request, **kwargs)
        577 
        578         # Total elapsed time of the request (approximately)

    /Users/me/anaconda/lib/python2.7/site-packages/requests/adapters.pyc in send(self, request, stream, timeout, verify, cert, proxies)
        368                     decode_content=False,
        369                     retries=self.max_retries,
    --&gt; 370                     timeout=timeout
        371                 )
        372 

    /Users/me/anaconda/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.pyc in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, **response_kw)
        542             httplib_response = self._make_request(conn, method, url,
        543                                                   timeout=timeout_obj,
    --&gt; 544                                                   body=body, headers=headers)
        545 
        546             # If we're going to release the connection in ``finally:``, then

    /Users/me/anaconda/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.pyc in _make_request(self, conn, method, url, timeout, **httplib_request_kw)
        339         # Trigger any extra validation we need to do.
        340         try:
    --&gt; 341             self._validate_conn(conn)
        342         except (SocketTimeout, BaseSSLError) as e:
        343             # Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.

    /Users/me/anaconda/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.pyc in _validate_conn(self, conn)
        760         # Force connect early to allow us to validate the connection.
        761         if not getattr(conn, 'sock', None):  # AppEngine might not have  `.sock`
    --&gt; 762             conn.connect()
        763 
        764         if not conn.is_verified:

    /Users/me/anaconda/lib/python2.7/site-packages/requests/packages/urllib3/connection.pyc in connect(self)
        236                                     ca_certs=self.ca_certs,
        237                                     server_hostname=hostname,
    --&gt; 238                                     ssl_version=resolved_ssl_version)
        239 
        240         if self.assert_fingerprint:

    /Users/me/anaconda/lib/python2.7/site-packages/requests/packages/urllib3/util/ssl_.pyc in ssl_wrap_socket(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context)
        261             raise
        262     if certfile:
    --&gt; 263         context.load_cert_chain(certfile, keyfile)
        264     if HAS_SNI:  # Platform-specific: OpenSSL with enabled SNI
        265         return context.wrap_socket(sock, server_hostname=server_hostname)

    IOError: [Errno 22] Invalid argument
</code></pre>

<h2>Environment</h2>

<p><em>Python</em>: Python 2.7.11 :: Anaconda 2.4.1 (x86_64)<br>
<em>Requests</em>: 2.6.0<br>
<em>Mac OSX</em>: Yosemite (10.10.5)</p>
",16,1455740429,python;ssl;python-requests;urllib3,True,4785,2,1652637307,https://stackoverflow.com/questions/35466918/python-requests-ioerror-errno-22-invalid-argument
28904607,Cannot import requests.packages.urllib3.util &#39;Retry&#39;,"<p>I am using Python 2.7 64 bit on Windows 8. I have Requests version 2.3 installed. I am trying to run this import statement as part of bringing in number of retries within my code:</p>

<pre><code>from requests.packages.urllib3.util import Retry
</code></pre>

<p>I have urllib3 installed also (I've just installed it now via Pip). I am getting the error message:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Python27\counter.py"", line 3, in &lt;module&gt;
    from requests.packages.urllib3.util import Retry
ImportError: cannot import name Retry
</code></pre>

<p>Can anyone tell me why this is? Are there any other dependencies I am unaware of to run this line of code successfully?</p>

<p>Thanks</p>
",31,1425664028,python;python-requests;urllib3,True,32881,2,1652371015,https://stackoverflow.com/questions/28904607/cannot-import-requests-packages-urllib3-util-retry
72194244,Unable to make requests.packages.urllib3 work,"<p>I am having an issue trying to use requests.packages.urllib3.exception, but i keep getting &quot;could not be resolved&quot;.</p>
<p>It used to work on another project, but i tried to merge this project with a PySide6 project and it wont work anymore.</p>
<p>The code is from this Module: <a href=""https://github.com/unistra/python-glpi-api"" rel=""nofollow noreferrer"">https://github.com/unistra/python-glpi-api</a></p>
<p>I tried deleting python, all the venv and reinstalling python + the modules and nothing works.
I am a novice python programmer, so my knowledge is quite limited.</p>
<p>thank you very much in advance !</p>
<p><a href=""https://i.stack.imgur.com/pnyOa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pnyOa.png"" alt=""Code"" /></a></p>
",0,1652229523,python;python-requests;urllib3,True,1098,1,1652323326,https://stackoverflow.com/questions/72194244/unable-to-make-requests-packages-urllib3-work
72047906,TypeError about needing &#39;bytes&#39; not &#39;str&#39; in Python,"<p>I'm attempting to use the Backblaze B2 API to eventually iterate through some .ai files I have there, but the code that Backblaze provides in <a href=""https://www.backblaze.com/b2/docs/b2_authorize_account.html"" rel=""nofollow noreferrer"">their documentation</a> is giving me an error.</p>
<p>Here's the code</p>
<pre><code>import base64
import json
import urllib3

id_and_key = 'applicationKeyId_value:applicationKey_value'
basic_auth_string = 'Basic ' + base64.b64encode(id_and_key)
headers = { 'Authorization': basic_auth_string }

request = urllib3.Request(
    'https://api.backblazeb2.com/b2api/v2/b2_authorize_account',
    headers = headers
    )
response = urllib3.urlopen(request)
response_data = json.loads(response.read())
response.close()
</code></pre>
<p>When I ran the code (replacing the id_and_key with my master key ID which looked something like '6b5*********') I got an error that looked like this</p>
<pre><code>Traceback (most recent call last):
  File &quot;/Users/jacobpatty/vscode_projects/badger_colors/backblaze_test.py&quot;, line 6, in &lt;module&gt;
    basic_auth_string = base64.b64encode(id_and_key)
  File &quot;/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/base64.py&quot;, line 58, in b64encode
    encoded = binascii.b2a_base64(s, newline=False)
TypeError: a bytes-like object is required, not 'str'
</code></pre>
<p>I looked at the base64 documentation, but I couldn't find any useful info there. Or is it possible it has something to do with urllib3?  Anyone know why I'm getting this error, or how to fix it?</p>
",1,1651166740,python;base64;urllib3;backblaze,True,84,1,1652287047,https://stackoverflow.com/questions/72047906/typeerror-about-needing-bytes-not-str-in-python
50414913,Reenable urllib3 warnings,"<p>I have a portion of my code where I knowingly make an Insecure Request. So I disable warnings with </p>

<pre><code>urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
</code></pre>

<p>After that part, how do I reenable/reset <code>urllib3</code> warnings in my script?</p>
",9,1526658094,python;urllib3,True,2093,2,1651176712,https://stackoverflow.com/questions/50414913/reenable-urllib3-warnings
71998897,Accessing a snowflake db within Flask and Windows IIS,"<p>I've an issue that I can't solve by myself, maybe some of you did have the same problem before.</p>
<p>I'm running a Flask-App on a Windows Server using Windows IIS.
The Flask-App itself is running without any issues, however the app has some functionalities which requires the connection to a Snowflake database.</p>
<p>And here comes the issue, I'm not able to connect to the snowflake database within Windows IIS.
I've tested to run the app with waitress-serve which worked very well.
I'd like to use Windows IIS however, because of it's functionality to use the Windows Authentication to read the REMOTE USER.</p>
<p>The only error I'm getting is</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\git-projects\test-project\venv\lib\site-packages\snowflake\connector\vendored\urllib3\connection.py&quot;, line 170, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw
  File &quot;C:\git-projects\test-project\venv\lib\site-packages\snowflake\connector\vendored\urllib3\util\connection.py&quot;, line 73, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
  File &quot;C:\Python\Python-3.7.2.amd64\lib\socket.py&quot;, line 748, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
socket.gaierror: [Errno 11001] getaddrinfo failed
</code></pre>
<p>Does anyone have an idea?</p>
<p>Thank you in advance!</p>
",0,1650886474,python;iis;snowflake-cloud-data-platform;urllib3,False,332,1,1651046078,https://stackoverflow.com/questions/71998897/accessing-a-snowflake-db-within-flask-and-windows-iis
71893233,How do I fix the ImportError?,"<p>So here is the import section:</p>
<pre><code>from  urllib3.packages import six
from .packages.six import HTTPConnection as _HTTPConnection 
from .packages.six import HTTPException
</code></pre>
<p>The Import Error raised at Line 2 is: <code>ImportError: cannot import name 'HTTPConnection' from 'urllib3.packages.six'</code>
And also there is a prompt which says: <code>&quot;HTTPException&quot; is not accessedPylance</code></p>
<p><a href=""https://i.stack.imgur.com/qZLJG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qZLJG.png"" alt=""Everything related to above code section is marked with arrow"" /></a></p>
",-1,1650106282,python;urllib3,False,363,1,1650230443,https://stackoverflow.com/questions/71893233/how-do-i-fix-the-importerror
71837596,"Urllib3 Python3, Getting Error No host specified, but the url works fine","<p>I am trying to write images to database from excel sheet. I have this function to download images.</p>
<pre><code>def downloadImage(url, Name):
    try:
        http    = urllib3.PoolManager()
        image   = http.request(&quot;GET&quot;, url, preload_content=False).data

    with File(open(f'{Name}-{timezone.now()}.jpg', 'wb')) as handler:
        handler.write(image)
    with handler.open(mode='rb') as nana:
        pass
except Exception as e:
    raise ValueError(e)
return handler
</code></pre>
<p>This is one of the url:</p>
<p><code>https://i.imgur.com/BWomMaV.jpg</code>, When I print the url, it is similar coz I have seen posts where http/https is not added, but here the url works fine.</p>
<p>Can anyone tell me where the issue is?</p>
<p>This is how the function is called</p>
<pre><code>Image(obj=instances[i], images=downloadImage(user['image1'], user['name']).open(mode='rb'))
</code></pre>
<p>Thanks</p>
",0,1649741811,python;python-3.x;python-requests;urllib3,False,241,0,1649753421,https://stackoverflow.com/questions/71837596/urllib3-python3-getting-error-no-host-specified-but-the-url-works-fine
71818804,Json-like response not recognized by Python,"<p>I got a response with urllib3 (also tried requests), decoded it and took in into json.loads,</p>
<p>This is the data after decoding with <code>decode('utf-8')</code> which I want to have as json:</p>
<p><code>json_thing = b'{\n&quot;stuff&quot;: {\n&quot;a&quot;: &quot;1&quot;,\n&quot;b&quot;: &quot;2&quot;,\n&quot;d&quot;: &quot;3&quot;,\n&quot;e&quot;: &quot;4&quot;,\n&quot;f&quot;: &quot;5&quot;,\n&quot;g&quot;: &quot;&quot;,\n&quot;h&quot;: &quot;8&quot;,\n&quot;i&quot;: &quot;9&quot;,\n&quot;j&quot;: &quot;10&quot;,\n&quot;k&quot;: &quot;&quot;,\n&quot;l&quot;: &quot;13&quot;,\n&quot;m&quot;: &quot;&quot;,\n&quot;n&quot;: &quot;&quot;,\n&quot;o&quot;: &quot;&quot;,\n&quot;p&quot;: [{\n&quot;q&quot;:\xc2\xa0&quot;19&quot;,\n&quot;r&quot;: 1\n}],\n&quot;s&quot;: &quot;Jan 1, 2020 1:44 pm&quot;,\n&quot;t&quot;: &quot;Jan 1, 2020 1:44 pm&quot;\n}\n}'</code></p>
<p>but I always get and error</p>
<p><code>raise JSONDecodeError(&quot;Expecting value&quot;, s, err.value) from None json.decoder.JSONDecodeError: Expecting value: line 18 column 14 (char 482)</code></p>
<p>When looking on it, after &quot;r&quot;: there is only a regular integer and there is at &quot;q&quot; \xc2\xa0, is it possible that I need to clean that out first?</p>
",0,1649610225,python;json;python-requests;response;urllib3,True,122,1,1649614138,https://stackoverflow.com/questions/71818804/json-like-response-not-recognized-by-python
71795242,swagger-codegen 3 python not working with Authorization,"<pre><code># Configure API key authorization: sso_auth
# configuration = pingdom_client.Configuration()
# configuration.api_key = {
#     'Authorization': 'Bearer &lt;token&gt;'}
# configuration.api_key_prefix['Authorization'] = 'Bearer'
# handle = pingdom_client.ApiClient(header_name='Authorization',
#                                   header_value='Bearer &lt;token&gt;')
# api_instance = pingdom_client.MarketApi(pingdom_client.ApiClient(configuration))
handle = pingdom_client.ApiClient(header_name='Authorization',
                                  header_value='Bearer &lt;token&gt;')

check = pingdom_client.ChecksApi(api_client=handle)
thread = check.checks_get_with_http_info(async_req=True)
result = thread.get()
print(result)
</code></pre>
<p>I was trying to use pingdom api using swagger. and it is not working.</p>
",1,1649412700,python;swagger-codegen;urllib3,False,193,0,1649412700,https://stackoverflow.com/questions/71795242/swagger-codegen-3-python-not-working-with-authorization
71788453,Python urllib3 doesn&#39;t seem to be sending fields data,"<p>I am trying to utilise the authentication here: <a href=""https://api.graphnethealth.com/system-auth"" rel=""nofollow noreferrer"">https://api.graphnethealth.com/system-auth</a> using Python urllib3 and have the following</p>
<pre><code>import urllib3
http = urllib3.PoolManager()
resp = http.request(
        &quot;POST&quot;,
        &quot;https://core.syhapp.com/hpca/oauth/token&quot;,
        headers={
            &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;
        },
        fields={
            &quot;grant_type&quot;: &quot;client_credentials&quot;,
            &quot;client_id&quot;: &quot;YYYYYYYYY&quot;,
            &quot;client_secret&quot;: &quot;XXXXXXXXX&quot;
        }
    )
print(resp.data)
</code></pre>
<p>I get an error saying that <code>grant_type</code> has not been sent.</p>
<pre><code>b'{\r\n  &quot;error&quot;: {\r\n    &quot;code&quot;: &quot;400&quot;,\r\n    &quot;message&quot;: &quot;Validation Errors&quot;,\r\n    &quot;target&quot;: &quot;/oauth/token&quot;,\r\n    &quot;details&quot;: [\r\n      {\r\n        &quot;message&quot;: &quot;grant_type is required&quot;,\r\n        &quot;target&quot;: &quot;GrantType&quot;\r\n      },\r\n      {\r\n        &quot;message&quot;: &quot;Value should be one of the following password,refresh_token,trusted_token,handover_token,client_credentials,pin&quot;,\r\n        &quot;target&quot;: &quot;GrantType&quot;\r\n      }\r\n    ]\r\n  }\r\n}'
</code></pre>
<p>Any suggestions?</p>
",0,1649362759,python;python-3.x;urllib3,True,962,2,1649363808,https://stackoverflow.com/questions/71788453/python-urllib3-doesnt-seem-to-be-sending-fields-data
71760536,create list of tuples with download url + &quot;href&quot;,"<p>I'm trying to make a list of tuples, the first element being the download URL and the second being the file name from the URL string with below code:</p>
<pre><code>import urllib
import requests
from bs4 import BeautifulSoup
import pandas as pd
import io
url = r&quot;https://www.ers.usda.gov/data-products/livestock-meat-domestic-data&quot;
my_bytes = urllib.request.urlopen(url)
my_bytes = my_bytes.read().decode(&quot;utf8&quot;)
parsed_html = BeautifulSoup(my_bytes, features = &quot;lxml&quot;)
table_data = parsed_html.body.find('table', attrs = {'id':'data_table'})
download_url = &quot;https://www.ers.usda.gov&quot;
full_download_url = [tuple(download_url,i[&quot;href&quot;]) for i in table_data.find_all('a')]
</code></pre>
<p>But I've been getting <code>TypeError: must be str, not list</code> all along and I'm not sure how to fix this, please help? Thanks!</p>
",0,1649213571,python;string;python-requests;tuples;urllib3,True,105,2,1649217778,https://stackoverflow.com/questions/71760536/create-list-of-tuples-with-download-url-href
27387783,How to download a file with urllib3?,"<p>This is based on another question on this site: <a href=""https://stackoverflow.com/questions/17285464/whats-the-best-way-to-download-file-using-urllib3"">What&#39;s the best way to download file using urllib3</a>
However, I cannot comment there so I ask another question:</p>

<p>How to download a (larger) file with urllib3?</p>

<p>I tried to use the same code that works with urllib2 (<a href=""https://stackoverflow.com/questions/7243750/download-file-from-web-in-python-3"">Download file from web in Python 3</a>), but it fails with urllib3:</p>

<pre><code>http = urllib3.PoolManager()

with http.request('GET', url) as r, open(path, 'wb') as out_file:       
    #shutil.copyfileobj(r.data, out_file) # this writes a zero file
    shutil.copyfileobj(r.data, out_file)
</code></pre>

<p>This says that 'bytes' object has no attribute 'read'</p>

<p>I then tried to use the code in that question but it gets stuck in an infinite loop because data is always '0':</p>

<pre><code>http = urllib3.PoolManager()
r = http.request('GET', url)

with open(path, 'wb') as out:
    while True:
        data = r.read(4096)         
        if data is None:
            break
        out.write(data)
r.release_conn()
</code></pre>

<p>However, if I read everything in memory, the file gets downloaded correctly:</p>

<pre><code>http = urllib3.PoolManager()
r = http.request('GET', url)
with open(path, 'wb') as out:
    out.write(data)
</code></pre>

<p>I do not want to do this, as I might potentially download very large files.
It is unfortunate that the urllib documentation does not cover the best practice in this topic.</p>

<p>(Also, please do not suggest requests or urllib2, because they are not flexible enough when it comes to self-signed certificates.)</p>
",10,1418155402,python;python-3.x;urllib3,True,15957,1,1648483007,https://stackoverflow.com/questions/27387783/how-to-download-a-file-with-urllib3
71574862,Trouble parsing string for url request,"<p>Prompt: I am attempting to access the SEC EDGAR Database to extract specific company files. I'm having trouble with my urllib.request.request(). Currently I need to access the source code of the site. After that I would parse with re for the body paragraphs</p>
<pre><code>**import re
import urllib.request as request
import urllib.parse as parse
import pandas
import csv
'''
WE ARE finding &amp; parsing information to find https://www.sec.gov/Archives/edgar/data/1018724/0001018724-20-000030.txt
'''
frm_type = input('Enter the file type (e.g. 10-k, 8-q): ')
year = input('Enter fiscal year(4 digit number): ')
quarter = input('Enter quarter (NOTE. Must be in format QTX, with x being 1-4): ')
CIK = input('Enter CIK (company identifier): ')
def find_sec_filings(cik, year, quarter, filetype):
    quarter = quarter.upper()
    &quot;&quot;&quot;sources relevant file from EDGAR Database.&quot;&quot;&quot;
    lookup = 'edgar/data/'
    web = 'https://www.sec.gov/Archives/edgar/full-index/'
    direction = web + str(year) + '/' + str(quarter) + '/' + 'master.idx'
    try:
        idx = request.urlopen(direction)  
        for line in idx:
            if year in line and cik in line:
                for element in line.split('|'):
                    if lookup in element:
                        file_direction = str(element\[lookup:\])
                        return file_direction
     except:
         print(&quot;No file with the specifications were found&quot;)
#Path to 10-k
fd = find_sec_filings(CIK,year,quarter,frm_type)
print(fd)
url1 = 'https://www.sec.gov/Archives/'+ fd

ERROR MESSAGE:
No file with the specifications were found
None
File &quot;C:\\Users\\trisy\\OneDrive\\Desktop\\classes\\SP_22_courses\\CS1110\\pye_files\\edgar.py&quot;, line 44, in \&lt;module\&gt;
url1 = 'https://www.sec.gov/Archives/'+ fd
TypeError: can only concatenate str (not &quot;NoneType&quot;) to str\`
</code></pre>
",0,1647963424,python;urllib3,False,55,2,1647964296,https://stackoverflow.com/questions/71574862/trouble-parsing-string-for-url-request
71548246,Python script to download PDF not downloading the PDF?,"<p>I have a Python 3.10 script to download a PDF from a URL, I get no errors but when I run the code the PDF does not download. I've done a sanity check to ensure the PDF is actually on the URL (which it is)</p>
<p>I'm not sure if this maybe has something to do with HTTP/ HTTPS? This site does have an expired HTTPS certificate, but it is a government site and this is really for testing only so I am not worried about that and can ignore the error</p>
<pre><code>from fileinput import filename
import os
import os.path
from datetime import datetime
import urllib.request
import requests

import urllib3
urllib3.disable_warnings()

resp = requests.get('http:// url domain .org', verify=False)
urllib.request.urlopen('http:// my url .pdf')

filename = datetime.now().strftime(&quot;%Y_%m_%d-%I_%M_%S_%p&quot;)
save_path = &quot;C:/Users/bob/Desktop/folder&quot;
</code></pre>
<p>Or maybe is the issue something to do with urllib3 ignoring the error and urllib downloading the file?</p>
<p>Redacted the specific URL here</p>
",0,1647791420,python;pdf;urllib;urllib3,True,240,1,1647792339,https://stackoverflow.com/questions/71548246/python-script-to-download-pdf-not-downloading-the-pdf
71062913,what is the proper way to use proxies with requests in python,"<p>Requests is not honoring the proxies flag.</p>
<p>There is something I am missing about making a request over a proxy with python requests library.</p>
<p>If I enable the OS system proxy, then it works, but if I make the request with just requests module proxies setting, the remote machine will not see the proxy set in requests, but will see my real ip, it is as if not proxy was set.</p>
<p>The bellow example will show this effect, at the time of this post the bellow proxy is alive but any working proxy should replicate the effect.</p>
<pre><code>import requests

proxy ={
  'http:': 'https://143.208.200.26:7878',
  'https:': 'http://143.208.200.26:7878'
}
data = requests.get(url='http://ip-api.com/json', proxies=proxy).json()
print('Ip: %s\nCity: %s\nCountry: %s' % (data['query'], data['city'], data['country']))
</code></pre>
<p>I also tried changing the proxy_dict format:</p>
<pre><code>proxy ={
          'http:': '143.208.200.26:7878',
          'https:': '143.208.200.26:7878'
       }
</code></pre>
<p>But still it has not effect.</p>
<p>I am using:
-Windows 10
-python 3.9.6
-urllib 1.25.8</p>
<p>Many thanks in advance for any response to help sort this out.</p>
",0,1644486602,python;python-requests;proxy;urllib3;proxies,True,5282,1,1647474039,https://stackoverflow.com/questions/71062913/what-is-the-proper-way-to-use-proxies-with-requests-in-python
71496124,How to extract these &#39;video_url&#39;,"<p>Here the text/javascript code I extracted.</p>
<p>And also, I want to extract values from 'video_id', 'video_url', 'video_alt_url' from these script!</p>
<pre><code>&quot;&quot;&quot;{                                                                                                                                   
    video_id: '000101',
    video_categories: 'Categorie01, Categorie02',
    video_tags: 'Categorie01, Categorie02',                                                                                                                                        license_code: '$603825119921245',                                                                                                                                       rnd: '1647426812',
    video_url:'https://www.example.com/get_file/5/bb6a5e180f5037a3f348fbdee96a8c6f681c4c0bab/107000/107389/107389.mp4/?br=709',
    postfix: '.mp4',
    video_url_text: '480p',
    video_alt_url:'https://www.example.com/get_file/5/47601c7136bcbe38e6eb0b2cfa04dd9d917aa6263b/107000/107389/107389_720p.mp4/?br=1243',
    video_alt_url_text: '720p',
    video_alt_url_hd: '1',
    preview_url: 'https://www.example.com/contents/videos_screenshots/107000/107389/preview.jpg',
    preview_url1:'https://www.example.com/contents/videos_screenshots/107000/107389/preview.mp4.jpg',
    preview_height1: '480',
    preview_url2:'https://www.example.com/contents/videos_screenshots/107000/107389/preview_720p.mp4.jpg',
    preview_height2: '720',
    skin: 'youtube.css',
    logo_position: '0,0',
    logo_anchor: 'topleft',
    hide_controlbar: '1',
    hide_style: 'fade',
    volume: '1',
    related_src: 'https://www.example.com/related_videos_html/107389/',                                                                                                                                  adv_pre_vast: 'https://twinrdsrv.com/preroll.engine?id=613eb379-62dd-49ef-8299-db2b5b2af4d7&amp;zid=12861&amp;cvs={ClientVideoSupport}&amp;time={TimeOffset}&amp;stdtime={StdTimeOffset}&amp;abr={IsAdblockRequest}&amp;pageurl={PageUrl}&amp;tid={TrackingId}&amp;res={Resolution}&amp;bw={BrowserWidth}&amp;bh={BrowserHeight}&amp;kw={Keywords}&amp;referrerUrl={ReferrerUrl}&amp;pw={PlayerWidth}&amp;ph={PlayerHeight}',
    adv_pre_skip_duration: '5',
    adv_pre_skip_text_time: 'Skip ad in %time',
    adv_pre_skip_text: 'Skip ad',
    adv_post_vast: 'https://twinrdsrv.com/preroll.engine?id=613eb379-62dd-49ef-8299-db2b5b2af4d7&amp;zid=12861&amp;cvs={ClientVideoSupport}&amp;time={TimeOffset}&amp;stdtime={StdTimeOffset}&amp;abr={IsAdblockRequest}&amp;pageurl={PageUrl}&amp;tid={TrackingId}&amp;res={Resolution}&amp;bw={BrowserWidth}&amp;bh={BrowserHeight}&amp;kw={Keywords}&amp;referrerUrl={ReferrerUrl}&amp;pw={PlayerWidth}&amp;ph={PlayerHeight}',
    adv_post_skip_duration: '5',
    adv_post_skip_text_time: 'Skip ad in %time',
    adv_post_skip_text: 'Skip ad',
    lrcv: '1651572296480833989009946',
    vast_timeout1: '10',
    player_width: '882',
    player_height: '496.9014084507',
    embed: '1'
}&quot;&quot;&quot;
</code></pre>
",1,1647429358,python;json;web-scraping;beautifulsoup;urllib3,True,292,2,1647440522,https://stackoverflow.com/questions/71496124/how-to-extract-these-video-url
71400581,"Docker Container with 4 Nodes, No connection could be established using urllib3","<p>I'm setting up a docker container which generates 4 nodes that will execute two python scripts: blockchain and import. More details below:</p>
<p>Docker File:</p>
<pre><code>FROM ubuntu:latest

RUN apt-get update -y
RUN apt-get install -y python3 python3-pip python3-dev build-essential

RUN mkdir /myApp
WORKDIR /myApp

COPY blockchain/requirements.txt /myApp/
RUN pip3 install -r requirements.txt

COPY blockchain/blockchain.py /myApp/

ENTRYPOINT [&quot;python3&quot;]
CMD [&quot;blockchain.py&quot;]
</code></pre>
<p>Docker Compose (docker-compose.yml):</p>
<pre><code>version: '3'

services:
  node1:
    build: .
    hostname: node1
    ports:
      - &quot;5001:5000&quot;
  node2:
    build: .
    hostname: node2
    ports:
      - &quot;5002:5000&quot;
  node3:
    build: .
    hostname: node3
    ports:
      - &quot;5003:5000&quot;
  node4:
    build: .
    hostname: node4
    ports:
      - &quot;5004:5000&quot;
</code></pre>
<p>This means that I want to create 4nodes, and on the import script I will need to connect to this nodes using POST method. In the variable localIP I used my local @IP :</p>
<pre><code>#I've tried to get the @ using Python
localIP = socket.gethostbyname(hostname)

#I've also used windows CMD: ipconfig to get the @IP
#localIP = '192.168.XX.XX'

#I've also tried the @IP we can get from google: what is my IP address
#localIP = '196.12X.XX.XX'

hosts = [ 
    'https://'+localIP+':5001',
    'https://'+localIP+':5002',
    'https://'+localIP+':5003',
    'https://'+localIP+':5004'
    ]

http = urllib3.PoolManager()

# register nodes - for each node, don't add itself the list
for host in hosts:
    data = { &quot;nodes&quot;: [x for x in hosts if x != host] }
    data= json.dumps(data).encode('utf-8')
    res = http.request('POST', host, body=data, headers={'Content-Type': 'application/json'})
</code></pre>
<p>But I get the following error:</p>
<pre><code>urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='192.168.XX.XX', port=5001): Max retries exceeded with url: 
/ (Caused by NewConnectionError('&lt;urllib3.connection.HTTPSConnection object at 0x000001FB210ACBB0&gt;: Failed to establish 
a new connection: [WinError 10061] No connection could be established because the target computer expressly refused it'))
</code></pre>
<p>When I try get from terminal the @IP of the nodes it returns blank value:</p>
<pre><code>docker inspect -f '{{ .NetworkSettings.IPAddress }}' NODE-01-ID
</code></pre>
<p><strong>The question is, Is the IP address wrong ? How can we provide the right IP address ?</strong></p>
<p>Thank you</p>
",1,1646768229,python;docker;docker-compose;dockerfile;urllib3,True,183,1,1646908912,https://stackoverflow.com/questions/71400581/docker-container-with-4-nodes-no-connection-could-be-established-using-urllib3
71145997,Add verbosity to urllib3 Retry with requests?,"<p>I am trying to use the urllib3 Retry functionality when scraping a website, but have found that it's not very verbose in the way it lets you know when it's retrying.</p>
<p>My class looks like this:</p>
<pre><code>class Session:
    def __init__(self):
        self.session = requests.session()
        retry = Retry(total=10, backoff_factor=1, status_forcelist=[413,429,500,502,503,504])
        adapter = HTTPAdapter(max_retries=retry)
        self.session.mount(&quot;http://&quot;, adapter)
        self.session.mount(&quot;https://&quot;, adapter)

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.session.__exit__()

    def get(self, url, params={}, headers=None):
        &quot;&quot;&quot;
        Implements the get function defined below
        &quot;&quot;&quot;
        return get(url, self.session, params, headers)


def get(url: str, session=None, params=None, headers=None,) -&gt; requests.Response:
    &quot;&quot;&quot;
    Function to return the response from a request given certain parameters, including auth and session
    &quot;&quot;&quot;
    logger.info(&quot;Get request to url: %s&quot;, url)

    if session:
        response = session.get(url, params=params, headers=headers,)
    else:
        response = requests.get(url, params=params, headers=headers,)
    response.raise_for_status()

    return response
</code></pre>
<p>I want to be able to add some logging, such as:</p>
<pre><code>failed on 1st attempt, backing off for 1s...
failed on 2nd attempt, backing off for 2s...
</code></pre>
<p>etc. Is there any way to do this with the Retry functionality other than writing something bespoke?</p>
",1,1645030369,python;web-scraping;python-requests;urllib;urllib3,False,865,0,1645030499,https://stackoverflow.com/questions/71145997/add-verbosity-to-urllib3-retry-with-requests
71081276,How to set a timeout for the connections in HTTPConnectionPool when using requests,"<p>I have the following script:</p>
<pre><code>from requests import Session
from time import sleep


session = Session()
url = 'http://tsetmc.com'
r = session.get(url, timeout=5)  # 200 OK

sleep(200)  # if the idle time is greater than ~120 seconds, then the next `session.get` attempt will fail

# the following line fails with 
# `requests.exceptions.ReadTimeout: HTTPConnectionPool(host='tsetmc.com', port=80): Read timed out. (read timeout=5)` 
# but will work if I use headers={&quot;Connection&quot;: &quot;close&quot;} parameter
session.get(url, timeout=5)
</code></pre>
<p>It fails with:</p>
<pre><code>Traceback (most recent call last):
  File &quot;...\Python310\lib\site-packages\urllib3\connectionpool.py&quot;, line 449, in _make_request
    six.raise_from(e, None)
  File &quot;&lt;string&gt;&quot;, line 3, in raise_from
  File &quot;...\Python310\lib\site-packages\urllib3\connectionpool.py&quot;, line 444, in _make_request
    httplib_response = conn.getresponse()
  File &quot;...\Python310\lib\http\client.py&quot;, line 1374, in getresponse
    response.begin()
  File &quot;...\Python310\lib\http\client.py&quot;, line 318, in begin
    version, status, reason = self._read_status()
  File &quot;...\Python310\lib\http\client.py&quot;, line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), &quot;iso-8859-1&quot;)
  File &quot;...\Python310\lib\socket.py&quot;, line 705, in readinto
    return self._sock.recv_into(b)
TimeoutError: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;...\Python310\lib\site-packages\requests\adapters.py&quot;, line 440, in send
    resp = conn.urlopen(
  File &quot;...\Python310\lib\site-packages\urllib3\connectionpool.py&quot;, line 785, in urlopen
    retries = retries.increment(
  File &quot;...\Python310\lib\site-packages\urllib3\util\retry.py&quot;, line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File &quot;...\Python310\lib\site-packages\urllib3\packages\six.py&quot;, line 770, in reraise
    raise value
  File &quot;...\Python310\lib\site-packages\urllib3\connectionpool.py&quot;, line 703, in urlopen
    httplib_response = self._make_request(
  File &quot;...\Python310\lib\site-packages\urllib3\connectionpool.py&quot;, line 451, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File &quot;...\Python310\lib\site-packages\urllib3\connectionpool.py&quot;, line 340, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='tsetmc.com', port=80): Read timed out. (read timeout=5)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;F:\stock\stock-market-scripts\ETFs\temp\sessiontest3.py&quot;, line 14, in &lt;module&gt;
    session.get(url, timeout=5)
  File &quot;...\Python310\lib\site-packages\requests\sessions.py&quot;, line 542, in get
    return self.request('GET', url, **kwargs)
  File &quot;...\Python310\lib\site-packages\requests\sessions.py&quot;, line 529, in request
    resp = self.send(prep, **send_kwargs)
  File &quot;...\Python310\lib\site-packages\requests\sessions.py&quot;, line 645, in send
    r = adapter.send(request, **kwargs)
  File &quot;...\Python310\lib\site-packages\requests\adapters.py&quot;, line 532, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: HTTPConnectionPool(host='tsetmc.com', port=80): Read timed out. (read timeout=5)

Process finished with exit code 1
</code></pre>
<p>As you can see the second call to <code>session.get</code> will raise an error if there has been a significant idle time during the script run.</p>
<p>I believe the cause of this error is a server configuration that causes connections to be dropped after a certain period of time (~120s), but <a href=""https://2.python-requests.org/en/master/user/advanced/#keep-alive"" rel=""nofollow noreferrer"">requests tries to reuse</a> the expired connection.</p>
<p>I have also noticed that passing <code>headers={&quot;Connection&quot;: &quot;close&quot;}</code> argument to <code>session.get</code> will fix the issue, but I believe that also means the connections won't be sent back to the pool?</p>
<p>My question: Is there a way to set a <code>timeout</code> for the connections in the HTTPConnectionPool so that they won't be used if expired?</p>
<p>(
other options I can think of are:</p>
<ul>
<li>the <code>headers={&quot;Connection&quot;: &quot;close&quot;}</code> above (inefficient?)</li>
<li>using <code>threading.Timer</code> to create and use a new session object which I belive will have a fresh connection pool. (sounds too complicated?)</li>
<li>catch the error and retry (too slow?)
)</li>
</ul>
",1,1644587772,python;python-requests;connection-pooling;urllib3,True,6496,1,1644650963,https://stackoverflow.com/questions/71081276/how-to-set-a-timeout-for-the-connections-in-httpconnectionpool-when-using-reques
71035442,How to filter some urls from python list?,"<p>I wrote this code for extract images urls from a web page as I given. And it shows all images urls. But I need to filter &quot;https://images.unsplash.com/profile&quot; urls and print them.</p>
<pre><code>from bs4 import BeautifulSoup
import urllib.request
import re

url= &quot;https://unsplash.com/t/nature&quot;

html_page= urllib.request.urlopen(url)
soup= BeautifulSoup(html_page)
images= []
for img in soup.findAll('img'):
    images.append(img.get('src'))

print(images)
</code></pre>
<p>I tried;</p>
<pre><code>from bs4 import BeautifulSoup
import urllib.request
import re

url= &quot;https://unsplash.com/t/nature&quot;

html_page= urllib.request.urlopen(url)
soup= BeautifulSoup(html_page)
images= []
for img in soup.findAll('img'):
    images.append(img.get('src'))

if &quot;https://images.unsplash.com/profile&quot; in images:
    print(images)
</code></pre>
<p>And didn't worked!</p>
",2,1644329919,python;beautifulsoup;urllib;python-re;urllib3,True,188,2,1644332242,https://stackoverflow.com/questions/71035442/how-to-filter-some-urls-from-python-list
70916953,How can I calculate the bandwidth of the incoming compressed response to Requests object?,"<p>I am able to calculate the <strong>un</strong>compressed response using the below code:</p>
<pre><code>headers = {
'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
'accept-encoding': 'gzip, deflate, br',
'accept-language': 'en-US,en;q=0.9',
'cache-control': 'max-age=0',
'cookie': '1P_JAR=2022-01-29-22; NID=511=SULQVuezm5OrzjpKvZtmBvne4q5ZsAQS8JmtkASRvog38SdZbGgM3-Z_gD0yuk2Z1HeFpk0BLlRSoVzKWvLL1CEWlLdJs5EQun_xoY6pLotnuiuZjhlEvCkx2rJ8wbsaz6jzAl6SoAoHsEqxOviuOoaCZnAMzSf29iMM7FUlSJA; DV=c4G23wRj8NImUJHHJ0xN1v_kltiA6ldQvrLpXtnDmAQAAAA',
'sec-ch-ua': '&quot; Not;A Brand&quot;;v=&quot;99&quot;, &quot;Google Chrome&quot;;v=&quot;97&quot;, &quot;Chromium&quot;;v=&quot;97&quot;',
'sec-ch-ua-arch': '&quot;arm&quot;',
'sec-ch-ua-bitness': '&quot;64&quot;',
'sec-ch-ua-full-version': '&quot;97.0.4692.99&quot;',
'sec-ch-ua-mobile': '?0',
'sec-ch-ua-model': '&quot;&quot;',
'sec-ch-ua-platform': '&quot;macOS&quot;',
'sec-ch-ua-platform-version': '&quot;12.0.0&quot;',
'sec-fetch-dest': 'document',
'sec-fetch-mode': 'navigate',
'sec-fetch-site': 'same-origin',
'sec-fetch-user': '?1',
'upgrade-insecure-requests': '1',
'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36'    
}

r = requests.get('https://www.google.com/search?q=test&amp;oq=test&amp;aqs=chrome..69i57j0i271l3j69i61j69i60l2.747j0j1&amp;sourceid=chrome&amp;ie=UTF-8', headers=headers)


def header_size(headers):
    return sum(len(key) + len(value) + 4 for key, value in headers.items()) + 2
 
request_line_size = len(r.request.method) + len(r.request.path_url) + 12
request_size = request_line_size + header_size(r.request.headers) + int(r.request.headers.get('content-length', 0))

response_line_size = len(r.reason) + 15
response_size = response_line_size + header_size(r.headers) + len(str(r.content))

total_size = request_size + response_size

print(total_size) # in bytes
</code></pre>
<p>In the request to Google, I get around 600 bytes. But this represents the uncompressed response on my end. Is there any way to calculate the compressed response (which is closer to 150-200 bytes in the case of Google)?</p>
",0,1643562449,python;python-requests;bandwidth;urllib3,True,321,1,1643566238,https://stackoverflow.com/questions/70916953/how-can-i-calculate-the-bandwidth-of-the-incoming-compressed-response-to-request
70792216,Count number of retries for each request,"<p>I use package <code>requests</code> together with <code>urllib3.util.retry.Retry()</code> to send tens of thousands of queries. I seek to count the number of queries and the number of necessary attempts until I successfully retrieve the desired data. My goal is to construct a measure for the reliability of the API.</p>
<p>To fix ideas, let's assume that the Response object of <code>requests</code> contains this data:</p>
<pre class=""lang-py prettyprint-override""><code>from requests import Session
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

def create_session():
    session = Session()
    retries = Retry(
        total = 15,
        backoff_factor = 0.5,
        status_forcelist = [401, 408, 429, 500, 502, 504],
        allowed_methods = frozenset([&quot;GET&quot;])
    )

    session.mount('http://', HTTPAdapter(max_retries=retries))
    session.mount('https://', HTTPAdapter(max_retries=retries))

    return session

urls = ['https://httpbin.org/status/500']
count_queries = len(urls)
count_attempts = 0

with create_session() as s:
    for url in urls:
        response = s.get(url)
        count_attempts += response.total_retries
</code></pre>
<p>Since there is no such variable, I am looking for alternatives to count the total number of retries.</p>
<p>While I am unable to identify an approach to this problem, I made the following observations during my search which is potentially helpful:</p>
<ul>
<li><code>urllib3</code> stores the retry-history in the Retry object. The <code>urllib3.HTTPResponse</code> stores the last Retry object (<a href=""https://github.com/urllib3/urllib3/blob/1dd0613b610df1269dbc11e9780b0fe7debaff3c/CHANGES.rst#117-2016-09-06"" rel=""nofollow noreferrer"">docs</a>). The <code>urllib3.HTTPResponse</code> (to be precise, its undecoded body) is stored in <code>requests.Response.raw</code>, however only when <code>stream=True</code> (<a href=""https://2.python-requests.org/en/master/user/advanced/#id5"" rel=""nofollow noreferrer"">docs</a>). In my understanding, I can't access this data.</li>
<li>One user provides a solution to a <a href=""https://stackoverflow.com/questions/51188661/adding-callback-function-on-each-retry-attempt-using-requests-urllib3"">similar question</a> that subclasses the <code>Retry</code> class. Essentially, a callback function is called which prints a string to a logger. This could be adapted to increment a counter instead of printing to logs. However, if possible, I prefer to track the retries specific to a particular <code>get</code>, as shown above, as opposed to all <code>get</code>s using the same session.</li>
<li>A very similar question was asked <a href=""https://stackoverflow.com/questions/58002114/history-of-retries-using-request-library"">here</a>, however no (working) solution was provided.</li>
</ul>
<p>I'm using Python 3.9, urllib3 1.26.8, requests 2.26.0.</p>
",6,1642706297,python;python-requests;urllib3,True,2187,1,1642938526,https://stackoverflow.com/questions/70792216/count-number-of-retries-for-each-request
70788238,What does: &quot;Unverified HTTPS request is being made&quot; mean?,"<p>First, I would not consider this a duplicate of <a href=""https://stackoverflow.com/questions/27981545/suppress-insecurerequestwarning-unverified-https-request-is-being-made-in-python"">Suppress Insecure Request Warning</a>, but I may be wrong.</p>
<p>When I try to run a external script (not important what the script is doing) I get the following message:</p>
<blockquote>
<p>InsecureRequestWarning: Unverified HTTPS request is being made to host
<code>myHost.example</code>. Adding certificate verification is strongly advised.
See: <a href=""https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings"" rel=""nofollow noreferrer"">Urllib3 SSL-warnings</a>.</p>
</blockquote>
<p>From my understanding, I know this occur because the certificate of the server which the request is being sent to has not been provided/installed by the client, but what does <code>Unverified HTTPS request</code> really mean? My understanding is that it is the same thing as when you visit a website with a untrusted certificate (e.g self signed) and are shown a warning site: &quot;Are you sure you want to proceed?&quot;  The traffic is still being encrypted though it is not trusted, same in this case, right?</p>
<p>I'd appreciate if anyone could verify this, or explain it.</p>
",4,1642689942,python;urllib3,False,3758,0,1642690970,https://stackoverflow.com/questions/70788238/what-does-unverified-https-request-is-being-made-mean
26109264,"pip, proxy authentication and &quot;Not supported proxy scheme&quot;","<p>Trying to install pip on a new python installation. I am stuck with proxy errors. Looks like a bug in <code>get-pip</code> or <code>urllib3</code>??</p>

<p>Question is do I have to go through the pain of setting up <a href=""https://stackoverflow.com/questions/14149422/using-pip-behind-a-proxy"">CNTLM as described here</a> or is there a shortcut?</p>

<p><a href=""http://pip.readthedocs.org/en/latest/installing.html"" rel=""noreferrer"">get-pip.py documentation</a> says use <code>--proxy=""[user:passwd@]proxy.server:port""</code> option to specify proxy and relevant authentication. But seems like pip passes on the whole thing as it is to <code>urllib3</code> which interprets ""myusr"" as the url scheme, because of the ':' I guess (?).</p>

<pre><code>C:\ProgFiles\Python27&gt;get-pip.py --proxy myusr:mypswd@111.222.333.444:80
Downloading/unpacking pip
Cleaning up...
Exception:
Traceback (most recent call last):
  File ""c:\users\sg0219~1\appdata\local\temp\tmpxwg_en\pip.zip\pip\basecommand.py"", line 122, in main
    status = self.run(options, args)
  File ""c:\users\sg0219~1\appdata\local\temp\tmpxwg_en\pip.zip\pip\commands\install.py"", line 278, in run
    requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)
  File ""c:\users\sg0219~1\appdata\local\temp\tmpxwg_en\pip.zip\pip\req.py"", line 1177, in prepare_files
    url = finder.find_requirement(req_to_install, upgrade=self.upgrade)
  File ""c:\users\sg0219~1\appdata\local\temp\tmpxwg_en\pip.zip\pip\index.py"", line 194, in find_requirement
    page = self._get_page(main_index_url, req)
  File ""c:\users\sg0219~1\appdata\local\temp\tmpxwg_en\pip.zip\pip\index.py"", line 568, in _get_page
    session=self.session,
  File ""c:\users\sg0219~1\appdata\local\temp\tmpxwg_en\pip.zip\pip\index.py"", line 670, in get_page
    resp = session.get(url, headers={""Accept"": ""text/html""})
  File ""c:\users\sg0219~1\appdata\local\temp\tmpxwg_en\pip.zip\pip\_vendor\requests\sessions.py"", line 468, in get
    return self.request('GET', url, **kwargs)
  File ""c:\users\sg0219~1\appdata\local\temp\tmpxwg_en\pip.zip\pip\download.py"", line 237, in request
    return super(PipSession, self).request(method, url, *args, **kwargs)
  File ""c:\users\sg0219~1\appdata\local\temp\tmpxwg_en\pip.zip\pip\_vendor\requests\sessions.py"", line 456, in request
    resp = self.send(prep, **send_kwargs)
  File ""c:\users\sg0219~1\appdata\local\temp\tmpxwg_en\pip.zip\pip\_vendor\requests\sessions.py"", line 559, in send
    r = adapter.send(request, **kwargs)
  File ""c:\users\sg0219~1\appdata\local\temp\tmpxwg_en\pip.zip\pip\_vendor\requests\adapters.py"", line 305, in send
    conn = self.get_connection(request.url, proxies)
  File ""c:\users\sg0219~1\appdata\local\temp\tmpxwg_en\pip.zip\pip\_vendor\requests\adapters.py"", line 215, in get_connection
    block=self._pool_block)
  File ""c:\users\sg0219~1\appdata\local\temp\tmpxwg_en\pip.zip\pip\_vendor\requests\packages\urllib3\poolmanager.py"", line 258, in proxy_fro
m_url
    return ProxyManager(proxy_url=url, **kw)
  File ""c:\users\sg0219~1\appdata\local\temp\tmpxwg_en\pip.zip\pip\_vendor\requests\packages\urllib3\poolmanager.py"", line 214, in __init__
    'Not supported proxy scheme %s' % self.proxy.scheme
AssertionError: Not supported proxy scheme myusr

Storing debug log for failure in C:\Users\myusr\pip\pip.log

C:\ProgFiles\Python27&gt;
</code></pre>

<p>When I run the command without the usrname and password it works fine, but proxy rejects the request saying it needs authentication (""407 authenticationrequired"").</p>

<pre><code>C:\ProgFiles\Python27&gt;get-pip.py --proxy 111.222.333.444:80
Downloading/unpacking pip
  Cannot fetch index base URL https://pypi.python.org/simple/
  Could not find any downloads that satisfy the requirement pip
Cleaning up...
No distributions at all found for pip
Storing debug log for failure in C:\Users\sg0219898\pip\pip.log

C:\ProgFiles\Python27&gt;cat C:\Users\sg0219898\pip\pip.log
------------------------------------------------------------
C:\ProgFiles\Python27\get-pip.py run on 09/29/14 16:23:26
Downloading/unpacking pip
  Getting page https://pypi.python.org/simple/pip/
  Could not fetch URL https://pypi.python.org/simple/pip/: connection error: ('Cannot connect to proxy.', error('Tunnel connection failed: 407 authenticationrequired',))
  Will skip URL https://pypi.python.org/simple/pip/ when looking for download links for pip
  Getting page https://pypi.python.org/simple/
  Could not fetch URL https://pypi.python.org/simple/: connection error: ('Cannot connect to proxy.', error('Tunnel connection failed: 407 authenticationrequired',))
  Will skip URL https://pypi.python.org/simple/ when looking for download links for pip
  Cannot fetch index base URL https://pypi.python.org/simple/
  URLs to search for versions for pip:
  * https://pypi.python.org/simple/pip/
  Getting page https://pypi.python.org/simple/pip/
  Could not fetch URL https://pypi.python.org/simple/pip/: connection error: ('Cannot connect to proxy.', error('Tunnel connection failed: 407 authenticationrequired',))
  Will skip URL https://pypi.python.org/simple/pip/ when looking for download links for pip
  Could not find any downloads that satisfy the requirement pip
Cleaning up...
  Removing temporary dir c:\users\sg0219~1\appdata\local\temp\pip_build_SG0219898...
No distributions at all found for pip
Exception information:
Traceback (most recent call last):
  File ""c:\users\sg0219~1\appdata\local\temp\tmp36ynxd\pip.zip\pip\basecommand.py"", line 122, in main
    status = self.run(options, args)
  File ""c:\users\sg0219~1\appdata\local\temp\tmp36ynxd\pip.zip\pip\commands\install.py"", line 278, in run
    requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)
  File ""c:\users\sg0219~1\appdata\local\temp\tmp36ynxd\pip.zip\pip\req.py"", line 1177, in prepare_files
    url = finder.find_requirement(req_to_install, upgrade=self.upgrade)
  File ""c:\users\sg0219~1\appdata\local\temp\tmp36ynxd\pip.zip\pip\index.py"", line 277, in find_requirement
    raise DistributionNotFound('No distributions at all found for %s' % req)
DistributionNotFound: No distributions at all found for pip

C:\ProgFiles\Python27&gt;
</code></pre>

<p>I had a brief look at <a href=""https://github.com/shazow/urllib3/blob/master/urllib3/poolmanager.py"" rel=""noreferrer""><code>urllib3\poolmanager.py</code></a> and it doesn't seem to have anything to do with username/passwords.</p>
",21,1412026276,python;pip;urllib;pypi;urllib3,True,85838,9,1641968526,https://stackoverflow.com/questions/26109264/pip-proxy-authentication-and-not-supported-proxy-scheme
70675499,JSON from an API request stores as a single key:value how do I convert it to a dictionary?,"<p>I've been attempting to use a get request from a rest API call.  When putting the following url in for a link the data displays in JSON.</p>
<pre><code>https://rxnav.nlm.nih.gov/REST/rxclass/classMembers.json?classId=C&amp;relaSource=ATC&amp;trans=0
</code></pre>
<p>Below is my code, which is mostly commented and mostly what I've attempted.</p>
<pre><code>import urllib3
from urllib3 import request
import json
import ndjson
import pandas as pd
from pandas.io.json import json_normalize

http = urllib3.PoolManager()
url ='https://rxnav.nlm.nih.gov/REST/rxclass/classMembers.json?classId=C&amp;relaSource=ATC&amp;trans=0'
resp = http.request(&quot;GET&quot;, url)
data = json.loads(resp.data.decode('utf-8'))


#with open(data) as f:                       ##expected str, byes or os.pathlike object not dict.  This was for a malformed json test.
#    source = f.read()
#    source = source.replace('}{', '}\n{')
#    data = ndjson.loads(source)

#print(data)

#print(data.key())  # drugMemberGroup
#print(data.values()) #prints the entire json

df = pd.json_normalize(data, 'drugMemberGroup')  # prints the entire json since it's stored as a single key
#print(df.head(10))

#with open(df) as f:                      # doesn't change anything still prints a single key:value for df
#    source = f.read()
#    source = source.replace('}{', '}\n{')
#    data = ndjson.loads(source)
#print(df)

#print(df.key())

#b = json.loads(data)  #TypeError: the JSON object must be str, bytes or bytearray, not dict
#print(b[&quot;rxcui&quot;])   #if I use pdf instead of data then I get the entire json for a single key

#rxcuijson = json.dumps(data)  #TypeError: the JSON object must be str, bytes or bytearray, not dict
#print(rxcuijson) #if I use pdf instead of data then I get the entire json for a single key
</code></pre>
<p>I've pasted the JSON in a JSON viewer and the tree looks like the following, so I don't believe it's malformed JSON.  However, I did try ndjson just in case.:</p>
<pre><code>object      {1}
    drugMemberGroup     {1}
    drugMember      [326]
    0       {2}
    minConcept      {3}
rxcui   :   1007489
name    :   felodipine / metoprolol
tty :   MIN
    nodeAttr        [3]
    0       {2}
attrName    :   SourceId
attrValue   :   C07FB02
    1       {2}
attrName    :   SourceName
attrValue   :   metoprolol and felodipine
    2       {2}
attrName    :   Relation
attrValue   :   INDIRECT
    1       {2}
    minConcept      {3}
rxcui   :   1009219
name    :   aliskiren / amlodipine
tty :   MIN
    nodeAttr        [3]
    0       {2}
attrName    :   SourceId
attrValue   :   C09XA53
    1       {2}
attrName    :   SourceName
attrValue   :   aliskiren and amlodipine
    2       {2}
attrName    :   Relation
attrValue   :   INDIRECT
    2       {2}
    minConcept      {3}
rxcui   :   1033889
name    :   amlodipine / perindopril
tty :   MIN
    nodeAttr        [3]
    0       {2}
attrName    :   SourceId
attrValue   :   C09BB04
    1       {2}
attrName    :   SourceName
attrValue   :   perindopril and amlodipine
    2       {2}
attrName    :   Relation
attrValue   :   INDIRECT
</code></pre>
<p>I'd like the JSON from the GET request to come through as a dictionary, so I can put it in tabular format, or work with it in a dataframe.  Everything I tried puts the JSON as a single key:value dictionary.  The only key available is the drugMemberGroup key, I believe my problem is around that.</p>
<p>As you can see I tried using pandas, json, ndjson, json_normalize, but nothing seems to work properly.  Anything that can point me in the correct direction would be appreciated.  Feel's like this should be an easy problem to solve, however it's been quite the headache.</p>
",0,1641952764,python;json;pandas;urllib3,True,358,1,1641953120,https://stackoverflow.com/questions/70675499/json-from-an-api-request-stores-as-a-single-keyvalue-how-do-i-convert-it-to-a-d
70413830,I try scrape barchart.com with &quot;html&quot; and urllib3.PoolManager(),"<p>this script retrieves the site data well but I don't know how to read the value ('lastPrice') &quot;&gt; 30.67 &lt;here someone can help me thank you</p>
<pre><code>#! /usr/bin/env python3
#-*- coding:Utf8 -*-
from lxml import html
import urllib3.request
url = &quot;https://www.barchart.com/stocks/quotes/$VIX&quot;
http = urllib3.PoolManager()
r = http.request('GET', url, headers={'User-agent':'Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.16 Safari/537.36','Cookie':'cookie_name=cookie_value'})        
page = r.data
data_string = page.decode('utf-8', errors='ignore')
tree = html.fromstring(data_string)
x_pat= &quot;/html/body/main/div/div[2]/div[2]/div/div[2]/div/div[1]/div/div[1]/div[2]/span[1]&quot;
valtab = tree.xpath(x_pat)
valtab= valtab[0].text
print(valtab.strip())
#[[ item.lastPrice ]]
# I also try!!!!!
valtab = tree.xpath(x_pat)
print(valtab)
#[&lt;Element span at 0x38700e0&gt;]
for line in valtab:
    l=line.items()
    print(l)
    #[('class', 'last-change'), ('data-ng-class', &quot;highlightValue('lastPrice')&quot;)]
    l=line.text_content()
    print(l)
    #[[ item.lastPrice ]]
</code></pre>
",0,1639935386,python;xpath;request;urllib3,False,58,0,1639936414,https://stackoverflow.com/questions/70413830/i-try-scrape-barchart-com-with-html-and-urllib3-poolmanager
70262272,Django: Could not found exception in Traceback,"<p>I have facing some issue with python requests in a Django project. It only occur in 2nd requests.post().</p>
<p>Although It exited with exception <code>TypeError: getresponse() got an unexpected keyword argument 'buffering'</code>.
But after updating urllib3. There is no exception in traceback.</p>
<pre><code>[2021-12-08 15:54:48 +0000] [11410] [CRITICAL] WORKER TIMEOUT (pid:11439)

Traceback (most recent call last):
  File &quot;/home/ubuntu/projects/project/api/views/a_view.py&quot;, line 765, in create_power_trace
    headers=power_trace_headers)
  File &quot;/home/ubuntu/projects/venv/lib/python3.7/site-packages/requests/api.py&quot;, line 117, in post
    return request('post', url, data=data, json=json, **kwargs)
  File &quot;/home/ubuntu/projects/venv/lib/python3.7/site-packages/requests/api.py&quot;, line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File &quot;/home/ubuntu/projects/venv/lib/python3.7/site-packages/requests/sessions.py&quot;, line 542, in request
    resp = self.send(prep, **send_kwargs)
  File &quot;/home/ubuntu/projects/venv/lib/python3.7/site-packages/requests/sessions.py&quot;, line 655, in send
    r = adapter.send(request, **kwargs)
  File &quot;/home/ubuntu/projects/venv/lib/python3.7/site-packages/requests/adapters.py&quot;, line 449, in send
    timeout=timeout
  File &quot;/home/ubuntu/projects/venv/lib/python3.7/site-packages/urllib3/connectionpool.py&quot;, line 677, in urlopen
    chunked=chunked,
  File &quot;/home/ubuntu/projects/venv/lib/python3.7/site-packages/urllib3/connectionpool.py&quot;, line 426, in _make_request
    six.raise_from(e, None)
  File &quot;&lt;string&gt;&quot;, line 3, in raise_from
  File &quot;/home/ubuntu/projects/venv/lib/python3.7/site-packages/urllib3/connectionpool.py&quot;, line 421, in _make_request
    httplib_response = conn.getresponse()
  File &quot;/usr/local/lib/python3.7/http/client.py&quot;, line 1373, in getresponse
    response.begin()
  File &quot;/usr/local/lib/python3.7/http/client.py&quot;, line 319, in begin
    version, status, reason = self._read_status()
  File &quot;/usr/local/lib/python3.7/http/client.py&quot;, line 280, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), &quot;iso-8859-1&quot;)
  File &quot;/usr/local/lib/python3.7/socket.py&quot;, line 589, in readinto
    return self._sock.recv_into(b)
  File &quot;/home/ubuntu/projects/venv/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 201, in handle_abort
    sys.exit(1)
SystemExit: 1 
</code></pre>
<p>For your information, my code is something like this:-</p>
<pre><code>res1 = requests.post(url1, data=data1)
result = res1.json()
print(result['id'])   # successfully prints
data2 = {'id': result['id']}
res2 = requests.post(url2, data=data2)   # System exited
print(res2)
</code></pre>
<p>Above snippet works fine outside of my project (tested in a different script with same environment same instance).
And of course, no issue found in local.</p>
",0,1638888707,python;django;nginx;python-requests;urllib3,False,258,1,1639071518,https://stackoverflow.com/questions/70262272/django-could-not-found-exception-in-traceback
70176892,"HTTP header cut in half with `urllib3.exceptions.HeaderParsingError: [MissingHeaderBodySeparatorDefect()], unparsed data`","<p>I spotted a weird warning in logs:</p>
<pre><code>[WARNING] urllib3.connectionpool:467: Failed to parse headers (url=https://REDACTED): [MissingHeaderBodySeparatorDefect()], unparsed data: 'trol,Content-Type\r\n\r\n'
Traceback (most recent call last):
File &quot;/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py&quot;, line 465, in _make_request
assert_header_parsing(httplib_response.msg)
File &quot;/usr/local/lib/python3.8/dist-packages/urllib3/util/response.py&quot;, line 91, in assert_header_parsing
raise HeaderParsingError(defects=defects, unparsed_data=unparsed_data)
urllib3.exceptions.HeaderParsingError: [MissingHeaderBodySeparatorDefect()], unparsed data: 'trol,Content-Type\r\n\r\n'
</code></pre>
<p>This is from calling a standard <code>requests.post()</code> on a web service I fully control (a Python app behind nginx).</p>
<p>When I turn on <code>debuglevel=1</code> in <code>http.client.HTTPResponse</code> I see this:</p>
<pre><code>reply: 'HTTP/1.1 200 OK\r\n'
header: Server: nginx/1.18.0 (Ubuntu)
header: Date: Tue, 30 Nov 2021 22:14:04 GMT
header: Content-Type: application/json
header: Transfer-Encoding: chunked
header: Connection: keep-alive
header: Vary: Accept-Encoding
header: Access-Control-Allow-Origin: *
header: Access-Control-Allow-Credentials: true
header: Access-Control-Allow-Methods: GET, POST, OPTIONS
header: Access-Control-Allow-Headers: DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Con
</code></pre>
<p>Note the last header ending abruptly in <code>,If-Modified-Since,Cache-Con</code>.</p>
<p>Clearly, <code>requests==2.26.0</code> (via <code>urllib3==1.26.7</code> via <code>http.client</code>) cuts the last header in half for some reason during parsing, and then later complains it has &quot;left over&quot; data with the remaining <code>trol,Content-Type\r\n\r\n</code>.</p>
<p>In this case the warning is not critical, because the header is not really needed. But it's scary this is happening, because… what else is being cut / misparsed?</p>
<p>The same endpoint works fine from e.g. <code>curl</code>:</p>
<pre class=""lang-sh prettyprint-override""><code>$ curl -i -XPOST https://REDACTED
HTTP/1.1 200 OK
Server: nginx/1.18.0 (Ubuntu)
Date: Sat, 04 Dec 2021 20:08:59 GMT
Content-Type: application/json
Content-Length: 53
Connection: keep-alive
Vary: Accept-Encoding
Access-Control-Allow-Origin: *
Access-Control-Allow-Credentials: true
Access-Control-Allow-Methods: GET, POST, OPTIONS
Access-Control-Allow-Headers: DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Con
trol,Content-Type

…JSON response…
</code></pre>
<p>Any idea what could be wrong? Many thanks.</p>
",2,1638311084,python;nginx;python-requests;python-3.8;urllib3,True,1396,1,1638789564,https://stackoverflow.com/questions/70176892/http-header-cut-in-half-with-urllib3-exceptions-headerparsingerror-missinghea
69890084,Mocking urllib3.PoolManager().request function with python,"<p>I have a function that makes a POST request using urllib3.PoolManager(). Now in the unit test I want to mock said request but having some difficulty. What's the correct way to do it?</p>
<p>My code:</p>
<pre><code>http = urllib3.PoolManager()
# my func
def func(event, context):
    ...
    resp = http.request('POST', url, body=encoded_msg)
    ...

# unit test
@patch('urllib3.PoolManager.request')
def test_lambda_handler(self, mock_instance):
    mock_instance.return_value = Mock(status = &quot;200&quot;, data = &quot;success&quot;)
    res = func(event, [])
    mock_instance.request.assert_called_once()
</code></pre>
<p>I get this error &quot;AssertionError: Expected 'request' to have been called once. Called 0 times.&quot;</p>
",1,1636406540,python;unit-testing;mocking;urllib3,True,4379,1,1636429221,https://stackoverflow.com/questions/69890084/mocking-urllib3-poolmanager-request-function-with-python
69696344,Syntax Error trying to open URL in XML Format,"<p>Python3
The URLs I'm trying to open:
<a href=""https://api.evemarketer.com/ec/marketstat?typeid=3534&amp;usesystem=30000142"" rel=""nofollow noreferrer"">https://api.evemarketer.com/ec/marketstat?typeid=3534&amp;usesystem=30000142</a>
The typeID is variable.</p>
<p>I've tried many different ways of opening this and have reached the furthest I can get. Here's the code:</p>
<pre><code>import xlrd
import xlwt
import urllib3
import xml.etree.ElementTree as ET
import xmltodict
loc = &quot;C:\\Users\\nhoff\\AutomatedTrader\\Data.xls&quot;
wb = xlrd.open_workbook(loc)
sheet = wb.sheet_by_index(0)
typeID = sheet.col_values(0)
wb = xlwt.Workbook()
score = 1
http = urllib3.PoolManager()
for i in typeID[1:]:
    url = &quot;https://api.evemarketer.com/ec/marketstat?typeid=&quot; + str(i) + &quot;&amp;usesystem=30000142&quot;
    try:
        resp = http.request(&quot;GET&quot;, url)
        print(resp.data.decode('utf-8'))
    except urllib3.exceptions.HTTPError as error:
        print(error)
</code></pre>
<p>The url reads as a b-string so I have the decode. When it runs, it doesn't run the exception. It appears to actually open. But when I run that print, I get this:</p>
<pre><code>&quot;strconv.ParseUint: parsing \&quot;18.0\&quot;: invalid syntax&quot;
&quot;strconv.ParseUint: parsing \&quot;19.0\&quot;: invalid syntax&quot;
&quot;strconv.ParseUint: parsing \&quot;20.0\&quot;: invalid syntax&quot;
&quot;strconv.ParseUint: parsing \&quot;21.0\&quot;: invalid syntax&quot;
</code></pre>
<p>I don't know what that invalid syntax is. The data is XML. Any ideas?</p>
",0,1635075360,python;url;urllib3,False,76,0,1635075360,https://stackoverflow.com/questions/69696344/syntax-error-trying-to-open-url-in-xml-format
69494186,Failed to call gmail api,"<p>Calling gmail api does not work
Running the example quickstart.py gives an error
When the program runs to &quot;# Call the Gmail API</p>
<pre><code>results = service.users().labels().list(userId='me').execute()
labels = results.get('labels', [])&quot; 
</code></pre>
<p>And it gives error Who has encountered the same error as me and how did you solve it?</p>
<p>The code is as follows：</p>
<pre><code>def main():
    creds = None
    if os.path.exists('token.json'):
        creds = Credentials.from_authorized_user_file('token.json', SCOPES)
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file(
                'credentials.json', SCOPES)
            creds = flow.run_local_server(port=0)
        with open('token.json', 'w') as token:
            token.write(creds.to_json())

    service = build('gmail', 'v1', credentials=creds)

    # Call the Gmail API
    results = service.users().labels().list(userId='me').execute()
    labels = results.get('labels', [])

    if not labels:
        print('No labels found.')
    else:
        print('Labels:')
        for label in labels:
            print(label['name'])
</code></pre>
<p>Error message：</p>
<pre><code>Traceback (most recent call last):
  File &quot;D:\Python\lib\site-packages\googleapiclient\http.py&quot;, line 190, in _retry_request
    resp, content = http.request(uri, method, *args, **kwargs)
  File &quot;D:\Python\lib\site-packages\google_auth_httplib2.py&quot;, line 225, in request
    **kwargs
  File &quot;D:\Python\lib\site-packages\httplib2\__init__.py&quot;, line 1712, in request
    conn, authority, uri, request_uri, method, body, headers, redirections, cachekey,
  File &quot;D:\Python\lib\site-packages\httplib2\__init__.py&quot;, line 1427, in _request
    (response, content) = self._conn_request(conn, request_uri, method, body, headers)
  File &quot;D:\Python\lib\site-packages\httplib2\__init__.py&quot;, line 1349, in _conn_request
    conn.connect()
  File &quot;D:\Python\lib\site-packages\httplib2\__init__.py&quot;, line 1185, in connect
    raise socket_err
  File &quot;D:\Python\lib\site-packages\httplib2\__init__.py&quot;, line 1141, in connect
    self.sock = self._context.wrap_socket(sock, server_hostname=self.host)
  File &quot;D:\Python\lib\ssl.py&quot;, line 423, in wrap_socket
    session=session
  File &quot;D:\Python\lib\ssl.py&quot;, line 870, in _create
    self.do_handshake()
  File &quot;D:\Python\lib\ssl.py&quot;, line 1139, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host
</code></pre>
<p>I've been struggling with this for days, does anyone know how to fix it. thanks</p>
",0,1633687982,python;http;gmail-api;urllib3,False,119,0,1634090911,https://stackoverflow.com/questions/69494186/failed-to-call-gmail-api
69407649,Takes 1 positional argument but 2 error while python requests module,"<p>I have the following test code</p>
<pre><code>import requests 
import json

r = requests.get('https://api.github.com/user')
print(r.status_code)
</code></pre>
<p>This is getting the following error, I reinstalled requests and urllib3 modules but didn't help</p>
<pre><code>File &quot;/data//temp/scratchprojects/data structures/test.py&quot;, line 4, in &lt;module&gt;
    r = requests.get('https://api.github.com/user')  
File &quot;/data/my_venv/lib/python3.8/site-packages/requests/api.py&quot;, line 75, in get
    return request('get', url, params=params, **kwargs)  
  File &quot;/data/my_venv/lib/python3.8/site-packages/requests/api.py&quot;, line 61, in request
    return session.request(method=method, url=url, **kwargs)  
  File &quot;/data/my_venv/lib/python3.8/site-packages/requests/sessions.py&quot;, line 542, in request  
    resp = self.send(prep, **send_kwargs)  
  File &quot;/data/my_venv/lib/python3.8/site-packages/requests/sessions.py&quot;, line 655, in send
    r = adapter.send(request, **kwargs)  
  File &quot;/data/my_venv/lib/python3.8/site-packages/requests/adapters.py&quot;, line 412, in send
    conn = self.get_connection(request.url, proxies)  
  File &quot;/data/my_venv/lib/python3.8/site-packages/requests/adapters.py&quot;, line 315, in get_connection
    conn = self.poolmanager.connection_from_url(url)  
  File &quot;/data/my_venv/lib/python3.8/site-packages/urllib3/poolmanager.py&quot;, line 298, in connection_from_url
    return self.connection_from_host(  
  File &quot;/data/my_venv/lib/python3.8/site-packages/urllib3/poolmanager.py&quot;, line 245, in connection_from_host
    return self.connection_from_context(request_context)  
  File &quot;/data/my_venv/lib/python3.8/site-packages/urllib3/poolmanager.py&quot;, line 260, in connection_from_context
    return self.connection_from_pool_key(pool_key, request_context=request_context)  
  File &quot;/data/my_venv/lib/python3.8/site-packages/urllib3/poolmanager.py&quot;, line 281, in connection_from_pool_key
    pool = self._new_pool(scheme, host, port, request_context=request_context)  
  File &quot;/data/my_venv/lib/python3.8/site-packages/urllib3/poolmanager.py&quot;, line 213, in _new_pool
    return pool_cls(host, port, **request_context)  
  File &quot;/data/my_venv/lib/python3.8/site-packages/urllib3/connectionpool.py&quot;, line 906, in __init__
    HTTPConnectionPool.__init__(  
  File &quot;/data/my_venv/lib/python3.8/site-packages/urllib3/connectionpool.py&quot;, line 197, in __init__
    self.pool = self.QueueCls(maxsize)  
TypeError: __init__() takes 1 positional argument but 2 were given  
</code></pre>
",0,1633098781,python;python-requests;urllib3,False,294,1,1633926998,https://stackoverflow.com/questions/69407649/takes-1-positional-argument-but-2-error-while-python-requests-module
69505218,How to download server SSL certificate with urllib3?,"<p>The title really is the question — how do I get <code>urllib3</code> to download the SSL cert from the remote server when trying to make an HTTPS connection?</p>
<p><strong>Background</strong>
Over in ServerVault land, I'm trying to get to the bottom of <a href=""https://serverfault.com/questions/1079741/how-to-diagnose-fix-cloudformation-autoscaling-ssl-errors-on-file-download"">a problem with AWS autoscaling groups</a> failing to be able to download phpMyAdmin as part of their init process. As part of trying to diagnose that, I'm now over in StackOverflow land, working with python scripts. Here's where I am:</p>
<pre><code>#! /usr/bin/python3
import cfnbootstrap
from cfnbootstrap.packages import requests
from requests import utils
from requests.utils import DEFAULT_CA_BUNDLE_PATH
from requests.packages import urllib3

conn = urllib3.connection_from_url(&quot;https://dev.mysql.com/&quot;, retries=False)
conn.cert_reqs = 'CERT_REQUIRED'
conn.ca_certs = DEFAULT_CA_BUNDLE_PATH
response = conn.request(&quot;GET&quot;, &quot;/get/&quot;)
conn.close()
print(response.status)
</code></pre>
<p>That works fine:</p>
<pre><code>$ ./urltest.py 
404
</code></pre>
<p>However, if I change it to be</p>
<pre><code>conn = urllib3.connection_from_url(&quot;https://www.phpmyadmin.net&quot;, retries=False)
response = conn.request(&quot;GET&quot;, &quot;/downloads/&quot;)
</code></pre>
<p>I run into my problem:</p>
<pre><code>$ ./urltest.py 
Traceback (most recent call last):
  File &quot;/usr/lib/python3.7/site-packages/cfnbootstrap/packages/requests/packages/urllib3/connectionpool.py&quot;, line 544, in urlopen
    body=body, headers=headers)
  File &quot;/usr/lib/python3.7/site-packages/cfnbootstrap/packages/requests/packages/urllib3/connectionpool.py&quot;, line 341, in _make_request
    self._validate_conn(conn)
  File &quot;/usr/lib/python3.7/site-packages/cfnbootstrap/packages/requests/packages/urllib3/connectionpool.py&quot;, line 762, in _validate_conn
    conn.connect()
  File &quot;/usr/lib/python3.7/site-packages/cfnbootstrap/packages/requests/packages/urllib3/connection.py&quot;, line 238, in connect
    ssl_version=resolved_ssl_version)
  File &quot;/usr/lib/python3.7/site-packages/cfnbootstrap/packages/requests/packages/urllib3/util/ssl_.py&quot;, line 265, in ssl_wrap_socket
    return context.wrap_socket(sock, server_hostname=server_hostname)
  File &quot;/usr/lib64/python3.7/ssl.py&quot;, line 423, in wrap_socket
    session=session
  File &quot;/usr/lib64/python3.7/ssl.py&quot;, line 870, in _create
    self.do_handshake()
  File &quot;/usr/lib64/python3.7/ssl.py&quot;, line 1139, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1091)
</code></pre>
<p>It seems very unlikely that the certificate has expired, as if I go check manually by using a web browser, all is good with the world; however, I'd like to see exactly what certificate <code>urllib3</code> is seeing as having expired. How do I do this?</p>
<p><strong>NB</strong>: I'm not calling <code>certify.where()</code> directly, as I'm not sure what shananigans AWS is performing in the background, but since their scripts are auto-installed as part of booting the instance, there's not much I can do about it, so I'm trying to use their internal processes.</p>
",1,1633770085,python;ssl;aws-cloudformation;urllib3,True,1522,1,1633777624,https://stackoverflow.com/questions/69505218/how-to-download-server-ssl-certificate-with-urllib3
69129216,Network is unreachable for Flask app on Apache,"<p>I have a simple Flask application hosting a form for a data input into local db.
I am validating one of the field with wdsl service with python zeep (simply checks if VAT exists with <a href=""http://ec.europa.eu/taxation_customs/vies/checkVatService.wsdl"" rel=""nofollow noreferrer"">http://ec.europa.eu/taxation_customs/vies/checkVatService.wsdl</a>)
The application server is behind corporate proxy. Both http_proxy and https_proxy are set. wdsl service returns data as intended while testing (localhost, or running validation function directly with python)
When I run the app with Apache2, suddenly the network is unreachable. I believe I have missing some configuration in Flaskapp.conf which would route the traffic through the corporate proxy.
Could anybody help/direct me here?
Thank you.</p>
<p>urlib3 error:</p>
<pre><code>File &quot;/home/administrator/v_env/flaskapp/lib/python3.6/site-packages/urllib3/connection.py&quot;, line 170, in _new_conn
(self._dns_host, self.port), self.timeout, **extra_kw
File &quot;/home/administrator/v_env/flaskapp/lib/python3.6/site-packages/urllib3/util/connection.py&quot;, line 96, in create_connection
raise err
File &quot;/home/administrator/v_env/flaskapp/lib/python3.6/site-packages/urllib3/util/connection.py&quot;, line 86, in create_connection
sock.connect(sa)
OSError: [Errno 101] Network is unreachable 
</code></pre>
<p>Flaskapp.conf</p>
<pre><code>&lt;VirtualHost *:80&gt;
                ServerName $stationIP
                ServerAdmin name@domain.com
                WSGIScriptAlias / /var/www/FlaskApp/flaskapp.wsgi
                &lt;Directory /var/www/FlaskApp/FlaskApp/&gt;
                        Order allow,deny
                        Allow from all
                &lt;/Directory&gt;
                Alias /static /var/www/FlaskApp/FlaskApp/static
                &lt;Directory /var/www/FlaskApp/FlaskApp/static/&gt;
                        Order allow,deny
                        Allow from all
                &lt;/Directory&gt;
                ErrorLog ${APACHE_LOG_DIR}/error.log
                LogLevel warn
                CustomLog ${APACHE_LOG_DIR}/access.log combined
&lt;/VirtualHost&gt;
</code></pre>
<p>validation function</p>
<pre><code>    def validate_vat(self, vat):
        transport = Transport(timeout=10)
        try:
            client = Client('http://ec.europa.eu/taxation_customs/vies/checkVatService.wsdl', transport=transport)
            client.transport.session.proxies = {'http':'http://&lt;proxy.server.com&gt;:9090'}
            response = client.service.checkVat(vat.data[0:2].upper(), vat.data[2:])
        except:
            raise wtforms.validators.ValidationError('VAT validation service unavailable. Please try again later')

        if not response.valid:
            raise wtforms.validators.ValidationError('Not a valid VAT')
</code></pre>
",-1,1631261681,python;apache;flask;urllib3;zeep,True,256,1,1633682015,https://stackoverflow.com/questions/69129216/network-is-unreachable-for-flask-app-on-apache
69303407,Python: How to only URL Encode a specific URL Parameter?,"<p>I have some big URLs that contain a lot of URL parameters.</p>
<p>For my specific case, I need to URL Encode the content of one specific URL Parameter (q) when the content after the &quot;q=&quot; starts with a slash (&quot;/&quot;)</p>
<p><strong>Example URL:</strong></p>
<pre><code>https://www.exmple.com/test?test1=abc&amp;test2=abc&amp;test3=abc&amp;q=/&quot;TEST&quot;/&quot;TEST&quot;
</code></pre>
<p>How can I only URL encode that last part of the URL which is within the &quot;q&quot; parameter?</p>
<p>The output of this example should be:</p>
<pre><code>https://www.exmple.com/test?test1=abc&amp;test2=abc&amp;test3=abc&amp;q=%2F%22TEST%22%2F%22TEST%22%20
</code></pre>
<p>I already tried some different things with urllib.parse but it doesnt work the way I want it.</p>
<p>Thanks for your help!</p>
",0,1632413068,python;urllib;urlencode;urllib3,True,1480,2,1632498435,https://stackoverflow.com/questions/69303407/python-how-to-only-url-encode-a-specific-url-parameter
68707369,Max retries exceeded with url : &#39;Connection to api.telegram.org timed out. (connect timeout=5.0)&#39;)),"<p>Recently I created a Telegram-bot which the bot works fine at the beginning. Then after several period of time (say few hours), the bot start to throw the below exception which I really don't understand:</p>
<pre><code>Error while getting Updates: urllib3 HTTPError SOCKSHTTPSConnectionPool(host='api.telegram.org', port=443): Max retries exceeded with url: /bot1924749319:&lt;mytoken&gt;/getUpdates (Caused by ConnectTimeoutError(&lt;telegram.vendor.ptb_urllib3.urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x116208c90&gt;, 'Connection to api.telegram.org timed out. (connect timeout=5.0)'))
No error handlers are registered, logging exception.
Traceback (most recent call last):
  File &quot;/Library/anaconda3/lib/python3.7/site-packages/socks.py&quot;, line 809, in connect
    negotiate(self, dest_addr, dest_port)
  File &quot;/Library/anaconda3/lib/python3.7/site-packages/socks.py&quot;, line 444, in _negotiate_SOCKS5
    self, CONNECT, dest_addr)
  File &quot;/Library/anaconda3/lib/python3.7/site-packages/socks.py&quot;, line 524, in _SOCKS5_request
    resp = self._readall(reader, 3)
  File &quot;/Library/anaconda3/lib/python3.7/site-packages/socks.py&quot;, line 276, in _readall
    d = file.read(count - len(data))
  File &quot;/Library/anaconda3/lib/python3.7/socket.py&quot;, line 589, in readinto
    return self._sock.recv_into(b)
socket.timeout: timed out

During handling of the above exception, another exception occurred:
</code></pre>
<p>My code :</p>
<pre><code>def papa_BT_scan(context: telegram.ext.CallbackContext):
    ws_nonFG_bot_web_scraping.papa_bt_scan(context)

#Server Start
#===========================================================
def server_start(update: telegram.Update, context: telegram.ext.CallbackContext):
    print(&quot;Telegram_bot_nonFG Started.&quot;)
    context.bot.send_message(chat_id=update.message.chat_id,text=':)')
    context.job_queue.run_daily(papa_BT_scan,datetime.time(hour=18, minute=00, tzinfo=pytz.timezone('Asia/Hong_Kong')),context=update.message.chat_id)
    

if __name__ == &quot;__main__&quot;:

    REQUEST_KWARGS = {'proxy_url': 'socks5h://127.0.0.1:9150' }
    u = Updater('&lt;myToken&gt;', use_context=True,request_kwargs=REQUEST_KWARGS)
  

    start_handler = CommandHandler('s', server_start,pass_job_queue=True)
    u.dispatcher.add_handler(start_handler)
    
    u.job_queue.start()
    u.start_polling()
    u.idle()
</code></pre>
<p>I searched from the internet, most of the people advise to use Proxy / VPN (ExpressVPN) which I have tried but still no use.</p>
",0,1628488175,python;telegram-bot;python-telegram-bot;urllib3,True,5103,1,1629093931,https://stackoverflow.com/questions/68707369/max-retries-exceeded-with-url-connection-to-api-telegram-org-timed-out-conn
68785469,http.client.RemoteDisconnected: Remote end closed connection without response SELENIUM/PYTHON,"<p>I'm using selenium - chrome for scraping pages and sometimes I get such an errors while getting website:</p>
<pre><code>http.client.RemoteDisconnected: Remote end closed connection without response

urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))

urllib3.exceptions.NewConnectionError: : Failed to establish a new connection: [Errno 111] Connection refused

urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=40097): Max retries exceeded with url: /session/503e38e9827bff7335d467e8ba31cb5c/screenshot (Caused by NewConnectionError(': Failed to establish a new connection: [Errno 111] Connection refused'))
</code></pre>
<p>I tried with page strategy and different chrome options like:</p>
<pre><code>options.page_load_strategy = 'eager'
options.add_argument('--disable-dev-shm-usage')
options.add_argument('--disable-gpu')
options.add_argument('--enable-features=NetworkServiceInProcess')  
options.add_argument('--disable-features=NetworkService') 
</code></pre>
<p>Nothing eliminate my errors.</p>
<p>I'm using such chrome and chrome driver version in container:</p>
<ul>
<li>browserVersion - '92.0.4515.131'</li>
<li>chromedriverVersion - '92.0.4515.107'</li>
</ul>
<p>Can you help me solve this problem?</p>
",2,1628961144,python;selenium;google-chrome;web-scraping;urllib3,True,4426,1,1628977265,https://stackoverflow.com/questions/68785469/http-client-remotedisconnected-remote-end-closed-connection-without-response-se
68517065,HTTP Error404 in pytube,"<p>its supposed to be a youtube downloader, but whenever i try running the code i get &quot;urllib.error.HTTPError: HTTP Error 404: Not Found &quot; even though the url/link used is an valid url/link which is openable with any browser. ide used : pycharm v2021.1.3
&amp; pytube v10.9.3</p>
<pre><code>from pytube import YouTube
link = input(&quot;Enter Link Here : &quot;)
url = YouTube(link)
print(&quot;Downloading....&quot;)
video = url.streams.first()
video.download()
print(&quot;Downloaded&quot;)
</code></pre>
<p><strong>#alternate code</strong></p>
<pre><code>from pytube import YouTube

link = input(&quot;Enter Youtube URL : &quot;)
yt = YouTube(link)
videos = yt.streams.all()
# this will stream all the format available for the video
video = list(enumerate(videos))
# this will be index all the format in list starting with zero
for i in video:
    print(i)
    # this will print all the available format of video with proper index
print(&quot;Enter the desired option to download the format&quot;)
dn_option = int(input(&quot;Enter the option : &quot;))
# ask user that which format he want to download
dn_video = videos[dn_option]
dn_video.download()
# for downloading the video
print(&quot;Downloaded successfully&quot;)
</code></pre>
",0,1627203616,python;python-3.x;youtube;urllib3;pytube,True,245,1,1628974818,https://stackoverflow.com/questions/68517065/http-error404-in-pytube
46648030,Python Request GET with Client Certificate is failing,"<p>I'm trying to send an HTTP GET request using Python Requests to an API that is requiring a Client Certificate.  I'm passing in a PEM and Key file into the GET request using </p>

<pre><code>session = requests.Session()
session.get('https://localhost/rest/containers/7uyeogzKQayw4mmQmcJb2Q/listeners', cert=('development.pem', 'development.key'))
</code></pre>

<p>When making this call, I'm using NGINX as the endpoint, I'm getting:</p>

<pre><code>2017-10-07 21:18:16,874 - containerLogger - DEBUG - code:400   text:
&lt;html&gt;
&lt;head&gt;&lt;title&gt;400 No required SSL certificate was sent&lt;/title&gt;&lt;/head&gt;
&lt;body bgcolor=""white""&gt;
&lt;center&gt;&lt;h1&gt;400 Bad Request&lt;/h1&gt;&lt;/center&gt;
&lt;center&gt;No required SSL certificate was sent&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx/1.12.1&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>I've tested the same API using a POST request against the same endpoint using the same certificates and it is successful.  I've validated the same GET using a browser (Firefox) successfully.</p>

<p>I'm currently trying to figure out why the POST is successful, but the GET is failing. I'm not having any luck in searching for an answer.</p>

<p>Not sure if this is due to requests or urllib3. </p>

<p>Versions:</p>

<ul>
<li>Java 1.8</li>
<li>Jython 2.7.1</li>
<li>Python 2.7.10</li>
<li>Requests 2.18.1</li>
<li>urllib3</li>
<li>certifi 2017.04.17</li>
</ul>

<p>Any help is greatly appreciated.</p>

<p>Cheers - Erik</p>
",0,1507556761,python;python-requests;urllib3,True,15155,2,1628410160,https://stackoverflow.com/questions/46648030/python-request-get-with-client-certificate-is-failing
68677122,python : Read HTTPResponse twice,"<p>I have <code>urllib3.response.HTTPResponse</code> (a &quot;file-like&quot; object) and <code>lxml.html.lxml_parse</code> that takes only a file path, an URL or a &quot;file-like&quot; object (in case of <code>HTTPResponse</code> it also extracts the URL). Also I need the content from the response.</p>
<p>The answers from <a href=""https://stackoverflow.com/questions/3906137/why-cant-i-call-read-twice-on-an-open-file"">Why can&#39;t I call read() twice on an open file?</a> do not fit the situation since the stream consumes while reading into a variable and <code>.seek</code> is not defined for <code>HTTPResponse</code>.</p>
<p><code>copy.copy</code> and <code>copy.deepcopy</code> do not work too.</p>
",1,1628231468,python;stream;httpresponse;urllib3,False,332,0,1628231468,https://stackoverflow.com/questions/68677122/python-read-httpresponse-twice
41527106,Can&#39;t Import Requests into Python: ImportError no module named urllib3,"<p>I've searched on this but other examples I've come across are people having issues installing Requests, my issue is around importing the module:</p>

<p>Using Putty (connected to a HDF 2.4 sandbox session) and Python to call an API.</p>

<p>Last night, I had this working - installed the Requests module no problem, defined Python, and then made the GET request, worked perfect.</p>

<p>Tonight, I've followed the exact same steps, but when I try to 'Import Requests' I'm getting the error:</p>

<pre><code>ImportError: No module named urllib3
</code></pre>

<p>The steps I've replicated from last night are:</p>

<pre><code>[root@sandbox ~]# pip install requests  
python –v
import requests 
</code></pre>

<p>Figure I must have inadvertently changed something... Can anyone please advise?</p>
",7,1483826809,python;urllib3;hortonworks-sandbox,False,1171,1,1628023258,https://stackoverflow.com/questions/41527106/cant-import-requests-into-python-importerror-no-module-named-urllib3
68501158,Python: fetching urllib3 request headers,"<p>We are injecting tracing information into request headers of all the http request calls in our API client library which is implemented based on urllib3</p>
<pre><code>def _init_jaeger_tracer():
    '''Jaeger tracer initialization'''
    config = Config(
        config={
            'sampler': {
                'type': 'const',
                'param': 1,
            },
        },
        service_name=&quot;session&quot;
        )
    return config.new_tracer()

class APIObject(rest.APIObject):
    '''Class for injecting traces into urllib3 request headers'''
    def __init__(self, configuration):
        print(&quot;RESTClientObject child class called####&quot;)
        self._tracer = None
        super().__init__(configuration)
        self._tracer = _init_jaeger_tracer()

    # pylint: disable=W0221
    def request(self, method, url, *args, **kwargs):
        lower_method = method.lower()
        with self._tracer.start_active_span('requests.{}'.format(lower_method)) as scope:
            span = scope.span
            span.set_tag(tags.SPAN_KIND, tags.SPAN_KIND_RPC_CLIENT)
            span.set_tag(tags.COMPONENT, 'request')
            span.set_tag(tags.HTTP_METHOD, lower_method)
            span.set_tag(tags.HTTP_URL, url)
            headers = kwargs.setdefault('headers', {})
            self._tracer.inject(span.context, Format.HTTP_HEADERS, headers)
    return r
</code></pre>
<p>After such implementation, urllib3 request headers look like this,</p>
<pre><code>headers = {
    'Content-Type': 'application/json', 
    'User-Agent': 'API-Generator/1.0.0/python', 
    # **
    'uber-trace-id': '30cef3e816482516:1a4fed2c4863b2f6:0:1'
    # **
}
</code></pre>
<p>Now we have to obtain trace-id from the injected request header.</p>
<p>In python &quot;requests&quot; library we have &quot;response.request.headers&quot; attribute to return request headers</p>
<p>In urllib3 i could not find a way return request headers for further processing</p>
<p>Could any of you share some thoughts</p>
<p>Now from this request headers, we need to fetch the 'uber-trace-id' and its value for further building URL.</p>
",6,1627052071,python;urllib3;jaeger;opentracing,True,2187,1,1627409983,https://stackoverflow.com/questions/68501158/python-fetching-urllib3-request-headers
31778800,How can I make a Post Request on Python with urllib3?,"<p>I've been trying to make a request to an API, I have to pass the following body:</p>

<pre><code>{
""description"":""Tenaris"",
""ticker"":""TS.BA"",
""industry"":""Metalúrgica"",
""currency"":""ARS""
}
</code></pre>

<p>Altough the code seems to be right and it finished with ""Process finished with exit code 0"", it's not working well. I have no idea of what I'm missing but this is my code:</p>

<pre><code>http = urllib3.PoolManager()
http.urlopen('POST', 'http://localhost:8080/assets', headers={'Content-Type':'application/json'},
                 data={
""description"":""Tenaris"",
""ticker"":""TS.BA"",
""industry"":""Metalúrgica"",
""currency"":""ARS""
})
</code></pre>

<p>By the way, this the first day working with Python so excuse me if I'm not specific enough.</p>
",15,1438570515,python;json;curl;urllib3,True,76723,3,1626722563,https://stackoverflow.com/questions/31778800/how-can-i-make-a-post-request-on-python-with-urllib3
68404313,Does requests.session expires if remained idle?,"<p>I am using a request.session()  in a retry request function. I would like to know how long the session lasts. If the <code>session</code> object remains idle for some time,  would that expire after some time?</p>
<pre><code>def create_requests_retry_session(
        retries=app.config[&quot;MAX_RETRY_FOR_SESSION&quot;],
        backoff_factor=app.config[&quot;BACKOFF_FACTOR&quot;]):
    &quot;&quot;&quot;
    implement failure mechanism while calling microservices to avoid system alert
    using parameters like a time limit to get a response. This function will attempt 3 times
    per session
    retries: Defaults to MAX_RETRY_FOR_SESSION
    backoff_factor: how long the processes will sleep between failed requests.
    Defaults to BACKOFF_FACTOR {backoff factor} * (2 ** ({number of total
    retries} - 1)). If the backoff_factor is 0.1, then sleep() will sleep for [0.0s, 0.2s, 0.4s, …] between
    retries. 1 second the successive sleeps will be 0.5, 1, 2, 4, 8, 16, 32, 64, 128, 256. 2 seconds - 1, 2, 4,
    8, 16, 32, 64, 128, 256, 512 10 seconds - 5, 10, 20, 40, 80, 160, 320, 640, 1280, 2560
    :return: Session object
    &quot;&quot;&quot;
    session = requests.Session()
    retry = Retry(total=retries, backoff_factor=backoff_factor, method_whitelist=frozenset(['POST']))
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    return session
</code></pre>
<p>I am using the session object from the above function as session.post()</p>
",2,1626416405,python;request;urllib3,True,922,1,1626417021,https://stackoverflow.com/questions/68404313/does-requests-session-expires-if-remained-idle
67114391,Problem whith parsing EpicGames Store on Python,"<p>I have a problem parsing the EpicGames Store in python. I would like to receive information about free games. I am writing code, but I only get []. I tested it on other sites and everything was fine. I have Python 3.8.8</p>
<pre><code>from lxml.html import fromstring
import urllib.request

response = urllib.request.urlopen('https://www.epicgames.com/store/pl/').read()
page = fromstring(response)
games = page.xpath('/html/body/div[1]/div/div[4]/main/div/div[3]/div/div/span[4]/div/div/section/div/div[1]/div/div/a/div/div/div[3]/span[1]')
print(games)
</code></pre>
",1,1618512428,python;parsing;request;lxml;urllib3,True,382,1,1626224871,https://stackoverflow.com/questions/67114391/problem-whith-parsing-epicgames-store-on-python
68281850,Error &quot;Failed to establish a new connection: [Errno 111] Connection refused&quot; While subscribing server sent events channel sseclient in Python,"<p>I am new to python, I have SSE server on Node</p>
<p>while client is created in Python using sseclient(<a href=""https://pypi.org/project/sseclient/"" rel=""nofollow noreferrer"">https://pypi.org/project/sseclient/</a>) i am trying to subscribe to sse channel.
sample code:</p>
<pre><code>import json
import logging
import threading
import time
from sseclient import SSEClient

class SSE_Handler(threading.Thread):
    def __init__(self,url):
        threading.Thread.__init__(self)
        self.sse_events = SSEClient(url)
        for msg,event in enumerate(self.sse_events,start=1):
            try:                
                if event.event == &quot;SYSTEM_RESTART&quot;:
                    print(&quot;System restart event&quot;)
                    reboot_system()
                    
                elif event.event == &quot;SERVICE_RESTART&quot;:
                    print(&quot;Service restart event&quot;)
                    restart_service()
                
                else:
                    print(&quot;Invalid event&quot;)
            except Exception as e:
                logging.error(e)
</code></pre>
<p>if the server is not active or server is stopped
I am getting exception and client stops working:</p>
<pre><code>requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=81): Max retries exceeded with url: /comm (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0xb5a10df0&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))
</code></pre>
<p>I want client to wait for the server to restart or become active without throwing exception.</p>
",1,1625643289,python;python-3.x;server-sent-events;urllib3,False,527,0,1625643289,https://stackoverflow.com/questions/68281850/error-failed-to-establish-a-new-connection-errno-111-connection-refused-whi
64149304,Max retries exceeded,"<p>I'm trying to fetch a request from urllib3 and my code works. However, few websites like <a href=""https://hackershala.com"" rel=""nofollow noreferrer"">https://hackershala.com</a> and etc which uses different TLS version are not being able to be fetched.
I tried changing useragent, but it didn't work for obvious reasons.</p>
<pre><code>urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='hackershala.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: TLSV1_ALERT_PROTOCOL_VERSION] tlsv1 alert protocol version (_ssl.c:852)'),))
</code></pre>
<p>My code is</p>
<pre><code>import urllib3

http = urllib3.PoolManager()
url = input(&quot;Website URL: &quot;)
r = http.request(&quot;GET&quot;, url, headers={
    'User-Agent': 'Mozilla/5.0'
})
rp = r.status
print(rp)
</code></pre>
",0,1601528237,python;python-3.x;urllib;urllib3,False,558,1,1625543160,https://stackoverflow.com/questions/64149304/max-retries-exceeded
68171363,How to trigger pipeline API in AzureDevOps from Python (urllib3),"<p>I have to trigger a pipeline in Azure DevOps from a python script. I have already found out that i need a private access token and that part is fine. I can, however, not get the script to work. I am trying something like this:</p>
<pre><code>data = [
     {
     }
    ]
http = urllib3.PoolManager()
    r = http.request('POST', api_url, headers={'Content-Type': 'application/json-patch+json', &quot;Authorization&quot;: private_access_token}, body=data)
    print(r.status)
    print(r.data)
</code></pre>
<p>Its a requirement that i have to use urllib3 because I cant use the requests package</p>
<p>data is empty, because looking at the parameters here <a href=""https://learn.microsoft.com/en-us/rest/api/azure/devops/pipelines/runs/run%20pipeline?view=azure-devops-rest-6.0"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/azure/devops/pipelines/runs/run%20pipeline?view=azure-devops-rest-6.0</a>. Then i dont need any input data? I just want to trigger a pipeline, nothing else</p>
<p>Error message is not very helpful. I get error message 203.</p>
",1,1624927748,python;azure-devops;urllib3,True,1684,1,1624929513,https://stackoverflow.com/questions/68171363/how-to-trigger-pipeline-api-in-azuredevops-from-python-urllib3
68134116,AWS lambda call with urllib3.PoolManager().request() -&gt; {&#39;message&#39;: &#39;Forbidden&#39;},"<p>I have a lambda on AWS and I use it with <code>urllib3</code>.
It worked for a month.</p>
<p>Today I tried it and now I can't access it. it returns me this: <code>{'message': 'Forbidden'}</code></p>
<pre><code>import urllib3, json

http = urllib3.PoolManager(cert_reqs = 'CERT_NONE')#
http.verify = True

def get_request(url):
    print(&quot;sending GET request at&quot;,url)
    try:
        r = http.request('GET', url)
        response = json.loads(r.data.decode(&quot;utf-8&quot;))
        print(&quot;api_connector request:&quot;,response)
        return response[&quot;statusCode&quot;], response[&quot;body&quot;]
    except Exception as e:
        print(e)
        print(&quot;http request failed&quot;)
        return -1, str(e)

err, err_msg = api_connector.get_request(
            &quot;https://xxxxx.execute-api.eu-west-3.amazonaws.com/xxx/xxxxxx?filename=&quot; + filename)

</code></pre>
<p>I saw some posts about this error but:</p>
<p>1 - talking about crawling others' websites when I'm talking about my lambda! so nobody should stop me?</p>
<p>2 - it was working before and one day, it stopped... so what should I try? I'm kinda lost :/</p>
",0,1624637640,python;amazon-web-services;aws-lambda;urllib3,True,1698,1,1624868615,https://stackoverflow.com/questions/68134116/aws-lambda-call-with-urllib3-poolmanager-request-message-forbidden
54871840,Retry requests mechanism,"<p>Im trying to build web <strong>scraper</strong> project
one of the thing im trying to do is smart retry mechanism
using urlib3 and requests and beautiful soup</p>
<p>when im set the timeout=1
in order to fail the retry and check retry its break with exception
code below :</p>
<pre><code>import requests
import re
from bs4 import BeautifulSoup
import json
import time
import sys
from requests.adapters import HTTPAdapter
from urllib3.util import Retry

# this get_items methods is for getting dict of link to scrape items per link

def get_items(self, dict):
        itemdict = {}
        for k, v in dict.items():
            boolean = True
        # here, we fetch the content from the url, using the requests library
            while (boolean):
             try:
                a =requests.Session()
                retries = Retry(total=3, backoff_factor=0.1, status_forcelist=[301,500, 502, 503, 504])
                a.mount(('https://'), HTTPAdapter(max_retries=retries))
                page_response = a.get('https://www.XXXXXXX.il' + v, timeout=1)
             except requests.exceptions.Timeout:
                print  (&quot;Timeout occurred&quot;)
                logging.basicConfig(level=logging.DEBUG)
             else:
                 boolean = False

            # we use the html parser to parse the url content and store it in a variable.
            page_content = BeautifulSoup(page_response.content, &quot;html.parser&quot;)
            for i in page_content.find_all('div', attrs={'class':'prodPrice'}):
                parent = i.parent.parent.contents[0]
                getparentfunc= parent.find(&quot;a&quot;, attrs={&quot;href&quot;: &quot;javascript:void(0)&quot;})
                itemid = re.search(&quot;.*'(\d+)'.*&quot;, getparentfunc.attrs['onclick']).groups()[0]
                itemName = re.sub(r'\W+', ' ', i.parent.contents[0].text)
                priceitem = re.sub(r'[\D.]+ ', ' ', i.text)
                itemdict[itemid] = [itemName, priceitem]
</code></pre>
<p>ill be appreciate for efficiency retry mechanism resolve or any other simple method
Thanks
Iso</p>
",0,1551116555,python;web-scraping;beautifulsoup;python-requests;urllib3,True,771,1,1623153029,https://stackoverflow.com/questions/54871840/retry-requests-mechanism
47675138,How to override BACKOFF_MAX while working with requests retry,"<p>I am using requests module with python3 to implement Retry mechanism on failure.</p>

<p>Following is my code-</p>

<pre><code>session = session or requests.Session()
retry = Retry(
    total=retries,
    read=retries,
    connect=retries,
    backoff_factor=backoff_factor,
    status_forcelist=status_forcelist,
    method_whitelist=method_whitelist)
retry.BACKOFF_MAX = 60
adapter = HTTPAdapter(max_retries=retry)
session.mount(BASE_URL, adapter)
return session
</code></pre>

<p>I do not see any named argument to set maximum backoff (BACKOFF_MAX in Retry class). I do not want sleep time between retries more than 60 seconds. 
How can I achieve it? Resetting BACKOFF_MAX doesn't work</p>
",3,1512565875,python;python-3.x;python-requests;urllib3,True,2374,4,1622185697,https://stackoverflow.com/questions/47675138/how-to-override-backoff-max-while-working-with-requests-retry
57783287,Scrape table html multipage with beautifulsoup4 and urllib3,"<p>Help me please,,
the code I made only works for 1 page, I want it for all pages. what should I do?</p>

<pre><code>import csv 
import urllib3
from bs4 import BeautifulSoup


outfile = open(""data.csv"",""w"",newline='')
    writer = csv.writer(outfile)


    for i in range(1,20) :
            url = f'http://ciumi.com/cspos/barcode-ritel.php?page={i}'
            req = urllib3.PoolManager()
            res = req.request('GET', url)
            tree = BeautifulSoup(res.data, 'html.parser')  
            table_tag = tree.select(""table"")[0]
    tab_data = [[item.text for item in row_data.select(""th,td"")]
                    for row_data in table_tag.select(""tr"")]

    for data in tab_data:
        writer.writerow(data)
        print( res, url, ' '.join(data))
</code></pre>
",0,1567581373,python;beautifulsoup;scrape;urllib3,True,246,1,1621957355,https://stackoverflow.com/questions/57783287/scrape-table-html-multipage-with-beautifulsoup4-and-urllib3
67614642,Python requests/urrllib3 - Retry on read timeout after 200 header received,"<p>I'm using requests to download some large files (100-5000 MB).  I'm using session and urllib3.Retry to get automatic retries.
It appears such retries only applies before the HTTP Header has been received and content has started streaming. After 200 has been sent, a network dip will be raised as a ReadTimeoutError.</p>
<p>See following example:</p>
<pre class=""lang-py prettyprint-override""><code>import requests, logging
from requests.adapters import HTTPAdapter
from urllib3 import Retry


def create_session():
    retries = Retry(total=5, backoff_factor=1)
    s = requests.Session()
    s.mount(&quot;http://&quot;, HTTPAdapter(max_retries=retries))
    s.mount(&quot;https://&quot;, HTTPAdapter(max_retries=retries))
    return s

logging.basicConfig(level=logging.DEBUG, stream=sys.stderr)
session = create_session()
response = session.get(url, timeout=(120, 10)) # Deliberate short read-timeout
</code></pre>
<p>This gives following log output:</p>
<pre><code>DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): example:443
DEBUG:urllib3.connectionpool:https://example:443 &quot;GET /example.zip HTTP/1.1&quot; 200 1568141974

&lt; UNPLUG NETWORK CABLE FOR 10-15 sec HERE &gt; 

Traceback (most recent call last):
  File &quot;urllib3/response.py&quot;, line 438, in _error_catcher
    yield
  File &quot;urllib3/response.py&quot;, line 519, in read
    data = self._fp.read(amt) if not fp_closed else b&quot;&quot;
  File &quot;/usr/lib/python3.8/http/client.py&quot;, line 458, in read
    n = self.readinto(b)
  File &quot;/usr/lib/python3.8/http/client.py&quot;, line 502, in readinto
    n = self.fp.readinto(b)
  File &quot;/usr/lib/python3.8/socket.py&quot;, line 669, in readinto
    return self._sock.recv_into(b)
  File &quot;/usr/lib/python3.8/ssl.py&quot;, line 1241, in recv_into
    return self.read(nbytes, buffer)
  File &quot;/usr/lib/python3.8/ssl.py&quot;, line 1099, in read
    return self._sslobj.read(len, buffer)
socket.timeout: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;requests/models.py&quot;, line 753, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File &quot;urllib3/response.py&quot;, line 576, in stream
    data = self.read(amt=amt, decode_content=decode_content)
  File &quot;urllib3/response.py&quot;, line 541, in read
    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)
  File &quot;/usr/lib/python3.8/contextlib.py&quot;, line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File &quot;urllib3/response.py&quot;, line 443, in _error_catcher
    raise ReadTimeoutError(self._pool, None, &quot;Read timed out.&quot;)
urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='example', port=443): Read timed out.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;example.py&quot;, line 14, in _download
    response = session.get(url, headers=headers, timeout=300)
  File &quot;requests/sessions.py&quot;, line 555, in get
    return self.request('GET', url, **kwargs)
  File &quot;requests/sessions.py&quot;, line 542, in request
    resp = self.send(prep, **send_kwargs)
  File &quot;requests/sessions.py&quot;, line 697, in send
    r.content
  File &quot;requests/models.py&quot;, line 831, in content
    self._content = b''.join(self.iter_content(CONTENT_CHUNK_SIZE)) or b''
  File &quot;requests/models.py&quot;, line 760, in generate
    raise ConnectionError(e)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='example', port=443): Read timed out. 
</code></pre>
<p>I can sort of understand why this would not work, things get even more obvious when you add the <code>stream=True</code> argument together with <code>response.iter_content()</code>.
I assume the rationale is that the read_timeout and TCP layer should handle this (in my example i set read_timeout deliberately low to provoke it). But we have cases where servers restart/crash or where firewalls drop connections in the middle of a stream and the only option for a client is to retry the entire thing.</p>
<p>Is there any simple solution to this problem, ideally built into requests? One could always wrap the whole thing with tenacity or manual retries, but ideally i want to avoid that as it means adding another layer and one needs to identify network-errors from other real errors, etc.</p>
",2,1621489748,python;python-requests;urllib3;retry-logic,False,1053,0,1621489748,https://stackoverflow.com/questions/67614642/python-requests-urrllib3-retry-on-read-timeout-after-200-header-received
67475470,while reading json data from url using python gives error &quot;urllib.error.HTTPError: HTTP Error 403: Forbidden&quot;,"<p>with this code I am reading a URL and using the data for filtration but urllib could not work</p>
<pre><code>url = &quot;myurl&quot;
response = urllib.request.urlopen(url)
data = json.loads(response.read())

</code></pre>
<p>yesterday it was working well but now giving me error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;vaccine_survey.py&quot;, line 22, in &lt;module&gt;
    response = urllib.request.urlopen(url)
  File &quot;/usr/lib/python3.6/urllib/request.py&quot;, line 223, in urlopen
    return opener.open(url, data, timeout)
  File &quot;/usr/lib/python3.6/urllib/request.py&quot;, line 532, in open
    response = meth(req, response)
  File &quot;/usr/lib/python3.6/urllib/request.py&quot;, line 642, in http_response
    'http', request, response, code, msg, hdrs)
  File &quot;/usr/lib/python3.6/urllib/request.py&quot;, line 570, in error
    return self._call_chain(*args)
  File &quot;/usr/lib/python3.6/urllib/request.py&quot;, line 504, in _call_chain
    result = func(*args)
  File &quot;/usr/lib/python3.6/urllib/request.py&quot;, line 650, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden

</code></pre>
",2,1620669594,python;json;urllib;urllib2;urllib3,True,1362,1,1620728973,https://stackoverflow.com/questions/67475470/while-reading-json-data-from-url-using-python-gives-error-urllib-error-httperro
67474679,Python requests &amp; urllib3 Retry - How to simulate a ConnectionError from inside the internal retry loop?,"<p>Given following example usage:</p>
<pre class=""lang-py prettyprint-override""><code>adapter = HTTPAdapter(max_retries=Retry(
    total=5,
    backoff_factor=0.1,
    status_forcelist=[429, 500, 502, 503, 504],
    method_whitelist=[&quot;HEAD&quot;, &quot;GET&quot;, &quot;OPTIONS&quot;]
))
session = requests.Session()
session.mount(&quot;http://&quot;, adapter)
session.mount(&quot;https://&quot;, adapter)
rsp = session.post(url, json=my_json, params=my_params)
</code></pre>
<p>I occasionally get:</p>
<pre><code>('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
</code></pre>
<p>I would like to establish retries around this as well (<a href=""https://stackoverflow.com/questions/67474641/python-requests-urllib3-retry-how-may-retries-were-made"">related question</a>), and I'd like to be able to test it by issuing such an error from inside the library's retry loop.</p>
<p>How do I do that?</p>
",6,1620665996,python;python-requests;urllib3,False,695,0,1620665996,https://stackoverflow.com/questions/67474679/python-requests-urllib3-retry-how-to-simulate-a-connectionerror-from-inside
67468173,"&quot;&#39;Connection aborted.&#39;, RemoteDisconnected&quot; or not getting any info with urllib3 and Django","<p>I am developing a web scraping application with BeautifulSoup and Django and I am experiencing some 'conexion issues' (I think).</p>
<p>The app has to check if any website is satisfying all the SEO requirements, and for that, I have to make different 'requests'... first to get the &quot;soup&quot; and then to check if the robots.txt and sitemap.xml, for example, exists... so I guess some sites are blocking my app because of that, and I keep getting the &quot;'Connection aborted.', RemoteDisconnected&quot; error or in other cases, I don't get the error but the &quot;soup&quot; is empty... is there a way to fix this? I have tried with <code>time.sleep()</code> but doesn't seem to work...</p>
<p>This is part of my code:</p>
<pre><code> http = PoolManager()
 r = http.request('GET', &quot;https://&quot; + url, headers={'User-Agent': &quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36&quot;, 'Accept-Encoding': 'br'})
    
 soup = BeautifulSoup(r.data, 'lxml')
</code></pre>
<p>And where I check if robots and sitemap exists:</p>
<pre><code>robots_url = url + &quot;/robots.txt&quot;
robot = requests.get(robots_url, headers)

if robot.ok:
    robot = True
else:
    robot = False

sleep(5)

sitemap_url = url + '/sitemap.xml'
sitemap = requests.get(sitemap_url, headers=headers)
if sitemap.ok:
    sitemap = True
else:
    sitemap = False
</code></pre>
<p>In most websites the code is working fine but there are some pages that I supposed have a higher security level that ends the connection with that error:</p>
<pre><code>During handling of the above exception (('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))), another exception occurred:
</code></pre>
<p>/app/.heroku/python/lib/python3.9/site-packages/django/core/handlers/exception.py, line 47, in inner</p>
<p>Thank you so much in advance for your time and advice.</p>
",1,1620639745,python;django;python-requests;screen-scraping;urllib3,False,218,0,1620647102,https://stackoverflow.com/questions/67468173/connection-aborted-remotedisconnected-or-not-getting-any-info-with-urllib3
67435240,ssl.SSLCertVerificationError: certificate verify failed: unable to get local issuer certificate (_ssl.c:1108),"<p>I use the code below to achieve one-way authentication, the file <code>ca.crt</code> is generated by server</p>
<pre><code>import ssl
import urllib.request

CERT_CA = './ca.crt'
context = ssl.SSLContext(ssl.PROTOCOL_TLS)
context.check_hostname = False
context.load_verify_locations(CERT_CA)
context.verify_mode = ssl.CERT_REQUIRED

server_api = 'https://xxxxxxxx'
request = urllib.request.Request(server_api, method='GET')
resp = urllib.request.urlopen(request, context=context)
</code></pre>
<p>but get the exception:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.8/urllib/request.py&quot;, line 1319, in do_open
    h.request(req.get_method(), req.selector, req.data, headers,
  File &quot;/usr/local/lib/python3.8/http/client.py&quot;, line 1230, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File &quot;/usr/local/lib/python3.8/http/client.py&quot;, line 1276, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File &quot;/usr/local/lib/python3.8/http/client.py&quot;, line 1225, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File &quot;/usr/local/lib/python3.8/http/client.py&quot;, line 1004, in _send_output
    self.send(msg)
  File &quot;/usr/local/lib/python3.8/http/client.py&quot;, line 944, in send
    self.connect()
  File &quot;/usr/local/lib/python3.8/http/client.py&quot;, line 1399, in connect
    self.sock = self._context.wrap_socket(self.sock,
  File &quot;/usr/local/lib/python3.8/ssl.py&quot;, line 500, in wrap_socket
    return self.sslsocket_class._create(
  File &quot;/usr/local/lib/python3.8/ssl.py&quot;, line 1040, in _create
    self.do_handshake()
  File &quot;/usr/local/lib/python3.8/ssl.py&quot;, line 1309, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;tst.py&quot;, line 20, in &lt;module&gt;
    resp = urllib.request.urlopen(request, context=context)
  File &quot;/usr/local/lib/python3.8/urllib/request.py&quot;, line 222, in urlopen
    return opener.open(url, data, timeout)
  File &quot;/usr/local/lib/python3.8/urllib/request.py&quot;, line 525, in open
    response = self._open(req, data)
  File &quot;/usr/local/lib/python3.8/urllib/request.py&quot;, line 542, in _open
    result = self._call_chain(self.handle_open, protocol, protocol +
  File &quot;/usr/local/lib/python3.8/urllib/request.py&quot;, line 502, in _call_chain
    result = func(*args)
  File &quot;/usr/local/lib/python3.8/urllib/request.py&quot;, line 1362, in https_open
    return self.do_open(http.client.HTTPSConnection, req,
  File &quot;/usr/local/lib/python3.8/urllib/request.py&quot;, line 1322, in do_open
    raise URLError(err)
urllib.error.URLError: &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)&gt;
</code></pre>
<p><code>certifi</code>==2020.6.20 and <code>urllib3</code>==1.25.10</p>
<p>How can I solve the problem, the request needs authentication, so please DON'T use something like <code>verify=False</code>.</p>
<p>Many thanks</p>
<hr />
<p>the problem is solved, because I use the wrong server host address.
<strong>the code above is effective for one-way authentication situation.</strong></p>
",0,1620391633,python;ssl;certificate;urllib;urllib3,True,1821,1,1620395008,https://stackoverflow.com/questions/67435240/ssl-sslcertverificationerror-certificate-verify-failed-unable-to-get-local-iss
67403914,How to open two differrent URLsat the same time using urllib.request.urlopen in python,"<p>I want to know if anyone knows how to open 2 different URLs at the same time using  urllib.request.urlopen? I can only open one URL at a time so far using try and except:</p>
<pre><code>
try:
    preflash = urllib.request.urlopen(&quot;http://192.168.100.6&quot;, timeout=5).getcode()
    print(&quot;Web page status code:&quot;, preflash, &quot;FAIL&quot;)  
    sys.exit(0)
    
except urllib.error.URLError:
    correct = urllib.request.urlopen(&quot;http://192.168.100.5&quot;, timeout=10).getcode()
    print(&quot;Web page status code:&quot;, correct)
    print(&quot;IP address: 192.168.100.5 is reachable&quot;)
</code></pre>
<p>How can i open both URLs at the same time and when 192.168.100.6 is reachable then the system just exits as you can clearly see in my code. But if 192.168.100.5 is reachable then the programm continues to function.
How can I open both 192.168.100.6 &amp; 192.168.100.5 at the same time and then using urllib or some other method and print &quot;Fail&quot; if 192.168.100.6 is reachable and print &quot;Pass&quot; if 192.168.100.5 is reachable.</p>
<p>I have read into threading but i really do not know how to implement it so please help me guys, show me how this could be done I really have no clue and i am so confused after reading about threading multi threading aiohttp aiosync I am just so confused.
If someone could show me how it is done I could use that as a base to understand what is going on.</p>
<p>I am just a beginner so please take it easy on me all.</p>
",0,1620227081,python;python-3.x;urllib3,False,65,0,1620227081,https://stackoverflow.com/questions/67403914/how-to-open-two-differrent-urlsat-the-same-time-using-urllib-request-urlopen-in
49530034,Tweepy OpenSSL.SSL.WantReadError,"<p>Python 3.6. I use the streamer of tweepy to get tweets. It works well. But sometimes, if I let it open for more than 24h, I have this error</p>

<pre><code>Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\requests\packages\urllib3\contrib\pyopenssl.py"", line 277, in recv_into
return self.connection.recv_into(*args, **kwargs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\OpenSSL\SSL.py"", line 1547, in recv_into
self._raise_ssl_error(self._ssl, result)
    File ""C:\ProgramData\Anaconda3\lib\site-packages\OpenSSL\SSL.py"", line 1353, in _raise_ssl_error
raise WantReadError()
  OpenSSL.SSL.WantReadError

During handling of the above exception, another exception occurred:


Traceback (most recent call last):
 File ""C:\ProgramData\Anaconda3\lib\site-packages\requests\packages\urllib3\contrib\pyopenssl.py"", line 277, in recv_into
return self.connection.recv_into(*args, **kwargs)
    File ""C:\ProgramData\Anaconda3\lib\site-packages\OpenSSL\SSL.py"", line 1547, in recv_into
self._raise_ssl_error(self._ssl, result)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\OpenSSL\SSL.py"", line 1370, in _raise_ssl_error
raise SysCallError(errno, errorcode.get(errno))
OpenSSL.SSL.SysCallError: (10054, 'WSAECONNRESET')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\requests\packages\urllib3\response.py"", line 302, in _error_catcher
yield
  File ""C:\ProgramData\Anaconda3\lib\site-packages\requests\packages\urllib3\response.py"", line 384, in read
data = self._fp.read(amt)
  File ""C:\ProgramData\Anaconda3\lib\http\client.py"", line 449, in read
n = self.readinto(b)
  File ""C:\ProgramData\Anaconda3\lib\http\client.py"", line 483, in readinto
return self._readinto_chunked(b)
  File ""C:\ProgramData\Anaconda3\lib\http\client.py"", line 578, in _readinto_chunked
chunk_left = self._get_chunk_left()
  File ""C:\ProgramData\Anaconda3\lib\http\client.py"", line 546, in _get_chunk_left
chunk_left = self._read_next_chunk_size()
  File ""C:\ProgramData\Anaconda3\lib\http\client.py"", line 506, in _read_next_chunk_size
line = self.fp.readline(_MAXLINE + 1)
  File ""C:\ProgramData\Anaconda3\lib\socket.py"", line 586, in readinto
return self._sock.recv_into(b)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\requests\packages\urllib3\contrib\pyopenssl.py"", line 293, in recv_into
return self.recv_into(*args, **kwargs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\requests\packages\urllib3\contrib\pyopenssl.py"", line 282, in recv_into
raise SocketError(str(e))
OSError: (10054, 'WSAECONNRESET')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\threading.py"", line 916, in _bootstrap_inner
self.run()
  File ""C:\ProgramData\Anaconda3\lib\threading.py"", line 864, in run
self._target(*self._args, **self._kwargs)
 File ""twitter_aspi_v0.8.py"", line 179, in _init_stream
tweepy.Stream(auth, listener).userstream()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tweepy\streaming.py"", line 396, in userstream
self._start(async)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tweepy\streaming.py"", line 363, in _start
self._run()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tweepy\streaming.py"", line 296, in _run
raise exception
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tweepy\streaming.py"", line 265, in _run
self._read_loop(resp)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tweepy\streaming.py"", line 315, in _read_loop
line = buf.read_line().strip()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tweepy\streaming.py"", line 180, in read_line
self._buffer += self._stream.read(self._chunk_size)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\requests\packages\urllib3\response.py"", line 401, in read
raise IncompleteRead(self._fp_bytes_read, self.length_remaining)
  File ""C:\ProgramData\Anaconda3\lib\contextlib.py"", line 100, in __exit__
self.gen.throw(type, value, traceback)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\requests\packages\urllib3\response.py"", line 320, in _error_catcher
raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: OSError(""(10054, \'WSAECONNRESET\')"",)', OSError(""(10054, 'WSAECONNRESET')"",))
</code></pre>

<p>My code is pretty long and regarding the error, it seems it comes from urllib3, OpenSSL and tweepy way of accessing the Twitter API. So I could handle this with a <code>try</code> before launching the streamer but I would like to know if there is maybe a better fix I could do to understand and avoid this? Thanks!</p>
",4,1522227045,python;python-3.x;openssl;tweepy;urllib3,True,8752,2,1619956364,https://stackoverflow.com/questions/49530034/tweepy-openssl-ssl-wantreaderror
67185598,How can I access a PDF file with Python through an automatic download link?,"<p>I am trying to create an automated Python script that goes to a webpage like <a href=""https://www.murphy.senate.gov/newsroom/press-releases/murphy-blumenthal-introduce-legislation-to-create-a-national-green-bank-thousands-of-clean-energy-jobs"" rel=""nofollow noreferrer"">this</a>, finds the link at the bottom of the body text (anchor text &quot;here&quot;), and downloads the PDF that loads after clicking said download link. I am able to retrieve the HTML from the original and find the download link, but I don't know how to get the <a href=""https://www.murphy.senate.gov/imo/media/doc/green_bank_act_2021.pdf"" rel=""nofollow noreferrer"">link to the PDF</a> from there. Any help would be much appreciated. Here's what I have so far:</p>
<pre><code>import urllib3
from urllib.request import urlopen
from bs4 import BeautifulSoup

# Open page and locate href for bill text
url = 'https://www.murphy.senate.gov/newsroom/press-releases/murphy-blumenthal-introduce-legislation-to-create-a-national-green-bank-thousands-of-clean-energy-jobs'
html = urlopen(url)
soup = BeautifulSoup(html, 'html.parser')
links = [] 
for link in soup.findAll('a', href=True, text=['HERE', 'here', 'Here']):
    links.append(link.get('href'))  
links2 = [x for x in links if x is not None]

# Open download link to get PDF
html = urlopen(links2[0])
soup = BeautifulSoup(html, 'html.parser')
links = [] 
for link in soup.findAll('a'):
    links.append(link.get('href'))  
links2 = [x for x in links if x is not None]
</code></pre>
<p>At this point the list of links I get does not include the PDF that I am looking for. Is there any way to grab this without hardcoding the link to the PDF in the code (that would be counterintuitive to what I am trying to do here)? Thanks!</p>
",3,1618948916,python;beautifulsoup;urllib3;urlopen,True,984,1,1618951321,https://stackoverflow.com/questions/67185598/how-can-i-access-a-pdf-file-with-python-through-an-automatic-download-link
11335825,authentication with urllib3,"<p>I am trying to connect to a webpage using urllib3. The code is provided below.</p>

<pre><code>import urllib3
http=urllib3.PoolManager()
fields={'username':'abc','password':'xyz'}
r=http.request('GET',url,fields)
</code></pre>

<p>If we assume that url is some webpage which needs to be authenticated using username and password, am i using the right code to authenticate ?</p>

<p>I have did this using urllib2 very comfortably but i was not able to do the same thing using urllib3. </p>

<p>Many Thanks</p>
",16,1341439105,python;authentication;urllib3,True,47538,2,1617794517,https://stackoverflow.com/questions/11335825/authentication-with-urllib3
66920796,getting error using self certificate verification in python ssl,"<p>Getting the following error :</p>
<p><code> [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1045)</code></p>
<p>I'm using self-signed certificates between many servers, now need to integrate python in the system but unable to verify self-signed certificates.</p>
<p>The code I'm using</p>
<pre><code>context = ssl.create_default_context()
context.load_verify_locations(&quot;/var/certs.crt&quot;)
context.load_cert_chain(certfile=cert_path, keyfile=key_path)
ctx.check_hostname = False
ctx.verify_mode = ssl.CERT_REQ

resp = urllib.request.urlopen(url_string, context=ctx)
</code></pre>
<p><strong>var/certs.crt</strong> containing the certificate of the specific server I'm starting an ssl connection with.</p>
<p><strong>cert_path &amp; key_path</strong> are my own cert and private key to establish 2 way ssl.</p>
<p>Things I've checked :</p>
<p>1.I can see my certs being loaded after <strong>load_cert_chain</strong> in <code>context.get_ca_certs()</code></p>
<p>2.I tried <code>context.verify_flags |= 0x80000</code> but it didn't work.</p>
<p>If <code>ctx.verify_mode = False</code> then I'm able to connect properly but it will not be secured.</p>
<p>Since the best existing answer on StackOverflow is to use <code>ctx.verify = False</code> and it's not the way, I'm hoping this time to find someone who actually fixed it.</p>
<p>Thanks</p>
",0,1617373625,python;python-3.x;ssl;urllib3,True,341,1,1617561184,https://stackoverflow.com/questions/66920796/getting-error-using-self-certificate-verification-in-python-ssl
11335825,authentication with urllib3,"<p>I am trying to connect to a webpage using urllib3. The code is provided below.</p>

<pre><code>import urllib3
http=urllib3.PoolManager()
fields={'username':'abc','password':'xyz'}
r=http.request('GET',url,fields)
</code></pre>

<p>If we assume that url is some webpage which needs to be authenticated using username and password, am i using the right code to authenticate ?</p>

<p>I have did this using urllib2 very comfortably but i was not able to do the same thing using urllib3. </p>

<p>Many Thanks</p>
",16,1341439105,python;authentication;urllib3,True,47538,2,1617794517,https://stackoverflow.com/questions/11335825/authentication-with-urllib3
66920796,getting error using self certificate verification in python ssl,"<p>Getting the following error :</p>
<p><code> [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1045)</code></p>
<p>I'm using self-signed certificates between many servers, now need to integrate python in the system but unable to verify self-signed certificates.</p>
<p>The code I'm using</p>
<pre><code>context = ssl.create_default_context()
context.load_verify_locations(&quot;/var/certs.crt&quot;)
context.load_cert_chain(certfile=cert_path, keyfile=key_path)
ctx.check_hostname = False
ctx.verify_mode = ssl.CERT_REQ

resp = urllib.request.urlopen(url_string, context=ctx)
</code></pre>
<p><strong>var/certs.crt</strong> containing the certificate of the specific server I'm starting an ssl connection with.</p>
<p><strong>cert_path &amp; key_path</strong> are my own cert and private key to establish 2 way ssl.</p>
<p>Things I've checked :</p>
<p>1.I can see my certs being loaded after <strong>load_cert_chain</strong> in <code>context.get_ca_certs()</code></p>
<p>2.I tried <code>context.verify_flags |= 0x80000</code> but it didn't work.</p>
<p>If <code>ctx.verify_mode = False</code> then I'm able to connect properly but it will not be secured.</p>
<p>Since the best existing answer on StackOverflow is to use <code>ctx.verify = False</code> and it's not the way, I'm hoping this time to find someone who actually fixed it.</p>
<p>Thanks</p>
",0,1617373625,python;python-3.x;ssl;urllib3,True,341,1,1617561184,https://stackoverflow.com/questions/66920796/getting-error-using-self-certificate-verification-in-python-ssl
52698283,Python urllib3 error - ImportError: cannot import name UnrewindableBodyError,"<p>I set my cronjob to call my script at particular time(<strong>ex- 2 4 5 10 * python3 mayank/exp/test.py</strong>).
When my <strong>test.py</strong> is called I'm activating the virtualenv within my test.py script as follows.</p>
<pre><code>activate = &quot;/home/myserver/schedule_py3/bin/activate_this.py&quot;
exec(open(activate).read())
</code></pre>
<p>After activating the virtual environment(which has python3 in it and the packages needed to run the script), I'm trying to <strong>import requests</strong> it is showing me error as:-</p>
<pre><code>File &quot;schedule_module/Schedule/notification_task.py&quot;, line 2, in &lt;module&gt;
    import requests
  File &quot;/usr/lib/python2.7/site-packages/requests/__init__.py&quot;, line 43, in &lt;module&gt;
    import urllib3
  File &quot;/usr/lib/python2.7/site-packages/urllib3/__init__.py&quot;, line 10, in &lt;module&gt;
    from .connectionpool import (
  File &quot;/usr/lib/python2.7/site-packages/urllib3/connectionpool.py&quot;, line 31, in &lt;module&gt;
    from .connection import (
  File &quot;/usr/lib/python2.7/site-packages/urllib3/connection.py&quot;, line 45, in &lt;module&gt;
    from .util.ssl_ import (
  File &quot;/usr/lib/python2.7/site-packages/urllib3/util/__init__.py&quot;, line 4, in &lt;module&gt;
    from .request import make_headers
  File &quot;/usr/lib/python2.7/site-packages/urllib3/util/request.py&quot;, line 5, in &lt;module&gt;
    from ..exceptions import UnrewindableBodyError
ImportError: cannot import name UnrewindableBodyError
</code></pre>
<p>As I can see that it is taking python2.7. Can anyone tell me where I'm wrong?</p>
<p><strong>Note</strong>- I had installed all the packages using pip3 inside my virtual environment.</p>
",16,1538987487,python;python-3.x;python-2.7;python-requests;urllib3,True,46880,5,1616607410,https://stackoverflow.com/questions/52698283/python-urllib3-error-importerror-cannot-import-name-unrewindablebodyerror
66721394,urllib3.... affected by certificate verification bypass. High risk!&quot; in my arch-audit stdout...?,"<p>Hello I have an message into the return of my arch-audit: &quot;python-urllib3 is affected by certificate verification bypass. High risk!&quot;</p>
<p>What is the signification ? It's the code of 'python-urllib3' have an security breach ? Or else ?
Because if I understand really good... it's an part of the code or this lib have incapacity of good check the certifications of asymmetric encrypted communication ?
So I think about reverse-shell and a lot of attacks possibles, it's dangerous, I think not tell bullshit if I tell: anybody can exploit this for usurpation of the certificate and take the control of an SSL/TLS transmission ? All right ? ...</p>
<p>I don't know, Sorry for my bad and poor english, and if my question is stupid at other look side.</p>
<p>I try to learn, I eat Arch Wiki.</p>
<ul>
<li>Rick.</li>
</ul>
",1,1616242106,python;python-3.8;urllib3;arch,True,167,1,1616243190,https://stackoverflow.com/questions/66721394/urllib3-affected-by-certificate-verification-bypass-high-risk-in-my-arch
66609339,How to get a file from a url and then read it as if it were local?,"<p>I have a jpg image that is stored at a url that I need to access and read the binary/byte data from.</p>
<p>I can get the file in Python by using:</p>
<pre><code>import urllib3

http = urllib3.PoolManager()

url = 'link to jpg'

contents = http.request('GET' url)

</code></pre>
<p>Purely reading the data from this request with <code>contents.data</code> doesn't provide the correct binary but if I download the file and read it locally, I get the correct binary. But I cannot continue with reading the file contents as such:</p>
<pre><code>with open(contents, &quot;rb&quot;) as image:
     f = image.read()
</code></pre>
<p>Using the bytes from the request doesn't work either:</p>
<pre><code>with open(contents.data, &quot;rb&quot;) as image:
     f = image.read()
</code></pre>
<p>How can I treat the jpg from the url as if it were local so that I can read the binary correctly?</p>
",1,1615599592,python;python-3.x;byte;binary-data;urllib3,True,1860,1,1615600532,https://stackoverflow.com/questions/66609339/how-to-get-a-file-from-a-url-and-then-read-it-as-if-it-were-local
29267838,URLLib3 Connection Pool creates only one pool,"<p>Currently i'm trying to scrape a site but the site didn't allow more than 100 request for one tcp connection. So, i tried to create multiple connection pool for requests. I tried the following code. Shouldn't it create 15 connection pool?</p>

<pre><code>from urllib3 import HTTPConnectionPool
for i in range(15):
    pool = HTTPConnectionPool('ajax.googleapis.com', maxsize=15)
    for j in range(15):
        resp= pool.request('GET', '/ajax/services/search/web')
    pool.num_connections
</code></pre>

<p>pool.num_connection always print 1</p>
",1,1427322934,python;connection-pooling;urllib3,False,1261,2,1615542915,https://stackoverflow.com/questions/29267838/urllib3-connection-pool-creates-only-one-pool
66499563,Python JA3 Fingerprint Spoofing with sockets,"<p>I would like to edit my own JA3 fingerprint in requests, I have modified my SSL, TLS, and Ciphers with UrlLib3 but there is no support for JA3 Fingerprints. How might I be able to spoof my JA3 Fingerprint within requests.</p>
",1,1614977975,python;ssl;python-requests;urllib3,False,457,0,1614977975,https://stackoverflow.com/questions/66499563/python-ja3-fingerprint-spoofing-with-sockets
66436010,Version conflict django-oauth-toolkit&gt;0.12.0 and urllib3==1.25.11,"<p>Why I have a problem with updating Django from version 1.11.29 to 2.0.13. When updating the library django-oauth-toolkit to version 1.2.0 - version support Django 2.0 I receive an error:  <code>__version__ = pkg_resources.require(&quot;django-oauth-toolkit&quot;)[0].version  pkg_resources.ContextualVersionConflict: (urllib3 1.25.11 (/.virtualenvs/django-oauth-tookit-conflict/lib/python3.6/site-packages), Requirement.parse('urllib3&lt;1.25,&gt;=1.21.1'), {'requests'})</code></p>
",0,1614674660,python;django;urllib3;django-oauth-toolkit,True,247,1,1614842761,https://stackoverflow.com/questions/66436010/version-conflict-django-oauth-toolkit0-12-0-and-urllib3-1-25-11
63088261,"Selenium giving error 111 after running a loop in python3 inside WSL (Debian for WSL, WSL2)","<p>Yes, I know the <s>main</s> general reason for this error. Im just unable to understand why Im getting it.</p>
<p>The Problem:
I made a script in python which uses selenium and bs4 to <strong>scrape</strong> a piece of information from websites in a list. When I put it in a loop, it doesn't show any urllib3 Error 111 but after the script stops or after forcedly stopping the loop when I try anything with browser.get(#anysite), it throws this error. If I do del browser and remake it then the error is nowhere to be found.</p>
<p>The error</p>
<blockquote>
<p>raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1',
port=51841): Max retries exceeded with url:
/session/50c8eae33ca98c4ceedb4988d72d27cd/url (Caused by
NewConnectionError('&lt;urllib3.connection.HTTPConnection object at
0x7f3d42858908&gt;: Failed to establish a new connection: [Errno 111]
Connection refused',))</p>
</blockquote>
<p>The platform Im running the script is Debian for wsl&lt;&lt;wsl2&lt;&lt;windows 10 2004</p>
<p>As for the script, it contains sensitive info so Im unable to show it. If the script is really needed, then I will try to replicate this error in another site. I hope I can understand as to the real reason Im getting this error.</p>
<p>PS: This is my first question after haunting stackoverflow for some years. As I almost always found my answers here I hadn't a need to do this in past.</p>
<p>Edit:
This piece of code may possibly be able to replicate the error.</p>
<pre><code>from selenium import webdriver
from selenium.webdriver.common.keys import Keys
chromeOptions = webdriver.ChromeOptions()  
chromeOptions.add_argument(&quot;--headless&quot;) 
browser = webdriver.Chrome(options=chromeOptions) 
c=1
while c==1:
    browser.get(&quot;http://users.ox.ac.uk/~csrob/ecdla/&quot;)
    x=1
    while x==1:
        soup = BeautifulSoup(browser.page_source, &quot;html.parser&quot;)
        y=soup.find(&quot;h2&quot;)
        if not y.find(&quot;This&quot;)==-1:
            x=0
            print(&quot;loaded&quot;)
</code></pre>
<p>After letting it run for a bit(say 2-5 min) Interrupt the code with Ctrl+C and then when browser.get(#anysite) is done urllib3 Error 111 appears.</p>
<p>So far what I've tried,</p>
<p>1)Searching for error on google, came across multiple sites with info on when error 111 occurs {when the server refuses(not applicable in my case), occurs when shifting from http to https(not applicable in my case), happened in old versions of urllib3}</p>
<p>2)Editing value of total to 0 in line 439 of /usr/local/lib/python3.5/dist-packages/urllib3/util/retry.py Gives some kind of recursive error.</p>
<p>3)Tried the same code in Ubuntu for wsl(in version 18.04 and 20.04), Fedora Remix for wsl both gave same error</p>
<p>4)Turning off Windows Defender Firewall</p>
<p>Edit2:
I think I found out why I was getting this error. Seems like sometimes Chrome gets stuck/hang during the process and is unable to get out of it and the only way is to close and open another instance of it. I found this out when running the script on windows normally(without WSL or headerless option). Still Im unable to figure out why this doesn't happen when in loop.</p>
",1,1595679762,python;selenium;debian;windows-subsystem-for-linux;urllib3,False,215,0,1614203720,https://stackoverflow.com/questions/63088261/selenium-giving-error-111-after-running-a-loop-in-python3-inside-wsl-debian-for
66312584,How to download files continuously using Python wget or urllib3,"<p>How to download files continuously using Python wget or urllib3.</p>
<p>My wish is to download automatic files at intervals, such as at intervals of 1 hour. on the official urllib3 page, there is Retrying Requests content for its users</p>
",0,1613984050,python;wget;urllib3,True,147,1,1613984452,https://stackoverflow.com/questions/66312584/how-to-download-files-continuously-using-python-wget-or-urllib3
52692120,Script Suddenly Stops Crawling Without Error or Exception,"<p>I'm not sure why, but my script always stops crawling once it hits <a href=""https://www.amazon.ca/gp/goldbox/ref=gbps_ftr_s-3_4bc8_page_9?gb_f_c2xvdC0z=discountRanges:10-25%252C25-50%252C50-70%252C70-,page:9,sortOrder:BY_DISCOUNT_DESCENDING,dealsPerPage:32&amp;pf_rd_p=f5836aee-0969-4c39-9720-4f0cacf64bc8&amp;pf_rd_s=slot-3&amp;pf_rd_t=701&amp;pf_rd_i=gb_main&amp;pf_rd_m=A3DWYIK6Y9EEQB&amp;pf_rd_r=5ZFYV9FYXCK98KYPJXMC&amp;ie=UTF8"" rel=""noreferrer"">page 9</a>. There are no errors, exceptions, or warnings, so I'm kind of at a loss.</p>

<p>Can somebody help me out?</p>

<p>P.S. <a href=""https://codepen.io/tOkyO1/pen/gBLPPr"" rel=""noreferrer"">Here is the full script in case anybody wants to test it for themselves!</a></p>

<pre><code>def initiate_crawl():
    def refresh_page(url):
        ff = create_webdriver_instance()
        ff.get(url)
        ff.find_element(By.XPATH, '//*[@id=""FilterItemView_sortOrder_dropdown""]/div/span[2]/span/span/span/span').click()
        ff.find_element(By.XPATH, '//a[contains(text(), ""Discount - High to Low"")]').click()
        items = WebDriverWait(ff, 15).until(
            EC.visibility_of_all_elements_located((By.XPATH, '//div[contains(@id, ""100_dealView_"")]'))
        )
        print(len(items))
        for count, item in enumerate(items):
            slashed_price = item.find_elements(By.XPATH, './/span[contains(@class, ""a-text-strike"")]')
            active_deals = item.find_elements(By.XPATH, './/*[contains(text(), ""Add to Cart"")]')
            if len(slashed_price) &gt; 0 and len(active_deals) &gt; 0:
                product_title = item.find_element(By.ID, 'dealTitle').text
                if product_title not in already_scraped_product_titles:
                    already_scraped_product_titles.append(product_title)
                    url = ff.current_url
                    ff.quit()
                    refresh_page(url)
                    break
            if count+1 is len(items):
                try:
                    next_button = WebDriverWait(ff, 15).until(
                        EC.text_to_be_present_in_element((By.PARTIAL_LINK_TEXT, 'Next→'), 'Next→')
                    )
                    ff.find_element(By.PARTIAL_LINK_TEXT, 'Next→').click()
                    url = ff.current_url
                    ff.quit()
                    refresh_page(url)
                except Exception as error:
                    print(error)
                    ff.quit()

    refresh_page('https://www.amazon.ca/gp/goldbox/ref=gbps_ftr_s-3_4bc8_dct_10-?gb_f_c2xvdC0z=sortOrder:BY_SCORE,discountRanges:10-25%252C25-50%252C50-70%252C70-&amp;pf_rd_p=f5836aee-0969-4c39-9720-4f0cacf64bc8&amp;pf_rd_s=slot-3&amp;pf_rd_t=701&amp;pf_rd_i=gb_main&amp;pf_rd_m=A3DWYIK6Y9EEQB&amp;pf_rd_r=CQ7KBNXT36G95190QJB1&amp;ie=UTF8')

initiate_crawl()
</code></pre>

<p>Printing the length of <code>items</code> invokes some strange behaviour too. Instead of it always returning 32, which would correspond to the number of items on each page, it prints <code>32</code> for the first page, <code>64</code> for the second, <code>96</code> for the third, so on and so forth. I fixed this by using <code>//div[contains(@id, ""100_dealView_"")]/div[contains(@class, ""dealContainer"")]</code> instead of <code>//div[contains(@id, ""100_dealView_"")]</code> as the XPath for the <code>items</code> variable. I'm hoping this is the reason why it runs into issues on page 9. I'm running tests right now. <strong>Update:</strong> It is now scraping page 10 and beyond, so the issue is resolved.</p>
",12,1538940686,python;selenium;python-requests;geckodriver;urllib3,True,2796,2,1613678541,https://stackoverflow.com/questions/52692120/script-suddenly-stops-crawling-without-error-or-exception
66255609,How to Edit document saved in Elasticsearch using python,"<p>I am new to <code>elasticsearch</code> and I am trying to perform CRUD operations using python. I have created an Index and I am able to save the document in the Elasticsearch. However when I try to update the document, the whole data gets overwritten. For instance, while creating a document there are 3 fields:</p>
<pre><code>data = {
 &quot;typeId&quot;:&quot;someValue&quot;,
 &quot;typeStatus&quot;:&quot;someValue&quot;,
 &quot;typeLists&quot;:&quot;someValue&quot;,
 &quot;createdDate&quot;,&quot;someValue&quot;
}
</code></pre>
<p>The above gets saved while saving the document. While Editing the above document the createdDate gets deleted. Below is the Update document.</p>
<pre><code>data = {
 &quot;typeId&quot;:&quot;someValue&quot;,
 &quot;typeStatus&quot;:&quot;someValueEdited&quot;,
 &quot;typeLists&quot;:&quot;someValue&quot;,
 &quot;updatedDate&quot;,&quot;someValue&quot;
}
</code></pre>
<p>Is there a way to save/edit the document without loosing the fields? Below is the code.</p>
<pre><code>import urllib3
        
        
saveContext = '_doc'
updateContext = '_update'
httpClient = urllib3.PoolManager()
response = httpClient.request('PUT', 
                          elasticsearchURL, 
                          headers={'headersValue'},
                          body=json.dumps(items))
</code></pre>
<p>The context values are appended to the ES URL.</p>
",0,1613634326,python;python-3.x;elasticsearch;urllib3,True,137,1,1613634608,https://stackoverflow.com/questions/66255609/how-to-edit-document-saved-in-elasticsearch-using-python
66234828,Using python and selenium to download an image using the image&#39;s &quot;src&quot; attribute,"<p>I'm new to Python and Selenium.  My goal here is to download an image from the Google Image Search results page and save it as a file in a local directory, but I have been unable to initially download the image.</p>
<p>I'm aware there are other options (retrieving the image via the url using request, etc.), but I want to know if it's possible to do it using the image's &quot;src&quot; attribute, e.g., &quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAkGBxM...&quot;</p>
<p>My code is below (I have removed all imports, etc., for brevity.):</p>
<pre><code># This creates the folder to store the image in
if not os.path.exists(save_folder):
    os.mkdir(save_folder)

driver = webdriver.Chrome(PATH)

# Goes to the given web page
driver.get(&quot;https://www.google.com/imghp?hl=en&amp;ogbl&quot;)

# &quot;q&quot; is the name of the google search field input
search_bar = driver.find_element_by_name(&quot;q&quot;)

# Input the search term(s)
search_bar.send_keys(&quot;Ben Folds Songs for Silverman Album Cover&quot;)

# Returns the results (basically clicks &quot;search&quot;)
search_bar.send_keys(Keys.RETURN)

# Wait 10 seconds for the images to load on the page before moving on to the next part of the script
try:
    # Returns a list
    search_results = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.ID, &quot;islrg&quot;))
    )
    # print(search_results.text)

    # Gets all of the images on the page (it should be a list)
    images = search_results.find_elements_by_tag_name(&quot;img&quot;)

    # I just want the first result
    image = images[0].get_attribute('src')

    ### Need help here ###

except:
    print(&quot;Error&quot;)
    driver.quit()

# Closes the browser
driver.quit()
</code></pre>
<p>I have tried:</p>
<pre><code>urllib.request.urlretrieve(image, &quot;00001.jpg&quot;)
</code></pre>
<p>and</p>
<pre><code>urllib3.request.urlretrieve(image, f&quot;{save_folder}/captcha.png&quot;)
</code></pre>
<p>But I've always hit the &quot;except&quot; block using those methods.  After reading a promising post, I also tried:</p>
<pre><code>bufferedImage = imageio.read(image)
outputFile = f&quot;{save_folder}/image.png&quot;
imageio.write(bufferedImage, &quot;png&quot;, outputFile)
</code></pre>
<p>with similar results, though I believe the latter <a href=""https://stackoverflow.com/questions/6813704/how-to-download-an-image-using-selenium-any-version"">example</a> used Java in the post and I may have made an error in translating it to Python.</p>
<p>I'm sure it's something obvious, but what am I doing wrong?  Thank you for any help.</p>
",1,1613528032,python;selenium;urllib;urllib3;python-imageio,True,847,1,1613532727,https://stackoverflow.com/questions/66234828/using-python-and-selenium-to-download-an-image-using-the-images-src-attribute
18061640,Ignore certificate validation with urllib3,"<p>I'm using urllib3 against private services that have self signed certificates. Is there any way to have urllib3 ignore the certificate errors and make the request anyways?</p>

<pre><code>import urllib3
c = urllib3.HTTPSConnectionPool('10.0.3.168', port=9001)
c.request('GET', '/')
</code></pre>

<p>When using the following:</p>

<pre><code>import urllib3
c = urllib3.HTTPSConnectionPool('10.0.3.168', port=9001, cert_reqs='CERT_NONE')
c.request('GET', '/')
</code></pre>

<p>The following error is raised:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/lib/python3/dist-packages/urllib3/request.py"", line 67, in request
    **urlopen_kw)
  File ""/usr/lib/python3/dist-packages/urllib3/request.py"", line 80, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 415, in urlopen
    body=body, headers=headers)
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 267, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File ""/usr/lib/python3.3/http/client.py"", line 1061, in request
    self._send_request(method, url, body, headers)
  File ""/usr/lib/python3.3/http/client.py"", line 1099, in _send_request
    self.endheaders(body)
  File ""/usr/lib/python3.3/http/client.py"", line 1057, in endheaders
    self._send_output(message_body)
  File ""/usr/lib/python3.3/http/client.py"", line 902, in _send_output
    self.send(msg)
  File ""/usr/lib/python3.3/http/client.py"", line 840, in send
    self.connect()
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 103, in connect
    match_hostname(self.sock.getpeercert(), self.host)
  File ""/usr/lib/python3/dist-packages/urllib3/packages/ssl_match_hostname/__init__.py"", line 32, in match_hostname
    raise ValueError(""empty or no certificate"")
ValueError: empty or no certificate
</code></pre>

<p>Using <code>cURL</code> I'm able to get the expected response from the service</p>

<pre><code>$ curl -k https://10.0.3.168:9001/
Please read the documentation for API endpoints
</code></pre>
",29,1375715617,python;python-3.x;urllib3,True,64133,4,1613463603,https://stackoverflow.com/questions/18061640/ignore-certificate-validation-with-urllib3
36516183,What should I use to open a url instead of urlopen in urllib3,"<p>I wanted to write a piece of code like the following:</p>

<pre><code>from bs4 import BeautifulSoup
import urllib2

url = 'http://www.thefamouspeople.com/singers.php'
html = urllib2.urlopen(url)
soup = BeautifulSoup(html)
</code></pre>

<p>But I found that I have to install <code>urllib3</code> package now.</p>

<p>Moreover, I couldn't find any tutorial or example to understand how to rewrite the above code, for example, <code>urllib3</code> does not have <code>urlopen</code>.</p>

<p>Any explanation or example, please?!</p>

<p>P/S: I'm using python 3.4.</p>
",74,1460201595,python;web-scraping;beautifulsoup;urllib3,True,178599,6,1613129869,https://stackoverflow.com/questions/36516183/what-should-i-use-to-open-a-url-instead-of-urlopen-in-urllib3
66149768,how to print request body in urllib3 python,"<p>I can successfully make a post multipart request using <strong>urllib3 library in python</strong>. However how to print the request body when POST request was made?</p>
<p>In <em><strong>python requests library</strong></em>: we have the option like: <strong>print(response.request.body)</strong></p>
<p>code looks like this:</p>
<p>http = urllib3.PoolManager()</p>
<pre><code>path_bdl = os.path.dirname(__file__) + '/some_bin_file.xyz'
content_json = os.path.dirname(__file__) + '/some_json.json'


with open(path_, 'rb') as fp:
    binary_data = fp.read()

with open(content_json, 'r') as j:
    json_data1 = j.read()

url = &quot;//myurl&quot;

headers = {}  # dictionary

response = http.request(
    'POST',
    url,
    fields={
        &quot;abc&quot;: (content_json, json_data1, &quot;application/json&quot;),
        &quot;bbb&quot;: (path_, binary_data, &quot;xyz_Content_type&quot;)
    },
    headers=headers
)

print('\n StatusCode: ', response.status)
print(response.headers)
print(response.request) &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; HTTPResponse has no attribute 'request'
</code></pre>
",0,1613024896,python;json;python-3.x;python-requests;urllib3,False,5557,1,1613027611,https://stackoverflow.com/questions/66149768/how-to-print-request-body-in-urllib3-python
65115175,GeocoderUnavailable: HTTPSConnectionPool error for some addresses using geopy and Nominatim,"<p>I get errors for some addresses when geocoding with <a href=""https://geopy.readthedocs.io"" rel=""nofollow noreferrer"">geopy</a> (using Nominatim). I don't really see a pattern why an address gives an error and another does not, e.g. simply changing the house number can make the difference.</p>
<p>When I make the API request mentioned by the error message via urllib3 it works, so I suppose the error is caused by geopy, but I am not sure.</p>
<h1>Minimal reproducible example</h1>
<pre class=""lang-py prettyprint-override""><code>from geopy.geocoders import Nominatim
geolocator = Nominatim(user_agent=&quot;my-test-app&quot;)
geolocator.geocode({'country': 'DE', 'city': 'Erlangen', 'postalcode': '91052',
                                'street': 'Nürnberger Straße 6'}) # working

&gt;&gt;&gt; Location(Nürnberger Straße, Sebaldussiedlung, Erlangen, Bayern, 91052, Deutschland, (49.5772384, 11.015895, 0.0))

geolocator.geocode({'country': 'DE', 'city': 'Erlangen', 'postalcode': '91052',
                                'street': 'Nürnberger Straße 7'}) # error
</code></pre>
<h2>Error message</h2>
<pre class=""lang-py prettyprint-override""><code>---------------------------------------------------------------------------
timeout                                   Traceback (most recent call last)
C:\Users\USERNAME\Anaconda3\envs\crm_templates\lib\site-packages\urllib3\connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
    425                     # Otherwise it looks like a bug in the code.
--&gt; 426                     six.raise_from(e, None)
    427         except (SocketTimeout, BaseSSLError, SocketError) as e:

C:\Users\USERNAME\Anaconda3\envs\crm_templates\lib\site-packages\urllib3\packages\six.py in raise_from(value, from_value)

C:\Users\USERNAME\Anaconda3\envs\crm_templates\lib\site-packages\urllib3\connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
    420                 try:
--&gt; 421                     httplib_response = conn.getresponse()
    422                 except BaseException as e:

C:\Users\USERNAME\Anaconda3\envs\crm_templates\lib\http\client.py in getresponse(self)
   1353             try:
-&gt; 1354                 response.begin()
   1355             except ConnectionError:

C:\Users\USERNAME\Anaconda3\envs\crm_templates\lib\http\client.py in begin(self)
    305         while True:
--&gt; 306             version, status, reason = self._read_status()
    307             if status != CONTINUE:

C:\Users\USERNAME\Anaconda3\envs\crm_templates\lib\http\client.py in _read_status(self)
    266     def _read_status(self):
--&gt; 267         line = str(self.fp.readline(_MAXLINE + 1), &quot;iso-8859-1&quot;)
    268         if len(line) &gt; _MAXLINE:

C:\Users\USERNAME\Anaconda3\envs\crm_templates\lib\socket.py in readinto(self, b)
    588             try:
--&gt; 589                 return self._sock.recv_into(b)
    590             except timeout:

C:\Users\USERNAME\Anaconda3\envs\crm_templates\lib\ssl.py in recv_into(self, buffer, nbytes, flags)
   1070                   self.__class__)
-&gt; 1071             return self.read(nbytes, buffer)
   1072         else:

C:\Users\USERNAME\Anaconda3\envs\crm_templates\lib\ssl.py in read(self, len, buffer)
    928             if buffer is not None:
--&gt; 929                 return self._sslobj.read(len, buffer)
    930             else:

timeout: The read operation timed out

During handling of the above exception, another exception occurred:

ReadTimeoutError                          Traceback (most recent call last)
C:\Users\USERNAME\Anaconda3\envs\crm_templates\lib\site-packages\urllib3\connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    676                 headers=headers,
--&gt; 677                 chunked=chunked,
    678             )

C:\Users\USERNAME\Anaconda3\envs\crm_templates\lib\site-packages\urllib3\connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
    427         except (SocketTimeout, BaseSSLError, SocketError) as e:
--&gt; 428             self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
    429             raise

C:\Users\USERNAME\Anaconda3\envs\crm_templates\lib\site-packages\urllib3\connectionpool.py in _raise_timeout(self, err, url, timeout_value)
    335             raise ReadTimeoutError(
--&gt; 336                 self, url, &quot;Read timed out. (read timeout=%s)&quot; % timeout_value
    337             )

ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)

During handling of the above exception, another exception occurred:

MaxRetryError                             Traceback (most recent call last)
C:\Users\USERNAME\Anaconda3\envs\crm_templates\lib\site-packages\requests\adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    448                     retries=self.max_retries,
--&gt; 449                     timeout=timeout
    450                 )

C:\Users\USERNAME\Anaconda3\envs\crm_templates\lib\site-packages\urllib3\connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    766                 body_pos=body_pos,
--&gt; 767                 **response_kw
    768             )

C:\Users\USERNAME\Anaconda3\envs\crm_templates\lib\site-packages\urllib3\connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    766                 body_pos=body_pos,
--&gt; 767                 **response_kw
    768             )

C:\Users\USERNAME\Anaconda3\envs\crm_templates\lib\site-packages\urllib3\connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    726             retries = retries.increment(
--&gt; 727                 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]
    728             )

C:\Users\USERNAME\Anaconda3\envs\crm_templates\lib\site-packages\urllib3\util\retry.py in increment(self, method, url, response, error, _pool, _stacktrace)
    445         if new_retry.is_exhausted():
--&gt; 446             raise MaxRetryError(_pool, url, error or ResponseError(cause))
    447 

MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?country=DE&amp;city=Erlangen&amp;postalcode=91052&amp;street=N%C3%BCrnberger+Stra%C3%9Fe+7&amp;format=json&amp;limit=1 (Caused by ReadTimeoutError(&quot;HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)&quot;))

During handling of the above exception, another exception occurred:

ConnectionError                           Traceback (most recent call last)
C:\Users\USERNAME\Anaconda3\envs\crm_templates\lib\site-packages\geopy\adapters.py in _request(self, url, timeout, headers)
    382         try:
--&gt; 383             resp = self.session.get(url, timeout=timeout, headers=headers)
    384         except Exception as error:

C:\Users\USERNAME\Anaconda3\envs\crm_templates\lib\site-packages\requests\sessions.py in get(self, url, **kwargs)
    554         kwargs.setdefault('allow_redirects', True)
--&gt; 555         return self.request('GET', url, **kwargs)
    556 

C:\Users\USERNAME\Anaconda3\envs\crm_templates\lib\site-packages\requests\sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)
    541         send_kwargs.update(settings)
--&gt; 542         resp = self.send(prep, **send_kwargs)
    543 

C:\Users\USERNAME\Anaconda3\envs\crm_templates\lib\site-packages\requests\sessions.py in send(self, request, **kwargs)
    654         # Send the request
--&gt; 655         r = adapter.send(request, **kwargs)
    656 

C:\Users\USERNAME\Anaconda3\envs\crm_templates\lib\site-packages\requests\adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    515 
--&gt; 516             raise ConnectionError(e, request=request)
    517 

ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?country=DE&amp;city=Erlangen&amp;postalcode=91052&amp;street=N%C3%BCrnberger+Stra%C3%9Fe+7&amp;format=json&amp;limit=1 (Caused by ReadTimeoutError(&quot;HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)&quot;))

During handling of the above exception, another exception occurred:

GeocoderUnavailable                       Traceback (most recent call last)
&lt;ipython-input-4-aa66519ee9b9&gt; in &lt;module&gt;()
----&gt; 1 geolocator.geocode({'country': 'DE', 'city': 'Erlangen', 'postalcode': '91052', 'street': 'Nürnberger Straße 7'})

C:\Users\USERNAME\Anaconda3\envs\crm_templates\lib\site-packages\geopy\geocoders\nominatim.py in geocode(self, query, exactly_one, timeout, limit, addressdetails, language, geometry, extratags, country_codes, viewbox, bounded, featuretype, namedetails)
    292         logger.debug(&quot;%s.geocode: %s&quot;, self.__class__.__name__, url)
    293         callback = partial(self._parse_json, exactly_one=exactly_one)
--&gt; 294         return self._call_geocoder(url, callback, timeout=timeout)
    295 
    296     def reverse(

C:\Users\USERNAME\Anaconda3\envs\crm_templates\lib\site-packages\geopy\geocoders\base.py in _call_geocoder(self, url, callback, timeout, is_json, headers)
    358         try:
    359             if is_json:
--&gt; 360                 result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)
    361             else:
    362                 result = self.adapter.get_text(url, timeout=timeout, headers=req_headers)

C:\Users\USERNAME\Anaconda3\envs\crm_templates\lib\site-packages\geopy\adapters.py in get_json(self, url, timeout, headers)
    371 
    372     def get_json(self, url, *, timeout, headers):
--&gt; 373         resp = self._request(url, timeout=timeout, headers=headers)
    374         try:
    375             return resp.json()

C:\Users\USERNAME\Anaconda3\envs\crm_templates\lib\site-packages\geopy\adapters.py in _request(self, url, timeout, headers)
    393                     raise GeocoderServiceError(message)
    394                 else:
--&gt; 395                     raise GeocoderUnavailable(message)
    396             elif isinstance(error, requests.Timeout):
    397                 raise GeocoderTimedOut(&quot;Service timed out&quot;)

GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?country=DE&amp;city=Erlangen&amp;postalcode=91052&amp;street=N%C3%BCrnberger+Stra%C3%9Fe+7&amp;format=json&amp;limit=1 (Caused by ReadTimeoutError(&quot;HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)&quot;))
</code></pre>
<h1>Working example using urllib3</h1>
<pre class=""lang-py prettyprint-override""><code>import urllib3

http = urllib3.PoolManager(1, headers={'user-agent': 'my-test-app'})

url = 'https://nominatim.openstreetmap.org/search?country=DE&amp;city=Erlangen&amp;postalcode=91052&amp;street=N%C3%BCrnberger+Stra%C3%9Fe+7&amp;format=json&amp;limit=1'

resp = http.request('GET', url)

json.loads(resp.data.decode())

  
&gt;&gt;&gt; [{'place_id': 17025708,
&gt;&gt;&gt;   'licence': 'Data © OpenStreetMap contributors, ODbL 1.0. https://osm.org/copyright',
&gt;&gt;&gt;   'osm_type': 'node',
&gt;&gt;&gt;   'osm_id': 1641967158,
&gt;&gt;&gt;   'boundingbox': ['49.5924431', '49.5925431', '11.0043901', '11.0044901'],
&gt;&gt;&gt;   'lat': '49.5924931',
&gt;&gt;&gt;   'lon': '11.0044401',
&gt;&gt;&gt;   'display_name': 'Postbank, 7, Nürnberger Straße, Am Anger, Erlangen, Bayern, 91052, Deutschland',
&gt;&gt;&gt;   'class': 'amenity',
&gt;&gt;&gt;   'type': 'bank',
&gt;&gt;&gt;   'importance': 0.6309999999999999,
&gt;&gt;&gt;   'icon': 'https://nominatim.openstreetmap.org/ui/mapicons//money_bank2.p.20.png'}]
</code></pre>
",4,1606938658,python;urllib3;geopy;nominatim,True,17166,2,1612976190,https://stackoverflow.com/questions/65115175/geocoderunavailable-httpsconnectionpool-error-for-some-addresses-using-geopy-an
35704392,How to make python .post() requests to retry?,"<p>I'm trying to implement requests retry in Python.<br>
It works like charm with <code>.get()</code> requests, but a <code>.post()</code> request never retries, regardless of a status code. I'd like to use it with <code>.post()</code> requests.</p>

<p>My code:</p>

<pre><code>from requests.packages.urllib3.util import Retry
from requests.adapters import HTTPAdapter
from requests import Session, exceptions

s = Session()
s.mount('http://', HTTPAdapter(max_retries=Retry(total=2, backoff_factor=1, status_forcelist=[ 500, 502, 503, 504, 521])))
r = s.get('http://httpstat.us/500')
r2 = s.post('http://httpstat.us/500')
</code></pre>

<p>So, the <code>.get()</code> requests do retry and the <code>.post()</code> ones do not.</p>

<p>What's wrong?</p>
",39,1456762834,python;python-requests;urllib3,True,25958,2,1612797121,https://stackoverflow.com/questions/35704392/how-to-make-python-post-requests-to-retry
66099221,How to monkeypatch method that&#39;s deep inside a library for only a single module?,"<p>I want to monkeypatch the <code>assert_fingerprint</code> method defined for urllib3 in this file: <a href=""https://github.com/urllib3/urllib3/blob/main/src/urllib3/util/ssl_.py"" rel=""nofollow noreferrer"">https://github.com/urllib3/urllib3/blob/main/src/urllib3/util/ssl_.py</a></p>
<p>However, we're not using urllib3 directly. We're using the requests library, which uses urllib3 below the surface. Additionally, I only want a single class in my code to have the &quot;monkeypatched version&quot; when using the requests interface, everywhere else in my code the normal behavior is expected. This is for production use, not for (unit)testing usage.</p>
<p>There's a lot of information out and about on how to patch or monkeypatch, but they're usually very tailored to specific classes that can be imported directly. Would I have to track down the entire import-chain here starting from requests to get down to this file?</p>
",0,1612777691,python;python-3.x;python-requests;urllib3,False,191,0,1612777691,https://stackoverflow.com/questions/66099221/how-to-monkeypatch-method-thats-deep-inside-a-library-for-only-a-single-module
66038119,Download multiple files from FTP share?,"<p>I know this question has been asked multiple times, but none of the solutions actually worked so far.</p>
<p>I would like to pull some files to a web tool based on an <a href=""ftp://ftp.ebi.ac.uk/pub/databases/metabolights/studies/public/MTBLS1167"" rel=""nofollow noreferrer"">URL</a>.</p>
<p>This seems to be an FTP share but using</p>
<pre><code>import ftplib
url = 'ftp://ftp.ebi.ac.uk/pub/databases/metabolights/studies/public/MTBLS1167'
ftp = ftplib.FTP(url)
</code></pre>
<blockquote>
<p>6 ftp = ftplib.FTP(url)
gaierror: [Errno -2] Name or service not known</p>
</blockquote>
<p>It is easy to download single files with <code>wget</code>:</p>
<pre><code>wget.download(url+'/'+filename, out=ms_dir)
</code></pre>
<p>However, the python implementation of <code>wget</code> does not have all features of the Linux tool implemented. So, something like <code>wget.download(url+'/*.*', out=ms_dir)</code> does not work.</p>
<p>Therefore, I need to pull the list of files that I want to download first and download the files one by one. I tried beautifulsoup, requests, urllib. But all the solutions seem over-complicated for a problem that was probably solved a million times ten years ago, or don't work at all.</p>
<p>However, e.g.</p>
<pre><code>import requests
response = requests.get(url, params=params)
</code></pre>
<blockquote>
<p>InvalidSchema: No connection adapters were found for...</p>
</blockquote>
<pre><code>import urllib3
http = urllib3.PoolManager()
r = http.request('GET', url)
</code></pre>
<blockquote>
<p>URLSchemeUnknown: Not supported URL scheme ftp</p>
</blockquote>
<p>And so on. I am not sure what I am doing wrong here.</p>
",1,1612401897,python;python-requests;urllib3;ftplib,True,457,1,1612409818,https://stackoverflow.com/questions/66038119/download-multiple-files-from-ftp-share
57468606,Python scraping website with flight tickets,"<p>I am trying to extract information about prices of flight tickets with a python script. Please take a look at the picture:</p>
<p><a href=""https://i.stack.imgur.com/e78dI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e78dI.png"" alt=""Inspect element"" /></a></p>
<p>I would like to parse all the prices (such as &quot;121&quot; at the bottom of the tree). I have constructed a simple script and my problem is that I am not sure how to get the right parts from the code behind page's &quot;inspect element&quot;. My code is below:</p>
<pre><code>import urllib3
from bs4 import BeautifulSoup as BS

http = urllib3.PoolManager()


ULR = &quot;https://greatescape.co/?datesType=oneway&amp;dateRangeType=exact&amp;departDate=2019-08-19&amp;origin=EAP&amp;originType=city&amp;continent=europe&amp;flightType=3&amp;city=WAW&quot;
response = http.request('GET', URL)
soup = BS(response.data, &quot;html.parser&quot;)

body = soup.find('body')
__next = body.find('div', {'id':'__next'})
ui_container = __next.find('div', {'class':'ui-container'})
bottom_container_root = ui_container.find('div', {'class':'bottom-container-root'})

print(bottom_container_root)
</code></pre>
<p>The problem is that I am stuck at the level of <code>ui-container</code>. <code>bottom-container-root</code> is an empty variable, despite it is a direct child under <code>ui-container</code>. Could someone please let me know how to parse this tree properly?</p>
<p>I have no experience in web scraping, but as it happens it is one step in a bigger workflow I am building.</p>
",0,1565646185,python;python-3.x;web-scraping;beautifulsoup;urllib3,False,778,1,1612132596,https://stackoverflow.com/questions/57468606/python-scraping-website-with-flight-tickets
65977397,How to read .net file downloaded by urllib3?,"<p>I'm downloading the file <code>airports.net</code> from github with <code>urllib3</code> and read it as a graph object with <code>networkx.read_pajek</code> as follows:</p>
<pre><code>import urllib3
import networkx as nx


http = urllib3.PoolManager()
url = 'https://raw.githubusercontent.com/leanhdung1994/WebMining/main/airports.net'
f = http.request('GET', url)
G = nx.read_pajek(f.data(), encoding = 'UTF-8')
print(G)
</code></pre>
<p>Then there is an error</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-16-7728c1228755&gt; in &lt;module&gt;
     13 url = 'https://raw.githubusercontent.com/leanhdung1994/WebMining/main/airports.net'
     14 f = http.request('GET', url)
---&gt; 15 G = nx.read_pajek(f.data(), encoding = 'UTF-8')
     16 print(G)
     17 

TypeError: 'bytes' object is not callable
</code></pre>
<p>Could you please elaborate on how to do so?</p>
<p><strong>Update:</strong> If I change <code>f.data()</code> to <code>f.data</code>, then a new error appears</p>
<pre><code>/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-2-e96ad6eb1bfb&gt; in &lt;module&gt;()
      6 url = 'https://raw.githubusercontent.com/leanhdung1994/WebMining/main/airports.net'
      7 f = http.request('GET', url)
----&gt; 8 G = nx.read_pajek(f.data, encoding = 'UTF-8')
      9 print(G)

&lt;decorator-gen-781&gt; in read_pajek(path, encoding)

4 frames
/usr/local/lib/python3.6/dist-packages/networkx/readwrite/pajek.py in &lt;genexpr&gt;(.0)
    159     for format information.
    160     &quot;&quot;&quot;
--&gt; 161     lines = (line.decode(encoding) for line in path)
    162     return parse_pajek(lines)
    163 

AttributeError: 'int' object has no attribute 'decode'
</code></pre>
",1,1612084215,python;networkx;urllib3,True,204,1,1612090791,https://stackoverflow.com/questions/65977397/how-to-read-net-file-downloaded-by-urllib3
65912428,Python `urllib3`: sudden &quot;certificate verify failed: certificate has expired&quot; error,"<p>I have an old tool that can no longer successfully communicate with the outside world because it doesn't understand any of the modern TLS protocols or ciphers. I decided to put it behind a forwarding proxy that would bear the responsibility of dealing with HTTPS instead of it. I quickly wrote a working Python proxy based on code someone else had already had going to solve the problem but never finished it, however, in testing I quickly found the Stack Exchange network sites among some others do not open through it because of this error:</p>
<pre><code>&quot;GET https://stackoverflow.com/&quot; HTTPSConnectionPool(host='stackoverflow.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1123)')))
</code></pre>
<p>I did follow the advice on similar questions on manually downloading and adding the missing/expired CA certificates (seems like I lacked the Let's Encrypt Authority X3 (Let's Encrypt R3) in my case) to the CA bundle, but it didn't change a thing. What bothers me the most that I have a non-system up-to-date OpenSSL installation and IT IS ALSO AFFECTED out of the blue.</p>
<pre><code>&gt; openssl s_client -connect stackoverflow.com:443 -servername stackoverflow.com -quiet -CAfile .\cacert.pem
depth=2 O = Digital Signature Trust Co., CN = DST Root CA X3
verify return:1
depth=1 C = US, O = Let's Encrypt, CN = Let's Encrypt Authority X3
verify return:1
depth=0 CN = *.stackexchange.com
verify error:num=10:certificate has expired
notAfter=Jan  3 13:02:44 2021 GMT
verify return:1
depth=0 CN = *.stackexchange.com
notAfter=Jan  3 13:02:44 2021 GMT
verify return:1
</code></pre>
<p>But at the same time an old version of the proxy that I kept in the executable form turns out to be able to connect and serve content from any of the &quot;expired&quot; domains still just fine, but... WHAT? I'm now completely puzzled at all of this; it also doesn't have an up-to-date CA bundle unlike the new version.</p>
<p>The relevant parts of the code sending the request are here:</p>
<pre class=""lang-py prettyprint-override""><code>    self.timeout = urllib3.util.timeout.Timeout(connect=90.0, read=300.0)
    self.params = dict(maxsize = 10, block = True, timeout = timeout)
    self.sslparams = dict(cert_reqs='CERT_REQUIRED', ca_certs=CA_CERTS,
                          ssl_version=ssl.PROTOCOL_TLS_CLIENT)
    #...
    sslparams = {} if not isSSL else {**self.sslparams,
                                      'server_hostname': host,
                                      'assert_hostname': host,
                                     }
    params = {**sslparams, **self.params}
    pool = urllib3.PoolManager(**params)
    r = pool.urlopen(self.command, url, body=body, headers=headers,
                     enforce_content_length=False, retries=1, redirect=False,
                     preload_content=False, decode_content=False)
</code></pre>
<p>Additional info:</p>
<pre><code>&gt; python -c &quot;import ssl; print(ssl.OPENSSL_VERSION)&quot;
OpenSSL 1.1.1g  21 Apr 2020
&gt; python --version
Python 3.9.1
</code></pre>
<p>I have <code>pyOpenSSL</code> v20.0.1 and <code>urllib3</code> v1.26.2. I'm on Windows 10 x64 with the latest updates installed.</p>
",2,1611717248,python;ssl;openssl;urllib3,False,806,0,1611851161,https://stackoverflow.com/questions/65912428/python-urllib3-sudden-certificate-verify-failed-certificate-has-expired-er
65925477,ImportError: cannot import name &#39;IPV6_ADDRZ_RE&#39; from &#39;urllib3.util.url&#39;,"<p>I am trying to run a .ipynb file using jupyter notebook but I'm getting this error.</p>
<p>I've been scouring the internet for days but nothing seems to work. What I've tried:</p>
<ul>
<li>install the 'urlib3.util.url 'packages</li>
<li>pip install -Iv botocore==1.17</li>
<li>uninstall then reinstall urllib3</li>
<li>upgrade urllib3</li>
</ul>
<p><a href=""https://i.stack.imgur.com/foCAd.png"" rel=""nofollow noreferrer"">here's a screenshot of the error</a></p>
",0,1611773239,python;importerror;urllib3,False,1146,0,1611773239,https://stackoverflow.com/questions/65925477/importerror-cannot-import-name-ipv6-addrz-re-from-urllib3-util-url
65924713,Is it possible to use https proxies with python?,"<p>I'm trying to send requests, currently using the <code>requests</code> library, via an https proxy server. The issue is, as I understand it, urllib (the underlying request module) doesn't support it. Is there an alternative in python?</p>
",0,1611770216,python;python-3.x;python-requests;urllib;urllib3,False,73,0,1611770615,https://stackoverflow.com/questions/65924713/is-it-possible-to-use-https-proxies-with-python
65871852,Download Videos using python from ttdownloader,"<p>Hey guys I need some help, I am trying to download videos from this site<a href=""https://ttdownloader.com/dl.php?v=YTo0OntzOjk6IndhdGVybWFyayI7YjowO3M6NzoidmlkZW9JZCI7czoxOToiNjkxMjEwNzYyNzY1MjY5NzM1MCI7czozOiJ1aWQiO3M6MzI6Ijk0MTdiOWE3NWU2MmE3MDQ1NjZhYzk0MzJjMThlY2VlIjtzOjQ6InRpbWUiO2k6MTYxMTQ5NzE1ODt9"" rel=""nofollow noreferrer"">https://ttdownloader.com/dl.php?v=YTo0OntzOjk6IndhdGVybWFyayI7YjowO3M6NzoidmlkZW9JZCI7czoxOToiNjkxMjEwNzYyNzY1MjY5NzM1MCI7czozOiJ1aWQiO3M6MzI6Ijk0MTdiOWE3NWU2MmE3MDQ1NjZhYzk0MzJjMThlY2VlIjtzOjQ6InRpbWUiO2k6MTYxMTQ5NzE1ODt9</a> using python.</p>
<p>this is code I have tried.</p>
<pre><code>import requests

url ='''https://ttdownloader.com/dl.php?v=YTo0OntzOjk6IndhdGVybWFyayI7YjowO3M6NzoidmlkZW9JZCI7czoxOToiNjkxMjEwNzYyNzY1MjY5NzM1MCI7czozOiJ1aWQiO3M6MzI6Ijk0MTdiOWE3NWU2MmE3MDQ1NjZhYzk0MzJjMThlY2VlIjtzOjQ6InRpbWUiO2k6MTYxMTQ5NzE1ODt9'''
page = requests.get(url)
with open('output.mp4', 'wb') as file:
    file.write(page.content)
</code></pre>
<p>But it doesnt work as expected, when i check page.content all I see is b''</p>
",1,1611499057,python;beautifulsoup;python-requests;urllib3,True,259,2,1611501948,https://stackoverflow.com/questions/65871852/download-videos-using-python-from-ttdownloader
65847171,Convert list of String into list of URL using Python,"<p>I Have a list of string(which are actually URL)</p>
<pre><code>['https://part.of.url/P2000-01.xls',

 'https://part.of.url/P2001-02.xls',

 'https://part.of.url/P2002-03.xls',

 'https://part.of.url/P2003-04.xls',

 'https://part.of.url/P2004-05.xls',

 'https://part.of.url/P2005-06.xls',

 'https://part.of.url/P2006-07.xls',

 'https://part.of.url/P2007-08.xls',

 'https://part.of.url/P2008-09.xls',

 'https://part.of.url/P2009-10.xls',

 'https://part.of.url/P2010-11.xls',

 'https://part.of.url/P2011-12.xls',

 'https://part.of.url/P2012-13.xls',

 'https://part.of.url/P2013-14.xls',

 'https://part.of.url/P2014-15.xls',

 'https://part.of.url/P2015-16.xls',

 'https://part.of.url/P2016-17.xls']
</code></pre>
<p>I wish to convert it into link form and using the code</p>
<pre><code>
    for urlValue in url:
        print(urllib.parse.quote(urlValue))

</code></pre>
<p>output I am getting is</p>
<pre><code>https%3A//part.of.url/P2012-13.xls

https%3A//part.of.url/P2013-14.xls

https%3A//part.of.url/P2014-15.xls

https%3A//part.of.url/P2015-16.xls
</code></pre>
<p>It should be the list of URL containing list such as below.</p>
<p><a href=""https://part.of.url/P2012-13.xls"" rel=""nofollow noreferrer"">https://part.of.url/P2012-13.xls</a></p>
<p>How can it be solved?</p>
",-2,1611326191,python;urllib;urllib3,True,615,2,1611342622,https://stackoverflow.com/questions/65847171/convert-list-of-string-into-list-of-url-using-python
62599036,Python requests is slow and takes very long to complete HTTP or HTTPS request,"<p>When requesting a web resource or website or web service with the requests library, the request takes a long time to complete. The code looks similar to the following:</p>
<pre class=""lang-py prettyprint-override""><code>import requests
requests.get(&quot;https://www.example.com/&quot;)
</code></pre>
<p>This request takes over 2 minutes (exactly 2 minutes 10 seconds) to complete! Why is it so slow and how can I fix it?</p>
",57,1593188938,python;python-3.x;python-requests;urllib3,True,69884,1,1610980425,https://stackoverflow.com/questions/62599036/python-requests-is-slow-and-takes-very-long-to-complete-http-or-https-request
56781990,Disable HeaderParsingError appearing from python3 urllib3,"<p>how can I suppress the <strong>Failed to parse headers error</strong> that appears from the urllib3 library?</p>

<p>The following errors keep appearing:</p>

<pre><code>Failed to parse headers (url=https://test): [StartBoundaryNotFoundDefect(), MultipartInvariantViolationDefect()], unparsed data: ''
Traceback (most recent call last):
  File ""/opt/test_project/.venv/lib/python3.5/site-packages/urllib3/connectionpool.py"", line 399, in _make_request
    assert_header_parsing(httplib_response.msg)
  File ""/opt/test_project/.venv/lib/python3.5/site-packages/urllib3/util/response.py"", line 66, in assert_header_parsing
    raise HeaderParsingError(defects=defects, unparsed_data=unparsed_data)
urllib3.exceptions.HeaderParsingError: [StartBoundaryNotFoundDefect(), MultipartInvariantViolationDefect()], unparsed data: ''
</code></pre>

<p>I have tried to suppress it using </p>

<pre><code>import urllib3
# Disable SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
# Disbale urllib3.exceptions.HeaderParsingError:
urllib3.disable_warnings(urllib3.exceptions.HeaderParsingError)

</code></pre>

<p>but seems like it's not working as it is still appearing.</p>

<p>There are some solutions online but that is suppressing via the logs. I am looking for a method to suppress it not from the log level.</p>

<p>AFAIK, this is just the warning from urllib3 and it's been reported as a bug. Therefore, is there any way I can suppress this?</p>
",6,1561590530,python;python-requests;urllib3,True,3256,1,1610556299,https://stackoverflow.com/questions/56781990/disable-headerparsingerror-appearing-from-python3-urllib3
65491229,Python - HTTP module cannot parse response if the server answers before the PUT is complete,"<p>I'm using the <code>requests</code> (which uses <code>urllib3</code> and the Python http module under the hood) library to upload a file from a Python script.
My backend starts by inspecting the headers of the request and if it doesn't comply with the needed prerequisites, it stops the request right away and respond with a valid 400 response.</p>
<p>This behavior works fine in Postman, or with Curl; i.e. the client is able to parse the 400 response even though it hasn't completed the upload and the server answers prematurely.
However, while doing so in Python with <code>requests</code>/<code>urllib3</code>, the library is unable to process the backend response :</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\Neumann\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\urllib3\connectionpool.py&quot;, line 670, in urlopen
    httplib_response = self._make_request(
  File &quot;C:\Users\Neumann\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\urllib3\connectionpool.py&quot;, line 392, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File &quot;C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.8_3.8.1776.0_x64__qbz5n2kfra8p0\lib\http\client.py&quot;, line 1255, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File &quot;C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.8_3.8.1776.0_x64__qbz5n2kfra8p0\lib\http\client.py&quot;, line 1301, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File &quot;C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.8_3.8.1776.0_x64__qbz5n2kfra8p0\lib\http\client.py&quot;, line 1250, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File &quot;C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.8_3.8.1776.0_x64__qbz5n2kfra8p0\lib\http\client.py&quot;, line 1049, in _send_output
    self.send(chunk)
  File &quot;C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.8_3.8.1776.0_x64__qbz5n2kfra8p0\lib\http\client.py&quot;, line 971, in send
    self.sock.sendall(data)
ConnectionResetError: [WinError 10054] Une connexion existante a dû être fermée par l’hôte distant
</code></pre>
<p>Because the server answers before the transfer is complete, it mistakenly considers that the connection has been aborted, even though the server DOES return a valid response.</p>
<p>Is there a way to avoid this and parse the response nonetheless ?</p>
<p>Steps to reproduce the issue :</p>
<ul>
<li>Download minIO : <a href=""https://min.io/download#/"" rel=""noreferrer"">https://min.io/download#/</a></li>
<li>Run minIO :</li>
</ul>
<pre><code>export MINIO_ACCESS_KEY=&lt;access_key&gt;
export MINIO_SECRET_KEY=&lt;secret_key&gt;
.\minio.exe server &lt;data folder&gt;
</code></pre>
<ul>
<li>Run the following script :</li>
</ul>
<pre><code>import os
import sys
import requests
from requests_toolbelt.multipart.encoder import MultipartEncoder

def fatal(msg):
    print(msg)
    sys.exit(1)

def upload_file():
    mp_encoder = MultipartEncoder(fields={'file': (open('E:/Downloads/kek.mp3', 'rb'))})
    headers = { &quot;Authorization&quot;: &quot;invalid&quot; }

    print('Uploading file with headers : ' + str(headers))

    upload_endpoint = 'http://localhost:9000/mybucket/myobject'
    try:
        r = requests.put(upload_endpoint, headers=headers, data=mp_encoder, verify=False)
    except requests.exceptions.ConnectionError as e:
        print(e.status)
        for property, value in vars(e).items():
            print(property, &quot;:&quot;, value)
        fatal(str(e))
    
    if r.status_code != 201:
        for property, value in vars(r).items():
            print(property, &quot;:&quot;, value)
        fatal('Error while uploading file. Status ' + str(r.status_code))
    print('Upload successfully completed')

if __name__ == &quot;__main__&quot;:
    upload_file()

</code></pre>
<p>If you change the request line with this, it will work (i.e. the server returns 400 and the client is able to parse it) :</p>
<pre><code>r = requests.put(upload_endpoint, headers=headers, data='a string', verify=False)
</code></pre>
<p><em><strong>EDIT</strong></em> : I updated the traceback and changed the question title to reflect the fact that it's neither <code>requests</code> or <code>urllib3</code> fault, but the Python http module that is used by both of them.</p>
",9,1609240429,python;python-3.x;http;python-requests;urllib3,True,921,2,1610018598,https://stackoverflow.com/questions/65491229/python-http-module-cannot-parse-response-if-the-server-answers-before-the-put
31151615,How to handle proxies in urllib3,"<p>I am having trouble finding solid examples of how to build a simple script in urllib3 which opens a url (via a proxy), then reads it and finally prints it. The proxy requires a user/pass to authenticate however it's not clear to me how you do this? Any help would be appreciated.</p>
",15,1435717436,python;proxy;urllib3,True,34608,2,1609178533,https://stackoverflow.com/questions/31151615/how-to-handle-proxies-in-urllib3
62240041,Max retries exceed with url (Failed to establish a new connection: [Errno 110] Connection timed out),"<pre><code>raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='mycompanyurl.in',     
port=443): Max retries exceeded with url: /api/v1/issues.json (Caused by 
NewConnectionError('&lt;requests.packages.urllib3.connection.VerifiedHTTPSConnection object 
at 0x51047d0&gt;: Failed to establish a new connection: [Errno 110] Connection timed out',))
</code></pre>
<p>However, <code>mycompanyurl.in</code> is fine &amp; I can open it in a browser as well.
I'm using Python 2.7.5.</p>
",6,1591495073,python;python-requests;urllib;urllib3,True,42314,2,1608753245,https://stackoverflow.com/questions/62240041/max-retries-exceed-with-url-failed-to-establish-a-new-connection-errno-110-c
65376820,Tried scraping group on for practice only. Why it returns less results?,"<p>I am new to Python scraping, so as part of the practice I was trying few other sites where often data wasn't returned at all, but when I checked Groupon, I found that <code>urllib</code> only returns the first 8 results, while there are 36 results on the browser page.</p>
<p>I am using <code>urllib</code> and <code>BS4</code>. below is the code</p>
<pre><code>from urllib.request import Request, urlopen
from bs4 import BeautifulSoup
req = Request('https://www.groupon.com/browse/chicago?category=beauty-and-spas')
req.add_header('User-Agent',
               'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36')
try:
    with urlopen(req) as response:
        htmlcontent = response.read().decode('utf-8')
except:
    htmlcontent = None
soup = BeautifulSoup(htmlcontent, 'lxml')
all_links = soup.find('div', { 'id': 'pull-results' }).select('figure &gt; div &gt; a')
</code></pre>
<p>Can somebody please tell, what am I missing in the code to be able to extract all the data?
If this doesn't work or shouldn't work, then do we have selenium as the only option?</p>
",0,1608437046,python;web-scraping;beautifulsoup;urllib3,True,75,2,1608446623,https://stackoverflow.com/questions/65376820/tried-scraping-group-on-for-practice-only-why-it-returns-less-results
65294060,SODA API not returning all fields in data set,"<p>I am having trouble obtaining all of the fields in a dataset from the Socrata Open Data Network. I am using urllib3 to make the request as such:</p>
<pre><code>url = 'https://data.sfgov.org/resource/g8m3-pdis.json'
http = urllib3.PoolManager(ca_certs=certifi.where())
req = http.request_encode_url('GET', url)


data = json.loads(req.data.decode('utf-8'))
</code></pre>
<p>This request returns all but the last 6 fields in the dataset. I have tried using the</p>
<pre><code>$select=*,*
</code></pre>
<p>statement to specify which fields I would like to retrieve but that still leaves out the last 6 fields even if I specify them.</p>
<p>However, when I use the</p>
<pre><code>$limit
</code></pre>
<p>statement and set it to something greater than 1000 I get some of those fields that I am missing. But, still not all of them.</p>
<p>One thing that does work is specifying which row to grab, so if I use the following:</p>
<pre><code>https://data.sfgov.org/resource/g8m3-pdis.json?ttxid=0000028-02-001
</code></pre>
<p>It will return all of the fields that I am looking for.</p>
<p>I guess I could make a request for all of the ttxid fields and then send out a separate request for each one in order to grab all the fields but there has to be a better way to do this. Does anyone have any ideas?</p>
",2,1607968739,python;urllib3;socrata;soda,True,438,1,1608081429,https://stackoverflow.com/questions/65294060/soda-api-not-returning-all-fields-in-data-set
65250991,python - With urllib3 Retry add solution for retries based on response content,"<p>I am using the usual urllib3 solution for requests to have retry and timeout handling:</p>
<pre><code>DEFAULT_TIMEOUT = 20 # seconds

class TimeoutHTTPAdapter(HTTPAdapter):
    def __init__(self, *args, **kwargs):
        self.timeout = DEFAULT_TIMEOUT
        if &quot;timeout&quot; in kwargs:
            self.timeout = kwargs[&quot;timeout&quot;]
            del kwargs[&quot;timeout&quot;]
        super().__init__(*args, **kwargs)

    def send(self, request, **kwargs):
        timeout = kwargs.get(&quot;timeout&quot;)
        if timeout is None:
            kwargs[&quot;timeout&quot;] = self.timeout
        return super().send(request, **kwargs)

retry_strategy = Retry(
    total=5,
    backoff_factor=2,
    status_forcelist=[400, 408, 429, 502, 503, 504, 508],
    method_whitelist=[&quot;HEAD&quot;, &quot;GET&quot;, &quot;OPTIONS&quot;, &quot;POST&quot;]
)
adapter = TimeoutHTTPAdapter(max_retries=retry_strategy)
http = requests.Session()
http.mount(&quot;https://&quot;, adapter)
http.mount(&quot;http://&quot;, adapter)
</code></pre>
<p>But sometimes I receive post request responses with response code 200 who contain error messages and service timeouts.</p>
<p>Is it possible to handle such content based retries with the build in retry solution of urllib3 or with other solutions?</p>
",3,1607688039,python;python-requests;urllib3,False,530,0,1607688039,https://stackoverflow.com/questions/65250991/python-with-urllib3-retry-add-solution-for-retries-based-on-response-content
58678037,How can i post XML with urllib3,"<p>I create a sms gateway with GSM Modem that support Hilink. i stuck in to post XML. first step is get session and token. second. with token pass to header. it will granted access to send message. but, how to pass XML Data with urllib3?</p>

<pre><code>import urllib3
import xml.etree.ElementTree as XML

http = urllib3.PoolManager()

response_body = http.request('GET', 'http://192.168.8.1/api/webserver/SesTokInfo')

tree = XML.ElementTree(XML.fromstring(response_body.data))
root = tree.getroot()

token = root[1].text

data=""&lt;?xml version='1.0' encoding='UTF-8'?&gt;&lt;request&gt;&lt;Index&gt;-1&lt;/Index&gt;&lt;Phones&gt;&lt;Phone&gt;Number&lt;/Phone&gt;&lt;/Phones&gt;&lt;Sca&gt;&lt;/Sca&gt;&lt;Content&gt;test&lt;/Content&gt;&lt;Length&gt;4&lt;/Length&gt;&lt;Reserved&gt;1&lt;/Reserved&gt;&lt;Date&gt;-1&lt;/Date&gt;&lt;/request&gt;""

send_message = http.request('POST', 'http://192.168.8.1/api/sms/send-sms', data=data, headers={'__RequestVerificationToken': token, 'Content-Type': 'application/xml'})

print(send_message.status)
</code></pre>
",2,1572764011,python;xml;urllib3,True,1441,1,1607477033,https://stackoverflow.com/questions/58678037/how-can-i-post-xml-with-urllib3
65170395,Python post to ms teams,"<p>Trying to post a message to MS Teams webhook from Python below:</p>
<pre><code>pmr = urllib3.PoolManager()
text='hello world'
message = {&quot;Test&quot;:text}
enco_message = json.dumps(message).encode('utf-8')
r=pmr.request('POST',url, headers={'Content-Type': 'application/json'}, body=enco_message)
print(r.status)
</code></pre>
<p>But this does not send any message to teams and returns response status <code>400</code>. Not sure what is to be changed in the code. Thanks for the help.</p>
",2,1607272281,python;python-requests;webhooks;microsoft-teams;urllib3,True,3024,1,1607325148,https://stackoverflow.com/questions/65170395/python-post-to-ms-teams
65171598,How does urllib3 determine which TLS extensions to use?,"<p>I'd like to modify the Extensions that I send in the client Hello packet with python.
I've had a read of most of the source code found on GitHub for urllib3 but I still don't know how it determines which TLS extensions to use.
I am aware that it will be quite low level and the creators of urllib3 may just import another package to do this for them. If this is the case, which package do they use?
If not, how is this determined?
Thanks in advance for any assistance.</p>
",1,1607279352,python;ssl;httpclient;tls1.2;urllib3,True,461,1,1607287892,https://stackoverflow.com/questions/65171598/how-does-urllib3-determine-which-tls-extensions-to-use
65110021,Issues using Scrapy with Google Cloud Storage as a feed export,"<p>I am using GCS as a feed export on Scrapy as per the <a href=""https://docs.scrapy.org/en/latest/topics/feed-exports.html"" rel=""nofollow noreferrer"">scrapy docs</a>. What is strange is that it does work some of the time.</p>
<p>But other times it will fail at the point of upload and the only thing I can see that is different is it was trying to upload more data. Having said that, it still failed with a ~60Mb upload which makes me question whether scale of data is really an issue here. Could someone explain whether this is an issue with my configuration or perhaps with Scrapy itself? The error report is below:</p>
<pre><code>2020-12-01 23:07:26 [scrapy.extensions.feedexport] ERROR: Error storing csv feed (19826 items) in: gs://instoxi_amazon/com/Ngolo/Amazon_Beauty_&amp;_Personal_Care_Ngolo.csv
Traceback (most recent call last):
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py&quot;, line 600, in urlopen
    chunked=chunked)
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py&quot;, line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File &quot;C:\ProgramData\Anaconda3\lib\http\client.py&quot;, line 1244, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File &quot;C:\ProgramData\Anaconda3\lib\http\client.py&quot;, line 1290, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File &quot;C:\ProgramData\Anaconda3\lib\http\client.py&quot;, line 1239, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File &quot;C:\ProgramData\Anaconda3\lib\http\client.py&quot;, line 1065, in _send_output
    self.send(chunk)
  File &quot;C:\ProgramData\Anaconda3\lib\http\client.py&quot;, line 987, in send
    self.sock.sendall(data)
  File &quot;C:\ProgramData\Anaconda3\lib\ssl.py&quot;, line 1034, in sendall
    v = self.send(byte_view[count:])
  File &quot;C:\ProgramData\Anaconda3\lib\ssl.py&quot;, line 1003, in send
    return self._sslobj.write(data)
ssl.SSLWantWriteError: The operation did not complete (write) (_ssl.c:2361)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\requests\adapters.py&quot;, line 449, in send
    timeout=timeout
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py&quot;, line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py&quot;, line 399, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='storage.googleapis.com', port=443): Max retries exceeded with url: /upload/storage/v1/b/instoxi_amazon/o?uploadType=resumable&amp;upload_id=ABg5-Uwjc9Vs5HdgyQdhTTm0ph3N_xQIoZaAE44Oiv2MdMO6q-YhD31eRkWO6W7UNAlehUKm4FTgVv0KXq32SHmCrDU (Caused by SSLError(SSLWantWriteError(3, 'The operation did not complete (write) (_ssl.c:2361)')))
</code></pre>
<p>This is my first question so let me know if there's a better way to ask / present. Just to clarify, I have had no issues with using Python to interact with GCS outside of Scrapy. Cheers!</p>
",1,1606919182,python;python-requests;scrapy;google-cloud-storage;urllib3,True,502,1,1606990854,https://stackoverflow.com/questions/65110021/issues-using-scrapy-with-google-cloud-storage-as-a-feed-export
64981301,How to validate public hash key of google managed ssl certificate while doing http request to server in python?,"<p>I am using the google app engine for my application and it has google managed SSL certificates enable. Google automatically renews it also before expiration.</p>
<p>One thing I noticed that google uses the same private key for creating or renewing the new certificates so the hash of the public key is not changing.</p>
<p>Now what I wanted to do is validate this public key hash while doing a request to the server in python using urllib3 or with any other library. So that if it doesn't matches with the hash of the incoming public key hash it fails the request and alerts me about me.</p>
<p>I went through a lot of links about pinning and all but didn't able to find anything for such comparison. Can somebody help me here? Specifically how to do it with python3?</p>
<p>PS: I don't have the certificate or its private key since it is managed by Google (didn't find any option too to download the current certificate from the google app engine). I already have the hash of the public key as <code>Pin SHA256: xxxxxxxxxxxx</code></p>
",0,1606199425,python;google-app-engine;ssl;urllib3;public-key-pinning,False,287,0,1606199425,https://stackoverflow.com/questions/64981301/how-to-validate-public-hash-key-of-google-managed-ssl-certificate-while-doing-ht
54194784,No module named ordered_dict error for from twilio.rest import Client,"<p><strong>aks.py:</strong></p>

<pre><code>from twilio.rest import Client
</code></pre>

<p>When execute above code, I am getting the error: </p>

<pre><code>Traceback (most recent call last):
  File ""aks.py"", line 10, in &lt;module&gt;
    from twilio.rest import Client
  File ""/usr/local/lib/python2.7/dist-packages/twilio/rest/__init__.py"", line 14, in &lt;module&gt;
    from twilio.http.http_client import TwilioHttpClient
  File ""/usr/local/lib/python2.7/dist-packages/twilio/http/http_client.py"", line 1, in &lt;module&gt;
    from requests import Request, Session, hooks
  File ""/usr/lib/python2.7/dist-packages/requests/__init__.py"", line 63, in &lt;module&gt;
    from . import utils
  File ""/usr/lib/python2.7/dist-packages/requests/utils.py"", line 24, in &lt;module&gt;
    from ._internal_utils import to_native_string
  File ""/usr/lib/python2.7/dist-packages/requests/_internal_utils.py"", line 11, in &lt;module&gt;
    from .compat import is_py2, builtin_str, str
  File ""/usr/lib/python2.7/dist-packages/requests/compat.py"", line 46, in &lt;module&gt;
    from .packages.urllib3.packages.ordered_dict import OrderedDict
ImportError: No module named ordered_dict
</code></pre>

<p>I tried various things like degrading Urllib to 1.23, uninstal, reinstall twilio and  <code>sudo -H pip2.7 install twilio</code></p>

<ul>
<li>python version: Python 2.7.13</li>
<li>pip version: pip 18.1 from /usr/local/lib/python3.5/dist-packages/pip (python 3.5) </li>
<li>OS : Raspbian Stretch</li>
</ul>
",1,1547539383,python;python-2.7;twilio;twilio-api;urllib3,True,1646,3,1605791543,https://stackoverflow.com/questions/54194784/no-module-named-ordered-dict-error-for-from-twilio-rest-import-client
64908049,Problem with PDF download that I cannot open,"<p>I am working on a script to extract text from law cases using <a href=""https://case.law/docs/site_features/api"" rel=""nofollow noreferrer"">https://case.law/docs/site_features/api</a>. I have created methods for search and create-xlsx, which work well, but I am struggling with the method to open an online pdf link, write (wb) in a temp file, read and extract the data (core text), then close it. The ultimate goal is to use the content of these cases for NLP.</p>
<p>I have prepared a function (see below) to download the file:</p>
<pre><code>def download_file(file_id):
    http = urllib3.PoolManager()
    folder_path = &quot;path_to_my_desktop&quot;
    file_download = &quot;https://cite.case.law/xxxxxx.pdf&quot;
    file_content = http.request('GET', file_download)
    file_local = open( folder_path + file_id + '.pdf', 'wb' )
    file_local.write(file_content.read())
    file_content.close()
    file_local.close()
</code></pre>
<p>The script works well as it download the file and it created on my desktop, but, when I try to open manually the file on the desktop I have this message from acrobat reader:</p>
<blockquote>
<p>Adobe Acrobat Reader could not open 'file_id.pdf' because it is either not a supported file type or because the file has been damager (for example, it was sent as a email attachments and wasn't correctly decoded</p>
</blockquote>
<p>I thought it was the Library so I tried with <em><strong>Requests / xlswriter / urllib3...</strong></em> (example below - I also tried to read it from the script to see whether it was Adobe that was the issue, but apparently not)</p>
<pre><code># Download the pdf from the search results
URL = &quot;https://cite.case.law/xxxxxx.pdf&quot;
r = requests.get(URL, stream=True)
with open('path_to_desktop + pdf_name + .pdf', 'w') as f:
      f.write(r.text)

# open the downloaded file and remove '&lt;[^&lt;]+?&gt;' for easier reading
with open('C:/Users/amallet/Desktop/r.pdf', 'r') as ff:
      data_read = ff.read()
      stripped = re.sub('&lt;[^&lt;]+?&gt;', '', data_read)
      print(stripped)
</code></pre>
<p>the output is:</p>
<pre><code>document.getElementById('next').value = document.location.toString();
document.getElementById('not-a-bot-form').submit();
</code></pre>
<p>with 'wb'and 'rb' instead (and removing the *** stripped *** the sript is:</p>
<pre><code>r = requests.get(test_case_pdf, stream=True)
with open('C:/Users/amallet/Desktop/r.pdf', 'wb') as f:
      f.write(r.content)

with open('C:/Users/amallet/Desktop/r.pdf', 'rb') as ff:
      data_read = ff.read()
      print(data_read)
</code></pre>
<p>and the output is :</p>
<pre><code>&lt;html&gt;
&lt;head&gt;
&lt;noscript&gt;
&lt;meta http-equiv=&quot;Refresh&quot; content=&quot;0;URL=?no_js=1&amp;next=/pdf/7840543/In%20re%20the%20Extradition%20of%20Garcia,%20890%20F.%20Supp.%20914%
20(1994).pdf&quot; /&gt;
&lt;/noscript&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;form method=&quot;post&quot; id=&quot;not-a-bot-form&quot;&gt;
&lt;input type=&quot;hidden&quot; name=&quot;csrfmiddlewaretoken&quot; value=&quot;5awGW0F4A1b7Y6bx
rYBaA6GIvqx4Tf6DnK0qEMLVoJBLoA3ZqOrpMZdUXDQ7ehOz&quot;&gt;
&lt;input type=&quot;hidden&quot; name=&quot;not_a_bot&quot; value=&quot;yes&quot;&gt;
&lt;input type=&quot;hidden&quot; name=&quot;next&quot; value=&quot;/pdf/7840543/In%20re%20
the%20Extradition%20of%20Garcia,%20890%20F.%20Supp.%20914%20(1994).pdf&quot; id=&quot;next&quot;&gt;
&lt;/form&gt;
&lt;script&gt;
document.getElementById(\'next\').value = document.loc
ation.toString();
document.getElementById(\'not-a-bot-form\').submit();
&lt;/script&gt;
&lt;a href=&quot;?no_js=1&amp;next=/pdf/7840543/In%20re%20the%20Extradition%20of%20Garcia,%2
0890%20F.%20Supp.%20914%20(1994).pdf&quot;&gt;Click here to continue&lt;/a&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>but none are working. The pdf is not protected by a password, and I tried on other website and it doesn't work either.</p>
<p>Therefore, I am wondering whether I have another issue that is not link to the code itself.</p>
<p>Please let me know if you need additional information.</p>
<p>thank you</p>
",3,1605774992,python;pdf;python-requests;urllib3;data-collection,True,1298,1,1605783555,https://stackoverflow.com/questions/64908049/problem-with-pdf-download-that-i-cannot-open
64861069,Python unquote URL but retain hyperlink,"<p>I have a URL written in Cyrillic. When inserting it in python, I get its encoded form:</p>
<pre><code>link = https://dspace.udpu.edu.ua/bitstream/6789/1751/1/%D0%97%D0%B0%D0%B3%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D0%BD%D0%B0%D1%83%D0%BA%D0%BE%D0%B2%D1%96%20%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D0%B8%20%D0%B4%D0%BE%D1%81%D0%BB%D1%96%D0%B4%D0%B6%D0%B5%D0%BD%D0%BD%D1%8F.pdf
</code></pre>
<p>To convert it into a hyperlink with Cyrillic characters, I have tried using urllib.parse.unquote:</p>
<pre><code>from urllib.parse import unquote
unquoted_link = unquote(link)
</code></pre>
<p>The result for <em>unquoted_link</em> is:</p>
<p><a href=""https://dspace.udpu.edu.ua/bitstream/6789/1751/1/%D0%97%D0%B0%D0%B3%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D0%BD%D0%B0%D1%83%D0%BA%D0%BE%D0%B2%D1%96"" rel=""nofollow noreferrer"">https://dspace.udpu.edu.ua/bitstream/6789/1751/1/Загальнонаукові</a> методи дослідження.pdf</p>
<p>I.e., I have the hyperlink only before the first space, and then it is plain text. How can I make this whole string a hyperlink, just as StackOverflow does in the first hyperlink I provided?</p>
",3,1605541404,python;urllib;urllib3,True,166,1,1605632494,https://stackoverflow.com/questions/64861069/python-unquote-url-but-retain-hyperlink
64742206,python urllib3 lambda Error: LocationParseError Failed to parse,"<p>I'm using urllib3 library in Lambda and python3 code that fetches the webhook url of MSTeams from AWS Secret Manager and sends a http post request to publish a notification.</p>
<p>My webhook url starts with https and looks like this &quot;https://outlook.office.com/webhook/.......&quot;. On executing the lambda function, I get an error as shown below <code>LocationParseError Failed to parse:</code></p>
<p>Code</p>
<pre><code>import urllib3 

http = urllib3.PoolManager() 

MSTEAMS_WEBHOOK_SECRET_NAME = os.getenv('MSTEAMS_WEBHOOK_SECRET_NAME') 
HOOK_URL = get_secret(MSTEAMS_WEBHOOK_SECRET_NAME,&quot;eu-west-1&quot;) 

def get_secret(secret_name, region_name):

    # Create a Secrets Manager client
    session = boto3.session.Session()
    client = session.client(
        service_name='secretsmanager',
        region_name=region_name
    )
        get_secret_value_response = client.get_secret_value(
            SecretId=secret_name,
            VersionStage=&quot;AWSCURRENT&quot;
        )
        if 'SecretString' in get_secret_value_response:
            secret = get_secret_value_response['SecretString']
            return secret
        else:
            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])
            return decoded_binary_secret


def lambda_handler(event, context):
 message = {
      &quot;@context&quot;: &quot;https://schema.org/extensions&quot;,
      &quot;@type&quot;: &quot;MessageCard&quot;,
      &quot;themeColor&quot;: data[&quot;colour&quot;],
      &quot;title&quot;: title,
      &quot;text&quot;: &quot;accountId:\n&quot;  + account_id + &quot; &lt;br/&gt;\n&quot; 
    }
 
    webhook_encoded_body = json.dumps(message).encode('utf-8')
    response = http.request('POST',HOOK_URL, body=webhook_encoded_body)
</code></pre>
<p>errorMessage</p>
<pre><code>{
  &quot;errorMessage&quot;: &quot;Failed to parse: {\&quot;msteams-secret\&quot;:\&quot;https://outlook.office.com/webhook/dxxxxxx@d779xxxxx-xxxxxx/IncomingWebhook/axxxxxx5/ca746326-bxxx-4xxx-8x-xxxxx\&quot;}&quot;,
  &quot;errorType&quot;: &quot;LocationParseError&quot;,
  &quot;stackTrace&quot;: [
    [
      &quot;/var/task/lambda_function.py&quot;,
      145,
      &quot;lambda_handler&quot;,
      &quot;resp = http.request('POST',HOOK_URL, body=webhook_encoded_body)&quot;
    ],
    [
      &quot;/var/runtime/urllib3/request.py&quot;,
      80,
      &quot;request&quot;,
      &quot;method, url, fields=fields, headers=headers, **urlopen_kw&quot;
    ],
    [
      &quot;/var/runtime/urllib3/request.py&quot;,
      171,
      &quot;request_encode_body&quot;,
      &quot;return self.urlopen(method, url, **extra_kw)&quot;
    ],
    [
      &quot;/var/runtime/urllib3/poolmanager.py&quot;,
      324,
      &quot;urlopen&quot;,
      &quot;u = parse_url(url)&quot;
    ],
    [
      &quot;/var/runtime/urllib3/util/url.py&quot;,
      392,
      &quot;parse_url&quot;,
      &quot;return six.raise_from(LocationParseError(source_url), None)&quot;
    ],
    [
      &quot;&lt;string&gt;&quot;,
      3,
      &quot;raise_from&quot;,
      &quot;&quot;
    ]
  ]
}
</code></pre>
",0,1604863798,python;lambda;microsoft-teams;urllib3;aws-secrets-manager,False,1596,1,1604873318,https://stackoverflow.com/questions/64742206/python-urllib3-lambda-error-locationparseerror-failed-to-parse
64742085,Python urllib3 Lambda: LocationValueError: No host specified HTTP Webhook,"<p>I'm using urllib3 library and python3 lambda code that fetches the webhook url of MSTeams from AWS Secret Manager and sends a <code>http post</code> request to publish a SNS message.</p>
<p>My  <code>webhook url</code> stars with https a d looks like this &quot;https://outlook.office.com/webhook/.......&quot;. On executing the lambda function, I get an error as shown below for <code>LocationValueError: No host specified</code></p>
<p>Code</p>
<pre><code>import urllib3 

http = urllib3.PoolManager() 

MSTEAMS_WEBHOOK_SECRET_NAME = os.getenv('MSTEAMS_WEBHOOK_SECRET_NAME') 
HOOK_URL = get_secret(MSTEAMS_WEBHOOK_SECRET_NAME) 

def lambda_handler(event, context):
 message = {
      &quot;@context&quot;: &quot;https://schema.org/extensions&quot;,
      &quot;@type&quot;: &quot;MessageCard&quot;,
      &quot;themeColor&quot;: data[&quot;colour&quot;],
      &quot;title&quot;: title,
      &quot;text&quot;: &quot;accountId:\n&quot;  + account_id + &quot; &lt;br/&gt;\n&quot; 
    }
 
    webhook_encoded_body = json.dumps(message).encode('utf-8')
    response = http.request('POST',HOOK_URL, body=webhook_encoded_body)
 
</code></pre>
<p>**Error while testing on Python 3.7 lambda execution engine **</p>
<pre><code>Traceback (most recent call last):
  File &quot;/var/task/lambda_function.py&quot;, line 145, in lambda_handler
    resp = http.request('POST',HOOK_URL, body=webhook_encoded_body)
  File &quot;/var/runtime/urllib3/request.py&quot;, line 80, in request
    method, url, fields=fields, headers=headers, **urlopen_kw
  File &quot;/var/runtime/urllib3/request.py&quot;, line 171, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File &quot;/var/runtime/urllib3/poolmanager.py&quot;, line 325, in urlopen
    conn = self.connection_from_host(u.host, port=u.port, scheme=u.scheme)
  File &quot;/var/runtime/urllib3/poolmanager.py&quot;, line 231, in connection_from_host
    raise LocationValueError(&quot;No host specified.&quot;)```

References
https://aws.amazon.com/premiumsupport/knowledge-center/sns-lambda-webhooks-chime-slack-teams/
</code></pre>
",0,1604862866,python;http;lambda;webhooks;urllib3,False,752,0,1604862866,https://stackoverflow.com/questions/64742085/python-urllib3-lambda-locationvalueerror-no-host-specified-http-webhook
64683967,Finding a specific element from a website using beautiful soup in Python,"<p>I'm trying to write a code that coverts money currencies into each other. To do so, I need the most recent updates of money currencies. I've chosen a website which displays them online. I want to find the desired currency (f.e. USD). I read a few articles about <code>Beautiful Soup</code>, but I'm not able to properly work with it. I've also checked similar questions on Stackoverflow.</p>
<p>Here's the part of HTML code of the currency website I see when I inspect the element:</p>
<p><a href=""https://i.stack.imgur.com/NMCMw.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NMCMw.jpg"" alt=""enter image description here"" /></a></p>
<p>I've marked down the part which says <code>270,693</code>. I want to get it with help of <code>soup.find()</code>, but I somehow can't get it right. I'm not really familiar with how it works.</p>
<p>Here's what I've tried, but I know it's not correct:</p>
<pre><code>from bs4 import BeautifulSoup
import urllib.request

url = 'https://english.tgju.org/'
page = urllib.request.urlopen(url)
soup = BeautifulSoup(page.read(), 'html.parser')
rank = soup.find(#What_should I do here?!)
print(rank)
</code></pre>
<p>I really appreciate any of your helps in advance.</p>
",0,1604507282,python;python-3.x;beautifulsoup;urllib;urllib3,True,79,1,1604510102,https://stackoverflow.com/questions/64683967/finding-a-specific-element-from-a-website-using-beautiful-soup-in-python
64370007,SSL verification fails with requests but not with urllib3,"<p>I'm importing the certificates from anaconda.</p>
<pre><code>import certifi
cert_path=certifi.where()
print(&quot;cert path&quot;, cert_path)
</code></pre>
<p>and I try to call an API with requests</p>
<pre><code>import requests 
r = requests.post(url = API_ENDPOINT, data = data, headers=headers, verify = cert_path) 
</code></pre>
<p>And it fails with</p>
<pre><code>port=443): Max retries exceeded with url: ... (Caused by SSLError(SSLError(&quot;bad handshake: Error([('SSLroutines', 'ssl3_get_server_certificate', 'certificate verify failed')],)&quot;,),))
</code></pre>
<p>I've already resolved this issue <strong>without requests</strong> but following <a href=""http://%20https://urllib3.readthedocs.io/en/latest/user-guide.html#ssl"" rel=""nofollow noreferrer"">urllib3 docs</a>.</p>
<pre><code>http = urllib3.PoolManager(
     cert_reqs='CERT_REQUIRED',
     ca_certs=certifi.where()
     )
r = http.request(&quot;POST&quot;, url = API_ENDPOINT, body = encoded_data, headers=headers) 
</code></pre>
<p>Why aren't requests using urllib3 under the covers, and why is it failing, or what should I change in my code when using the requests module?
I've noticed the fascinating and helpful picture in this <a href=""https://stackoverflow.com/a/64225525/11323942"">answer</a>, but - before downloading new certs - I'd like to understand the reason better because I would prefer to stay with my current urllib3 solution if the requests module can't work with my standard anaconda certs as well as urllib3 is doing.
Notice that &quot;<em>downloading the appropriate intermediate certificate(s)</em>&quot; is <a href=""https://github.com/urllib3/urllib3/issues/1135#issuecomment-286733578"" rel=""nofollow noreferrer"">already needed</a> for urllib3 AFAIK, so if urllib3 validation passes, that should not be a problem for requests either.</p>
",0,1602759217,python;python-requests;urllib3,True,662,1,1604096377,https://stackoverflow.com/questions/64370007/ssl-verification-fails-with-requests-but-not-with-urllib3
64394719,"Python requests: NewConnectionError, urllib3, using cert and verify attributes","<p>So the program i am developing involves posting documents in bank DMS server. They have provided me server certificate in .cer format which i have inserted in my verify variable in code. They also provided client id and password which i have to embed in the header itself. I generated self signed client certificate and private key and gave them the client certificate in cer format and public key. Also in code i gave path of client certificate and private key in cert tuple.
Upon executing code, i am getting this error:</p>
<pre><code>    HTTPSConnectionPool(host='apimuat.xxxbank.com', port=9095): Max retries exceeded with url: /doc-mgmt/v1/uploadDoc (Caused by NewConnectionError('&lt;urllib3.connection.HTTPSConnection object at 0x7fb01bd8a160&gt;: Failed to establish a new connection: [Errno 60] Operation timed out'))


File &quot;/Users/fpl_mayank/Documents/FPL/python-virtual-env/uploadDocApi/server.py&quot;, line 164, in main
    result = requests.post(url,
  File &quot;/Users/fpl_mayank/Documents/FPL/python-virtual-env/uploadDocApi/server.py&quot;, line 189, in &lt;module&gt;
    main()
</code></pre>
<p>I have tested it with 'https://postman-echo.com/post' without mentioning cert and verify just to check if my request is going through or not. it is working fine there.</p>
<p>This is my code snippet where i am using request functions.</p>
<pre><code>url='https://apimuat.xxxbank.com:9095/doc-mgmt/v1/uploadDoc'
headers = {&quot;Content-Type&quot;: &quot;application/json&quot;, &quot;client_id&quot;:&quot;af197b22539647fba4db8b971b43e38&quot;, 
           &quot;client_secret&quot;:&quot;c1AA406e24074d8887954472C78a924&quot;}
data = req
result = requests.post(url,
             data=data,
             headers=headers,
             cert=('/Users/fpl_mayank/Documents/FPL/python-virtual- 
env/uploadDocApi/keystore/dms_csr_certificate_self.cer','/Users/fpl_mayank/Documents/FPL/python-virtual-env/uploadDocApi/keystore/dms_private_key.key'),
             verify='/Users/fpl_mayank/Documents/FPL/python-virtual-env/uploadDocApi/truststore/APIM-UAT.cer'
             )
        
        res = result.json()
</code></pre>
<p>In apidoc it was mentioned, 2-way SSL authentication will be implemented bw client and server. Also i have made virtual-env for this program for that matter. Please help. I am the first one to write an API using python in my company so only way to get my issue resolve is through good ol stackoverflow.</p>
",0,1602873618,python;ssl;https;python-requests;urllib3,False,534,1,1603808393,https://stackoverflow.com/questions/64394719/python-requests-newconnectionerror-urllib3-using-cert-and-verify-attributes
64459187,Python Urllib ContextualVersionConflict,"<p>Till Oct 20, it was fine. Oct 21, it failed with the below messages. Can any one help here</p>
<pre><code>     command: /usr/local/bin/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'/tmp/pip-install-i8sl36tr/elasticsearch-curator/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'/tmp/pip-install-i8sl36tr/elasticsearch-curator/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' egg_info --egg-base /tmp/pip-pip-egg-info-et0c4kgw
         cwd: /tmp/pip-install-i8sl36tr/elasticsearch-curator/
    Complete output (20 lines):
    Traceback (most recent call last):
      File &quot;/tmp/pip-install-i8sl36tr/elasticsearch-curator/setup.py&quot;, line 39, in &lt;module&gt;
        from cx_Freeze import setup, Executable
    ModuleNotFoundError: No module named 'cx_Freeze'
</code></pre>
<pre><code>    Traceback (most recent call last):
      File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
      File &quot;/tmp/pip-install-i8sl36tr/elasticsearch-curator/setup.py&quot;, line 177, in &lt;module&gt;
        tests_require = [&quot;mock&quot;, &quot;nose&quot;, &quot;coverage&quot;, &quot;nosexcover&quot;]
      File &quot;/usr/local/lib/python3.7/site-packages/setuptools/__init__.py&quot;, line 152, in setup
        _install_setup_requires(attrs)
      File &quot;/usr/local/lib/python3.7/site-packages/setuptools/__init__.py&quot;, line 147, in _install_setup_requires
        dist.fetch_build_eggs(dist.setup_requires)
      File &quot;/usr/local/lib/python3.7/site-packages/setuptools/dist.py&quot;, line 676, in fetch_build_eggs
        replace_conflicting=True,
      File &quot;/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py&quot;, line 775, in resolve
        raise VersionConflict(dist, req).with_context(dependent_req)
    pkg_resources.ContextualVersionConflict: (urllib3 1.24.3 (/tmp/pip-install-i8sl36tr/elasticsearch-curator/.eggs/urllib3-1.24.3-py3.7.egg), Requirement.parse('urllib3&lt;1.26,&gt;=1.25.4'), {'botocore'})
    ----------------------------------------
ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.```
</code></pre>
",1,1603267347,python;python-3.x;urllib2;urllib3;pyelasticsearch,True,5675,2,1603525162,https://stackoverflow.com/questions/64459187/python-urllib-contextualversionconflict
64310395,Intermittent Proxy Errors while using O365 Library to access Sharepoint Site,"<p>I am currently using the O365 Library through an Airflow Scheduler within a Docker Container.
The library works fine for me outside the Airflow Docker Container, however within Airflow I have been getting intermittent Proxy Errors especially when I access the API simultaneously or in very short intervals between one another,
Such as when running different notebooks sequentially or simultaneously. However it does sometimes work if I space out my script runs.</p>
<p>I am using the API through an Azure AD App with Sites.ReadWriteAll permission.</p>
<p>I am using the 2.0.11 version of the O365 library.</p>
<p>I am using the Client Credentials grant type with the Account method.</p>
<pre><code>from O365 import Account

credentials = ('my_client_id', 'my_client_secret')

# the default protocol will be Microsoft Graph

account = Account(credentials, auth_flow_type='credentials', tenant_id='my-tenant-id')
if account.authenticate():
   print('Authenticated!')
</code></pre>
<p>My Proxy should be working fine as I am still able to access the Sharepoint site occasionally.</p>
<p>I am wondering if this error has been faced by others before and if so how to fix it?</p>
<p>Thank you</p>
<p>Here is a snippet of the error</p>
<pre><code>ConnectionRefusedError                    Traceback (most recent call last)
/usr/local/lib/python3.7/site-packages/urllib3/connection.py in _new_conn(self)
    159             conn = connection.create_connection(
--&gt; 160                 (self._dns_host, self.port), self.timeout, **extra_kw
    161             )

/usr/local/lib/python3.7/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options)
     83     if err is not None:
---&gt; 84         raise err
     85 

/usr/local/lib/python3.7/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options)
     73                 sock.bind(source_address)
---&gt; 74             sock.connect(sa)
     75             return sock

ConnectionRefusedError: [Errno 111] Connection refused


During handling of the above exception, another exception occurred:

NewConnectionError                        Traceback (most recent call last)
/usr/local/lib/python3.7/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    666             if is_new_proxy_conn:
--&gt; 667                 self._prepare_proxy(conn)
    668 

/usr/local/lib/python3.7/site-packages/urllib3/connectionpool.py in _prepare_proxy(self, conn)
    931         conn.set_tunnel(self._proxy_host, self.port, self.proxy_headers)
--&gt; 932         conn.connect()
    933 

/usr/local/lib/python3.7/site-packages/urllib3/connection.py in connect(self)
    308         # Add certificate verification
--&gt; 309         conn = self._new_conn()
    310         hostname = self.host

/usr/local/lib/python3.7/site-packages/urllib3/connection.py in _new_conn(self)
    171             raise NewConnectionError(
--&gt; 172                 self, &quot;Failed to establish a new connection: %s&quot; % e
    173             )

NewConnectionError: &lt;urllib3.connection.HTTPSConnection object at &gt;: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

MaxRetryError                             Traceback (most recent call last)
/usr/local/lib/python3.7/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    448                     retries=self.max_retries,
--&gt; 449                     timeout=timeout
    450                 )

/usr/local/lib/python3.7/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    766                 body_pos=body_pos,
--&gt; 767                 **response_kw
    768             )

/usr/local/lib/python3.7/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    766                 body_pos=body_pos,
--&gt; 767                 **response_kw
    768             )

/usr/local/lib/python3.7/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    766                 body_pos=body_pos,
--&gt; 767                 **response_kw
    768             )

/usr/local/lib/python3.7/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    726             retries = retries.increment(
--&gt; 727                 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]
    728             )

/usr/local/lib/python3.7/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace)
    438         if new_retry.is_exhausted():
--&gt; 439             raise MaxRetryError(_pool, url, error or ResponseError(cause))
    440 

MaxRetryError: HTTPSConnectionPool(host='graph.microsoft.com', port=443): Max retries exceeded with url: (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('&lt;urllib3.connection.HTTPSConnection object at &gt;: Failed to establish a new connection: [Errno 111] Connection refused')))

</code></pre>
",2,1602463903,python;sharepoint;proxy;microsoft-graph-api;urllib3,False,513,0,1602471446,https://stackoverflow.com/questions/64310395/intermittent-proxy-errors-while-using-o365-library-to-access-sharepoint-site
16703936,Proxy connection with Python,"<p>I have been attempting to connect to URLs from python.  I have tried:
urllib2, urlib3, and requests.  It is the same issue that i run up against in all cases.  Once I get the answer I imagine all three of them would work fine.</p>

<p>The issue is connecting via proxy.  I have entered our proxy information but am not getting any joy.  I am getting 407 codes and error messages like:
HTTP Error 407: Proxy Authentication Required ( Forefront TMG requires authorization to fulfill the request. Access to the Web Proxy filter is denied.  )</p>

<p>However, I can connect using another of other applications that go through the proxy, git for example.  When I run <code>git config --get htpp.proxy</code> it returns the same values and format that I am entering in Python namely </p>

<pre><code>http://username:password@proxy:8080
</code></pre>

<p>An example of code in requests is </p>

<pre><code>import requests
proxy = {""http"": ""http://username:password@proxy:8080""}
url = 'http://example.org'
r = requests.get(url,  proxies=proxy)
print r.status_code
</code></pre>

<p>Thanks for your time</p>
",5,1369270317,python;proxy;urllib3,True,20668,4,1602449588,https://stackoverflow.com/questions/16703936/proxy-connection-with-python
64292120,List all media and document files loaded with webpage with requests python,"<p>I'm looking for a way to list all loaded files with the requests module.
Like there is in chrome's Inspector Network tab, you can see all kinds of files that have been loaded by the webpage.</p>
<p><a href=""https://i.stack.imgur.com/klNli.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/klNli.png"" alt=""enter image description here"" /></a></p>
<p>The problem is the file(in this case .pdf file) I want to fetch does not have a specific tab, and the webpage loads it by javascript and AJAX I guess, because even after the page loaded completely, I couldn't find a tag that has a link to the .pdf file or something like that, so every time I should goto Networks tab and reload the page and find the file in the loaded resources list.
Is there any way to catch all the loaded files and list them using the Requests module?</p>
",2,1602323587,python;python-requests;urllib3,False,1704,1,1602324632,https://stackoverflow.com/questions/64292120/list-all-media-and-document-files-loaded-with-webpage-with-requests-python
64207556,The &#39;html2text&#39; module not working when using with &#39;urllib.request&#39; module,"<p>I want to  get all the text of a webpage and therefore I am trying to use html2text module with the urllib.request module--</p>
<pre><code>import urllib.request 
import html2text
request_url = urllib.request.urlopen('https://dev.to/justdevasur/let-s-perform-google-search-with-python-2gpi') 
u=request_url.read()
print(html2text.html2text(u))
print('Done')
</code></pre>
<p>But I am getting the following error--</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;C:\Users\rauna\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\html2text\__init__.py&quot;, line 947, in html2text
    return h.handle(html)
  File &quot;C:\Users\rauna\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\html2text\__init__.py&quot;, line 142, in handle
    self.feed(data)
  File &quot;C:\Users\rauna\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\html2text\__init__.py&quot;, line 138, in feed
    data = data.replace(&quot;&lt;/' + 'script&gt;&quot;, &quot;&lt;/ignore&gt;&quot;)
TypeError: a bytes-like object is required, not 'str'
</code></pre>
",-1,1601896570,python;python-3.x;web-scraping;urllib3,False,781,1,1601971931,https://stackoverflow.com/questions/64207556/the-html2text-module-not-working-when-using-with-urllib-request-module
64113381,Urllib3 &amp; MITMProxy: sslv3 alert handshake failure,"<p>My client is on urllib3 <code>HTTPSConnectionPool</code>. I've installed a SSL version 1 certificate to the server and it works fine on request.
<a href=""https://i.stack.imgur.com/FAW6k.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FAW6k.png"" alt=""Certificate for my server"" /></a></p>
<p>On next instance of <code>HTTPSConnectionPool</code>, I've used a proxy to intercept. The problem is that client exits with following traceback:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/user/Documents/project/cert-pinning/lib/python3.8/site-packages/urllib3/connectionpool.py&quot;, line 667, in urlopen
    self._prepare_proxy(conn)
  File &quot;/home/user/Documents/project/cert-pinning/lib/python3.8/site-packages/urllib3/connectionpool.py&quot;, line 932, in _prepare_proxy
    conn.connect()
  File &quot;/home/user/Documents/project/cert-pinning/lib/python3.8/site-packages/urllib3/connection.py&quot;, line 362, in connect
    self.sock = ssl_wrap_socket(
  File &quot;/home/user/Documents/project/cert-pinning/lib/python3.8/site-packages/urllib3/util/ssl_.py&quot;, line 384, in ssl_wrap_socket
    return context.wrap_socket(sock, server_hostname=server_hostname)
  File &quot;/usr/lib/python3.8/ssl.py&quot;, line 500, in wrap_socket
    return self.sslsocket_class._create(
  File &quot;/usr/lib/python3.8/ssl.py&quot;, line 1040, in _create
    self.do_handshake()
  File &quot;/usr/lib/python3.8/ssl.py&quot;, line 1309, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1123)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;client2.py&quot;, line 76, in &lt;module&gt;
    r = pool_with_proxy.urlopen('GET', '/')
  File &quot;/home/user/Documents/project/cert-pinning/lib/python3.8/site-packages/urllib3/connectionpool.py&quot;, line 754, in urlopen
    return self.urlopen(
  File &quot;/home/user/Documents/project/cert-pinning/lib/python3.8/site-packages/urllib3/connectionpool.py&quot;, line 754, in urlopen
    return self.urlopen(
  File &quot;/home/user/Documents/project/cert-pinning/lib/python3.8/site-packages/urllib3/connectionpool.py&quot;, line 754, in urlopen
    return self.urlopen(
  File &quot;/home/user/Documents/project/cert-pinning/lib/python3.8/site-packages/urllib3/connectionpool.py&quot;, line 726, in urlopen
    retries = retries.increment(
  File &quot;/home/user/Documents/project/cert-pinning/lib/python3.8/site-packages/urllib3/util/retry.py&quot;, line 439, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: TestHTTPSConnectionPool(host='https://cod-avatar.com', port=8000): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1123)')))
</code></pre>
<p>I am pretty sure its due to the Version 3 certificate of HTTPS_PROXY.</p>
<p><a href=""https://i.stack.imgur.com/nqLby.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nqLby.png"" alt=""HTTPS Proxy Certificate"" /></a></p>
<p>I want to know. How do I enable Version 3 certificate for my urllib3 client.</p>
<h2>UPDATE:</h2>
<p>My certificates are all self-signed. When I start the mitmproxy without defining my ca-certs. It gives Bad Gateway which is obvious as it wont trust my certificates.</p>
<p><code>Certificate verification error for cod****.com: unable to get local issuer certificate (errno: 20, depth: 0)</code></p>
<p>But when I define my ca-certs while starting mitmproxy</p>
<blockquote>
<p>mitmweb --cacerts cod*****.com=new-cacert.pem</p>
</blockquote>
<p>The previous stack trace occurs</p>
<h3>Trying with curl:</h3>
<blockquote>
<p>curl -X GET &quot;https://cod*****.com:8000&quot; --cacert new/new-cacert.pem --proxy 127.0.0.1:6666</p>
</blockquote>
<p>also gives following response, but works fine without proxy</p>
<p><code>curl: (35) error:14094410:SSL routines:ssl3_read_bytes:sslv3 alert handshake failure</code></p>
",0,1601357048,python;python-3.x;ssl;urllib3;mitmproxy,True,1715,1,1601437402,https://stackoverflow.com/questions/64113381/urllib3-mitmproxy-sslv3-alert-handshake-failure
64110501,"SSL: CERTIFICATE_VERIFY_FAILED certificate verify failed: Hostname mismatch, certificate is not valid for &#39;url&#39;&#39;","<p>I am trying to access Solr using urllib2 as instaructed here: <a href=""https://lucene.apache.org/solr/guide/7_3/using-python.html"" rel=""nofollow noreferrer"">https://lucene.apache.org/solr/guide/7_3/using-python.html</a> using urllib</p>
<p>but running into error <code>raise URLError(err) urllib.error.URLError: &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'the_dns-for-lb'</code></p>
<p>The cert exist in AWS ACM and there is custom auth, can someone guide me how to establish connection?</p>
<pre><code>from urllib.request import urlopen
connection = urlopen('https://dns-for-lb/solr/design').read()
response = eval(connection.read())
</code></pre>
<p>tried:
<a href=""https://stackoverflow.com/questions/48068772/using-urllib-gives-ssl-error"">Using urllib gives SSL error</a>
<a href=""https://stackoverflow.com/questions/48068772/using-urllib-gives-ssl-error"">Using urllib gives SSL error</a>
<a href=""https://stackoverflow.com/questions/25981703/pip-install-fails-with-connection-error-ssl-certificate-verify-failed-certi"">pip install fails with &quot;connection error: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:598)&quot;</a></p>
",3,1601332608,python;amazon-web-services;solr;urllib2;urllib3,False,2404,0,1601332608,https://stackoverflow.com/questions/64110501/ssl-certificate-verify-failed-certificate-verify-failed-hostname-mismatch-cer
63700904,"Python code for sending requests with certificate, private encrypted key and password","<p>I'm trying to fetch response from an https call which has certificate installed atits side. This is my code</p>
<pre><code>import requests
import urllib3

urllib3.disable_warnings()

cert_file_path = &quot;/path/output-crt-file-name.crt&quot;
key_file_path = &quot;/path/output-key-file-name.key&quot;
passwd = 'secretpass'
print(passwd)
url = &quot;https://url/to/fetch/response&quot;
params = {&quot;AppID&quot;: &quot;xxxx&quot;, &quot;Safe&quot;: &quot;xxxx&quot;, &quot;Folder&quot;: &quot;Root&quot;,
          &quot;Object&quot;: &quot;xxxx&quot;}
cert = (cert_file_path, key_file_path, passwd)
r = requests.get(url, params=params, cert=cert, verify=True )
print(r.text)
</code></pre>
<p>which throws error</p>
<blockquote>
<p>Caused by SSLError('Client private key is encrypted, password is required'</p>
</blockquote>
<p>Please suggest.</p>
",4,1599032605,python;urllib3,True,10117,1,1601215872,https://stackoverflow.com/questions/63700904/python-code-for-sending-requests-with-certificate-private-encrypted-key-and-pas
64089292,Python: conversion from requests library to urllib3,"<p>I need to convert the following CURL command into an http request in Python:</p>
<pre><code>curl -X POST https://some/url
-H 'api-key: {api_key}'
-H 'Content-Type: application/json'
-H 'Accept: application/json'
-d '{ &quot;data&quot;: { &quot;dirname&quot;: &quot;{dirname}&quot;, &quot;basename&quot;: &quot;{filename}&quot;, &quot;contentType&quot;: &quot;application/octet-stream&quot; } }'

</code></pre>
<p>I initially successfully implemented the request using Python's requests library.</p>
<pre><code>import requests

url = 'https://some/url'
api_key = ...
dirname = ...
filename = ...

headers = {
    'api-key': f'{api_key}',
    'Content-Type': 'application/json',
    'Accept': 'application/json',
}

payload = json.dumps({
    'data': {
        'dirname': f'{dirname}',
        'basename': f'{filename}',
        'contentType': 'application/octet-stream'
    }
})

response = requests.post(url, headers=headers, data=payload)
</code></pre>
<p>The customer later asked not to use pip to install the requests library. For this I am trying to use the urllib3 library as follows:</p>
<pre><code>import urllib3

url = 'https://some/url'
api_key = ...
dirname = ...
filename = ...

headers = {
    'api-key': f'{api_key}',
    'Content-Type': 'application/json',
    'Accept': 'application/json',
}

payload = json.dumps({
    'data': {
        'dirname': f'{dirname}',
        'basename': f'{filename}',
        'contentType': 'application/octet-stream'
    }
})

http = urllib3.PoolManager()
response = http.request('POST', url, headers=headers, body=payload)
</code></pre>
<p>The problem is that now the request returns me an error 400 and I don't understand why.</p>
",2,1601215433,python;curl;python-requests;urllib3,True,1595,1,1601215795,https://stackoverflow.com/questions/64089292/python-conversion-from-requests-library-to-urllib3
64008786,Python urllib3 AWS Elastic Search ConnectionError,"<p>I have written a python script that uses <strong>elasticsearch</strong> library to connect to AWS elastic search cluster. Things worked fine for sometime but suddenly started throwing <strong>ConnectionError(&lt;urllib3.connection.HTTPSConnection object at 0x7fec57c238e0&gt;: Failed to establish a new connection: [Errno -2] Name or service not known) caused by: NewConnectionError(&lt;urllib3.connection.HTTPSConnection object at 0x7fec57c238e0&gt;: Failed to establish a new connection: [Errno -2] Name or service not known)</strong>. <br/> The error goes away when i restart the server but comes back after sometime. Can anyone help me resolve this? <br/> <br/>
Posting the log for reference:<br/></p>
<pre><code>Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.8/dist-packages/urllib3/connection.py&quot;, line 159, in _new_conn
    conn = connection.create_connection(
  File &quot;/usr/local/lib/python3.8/dist-packages/urllib3/util/connection.py&quot;, line 61, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
  File &quot;/usr/lib/python3.8/socket.py&quot;, line 918, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
socket.gaierror: [Errno -2] Name or service not known
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.8/dist-packages/elasticsearch/connection/http_urllib3.py&quot;, line 245, in perform_request
    response = self.pool.urlopen(
  File &quot;/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py&quot;, line 726, in urlopen
    retries = retries.increment(
  File &quot;/usr/local/lib/python3.8/dist-packages/urllib3/util/retry.py&quot;, line 379, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File &quot;/usr/local/lib/python3.8/dist-packages/urllib3/packages/six.py&quot;, line 735, in reraise
    raise value
  File &quot;/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py&quot;, line 670, in urlopen
    httplib_response = self._make_request(
  File &quot;/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py&quot;, line 381, in _make_request
    self._validate_conn(conn)
  File &quot;/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py&quot;, line 978, in _validate_conn
    conn.connect()
  File &quot;/usr/local/lib/python3.8/dist-packages/urllib3/connection.py&quot;, line 309, in connect
    conn = self._new_conn()
  File &quot;/usr/local/lib/python3.8/dist-packages/urllib3/connection.py&quot;, line 171, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: &lt;urllib3.connection.HTTPSConnection object at 0x7fec57c238e0&gt;: Failed to establish a new connection: [Errno -2] Name or service not known
</code></pre>
",0,1600774099,python;amazon-web-services;elasticsearch;python-requests;urllib3,False,499,0,1600774099,https://stackoverflow.com/questions/64008786/python-urllib3-aws-elastic-search-connectionerror
63940850,Python Requests - Connection Refused,"<p>I'm trying to get the status_code from various URLs in a csv file using the requests Python module.
It works for some websites, but for most of them it shows <strong>'Connection Refused'</strong>, even though if I visit the websites through the browser they load just fine.</p>
<p>The code looks like this:</p>
<pre><code>import pandas as pd 
import requests 
from requests.adapters import HTTPAdapter
from fake_useragent import UserAgent
import time
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

df = pd.read_csv('Websites.csv')
output_data = pd.DataFrame(columns=['url', 'status'])
number_urls = df.shape[0]

i = 0

for url in df['urls']:

    session = requests.Session()
    adapter = HTTPAdapter(max_retries=3)
    adapter.max_retries.respect_retry_after_header = False
    session.mount('http://', adapter)
    session.mount('https://', adapter)

    print(url)

    ua = UserAgent()
    header = {'User-Agent':str(ua.chrome)}
    
    try:
        # Status
        start = time.time()
        response = session.get(url, headers=header, verify=False, timeout=0.5)
        request_time = time.time() - start
        info = &quot;Request completed in {0:.0f}ms&quot;.format(request_time)
        print(info)
        status = response.status_code
        if (status == 200):
            status = &quot;Connection Successful&quot;
        if (status == 404):
            status = &quot;404 Error&quot;
        if (status == 403):
            status = &quot;403 Error&quot;
        if (status == 503):
            status = &quot;503 Error&quot;
        print(status)

        output_data.loc[i] = [df.iloc[i, 0], status]

        i += 1

    except requests.exceptions.Timeout:
        status = &quot;Connection Timed Out&quot;
        print(status)
        request_time = time.time() - start
        info = &quot;TimeOut in {0:.0f}ms&quot;.format(request_time)
        print(info)

        output_data.loc[i] = [df.iloc[i, 0], status]
        i += 1

    except requests.exceptions.ConnectionError:
        status = &quot;Connection Refused&quot;
        print(status)
        request_time = time.time() - start
        info = &quot;Connection Error in {0:.0f}ms&quot;.format(request_time)
        print(info)

        output_data.loc[i] = [df.iloc[i, 0], status]
        i += 1

output_data.to_csv('dead_blocked2.csv', index=False)
print('CSV file created!')
</code></pre>
<p>Here's an example of one website that shows Connection Refused, even though it works: <a href=""https://www.dytt8.net"" rel=""nofollow noreferrer"">https://www.dytt8.net</a></p>
<p>I've tried using different TLS versions using the following piece of code and updating my session, but it still doesn't work:</p>
<pre><code>class MyAdapter(HTTPAdapter):
def init_poolmanager(self, connections, maxsize, block=False):
    self.poolmanager = PoolManager(num_pools=connections,
                            maxsize=maxsize,
                            block=block,
                            ssl_version=ssl.PROTOCOL_TLSv1)
</code></pre>
<p>Can anyone help?</p>
<p>Thanks!</p>
",0,1600354747,python;python-requests;urllib3;connection-refused,False,1400,0,1600356367,https://stackoverflow.com/questions/63940850/python-requests-connection-refused
63933525,Import Error:No module request (on Windows command prompt),"<p>I have been searching tirelessly for days now. I cannot do 'import urllib' on my command prompt, that is when I run a file that contains 'import urllib' I start to get Import Error:No module request. But if I run it on python.exe, it runs fine. Please help me, what should I do to run my files on the command line to get outputs?Thanks.</p>
",0,1600329021,python;python-requests;python-3.7;urllib3,False,160,0,1600329021,https://stackoverflow.com/questions/63933525/import-errorno-module-request-on-windows-command-prompt
63883656,ImportError: cannot import name create_urllib3_context,"<p>I'm working on a script that use google API, with reference at the following tutorial:</p>
<p><a href=""https://developers.google.com/sheets/api/reference/rest/v4/spreadsheets.values/append#InsertDataOption"" rel=""nofollow noreferrer"">https://developers.google.com/sheets/api/reference/rest/v4/spreadsheets.values/append#InsertDataOption</a></p>
<p>the script works fine in the windows PC with python 3.6, while i've some problem if i run the same script in raspbian with python 3.4.
the script rises the following error:</p>
<pre><code>    from google_auth_oauthlib.flow import InstalledAppFlow,Flow
  File &quot;/usr/local/lib/python3.4/dist-packages/google_auth_oauthlib/__init__.py&quot;, line 21, in &lt;module&gt;
    from .interactive import get_user_credentials
  File &quot;/usr/local/lib/python3.4/dist-packages/google_auth_oauthlib/interactive.py&quot;, line 24, in &lt;module&gt;
    import google_auth_oauthlib.flow
  File &quot;/usr/local/lib/python3.4/dist-packages/google_auth_oauthlib/flow.py&quot;, line 67, in &lt;module&gt;
    import google.auth.transport.requests
  File &quot;/usr/local/lib/python3.4/dist-packages/google/auth/transport/requests.py&quot;, line 39, in &lt;module&gt;
    from requests.packages.urllib3.util.ssl_ import (
ImportError: cannot import name 'create_urllib3_context'
</code></pre>
<p>I've also checkd the pip3 list for the installed libraries:</p>
<pre><code>cachetools (4.1.1)
certifi (2020.6.20)
chardet (2.3.0)
codebug-i2c-tether (0.2.3)
codebug-tether (0.7.0)
colorama (0.3.2)
emoji (0.5.4)
Flask (0.10.1)
google-api-core (1.22.2)
google-api-python-client (1.11.0)
google-auth (1.21.1)
google-auth-httplib2 (0.0.4)
google-auth-oauthlib (0.4.1)
googleapis-common-protos (1.52.0)
gpiozero (1.3.1)
html5lib (0.999)
httplib2 (0.18.1)
idna (2.10)
itsdangerous (0.24)
Jinja2 (2.7.3)
lxml (3.4.0)
MarkupSafe (0.23)
matplotlib (1.4.2)
mcpi (0.1.1)
nose (1.3.4)
numpy (1.8.2)
oauthlib (3.1.0)
pgzero (1.1)
picamera (1.12)
picraft (0.6)
pifacecommon (4.2.1)
pifacedigitalio (3.1.0)
pigpio (1.30)
Pillow (2.6.1)
pip (1.5.6)
protobuf (3.13.0)
pushbullet.py (0.10.0)
pyasn1 (0.4.8)
pyasn1-modules (0.2.8)
pygame (1.9.2a0)
pygobject (3.14.0)
pyinotify (0.9.4)
pyOpenSSL (0.13.1)
pyparsing (2.0.3)
pyserial (2.6)
python-apt (0.9.3.12)
python-dateutil (2.2)
python-debian (0.1.27)
python-magic (0.4.12)
pytz (2012c)
requests (2.4.3)
requests-oauthlib (1.3.0)
RPi.GPIO (0.6.2)
rsa (4.5)
RTIMULib (7.2.1)
sense-emu (1.0)
sense-hat (2.2.0)
setuptools (5.5.1)
six (1.8.0)
smbus (1.1)
spidev (3.0)
telepot (12.7)
twython (3.1.2)
uritemplate (3.0.1)
urllib3 (1.9.1)
websocket-client (0.40.0)
Werkzeug (0.9.6)
wheel (0.24.0)
</code></pre>
<p>many thanks for the support.
Matteo</p>
",2,1600083984,python;google-api;urllib3,False,1812,0,1600083984,https://stackoverflow.com/questions/63883656/importerror-cannot-import-name-create-urllib3-context
63775458,TypeError: request() missing 1 required positional argument: &#39;url&#39; in urllib3,"<p>I'm getting deep into Python and was going through urllib3's docs. Tried running the code but seems like it's not working the way it's expected to.
My code is</p>
<pre><code>import urllib3

t = urllib3.PoolManager
test = t.request('GET', 'https://shadowhosting.net/')
print(test.data)
</code></pre>
<p>And the error I get is</p>
<pre><code>TypeError: request() missing 1 required positional argument: 'url'
</code></pre>
<p>I tried swapping place but still not working. I'm following the very beginning of the documentation(User Guide)
For reference - <a href=""https://urllib3.readthedocs.io/en/latest/user-guide.html"" rel=""nofollow noreferrer"">https://urllib3.readthedocs.io/en/latest/user-guide.html</a></p>
",2,1599473232,python;urllib3,True,6179,3,1599475048,https://stackoverflow.com/questions/63775458/typeerror-request-missing-1-required-positional-argument-url-in-urllib3
63771839,How to download resized image using python?,"<p>I am new to web-scraping ,I want a program that download images using bs4 and urllib3 but I want to download resized image ,I have previously used the following code</p>
<pre><code>urllib.request.urlretrieve(&quot;https://img-prod-cms-rt-microsoft-com.akamaized.net/cms/api/am/imageFileData/RE4wyTq?ver=c8e5&amp;quot&quot;,&quot;image.jpg&quot;)
</code></pre>
<p>but this downloads image in it's real size I want to download this image with different height and weight
What can I do?</p>
",1,1599456385,python;web-scraping;beautifulsoup;urllib3,True,1315,1,1599460837,https://stackoverflow.com/questions/63771839/how-to-download-resized-image-using-python
63732429,How install python-pysocks in CentOS/oracle linux?,"<p>When I try to update using the command <code>sudo yum update</code> I get the error:</p>
<pre><code>Error: Package: python2-urllib3-1.22-2.el7.noarch (ol7_developer)
           Requires: python-pysocks
</code></pre>
<p>so when I try to install the <code>python2-urllib3-1.22-2.el7.noarch.rpm</code> I get the error:</p>
<pre><code>Error: Package: python2-urllib3-1.22-2.el7.noarch (/python2-urllib3-1.22-2.el7.noarch)
           Requires: python-pysocks
</code></pre>
<p>Then I tried to download the package <a href=""https://download-ib01.fedoraproject.org/pub/epel/7/SRPMS/Packages/p/python-pysocks-1.6.8-7.el7.src.rpm"" rel=""nofollow noreferrer"">https://download-ib01.fedoraproject.org/pub/epel/7/SRPMS/Packages/p/python-pysocks-1.6.8-7.el7.src.rpm</a> but when I try to install: <code>sudo yum install /home/&lt;USER&gt;/python-pysocks-1.6.8-7.el7.src.rpm</code> I get the error:</p>
<pre><code>Cannot add package /home/bigdata/python-pysocks-1.6.8-7.el7.src.rpm to transaction. Not a compatible architecture: src
</code></pre>
<p>How do I correct? Where should I download this package?</p>
",0,1599171800,python;centos;urllib3;oraclelinux;pysocks,True,503,1,1599235823,https://stackoverflow.com/questions/63732429/how-install-python-pysocks-in-centos-oracle-linux
63725498,Multipart upload with urllib3 - value error,"<p>I'm writing a script to upload a large file (50GB) to our service at work which utilizes minio.</p>
<pre><code>config = TransferConfig(multipart_threshold=1024 * 100 * 1024, max_concurrency=10,
                        multipart_chunksize=1024 * 100 * 1024, use_threads=True)

session = boto3.Session(
    aws_access_key_id=upload_data['access_key_id'],
    aws_secret_access_key=upload_data['secret_access_key'],
    aws_session_token=upload_data['session_token'],
    region_name=upload_data['region']
)

client = session.client('s3', endpoint_url='https://maurice-storage.vdoo.team/', verify=False)
client.upload_file(
    image_file,
    upload_data['bucket'],
    upload_data['key'],
    ExtraArgs={'Metadata': {
        'name': os.path.basename(image_file),
        'size': str(os.stat(image_file).st_size),
    }},
    Config = config,
    Callback=ProgressPercentage(image_file)
)
</code></pre>
<p>I've set the upload to upload chunks of 100 MB (total 500 chunks) as the file is 50GB.</p>
<p>However, once the upload reaches 100%, a ValueError exception is thrown:</p>
<pre><code>File &quot;C:\Users\user\AppData\Local\Programs\Python\Python37\lib\site-packages\urllib3\response.py&quot;, line 696, in _update_chunk_length
self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''
</code></pre>
<p>I <strong>know</strong> why it happens - it tries to call int with an empty binary value (b''), but i'm not sure why that value is there.</p>
<p>Any idea why that could happen?</p>
",0,1599142504,python;urllib3;minio,False,285,1,1599146608,https://stackoverflow.com/questions/63725498/multipart-upload-with-urllib3-value-error
39012081,Python - urllib3 ClosedPoolError on crawling,"<p>I am building a crawler with <code>python3</code> and <code>urllib3</code>. I am using a <code>PoolManager</code> instance that is used by 15 different threads. While crawling thousands of website i get a lot of <code>ClosedPoolError</code> from different website. </p>

<p>On the documentation - <code>ClosedPoolError</code>:</p>

<blockquote>
  <p>Raised when a request enters a pool after the pool has been closed.</p>
</blockquote>

<p>It appears that the <code>PoolManager</code> instance is trying to use a closed connection.</p>

<p>My code:</p>

<pre><code>from urllib3 import PoolManager, util, Retry
from urllib3.exceptions import MaxRetryError

# Instance of PoolManager is started on init
manager = PoolManager(num_pools=15,
                      maxsize=6,
                      timeout=40.0,
                      retries=Retry(connect=2, read=2, redirect=10))

# Every thread execute download by using the pool manager instance
url_to_download = ""**""
headers = util.make_headers(accept_encoding='gzip, deflate',
                            keep_alive=True,
                            user_agent=""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0"")
headers['Accept-Language'] = ""en-US,en;q=0.5""
headers['Accept'] = ""text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8""
try:
   response = manager.request('GET',
                                   url_to_download,
                                   preload_content=False,
                                   headers=headers)
except MaxRetryError as ex:
   raise FailedToDownload()
</code></pre>

<p>How can i make the <code>PoolManager</code> renew the connection and try again?</p>
",1,1471504634,python;python-3.x;urllib;python-multithreading;urllib3,True,1240,1,1599114892,https://stackoverflow.com/questions/39012081/python-urllib3-closedpoolerror-on-crawling
63690544,HTTPS connection error with python requests,"<p>I'm trying to run a simple requests code but it's giving me HTTPS connection error.... Please help</p>
<pre><code>import requests
x=requests.get('https://www.google.ca')
print(x.status_code)
</code></pre>
<p>Here's the error message:</p>
<pre><code>  File &quot;C:\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py&quot;, line 827, in runfile
    execfile(filename, namespace)

  File &quot;C:\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py&quot;, line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File &quot;C:/Users/129881249/Desktop/test.py&quot;, line 9, in &lt;module&gt;
    x=requests.get('https://www.google.ca')

  File &quot;C:\Anaconda3\lib\site-packages\requests\api.py&quot;, line 75, in get
    return request('get', url, params=params, **kwargs)

  File &quot;C:\Anaconda3\lib\site-packages\requests\api.py&quot;, line 60, in request
    return session.request(method=method, url=url, **kwargs)

  File &quot;C:\Anaconda3\lib\site-packages\requests\sessions.py&quot;, line 533, in request
    resp = self.send(prep, **send_kwargs)

  File &quot;C:\Anaconda3\lib\site-packages\requests\sessions.py&quot;, line 646, in send
    r = adapter.send(request, **kwargs)

  File &quot;C:\Anaconda3\lib\site-packages\requests\adapters.py&quot;, line 516, in send
    raise ConnectionError(e, request=request)

ConnectionError: HTTPSConnectionPool(host='www.google.ca', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x0000015CCB7B4388&gt;: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond'))
</code></pre>
",0,1598973304,python;python-requests;urllib3,False,1595,1,1598973527,https://stackoverflow.com/questions/63690544/https-connection-error-with-python-requests
63680800,ModuleNotFound in python 3.7,"<p>i am getting this error in cmd prompt : No module named 'urllib.request'
i have installed pip and requests packages still i am getting this</p>
<pre class=""lang-py prettyprint-override""><code>import urllib.request, urllib.parse, urllib.error
fhand = urllib.request.open('https://data.pr4e.org/romeo.txt')
for line in fhand:
    print(line.decode().strip())
</code></pre>
",-1,1598929952,python;python-requests;urllib;urllib3,False,325,2,1598939110,https://stackoverflow.com/questions/63680800/modulenotfound-in-python-3-7
63650313,How to check if user is connected to internet in python?,"<p>I am python 3.5 and using urllib3 and I have seen various examples but they were of urllib2 that's why I asked this question ,I have a bit of code which need internet but if user is not connected I want to show them a warning that device is not connected to internet so how can I do this.</p>
",1,1598724803,python;web-scraping;urllib3,True,159,2,1598725666,https://stackoverflow.com/questions/63650313/how-to-check-if-user-is-connected-to-internet-in-python
63572684,How to join https to a text using urljoin?,"<p>I am trying to add <code>https</code> to a text <code>business.com</code> as follows:</p>
<pre><code>from urllib.request import urljoin
datax = &quot;business.com&quot;
base_url = &quot;https://www.&quot;
aa11 = urljoin(base_url, datax)
print(aa11)
</code></pre>
<p>I get the following. Actually, I don't know to avoid adding <code>/</code> between <code>.</code> and <code>business</code>.</p>
<pre><code>https://www./business.com
</code></pre>
<p>Desired output:</p>
<pre><code>https://www.business.com
</code></pre>
<p>Or Is there anyother way to get the desired output?</p>
<p>Thanks in advance for your help.</p>
",0,1598334227,python;urllib3,True,65,2,1598345819,https://stackoverflow.com/questions/63572684/how-to-join-https-to-a-text-using-urljoin
63508878,Python intermittent OpenSSL error: FileNotFoundError: [Errno 2] No such file or directory,"<p>I'm getting an intermittent error on my production Django website (running under UWSGI) when trying to use the kubernetes client library. The error seems to be urllib3, or perhaps OpenSSL. The strange thing is, I can simply restart my server and the problem goes away, but only temporarily. After a while I start seeing this error again:</p>
<pre><code>Traceback (most recent call last):                                                
  File &quot;/srv/hive/lib/python3.6/site-packages/urllib3/util/ssl_.py&quot;, line 353, in ssl_wrap_socket
    context.load_verify_locations(ca_certs, ca_cert_dir, ca_cert_data)           
FileNotFoundError: [Errno 2] No such file or directory 
</code></pre>
<p>This is extremely difficult to troubleshoot, because restarting the server makes the problem go away for a few hours. I added some extra logging to the <code>load_verify_locations</code> function in <code>urllib3/util/ssl_.py</code> to see what file it's trying to open. The <code>ca_certs</code> argument is passing in a temp file, <code>/tmp/tmp2xng8a6e</code>. The other two arguments are <code>None</code>.</p>
<p>So, why a temp file, and why is it missing? Is there a reason why the temp file would get deleted, or why urllib3 might not be creating the temp file in the first place?</p>
<p>Here's the full exception with traceback:</p>
<pre><code>Aug 20 10:36:05 ERROR django.request: Internal Server Error: /code/projects/test-project/
Traceback (most recent call last):
  File &quot;/srv/hive/lib/python3.6/site-packages/urllib3/util/ssl_.py&quot;, line 353, in ssl_wrap_socket
    context.load_verify_locations(ca_certs, ca_cert_dir, ca_cert_data)
FileNotFoundError: [Errno 2] No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/srv/hive/lib/python3.6/site-packages/urllib3/connectionpool.py&quot;, line 677, in urlopen
    chunked=chunked,
  File &quot;/srv/hive/lib/python3.6/site-packages/urllib3/connectionpool.py&quot;, line 381, in _make_request
    self._validate_conn(conn)
  File &quot;/srv/hive/lib/python3.6/site-packages/urllib3/connectionpool.py&quot;, line 976, in _validate_conn
    conn.connect()
  File &quot;/srv/hive/lib/python3.6/site-packages/urllib3/connection.py&quot;, line 370, in connect
    ssl_context=context,
  File &quot;/srv/hive/lib/python3.6/site-packages/urllib3/util/ssl_.py&quot;, line 355, in ssl_wrap_socket
    raise SSLError(e)
urllib3.exceptions.SSLError: [Errno 2] No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/srv/hive/lib/python3.6/site-packages/django/core/handlers/exception.py&quot;, line 34, in inner
    response = get_response(request)
  File &quot;/srv/hive/lib/python3.6/site-packages/django/core/handlers/base.py&quot;, line 115, in _get_response
    response = self.process_exception_by_middleware(e, request)
  File &quot;/srv/hive/lib/python3.6/site-packages/django/core/handlers/base.py&quot;, line 113, in _get_response
    response = wrapped_callback(request, *callback_args, **callback_kwargs)
  File &quot;/srv/hive/lib/python3.6/site-packages/django/contrib/auth/decorators.py&quot;, line 21, in _wrapped_view
    return view_func(request, *args, **kwargs)
  File &quot;/srv/hive/src/hive/ide/views.py&quot;, line 67, in ide
    utils.init_workspace(project)
  File &quot;/srv/hive/src/hive/ide/utils.py&quot;, line 252, in init_workspace
    get_user_storage_resource(project.user).apply()
  File &quot;/srv/hive/src/hive/ide/utils.py&quot;, line 79, in apply
    responses.append(item.apply())
  File &quot;/srv/hive/src/hive/ide/utils.py&quot;, line 221, in apply
    if self.exists:
  File &quot;/srv/hive/src/hive/ide/utils.py&quot;, line 165, in exists
    self._call('read')
  File &quot;/srv/hive/src/hive/ide/utils.py&quot;, line 152, in _call
    return func(**kwargs)
  File &quot;/srv/hive/lib/python3.6/site-packages/kubernetes/client/api/core_v1_api.py&quot;, line 18854, in read_namespaced_persistent_volume_claim
    (data) = self.read_namespaced_persistent_volume_claim_with_http_info(name, namespace, **kwargs)  # noqa: E501
  File &quot;/srv/hive/lib/python3.6/site-packages/kubernetes/client/api/core_v1_api.py&quot;, line 18945, in read_namespaced_persistent_volume_claim_with_http_info
    collection_formats=collection_formats)
  File &quot;/srv/hive/lib/python3.6/site-packages/kubernetes/client/api_client.py&quot;, line 345, in call_api
    _preload_content, _request_timeout)
  File &quot;/srv/hive/lib/python3.6/site-packages/kubernetes/client/api_client.py&quot;, line 176, in __call_api
    _request_timeout=_request_timeout)
  File &quot;/srv/hive/lib/python3.6/site-packages/kubernetes/client/api_client.py&quot;, line 366, in request
    headers=headers)
  File &quot;/srv/hive/lib/python3.6/site-packages/kubernetes/client/rest.py&quot;, line 241, in GET
    query_params=query_params)
  File &quot;/srv/hive/lib/python3.6/site-packages/kubernetes/client/rest.py&quot;, line 214, in request
    headers=headers)
  File &quot;/srv/hive/lib/python3.6/site-packages/urllib3/request.py&quot;, line 76, in request
    method, url, fields=fields, headers=headers, **urlopen_kw
  File &quot;/srv/hive/lib/python3.6/site-packages/urllib3/request.py&quot;, line 97, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
  File &quot;/srv/hive/lib/python3.6/site-packages/urllib3/poolmanager.py&quot;, line 336, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File &quot;/srv/hive/lib/python3.6/site-packages/urllib3/connectionpool.py&quot;, line 765, in urlopen
    **response_kw
  File &quot;/srv/hive/lib/python3.6/site-packages/urllib3/connectionpool.py&quot;, line 765, in urlopen
    **response_kw
  File &quot;/srv/hive/lib/python3.6/site-packages/urllib3/connectionpool.py&quot;, line 765, in urlopen
    **response_kw
  File &quot;/srv/hive/lib/python3.6/site-packages/urllib3/connectionpool.py&quot;, line 725, in urlopen
    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]
  File &quot;/srv/hive/lib/python3.6/site-packages/urllib3/util/retry.py&quot;, line 439, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='ide-ideresourcegroup-207977-dbdb0faf.hcp.centralus.azmk8s.io', port=443): Max retries exceeded with url: /api/v1/namespaces/ide/persistentvolumeclaims/workspace-storage-5p2lm91r (Caused by SSLError(FileNotFoundError(2, 'No such file or directory'),))
</code></pre>
",1,1597939023,python;django;kubernetes;openssl;urllib3,False,872,0,1597939023,https://stackoverflow.com/questions/63508878/python-intermittent-openssl-error-filenotfounderror-errno-2-no-such-file-or
63367092,Request time out in python,"<p>I am trying to send a request to the Louis Vuitton, but none of the the libraries that I have used are able to get a response. Even though i can access the site fine from the browser. Is there something im doing wrong?</p>
<p>I have used <em>requests</em>,<em>urllib3</em> and <em>BeautifulSoup</em> and none of my trials with these libraries have been successful.</p>
<p>Code:</p>
<pre><code>from urllib.request import Request, urlopen
from bs4 import BeautifulSoup as soup


url = 'https://us.louisvuitton.com/eng-us/products/pocket-organizer-monogram-other-nvprod2380073v'
req = Request(url , headers={'User-Agent': 'Chrome/84.0.4147.105'})

webpage = urlopen(req).read()
page_soup = soup(webpage, &quot;html.parser&quot;)
title = page_soup.find(&quot;title&quot;)
print(title)
</code></pre>
<p>Output:
Doesn't give me one, just stays blank until I KeyboardInterrupt.</p>
<p>Notes:
The purpose of this project will be to monitor the website to detect if any changes occur. (Wallet goes back in stock)</p>
",0,1597184015,python;beautifulsoup;python-requests;urllib3,True,304,1,1597191104,https://stackoverflow.com/questions/63367092/request-time-out-in-python
63264392,Python scraping: Error 54 &#39;Connection reset by peer&#39;,"<p>I have wrote simple script to get html's from multiple website. Although I didn't have any issue with the script up until yesterday. It suddenly started throwing the exception bellow.</p>
<pre><code>Traceback (most recent call last):
  File &quot;crowling.py&quot;, line 45, in &lt;module&gt;
    result = requests.get(url)
  File &quot;/Users/gen/.pyenv/versions/3.7.1/lib/python3.7/site-packages/requests/api.py&quot;, line 76, in get
    return request('get', url, params=params, **kwargs)
  File &quot;/Users/gen/.pyenv/versions/3.7.1/lib/python3.7/site-packages/requests/api.py&quot;, line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File &quot;/Users/gen/.pyenv/versions/3.7.1/lib/python3.7/site-packages/requests/sessions.py&quot;, line 530, in request
    resp = self.send(prep, **send_kwargs)
  File &quot;/Users/gen/.pyenv/versions/3.7.1/lib/python3.7/site-packages/requests/sessions.py&quot;, line 685, in send
    r.content
  File &quot;/Users/gen/.pyenv/versions/3.7.1/lib/python3.7/site-packages/requests/models.py&quot;, line 829, in content
    self._content = b''.join(self.iter_content(CONTENT_CHUNK_SIZE)) or b''
  File &quot;/Users/gen/.pyenv/versions/3.7.1/lib/python3.7/site-packages/requests/models.py&quot;, line 754, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: (&quot;Connection broken: ConnectionResetError(54, 'Connection reset by peer')&quot;, ConnectionResetError(54, 'Connection reset by peer'))
</code></pre>
<p>The main part of the script is this.</p>
<pre class=""lang-py prettyprint-override""><code>c = 0
#urls is the list of urls as strings
for url in urls:
    result = requests.get(url)
    c += 1
    with open('htmls/p{}.html'.format(c),'w',encoding='UTF-8') as f:
        f.write(result.text)
</code></pre>
<p>The list urls is generated by my other codes and I have checked that the urls are correct. Also the timing of the exception is not constant. Sometimes it stops when scraping 20th htmls and sometimes it goes until 80th then stop. As the exception suddenly appeared without changing codes, I am guessing that the exception is due to the Internet connection. Yet, I want to ensure that the script works stably. Is there any possible cause of this error?</p>
",2,1596627575,python;web-scraping;python-requests;urllib3,True,10018,1,1596630238,https://stackoverflow.com/questions/63264392/python-scraping-error-54-connection-reset-by-peer
63263162,Python import error even though relevant module is installed,"<p>I'm trying to run Python behave tests but get the following error:</p>
<pre><code>[moadmin@modevvm12 e2e-tests]$ behave -i features/tests/connmgr_2_scan_management_container.feature 
Exception ImportError: No module named request
Traceback (most recent call last):
  File &quot;/usr/bin/behave&quot;, line 11, in &lt;module&gt;
    sys.exit(main())
  File &quot;/usr/lib/python2.7/site-packages/behave/__main__.py&quot;, line 183, in main
    return run_behave(config)
  File &quot;/usr/lib/python2.7/site-packages/behave/__main__.py&quot;, line 127, in run_behave
    failed = runner.run()
  File &quot;/usr/lib/python2.7/site-packages/behave/runner.py&quot;, line 804, in run
    return self.run_with_paths()
  File &quot;/usr/lib/python2.7/site-packages/behave/runner.py&quot;, line 808, in run_with_paths
    self.load_hooks()
  File &quot;/usr/lib/python2.7/site-packages/behave/runner.py&quot;, line 784, in load_hooks
    exec_file(hooks_path, self.hooks)
  File &quot;/usr/lib/python2.7/site-packages/behave/runner_util.py&quot;, line 386, in exec_file
    exec(code, globals_, locals_)
  File &quot;features/environment.py&quot;, line 20, in &lt;module&gt;
    from framework.sd.api.auth_rbac import AuthRBAC
  File &quot;/tmp/e2e-tests/framework/sd/api/auth_rbac.py&quot;, line 10, in &lt;module&gt;
    from framework.sd.api.sd_endpoint import SDEndpoint
  File &quot;/tmp/e2e-tests/framework/sd/api/sd_endpoint.py&quot;, line 12, in &lt;module&gt;
    import urllib.request, urllib.parse, urllib.error
ImportError: No module named request
</code></pre>
<p>urllib is installed so I can't understand why I get this error.</p>
",1,1596623084,python;importerror;urllib3;python-behave,False,159,0,1596623084,https://stackoverflow.com/questions/63263162/python-import-error-even-though-relevant-module-is-installed
63064707,Change request headers between subsequent retries,"<p>Consider an http request using an OAuth token. The access token needs to be included in the header as bearer. However, if the token is expired, another request needs to be made to refresh the token and then try again. So the custom Retry object will look like:</p>
<pre><code>s = requests.Session()
### token is added to the header here
s.headers.update(token_header)
retry = OAuthRetry(
        total=2,
        read=2,
        connect=2,
        backoff_factor=1,
        status_forcelist=[401],
        method_whitelist=frozenset(['GET', 'POST']),
        session=s
    )
adapter = HTTPAdapter(max_retries=retry)
s.mount('http://', adapter)
s.mount('https://', adapter)
r = s.post(url, data=data)
</code></pre>
<p>The Retry class:</p>
<pre><code>class OAuthRetry(Retry):
    def increment(self, method, url, *args, **kwargs):
        # refresh the token here. This could be by getting a reference to the session or any other way.
        return super(OAuthRetry, self).increment(method, url, *args, **kwargs)
</code></pre>
<p>The problem is that after the token is refreshed, HTTPConnectionPool is still using the same headers to make the request after calling increment. See: <a href=""https://github.com/urllib3/urllib3/blob/master/src/urllib3/connectionpool.py#L787"" rel=""noreferrer"">https://github.com/urllib3/urllib3/blob/master/src/urllib3/connectionpool.py#L787</a>.
Although the instance of the pool is passed in increment, changing the headers there will not affect the call since it is using a local copy of the headers.</p>
<p>This seems like a use case that should come up frequently for the request parameters to change in between retries.</p>
<p>Is there a way to change the request headers in between two subsequent retries?</p>
",14,1595547807,python;python-requests;urllib3,True,1351,1,1596529678,https://stackoverflow.com/questions/63064707/change-request-headers-between-subsequent-retries
60771889,AWS API gateway Lambda autherizer warning,"<p>AWS API Gateway Autherizer lambda is working with warning:</p>

<p>/var/runtime/botocore/vendored/requests/api.py:64: DeprecationWarning: You are using the post() function from 'botocore.vendored.requests'. This dependency was removed from Botocore and will be removed from Lambda after 2020/03/31. <a href=""https://aws.amazon.com/blogs/developer/removing-the-vendored-version-of-requests-from-botocore/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/developer/removing-the-vendored-version-of-requests-from-botocore/</a>. Install the requests package, 'import requests' directly, and use the requests.post() function instead.</p>

<p>I know this can be solved by importing requests package, but I tried if I could solve it using urllib3(default package in boto3). Below is my code </p>

<pre><code>data = {
    'token': access_token,
    'client_id': client_id,
    'client_secret': client_secret,
    'token_type_hint': 'access_token'
}
http = urllib3.PoolManager()

encoded_data = json.dumps(data).encode('utf-8')
introspection = http.request(method='POST',
                             url=INTROSPECTION_ENDPOINT,
                             body=encoded_data,
                             headers={'Content-Type': 'application/x-www-form-urlencoded'}
                             )
</code></pre>

<p>after running the code I am getting the following error:
{""error_description"":""token parameter is required for the introspection endpoint."",""error"":""invalid_request""}'</p>

<p>but the same is working using requests </p>

<pre><code>data = {
    'token': access_token,
    'client_id': client_id,
    'client_secret': client_secret,
    'token_type_hint': 'access_token'
}

introspection = requests.post(
     url=INTROSPECTION_ENDPOINT,
     data=data,
     headers={
         'Content-Type': 'application/x-www-form-urlencoded'
     }
 )
</code></pre>
",0,1584698035,python;aws-lambda;python-requests;urllib3,True,177,2,1595959716,https://stackoverflow.com/questions/60771889/aws-api-gateway-lambda-autherizer-warning
62955727,not getting a urllib response on some urls that opens regularly in the webbrowser,"<p>I am having an issue with urllib on python3 that I dont know how to solve.
this is my code :</p>
<pre><code>import urllib.request
       hdr = { 'User-Agent' : 'super happy flair bot by /u/spladug' }
       req = urllib.request.Request(url,headers=hdr)
       data = urllib.request.urlopen(req,timeout=10).read()
       return data
</code></pre>
<p>Stuff I tried:</p>
<ol>
<li>Changing user agent</li>
<li>Adding and removing timeouts</li>
<li>curl these sites</li>
</ol>
<p>but still there are some sites(that ends with image file extension like <a href=""http://xxx.xxx/aaa.png"" rel=""nofollow noreferrer"">http://xxx.xxx/aaa.png</a> or .jpg) that I just can't get a response,
and if I am setting a timeout I am getting TimeOutError. but if i open the same site in the chrome everything working fine.
Does someone have any solution or faced the issue?</p>
<p>site example -
<a href=""https://sgfm.elcorteingles.es/SGFM/dctm/MEDIA03/202006/24/00117731276964____5__210x210.jpg"" rel=""nofollow noreferrer"">https://sgfm.elcorteingles.es/SGFM/dctm/MEDIA03/202006/24/00117731276964____5__210x210.jpg</a></p>
",2,1594994823,python;urllib;urllib3,False,209,1,1594997711,https://stackoverflow.com/questions/62955727/not-getting-a-urllib-response-on-some-urls-that-opens-regularly-in-the-webbrowse
62864010,I can&#39;t use urllib3 or urllib in python 3.8,"<p>I just installed urllib3 using pip command and was successful, and when i use <code>import urllib3</code> or <code>import urllib</code> its showing a huge error. Why this happening?. Please help.</p>
<p>This is my error: when use <code>import urllib</code> or <code>import urllib3</code> and running in sublime text.</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\Toshiba\Desktop\dd.py&quot;, line 1, in &lt;module&gt;
    import urllib3
  File &quot;C:\Users\Toshiba\AppData\Local\Programs\Python\Python38-32\lib\site-packages\urllib3\__init__.py&quot;, line 7, in &lt;module&gt;
    from .connectionpool import HTTPConnectionPool, HTTPSConnectionPool, connection_from_url
  File &quot;C:\Users\Toshiba\AppData\Local\Programs\Python\Python38-32\lib\site-packages\urllib3\connectionpool.py&quot;, line 11, in &lt;module&gt;
    from .exceptions import (
  File &quot;C:\Users\Toshiba\AppData\Local\Programs\Python\Python38-32\lib\site-packages\urllib3\exceptions.py&quot;, line 2, in &lt;module&gt;
    from .packages.six.moves.http_client import IncompleteRead as httplib_IncompleteRead
  File &quot;C:\Users\Toshiba\AppData\Local\Programs\Python\Python38-32\lib\site-packages\urllib3\packages\six.py&quot;, line 199, in load_module
    mod = mod._resolve()
  File &quot;C:\Users\Toshiba\AppData\Local\Programs\Python\Python38-32\lib\site-packages\urllib3\packages\six.py&quot;, line 113, in _resolve
    return _import_module(self.mod)
  File &quot;C:\Users\Toshiba\AppData\Local\Programs\Python\Python38-32\lib\site-packages\urllib3\packages\six.py&quot;, line 82, in _import_module
    __import__(name)
  File &quot;C:\Users\Toshiba\AppData\Local\Programs\Python\Python38-32\lib\http\client.py&quot;, line 71, in &lt;module&gt;
    import email.parser
  File &quot;C:\Users\Toshiba\AppData\Local\Programs\Python\Python38-32\lib\email\parser.py&quot;, line 12, in &lt;module&gt;
    from email.feedparser import FeedParser, BytesFeedParser
  File &quot;C:\Users\Toshiba\AppData\Local\Programs\Python\Python38-32\lib\email\feedparser.py&quot;, line 27, in &lt;module&gt;
    from email._policybase import compat32
  File &quot;C:\Users\Toshiba\AppData\Local\Programs\Python\Python38-32\lib\email\_policybase.py&quot;, line 9, in &lt;module&gt;
    from email.utils import _has_surrogates
  File &quot;C:\Users\Toshiba\AppData\Local\Programs\Python\Python38-32\lib\email\utils.py&quot;, line 31, in &lt;module&gt;
    import urllib.parse
  File &quot;C:\Users\Toshiba\Desktop\urllib.py&quot;, line 2, in &lt;module&gt;
    from urllib3 import PoolManager
ImportError: cannot import name 'PoolManager' from partially initialized module 'urllib3' (most likely due to a circular import) (C:\Users\Toshiba\AppData\Local\Programs\Python\Python38-32\lib\site-packages\urllib3\__init__.py)
</code></pre>
",-1,1594573347,python;urllib;urllib3,True,1039,1,1594621792,https://stackoverflow.com/questions/62864010/i-cant-use-urllib3-or-urllib-in-python-3-8
50632089,python3 HTTP request taking a long time,"<p>I'm using the requests library to fetch a simple URL (I've put a dummy URL here, a normal URL is used in code):</p>

<pre><code>import requests
response = requests.get(""http://example.com/foo/bar/"", headers={""User-Agent"": ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36""})
</code></pre>

<p>Locally it works fine, but when I put the same code on my server, this request takes forever to finish. I've enabled logging output for all of these loggers:</p>

<pre><code>urllib3.util.retry
urllib3.util
urllib3
urllib3.connection
urllib3.response
urllib3.connectionpool
urllib3.poolmanager
requests
</code></pre>

<p>This is the only output produced by them:</p>

<pre><code>2018-05-31 19:55:56,894 - urllib3.connectionpool - DEBUG - Starting new HTTP connection (1): example.com
2018-05-31 19:58:06,676 - urllib3.connectionpool - DEBUG - http://example.com:80 ""GET /foo/bar/ HTTP/1.1"" 200 None
</code></pre>

<p>The funny thing is that it always takes exactly 2 minutes and 10 seconds for the request to finish (if you disregard milliseconds). Locally it's instant.</p>

<p>Any clues where I should look next?</p>
",3,1527797628,python;python-3.x;python-requests;urllib3,True,1094,1,1593189262,https://stackoverflow.com/questions/50632089/python3-http-request-taking-a-long-time
62367611,python3 - urllib - ssl - UnicodeDecodeError,"<p>I want to run this code. But error why is it error?</p>

<pre><code>import requests
import ssl
from urllib import request

urlMain = ""https://www.example.com/""
urlLogin = ""https://www.example.com/join/""

istekBaba = requests.get(urlMain)
cookies = istekBaba.cookies.get_dict()
baslik = {
    ""User-Agent"" : ""Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:74.0) Gecko/20100101 Firefox/74.0"",
    ""Sec-Fetch-Site"" :""none"",
    ""Accept"" : ""text/html"",
    ""Accept-Language"" : ""en-US,en;q=0.5"",
    ""Accept-Encoding"" : ""gzip, deflate"",
    ""Referer"" : ""https://www.example.com/""
}

req = request.Request(url=urlLogin,headers=baslik)
req.add_header(""Cookie"",""ud_rule_vars""+""=""+cookies[""ud_rule_vars""]+"";"")
contextssl = ssl.SSLContext(ssl.PROTOCOL_SSLv23)
res = request.urlopen(req,context=contextssl)
with res as f:
    print(f.read().decode('utf-8'))

</code></pre>

<p>Why it is error?</p>

<p>Error code is here</p>

<pre><code>C:\Users\emRe\Documents\works\python\test&gt;python test.py
Traceback (most recent call last):
  File ""test.py"", line 24, in &lt;module&gt;
    print(f.read().decode('utf-8'))
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte
</code></pre>

<p>error why is error? what is wrong?
thank you guys for helping....</p>

<p>edit: 
if delete utf-8, i see it.</p>

<pre><code>b'\x1f\x8b\x08\x00\x00\x00\x00\x00\x00\x03\xe2\xb2I\xc9,SH\xceI,.\xb6U\xca\xc9O\xcf\xccK\xca\xaf\xd0-3Q\xc8\xcd\x07\x00\x00\x00\xff\xff\x9cV\xdfS\xe36\x10~\xcf_\xa1\xea\xe1x\xa9p`\xca\xaf\xbb\xc4L\xb9\xbb^\xb92\x1cS\xa0\xed\xb5\xd3\xc9\xc8\xd6\xc6\x16\x91\xb5B\x96I2\x9d\xfe\xef\x1dYvb\x87\xc0A\xf3\x10\xdb+\xed\xf7\xed~\xab\x95$\xb8b)j\x07\xda\xb1\xb9\xe5\xc6\x80%\x95`\xdc\x18\xa6\x90\x8b\xf0\x95baP\xfb)\xac\xc4Tr\xc5x\xe5r\xe6g\xd1\x01i\x7f\x82;\xce\n\x14\x95\x02&amp;\xc5\x98v\xa6&gt;1\x8b\xdb\xac\x1c\xd3\x7f\xde\xdcW\xe8\xde\x15( \xbc\xbd\r\x8f\x8b/\x9f\xce/\xc3\xeb\xbf\x9b\x00\xa6\xb2\x06Kh\xb2a\t.h&lt;\xe8f\xf9\\\xd4\xfe\xbf\x04\xd0,Y\xb2\xaa\x04\xcb\x9c\xe5\xe9\x0c,\x8dG\x91\x90\x0f}\xa0\x8e\\\x93I\x0e^\x12\x1a\x0f.0#\xe7\x9a8$_\xb1\xb2\xe4V@\xb1$?\xa6)V\xda}7\xf8\x16L#\xb8\x0f\xb94\\wb~R\xf7G\x82o\xaa\xdd\x95\xf9I\x8d\xb91\n\xae\x1dw}\xa5\xc1Vy\x9a\xdc\xe5\xe5b\xa6\x8a\xc5b\x11\xac\xdfw|\xde+\t\xda\x9d\x8b\x9e[\x8a\xc5n\xe53\xdf\x9dC\xd2\xf3\xc9\x10\xb3\'\x9c\x8e\x86\xc3\xfd\xe1\xe1p\x7fo8&lt;`\xc9\xd4\x82\x1e\xde\xdf\xed\xa1=6\xe6\xa04Z\x1f\xec\xef\xe5X\x9e\xa8\xe2D\xa9\x93]nL\xb9\x1b\xd0|\xa5\x1a\xe1vS,\xfa1*\x85s\x10\xd7u\x99\xcf*\xe7P\x97\r\xeb_\xe19\xe5)$\x88\xb3-a2\xa3\xaa\xf2q\xca\xe1\xf5\xef\xae\xe9W\x10\xd2B\xean\xad\xec\xe5\x94;g\xca\xb7Q4\x9f\xcf\x1bAR,\xa2;\x94:jV\x9e\xaf\xa4\x02\x07Q\x8d\x13\xd5\xab!\xeaq\xb6\x01\xae9T\x8f\xa3\x07\x97\x80\xf7o]\xa2\xd3\xa9\xc5b\\\x83\xbe\xe1\x85y\'di\x14_N\xdc\xd2\xc0\xd8\xa0\xa9LmV\x98r\x05c\xd0\x93\xdb\xeb-:\xbc\x88y\x95HG\xbb\xe8\xf4\xb5\x84&lt;u\x12\xf5\xcd\xd2\xf4\xd7a\xc8\xa0\xdf\xf1\xdd\xb5l\xacD+\xddrL\xd9\xfe\xd0wO\xe4\xdb\'\x1e\x8c\xa6h\x0bB\x02\xea\x98&gt;S\x8e\xb0]\xd4\x11\xbe(j\xda\x80\xb2zN\xe8b\xda\xf6k)3-5\xab\xc9\xc5\x1d%\xbe\r\x03\x817Q\xa2y\x01}K\x01.G1\xa6\x06KGILFR\x9b\xca\x91\x1a|\'\x97B\x80\xde\tn;ii\xa7\x85\x14B\xc1\x9c[p8\xf3C\x0f\\U~\xe6\x17}\xff\xe9\xf2su\xf8\xfbr\xaf&lt;;\xb8\xfe\xe5\xe8\xec\xf6\xfa\xcf\x0b\xa3\xaf2s\x979\x10\x7f|UG\x1f\xf2\xe3\xd9\xb19L\xd2\x93\x8b\x9b\xe5\xfe\xde\xcf\x97{\xef\xef\xcfo&gt;\xdb\xdf\x8e\x16;$\x8aI\xbd;\x91Z\xe5&amp;\x9f\x82k\x9e\x01\x9bJP\xa2lO\x83\xcd0i\x083\x94\xa7\xcd\xd0\x0b\x16,!B\x1a\xc4\xab-\xeb\xbf\x96\xb4\xa1\xf3\x92\x04\xb2\xfa\x04\xe2R\x83%D\xf1\x04\x14\x88\xe7\x8e\x1dq\xc7u\x86\x0c\n.U\x00\x08L^\xff\x1aT:(\xc2p\x18\x88\xc9\xa8\x86m\x99=\x9dE\xc5\x82\x91\x92)\xda1\x95b\x12\\\xe2\x8f\xfe1\x8a\xea\xd1&amp;d\x0f\xed\x10\x95\x93\x86Y\x98\x82\x05\x9dB\x97\xa2#\xe3\xa3y\xc4`\xc9,\xa1\x1bB\x06\xefF\xc3\xe6\xa3\xe0\x0b\x05:s\xf9\x98\x1e\xfe\x10V\xd4*\xae\x16\xbf\xfe\n@u\xb6M6\x94X\xb8\xaf\xa4\x05A\x8c\xe2)\xe4\xa8\x04\xd81\xfd\xd8 K\xdd""\x1f\xd1\x8dC\xb4\xc1\xf7\xf5iN\xae\xfa\xf1\x8ajm+\x80\xe1e9G+^W\x83\x95W|\xd5\xbc\xbd\xa4\x12}\xae\xffQ\x8c\x15@S\x8f\xf5\xf7\xf6\x92\xac\xc7[""X\xb8\x06\x10\x16\xee|KyzU\xb9Z\xf9\xaf\xaa\xd6\xa9\xd0\xe1f\x85\xd6t\x9bE\xdav\xd9\xa8y\xc3\xc6U\xd2m\x9d^VI!\x1d\xb38\x7f\xd4\xdea\xa8\xdb\xde]K\xd3\xde\xe1\xfa\xd3\xe9\xef\x0ev\xe24I\x9c\xf6\x9bu\xc1\xed\x92\xacKR_\n\x03\xb3\x14\xac\x0b;\xd8r\xb3\x13\xc8\xc2v\xdb\xec\x1d\xe1\xae\x14\xa3%\xcd\xb6?\xe2$\xb70\x1d\xd3\xc8_\r\xa2)\xda\x0c\xddj%l\xdd\xde\xfb[\xfbZ\xad\xae#SR\xcfh\xfcSm%\xebE\xc87\x14\'#OY\xc4\xdf\xbc\xedM\x11]}i\xfc\x80z\xc7\x91\x9c?\x00\xe1\x9a\xf0p[&lt;\x1d\x8cx\xf7@\t\xf4mf\xe1\xe8\x95\x99\xae\xcc\xabN\xad\xbe\x945neh&lt;\xb8\x96\x99&amp;\x95\x19&lt;N\xe7\xa9\xf8Y\x86,\xe1\xe9l2i\x0f\x84\x95\xc7\x7f\x00\x00\x00\xff\xff\x03\x00\x8c$8\xafW\x0c\x00\x00'
</code></pre>
",0,1592100443,python;python-3.x;urllib;urllib2;urllib3,False,316,0,1592100870,https://stackoverflow.com/questions/62367611/python3-urllib-ssl-unicodedecodeerror
62275876,why this https request only works with urllib3 and requests libraries?,"<p>I'm making this request and it only works with urllib3 and requests library i'm guessing it's because of ssl version or cert verification. any clue would be appreciated</p>

<p>this works</p>

<pre class=""lang-py prettyprint-override""><code>import urllib3

params = {...}
http = urllib3.PoolManager(cert_reqs='CERT_NONE', assert_hostname=False)
r = http.request(""POST"", ""https://android.clients.google.com/auth"" , fields=params)
print(r.data)
</code></pre>

<p>this return <code>Error=BadAuthentication</code></p>

<pre class=""lang-py prettyprint-override""><code>from urllib import request, parse
import urllib

params = {...}
data = parse.urlencode(params).encode()
req = request.Request(""https://android.clients.google.com/auth"", data=data)
try:
  resp = request.urlopen(req)
  print(resp.read())
except urllib.error.HTTPError as e:
  print(""error"", e.read())
</code></pre>
",0,1591681784,python;curl;python-requests;urllib;urllib3,True,2044,1,1591801192,https://stackoverflow.com/questions/62275876/why-this-https-request-only-works-with-urllib3-and-requests-libraries
62157976,Python Requests returns content of other random URL,"<p>So, I have a strange behavior when trying to scrape a webpage with python requests library. For some reason that I don't understand when I scrape the content of a webpage I get the data of another apparently random webpage. Here's an example:</p>

<pre><code>import requests
from bs4 import BeautifulSoup

def scrape_webpage(url):
    """"""
    Function to scrape some data from given url
    """"""
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    data = {'event_title': soup.find('h1').text.lower()}
    data['event_date'] = soup.find('li', {'class': 'header'}).text.split()[1]

    return data

# Test URL 
url = 'https://www.tapology.com/fightcenter/events/67412-ufc-on-espn-33'

# First try returns the correct info
first = scrape_webpage(url)
print(first)   
# {'event_date': '05.16.2020', 'event_title': 'ufc fight night: overeem vs. harris'}

# A second try changing nothing returns wrong info
second = scrape_webpage(url)
print(second)
# {'event_date': '06.20.2020', 'event_title': 'efm 3'}

# A third try also fails to retrieve the correct data
third = scrape_webpage(url)
print(third)
# {'event_date': '10.05.2010', 'event_title': 'bystriy fight club 1'}
</code></pre>

<p>And so this behavior repeats without apparent logic. Also worth to mention is that I'm using Google Colab to do this. If I try to scrape a list of urls only the first one gets the correct data (and only if it's the first try) and the rest returns data from a random url. So the questions is ¿how to fix this behavior?</p>
",0,1591119368,python;web-scraping;python-requests;urllib3,True,315,1,1591119939,https://stackoverflow.com/questions/62157976/python-requests-returns-content-of-other-random-url
17285464,What&#39;s the best way to download file using urllib3,"<p>I would like to download file over <code>HTTP</code> protocol using <code>urllib3</code>.
I have managed to do this using following code:</p>

<pre><code> url = 'http://url_to_a_file'
 connection_pool = urllib3.PoolManager()
 resp = connection_pool.request('GET',url )
 f = open(filename, 'wb')
 f.write(resp.data)
 f.close()
 resp.release_conn()
</code></pre>

<p>But I was wondering what is the <strong>proper</strong> way of doing this.
For example will it work well for big files and If no what to do to make this code more bug tolerant and scalable.</p>

<p>Note. It is important to me to use <code>urllib3</code> library not <code>urllib2</code> for example, because I want my code to be thread safe.</p>
",27,1372109428,python;download;urllib3,True,61938,3,1590739280,https://stackoverflow.com/questions/17285464/whats-the-best-way-to-download-file-using-urllib3
61889452,Add parameters to a given URL in Python,"<p>I want to add the following parameters:</p>

<p><code>params= {'Context' : { ""Country"":""US"", ""Region"":""US"", ""Language"":""en"", ""Segment"":""dhs"", ""CustomerSet"":""19""}, 'itemIdentifiers' : ['210-amsr','320-9704']}</code></p>

<p>to the base URL = <code>https://www.catalogue.com/getdetail?</code></p>

<p>so that the final URL looks like this: <code>https://www.catalogue.com/getdetail?Context={""Country"":""US"",""Region"":""US"",""Language"":""en"",""Segment"":""dhs"",""CustomerSet"":""19""}&amp;itemIdentifiers=210-amsr,320-9704</code></p>

<p>I've tried the following approach:
<code>api_url = url+parse.urlencode(params, doseq=True)</code></p>

<p>but I end up with: <code>https://www.catalogue.com/getdetail?Context=Country&amp;Context=Region&amp;Context=Language&amp;Context=Segment&amp;Context=CustomerSet&amp;itemIdentifiers=210-amsr&amp;itemIdentifiers=320-9704</code></p>
",1,1589886084,python;urllib;urllib3,True,617,1,1589887732,https://stackoverflow.com/questions/61889452/add-parameters-to-a-given-url-in-python
51686224,TypeError: urlopen() got multiple values for keyword argument &#39;body&#39; while executing tests through Selenium and Python on Kubuntu 14.04,"<p>im trying to run a selenium in python on Kubuntu 14.04. 
I get this error message trying with chromedriver or geckodriver, both same error.</p>

<pre><code>Traceback (most recent call last):
  File ""vse.py"", line 15, in &lt;module&gt;
    driver = webdriver.Chrome(chrome_options=options, executable_path=r'/root/Desktop/chromedriver')
  File ""/usr/local/lib/python3.4/dist-packages/selenium/webdriver/chrome/webdriver.py"", line 75, in __init__
    desired_capabilities=desired_capabilities)
  File ""/usr/local/lib/python3.4/dist-packages/selenium/webdriver/remote/webdriver.py"", line 156, in __init__
    self.start_session(capabilities, browser_profile)
  File ""/usr/local/lib/python3.4/dist-packages/selenium/webdriver/remote/webdriver.py"", line 251, in start_session
    response = self.execute(Command.NEW_SESSION, parameters)
  File ""/usr/local/lib/python3.4/dist-packages/selenium/webdriver/remote/webdriver.py"", line 318, in execute
    response = self.command_executor.execute(driver_command, params)
  File ""/usr/local/lib/python3.4/dist-packages/selenium/webdriver/remote/remote_connection.py"", line 375, in execute
    return self._request(command_info[0], url, body=data)
  File ""/usr/local/lib/python3.4/dist-packages/selenium/webdriver/remote/remote_connection.py"", line 397, in _request
    resp = self._conn.request(method, url, body=body, headers=headers)
  File ""/usr/lib/python3/dist-packages/urllib3/request.py"", line 79, in request
    **urlopen_kw)
  File ""/usr/lib/python3/dist-packages/urllib3/request.py"", line 142, in request_encode_body
    **urlopen_kw)
TypeError: urlopen() got multiple values for keyword argument 'body'
</code></pre>

<hr>

<pre><code>import time
import mapeamentos as map
from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.common.exceptions import TimeoutException
from random import randint
import datetime
from selenium.webdriver.chrome.options import Options
options = Options()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
driver = webdriver.Chrome(chrome_options=options, executable_path=r'/root/Desktop/chromedriver')
driver.get('http://192.168.1.11:66/webclient/login.html')
</code></pre>

<p>This error gets fired in:</p>

<pre><code>driver = webdriver.Chrome()
</code></pre>

<p>Ive tried with options, without options, without the hardcoded path ou with the path.</p>

<p>I have no ideia what is happening. thanks everyone.</p>
",8,1533390439,python;selenium;selenium-webdriver;urllib3;urlopen,True,7237,2,1588957311,https://stackoverflow.com/questions/51686224/typeerror-urlopen-got-multiple-values-for-keyword-argument-body-while-execu
61612465,"Requests cannot connect to server with supplied .pem file, but urrlib3 works even without it","<p>I am trying to send HTTP requests to server (only accessible from specific network, so all names are made up) via requests lib, but I am getting SSL error. </p>

<p>My code:</p>

<pre><code>site_auth = HTTPBasicAuth(""user"", ""password"")
response = requests.get(""https://my.hidden.site.com"", auth=site_auth , verify=""location/src/mycertfile.pem"")
</code></pre>

<p>I have tried also this:</p>

<pre><code>site_auth = HTTPBasicAuth(""user"", ""password"")
response = requests.get(""https://my.hidden.site.com"", auth=site_auth , verify=True, cert=""location/src/mycertfile.pem"")
</code></pre>

<p>But the result was the same stacktrace.</p>

<p>Stacktrace:</p>

<pre><code>Traceback (most recent call last):
  File ""location\.venv\lib\site-packages\urllib3\connectionpool.py"", line 670, in urlopen
    httplib_response = self._make_request(
  File ""location\.venv\lib\site-packages\urllib3\connectionpool.py"", line 381, in _make_request
    self._validate_conn(conn)
  File ""location\.venv\lib\site-packages\urllib3\connectionpool.py"", line 976, in _validate_conn
    conn.connect()
  File ""location\.venv\lib\site-packages\urllib3\connection.py"", line 361, in connect
    self.sock = ssl_wrap_socket(
  File ""location\.venv\lib\site-packages\urllib3\util\ssl_.py"", line 377, in ssl_wrap_socket
    return context.wrap_socket(sock, server_hostname=server_hostname)
  File ""C:\Users\Adam\AppData\Local\Programs\Python\Python38-32\lib\ssl.py"", line 500, in wrap_socket
    return self.sslsocket_class._create(
  File ""C:\Users\Adam\AppData\Local\Programs\Python\Python38-32\lib\ssl.py"", line 1040, in _create
    self.do_handshake()
  File ""C:\Users\Adam\AppData\Local\Programs\Python\Python38-32\lib\ssl.py"", line 1309, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""location\.venv\lib\site-packages\requests\adapters.py"", line 439, in send
    resp = conn.urlopen(
  File ""location\.venv\lib\site-packages\urllib3\connectionpool.py"", line 724, in urlopen
    retries = retries.increment(
  File ""location\.venv\lib\site-packages\urllib3\util\retry.py"", line 439, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='my.hidden.site.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""location/main_playground.py"", line 15, in &lt;module&gt;
    response = requests.get(""https://my.hidden.site.com"", auth=rt_auth, verify=""location/mycertfile.pem"")
  File ""location\.venv\lib\site-packages\requests\api.py"", line 76, in get
    return request('get', url, params=params, **kwargs)
  File ""location\.venv\lib\site-packages\requests\api.py"", line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File ""location\.venv\lib\site-packages\requests\sessions.py"", line 530, in request
    resp = self.send(prep, **send_kwargs)
  File ""location\.venv\lib\site-packages\requests\sessions.py"", line 643, in send
    r = adapter.send(request, **kwargs)
  File ""location\.venv\lib\site-packages\requests\adapters.py"", line 514, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='my.hidden.site.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)')))

</code></pre>

<p>I am using these versions of the libs:</p>

<pre><code>requests==2.23.0
urllib3==1.25.9
</code></pre>

<p>I have googled a lot, but nothing works for me. I have manually downloaded the certification chain and created my .pem file which I have checked with openssl verify. I have 3 certificates in it and I have them ordered from the root to my target site in top to bottom fashion like this:</p>

<pre><code>-----BEGIN CERTIFICATE-----
...
-----END CERTIFICATE-----
-----BEGIN CERTIFICATE-----
...
-----END CERTIFICATE-----
-----BEGIN CERTIFICATE-----
...
-----END CERTIFICATE-----
</code></pre>

<p>Weird thing is that urrlib3 works fine even without .pem file and explicitly enabled certificates, code:</p>

<pre><code>http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED')
r = http.request('GET', ""https://my.hidden.site.com"", fields={'user': ""user"", 'pass': ""password""})
print(r)
print(r.data.decode('utf-8'))
</code></pre>

<p>Another weird thing is that curl does not work by default, but works like it should when I supply the .pem file, so the requests should too, right?</p>

<pre><code>curl https://my.hidden.site.com --cacert mycertfile.pem
</code></pre>

<p>I tried also Postman, but he does not work even with selected .pem file in the settings.</p>

<pre><code>Could not get any response
There was an error connecting to https://my.hidden.site.com.
</code></pre>

<p>I have also tried installing root and intermediate certificates on my machine (Windows 10), but it has no effect. Has anyone any idea what can be wrong or what should I check next?</p>

<p>Thanks for any help</p>

<p>// EDIT 1</p>

<p>I had created the chain of certificates by using this command and extracting the 3 keys from it. I have changed the order in the .pem file like I have already mentioned so the key of the root CA would be first in the .pem file and also the intermediate is second and the key for my.hidden.site.com is third.</p>

<p>I can see there are lines <em>unable to get local issuer certificate</em> and
<em>verify error:num=21:unable to verify the first certificate</em>, but I do not know whether it is a problem or not, or maybe there is something missing.</p>

<pre><code>$ openssl s_client -showcerts -connect my.hidden.site.com:443
CONNECTED(0000017C)
---
Certificate chain
 0 s:SECRET LINE my.hidden.site.com
   i:SECRET LINE intermediate
-----BEGIN CERTIFICATE-----
...
-----END CERTIFICATE-----
 1 s:SECRET LINE intermediate
   i:SECRET LINE root ca
-----BEGIN CERTIFICATE-----
...
-----END CERTIFICATE-----
 2 s:SECRET LINE root ca
   i:SECRET LINE root ca
-----BEGIN CERTIFICATE-----
...
-----END CERTIFICATE-----
---
Server certificate
subject=SECRET LINE my.hidden.site.com

issuer=SECRET LINE intermediate

---
No client certificate CA names sent
Peer signing digest: SHA256
Peer signature type: RSA
Server Temp Key: ECDH, P-256, 256 bits
---
SSL handshake has read 4618 bytes and written 444 bytes
Verification error: unable to verify the first certificate
---
New, TLSv1.2, Cipher is ECDHE-RSA-AES256-GCM-SHA384
Server public key is 2048 bit
Secure Renegotiation IS supported
Compression: NONE
Expansion: NONE
No ALPN negotiated
SSL-Session:
    Protocol  : TLSv1.2
    Cipher    : ECDHE-RSA-AES256-GCM-SHA384
    Session-ID: &lt;HIDDEN_SESSION_ID&gt;
    Session-ID-ctx:
    Master-Key: &lt;HIDDEN_MASTER_KEY&gt;
    PSK identity: None
    PSK identity hint: None
    SRP username: None
    TLS session ticket lifetime hint: 300 (seconds)
    TLS session ticket:
    &lt;HIDDEN_SESSION_TICKET&gt;

    Start Time: 1588693969
    Timeout   : 7200 (sec)
    Verify return code: 21 (unable to verify the first certificate)
    Extended master secret: no
---

closed
depth=0 HIDDEN DETAILS, CN = my.hidden.site.com
verify error:num=20:unable to get local issuer certificate
verify return:1
depth=0 HIDDEN DETAILS CN = my.hidden.site.com
verify error:num=21:unable to verify the first certificate
verify return:1
</code></pre>
",0,1588679499,python;authentication;python-requests;certificate;urllib3,False,870,1,1588701843,https://stackoverflow.com/questions/61612465/requests-cannot-connect-to-server-with-supplied-pem-file-but-urrlib3-works-eve
28411546,What does the Python InsecureRequestWarning really mean?,"<p>I'm getting the warning:</p>

<pre><code>/.../local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py:734: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html
  InsecureRequestWarning)
</code></pre>

<p>I'm reading <a href=""https://urllib3.readthedocs.org/en/latest/security.html"" rel=""noreferrer"">the doc</a>.</p>

<p>I'm seeing lots of posts on how to disable it if I know what I'm doing, like <a href=""https://stackoverflow.com/questions/27981545/surpress-insecurerequestwarning-unverified-https-request-is-being-made-in-pytho"">this one</a>.</p>

<p>But I'm still having trouble figuring out what the error means. I gather that it means that I'm missing a certificate (because it only happens on my VPS, not on my Mac running the same version of a script), but <strong>I don't understand why I need a certificate to make a secure request to a third-party API</strong>.</p>

<p>A helpful summary (or just a point in the right direction) would be much appreciated so I can decide whether or not to disable it. My gut is that I shouldn't disable it, so I'd like to figure out how to address the problem properly.</p>
",14,1423490560,python;ssl-certificate;urllib3,True,26887,1,1588601456,https://stackoverflow.com/questions/28411546/what-does-the-python-insecurerequestwarning-really-mean
61442596,set source ipv4 address in urllib3,"<p>Im new in programming and now im trying to create python script that will periodically send http DELETE requests from a specific ipv4 address on machine. I don't understand what do I need to do to implement this feature.
I've read about class <code>urllib3.connection.HTTPConnection</code> in urllib3 docks. As far as i understood from docks <code>source_address</code> parameter need to be passed to this class to implement this, but what do i need to do with it next? I didn't find any examples about it in docks.
Thanks for help in advance!</p>

<p>EDIT:
Ok, I found a solution:</p>

<pre><code>real_create_conn = urllib3.util.connection.create_connection

def set_src_addr(address, timeout, *args, **kw):
    source_address = ('127.0.0.1', 0)
    return real_create_conn(address, timeout=timeout, source_address=source_address)

urllib3.util.connection.create_connection = set_src_addr

import requests
r = requests.get('http://httpbin.org')
</code></pre>
",0,1587911885,python;python-3.x;urllib;urllib3,False,564,1,1587974932,https://stackoverflow.com/questions/61442596/set-source-ipv4-address-in-urllib3
35541949,urllib3.urlencode googlescholar url from string,"<p>I am trying to encode a string to <code>url</code> to search google scholar, soon to realize, <code>urlencode</code> is not provided in <code>urllib3</code>.</p>

<pre><code>&gt;&gt;&gt; import urllib3
&gt;&gt;&gt; string = ""https://scholar.google.com/scholar?"" + urllib3.urlencode( {""q"":""rudra banerjee""} )
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
AttributeError: 'module' object has no attribute 'urlencode'
</code></pre>

<p>So, I checked <a href=""https://urllib3.readthedocs.org/en/latest/helpers.html#module-urllib3.request"" rel=""nofollow"">urllib3 doc</a> and found, I possibly need <code>request_encode_url</code>. But I have no experience in using that and failed.</p>

<pre><code>&gt;&gt;&gt; string = ""https://scholar.google.com/scholar?"" +""rudra banerjee""
&gt;&gt;&gt; url = urllib3.request_encode_url('POST',string)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
AttributeError: 'module' object has no attribute 'request_encode_url'
</code></pre>

<p>So, how I can encode a string to url?</p>

<p><strong>NB</strong> I don't have any particular fascination to urllib3. so, any other module will also do.</p>
",2,1456088262,python;python-3.x;urllib3,True,16535,2,1587658469,https://stackoverflow.com/questions/35541949/urllib3-urlencode-googlescholar-url-from-string
49093462,Do I need to close a urllib.request.urlopen connection in Python?,"<p>I'm working with the <code>urllib.request</code> module in Python 3.</p>

<p>My question is:</p>

<p>Do we need to close the connection if I open it using  <code>urllib.request.ulropen</code>?</p>
",5,1520152122,python;python-3.x;urllib;python-2.x;urllib3,True,4959,1,1587374631,https://stackoverflow.com/questions/49093462/do-i-need-to-close-a-urllib-request-urlopen-connection-in-python
61288559,How to download URLs that fit a pattern with urllib3,"<p>I'm trying to use the urllib3 package to download URLs that each fit a specific pattern. I'm trying to download .csv files from many URLs, each of which is of the format <a href=""https://website.com/data/something.pdf"" rel=""nofollow noreferrer"">https://website.com/data/something.pdf</a> where the ""something"" changes. How might I automate this?</p>
",0,1587210386,python;urllib3,False,76,0,1587210386,https://stackoverflow.com/questions/61288559/how-to-download-urls-that-fit-a-pattern-with-urllib3
61257078,Why am I seeing InvalidProxyConfigurationWarning when using an HTTPS proxy with urllib3?,"<p>When using a <code>urllib3.ProxyManager()</code> with an HTTPS proxy URL I'm seeing a warning called <code>InvalidProxyConfigurationWarning</code> on version 1.25.9 of <code>urllib3</code>. I didn't get this warning before, what does it mean?</p>
",2,1587060656,python;https;proxy;python-requests;urllib3,True,513,1,1587060656,https://stackoverflow.com/questions/61257078/why-am-i-seeing-invalidproxyconfigurationwarning-when-using-an-https-proxy-with
60972361,How to access proxy_url\auth in urllib3.ProxyManager,"<p>I'm working right now on project (not mine to clarify) which scraps some sites using urllib3 to make requests and some of them are under CF protection. I found some cfscrape (etc etc list of similar names) library that is a wrapper of requests.Session which may help with circumventing antibot measures of CF but there is a catch, I need proxies which are fetched by API and put into ProxyManager objects. In devenv I have no access to those proxies because of policy. Is there an easy way to get proxy url and auth from ProxyManager or do I need to add some square wheels (aka save them somewhere else as second copy) to integrate that library into project with little work as possible without degrading performance by that much? Don't really want to rewrite urllib3 usage to requests.Session</p>
",0,1585748858,python;urllib3,True,109,1,1586877983,https://stackoverflow.com/questions/60972361/how-to-access-proxy-url-auth-in-urllib3-proxymanager
61193811,How to cancel or pause a urllib request in python,"<p>So I have this program which requests a file from the web and the user can download it. I am using <strong>urllib.request and tkinter</strong> for my program. The problem is that when the user hits the 'Download' button there is no pause or cancel until the file gets downloaded and the program freezes too. I really want to create a pause or a cancel button, but I do not know how and I want to eliminate the freezing of the program. Should I use another library like 'requests'? Or should I try threading? Can someone guide me through this?
My code(BTW if you know any way to improve my program I would appreciate it a lot if you shared it with me):</p>

<pre><code>from tkinter import *
from tkinter import font as tkFont
import random
import urllib.request
import requests
from tqdm import tqdm
from tqdm.auto import tqdm


def printsth():
    print(""Yay it works! "")


def main_menu():
    root = Tk()
    # the top menu
    num = IntVar()
    # var = IntVar()
    menu = Menu(root)
    root.config(menu=menu)
    submenu = Menu(menu)
    menu.add_cascade(label=""Settings"", menu=submenu)

    def custom_op():
        custom = Tk()

        custom.mainloop()
    submenu.add_command(label=""Customization "", command=custom_op)

    def settings_op():
        set_win = Tk()

        set_win.mainloop()
    submenu.add_command(label=""Settings "", command=settings_op)
    submenu.add_separator()
    submenu.add_command(label=""Exit"", command=root.destroy)

    # the edit menu
    editmenu = Menu(menu)
    menu.add_cascade(label=""Edit"", menu=editmenu)
    editmenu.add_command(label=""Redo..."", command=printsth)

    # the tool bar
    toolbar = Frame(root, bg=""light gray"")
    insert_button = Button(toolbar, text=""Insert an image"", command=printsth)
    insert_button.pack(side=LEFT, padx=2, pady=2)
    print_button = Button(toolbar, text=""Print"", command=printsth)
    print_button.pack(side=LEFT, padx=2, pady=2)
    toolbar.pack(side=TOP, fill=X)

    # the download function
    def download_image():
        global formatname
        if num.get() == 1:
            name = random.randrange(1, 100000)
        else:
            name = str(name_entry.get())
        formatname = str(format_entry.get())
        '''if var.get() == 1:
            operator = str(url_entry.get())
            formatname = '.' + operator[-3] + operator[-2] + operator[-1]
        else:
            pass'''
        fullname = str(name) + formatname
        url = str(url_entry.get())
        fw = open('file-size.txt', 'w')
        file_size = int(requests.head(url, headers={'accept-encoding': ''}).headers['Content-Length'])
        fw.write(str(file_size))
        fw.close()
        path = str(output_entry.get()) + ""\\""
        urllib.request.urlretrieve(url, path.replace(""\\"", ""\\\\"") + fullname)

    # the status bar
    status_bar = Label(root, text=""Downloading..."", bd=1, relief=SUNKEN, anchor=W)
    status_bar.pack(side=BOTTOM, fill=X)

    # the download frame
    body_frame = Frame(root, bg=""light blue"")
    download_button = Button(body_frame, text=""Download! "", command=download_image, border=3, width=20, height=5)
    download_design = tkFont.Font(size=12, slant='italic')
    download_button['font'] = download_design
    download_button.pack(side=LEFT, pady=5, padx=5)
    body_frame.pack(side=LEFT, fill=Y)
    # the main interaction menu
    inter_frame = Frame(root)
    url_entry = Entry(inter_frame)
    label = Label(inter_frame, text=""Enter the image URL: "")
    file_format = Label(inter_frame, text=""Choose your file format: "")
    format_entry = Entry(inter_frame)
    file_name = Label(inter_frame, text=""File's name: "")
    name_entry = Entry(inter_frame)
    check_name = Checkbutton(inter_frame, text=""Give a random name"", variable=num)
    # check_format = Checkbutton(inter_frame, text=""Download with default format"", variable=var)
    output_path = Label(inter_frame, text=""Choose output path: "")
    output_entry = Entry(inter_frame)
    file_name.pack(anchor=CENTER, expand=1)
    name_entry.pack(anchor=CENTER, expand=1)
    check_name.pack(anchor=CENTER, expand=1)
    label.pack(anchor=CENTER, expand=1)
    url_entry.pack(anchor=CENTER, expand=1)
    file_format.pack(anchor=CENTER, expand=1)
    format_entry.pack(anchor=CENTER, expand=1)
    # check_format.pack(anchor=CENTER)
    output_path.pack(anchor=CENTER, expand=1)
    output_entry.pack(anchor=CENTER, expand=1)
    inter_frame.pack(expand=1)
    root.mainloop()

    # the end!


main_menu()
</code></pre>
",1,1586800853,python;tkinter;urllib;urllib3;python-3.8,True,854,1,1586871279,https://stackoverflow.com/questions/61193811/how-to-cancel-or-pause-a-urllib-request-in-python
61202258,How to create a download bar in python,"<p>So I am working on this program that is supposed to be downloading stuff from the web using <strong>tkinter and urllib.request</strong> but it is lacking a download bar(or a progress bar if that is the true word). So I found out that you can create a progress bar using <strong>TQDM</strong> but there are two problems: first of all, I didn't seem to find a way to use it in a tkinter program. Second of all, It doesn't show the download speed, you just set a limit and it fills up until it reaches that limit. I did a bit of research again and I found out that you can create a progress bar using <strong>ttk</strong> but it still has the second problem(It does not show the user's internet(download) speed and that how much of the file has been downloaded like chrome's download bar).
Can someone help me out on this?
My code(BTW if you have any suggestions how to make my program better I would appreciate it):</p>

<pre><code>from tkinter import *
from tkinter import font as tkFont
import random
import urllib.request
import requests
from tqdm import tqdm
from tqdm.auto import tqdm


def printsth():
    print(""Yay it works! "")


def main_menu():
    root = Tk()
    # the top menu
    num = IntVar()
    # var = IntVar()
    menu = Menu(root)
    root.config(menu=menu)
    submenu = Menu(menu)
    menu.add_cascade(label=""Settings"", menu=submenu)

    def custom_op():
        custom = Tk()

        custom.mainloop()
    submenu.add_command(label=""Customization "", command=custom_op)

    def settings_op():
        set_win = Tk()

        set_win.mainloop()
    submenu.add_command(label=""Settings "", command=settings_op)
    submenu.add_separator()
    submenu.add_command(label=""Exit"", command=root.destroy)

    # the edit menu
    editmenu = Menu(menu)
    menu.add_cascade(label=""Edit"", menu=editmenu)
    editmenu.add_command(label=""Redo..."", command=printsth)

    # the tool bar
    toolbar = Frame(root, bg=""light gray"")
    insert_button = Button(toolbar, text=""Insert an image"", command=printsth)
    insert_button.pack(side=LEFT, padx=2, pady=2)
    print_button = Button(toolbar, text=""Print"", command=printsth)
    print_button.pack(side=LEFT, padx=2, pady=2)
    toolbar.pack(side=TOP, fill=X)

    # the download function
    def download_image():
        global formatname
        if num.get() == 1:
            name = random.randrange(1, 100000)
        else:
            name = str(name_entry.get())
        formatname = str(format_entry.get())
        '''if var.get() == 1:
            operator = str(url_entry.get())
            formatname = '.' + operator[-3] + operator[-2] + operator[-1]
        else:
            pass'''
        fullname = str(name) + formatname
        url = str(url_entry.get())
        fw = open('file-size.txt', 'w')
        file_size = int(requests.head(url, headers={'accept-encoding': ''}).headers['Content-Length'])
        fw.write(str(file_size))
        fw.close()
        path = str(output_entry.get()) + ""\\""
        urllib.request.urlretrieve(url, path.replace(""\\"", ""\\\\"") + fullname)

    # the status bar
    status_bar = Label(root, text=""Downloading..."", bd=1, relief=SUNKEN, anchor=W)
    status_bar.pack(side=BOTTOM, fill=X)

    # the download frame
    body_frame = Frame(root, bg=""light blue"")
    download_button = Button(body_frame, text=""Download! "", command=download_image, border=3, width=20, height=5)
    download_design = tkFont.Font(size=12, slant='italic')
    download_button['font'] = download_design
    download_button.pack(side=LEFT, pady=5, padx=5)
    body_frame.pack(side=LEFT, fill=Y)
    # the main interaction menu
    inter_frame = Frame(root)
    url_entry = Entry(inter_frame)
    label = Label(inter_frame, text=""Enter the image URL: "")
    file_format = Label(inter_frame, text=""Choose your file format: "")
    format_entry = Entry(inter_frame)
    file_name = Label(inter_frame, text=""File's name: "")
    name_entry = Entry(inter_frame)
    check_name = Checkbutton(inter_frame, text=""Give a random name"", variable=num)
    # check_format = Checkbutton(inter_frame, text=""Download with default format"", variable=var)
    output_path = Label(inter_frame, text=""Choose output path: "")
    output_entry = Entry(inter_frame)
    file_name.pack(anchor=CENTER, expand=1)
    name_entry.pack(anchor=CENTER, expand=1)
    check_name.pack(anchor=CENTER, expand=1)
    label.pack(anchor=CENTER, expand=1)
    url_entry.pack(anchor=CENTER, expand=1)
    file_format.pack(anchor=CENTER, expand=1)
    format_entry.pack(anchor=CENTER, expand=1)
    # check_format.pack(anchor=CENTER)
    output_path.pack(anchor=CENTER, expand=1)
    output_entry.pack(anchor=CENTER, expand=1)
    inter_frame.pack(expand=1)
    root.mainloop()

    # the end!


main_menu()
</code></pre>
",0,1586847632,python;tkinter;urllib;urllib3;python-3.8,False,711,1,1586848369,https://stackoverflow.com/questions/61202258/how-to-create-a-download-bar-in-python
58345555,Python urllib3: close idle connection after some time,"<p>Is there a way to tell Python urllib3 to not reuse idle connections after some period of time, and instead to close them?</p>

<p>Looking in <a href=""https://urllib3.readthedocs.io/en/latest/reference/index.html#module-urllib3.connectionpool"" rel=""noreferrer"">https://urllib3.readthedocs.io/en/latest/reference/index.html#module-urllib3.connectionpool</a> doesn't seem to show anything relevant.</p>
",7,1570812837,python;http;tcp;keep-alive;urllib3,True,4201,2,1586335390,https://stackoverflow.com/questions/58345555/python-urllib3-close-idle-connection-after-some-time
58309029,SSL: CERTIFICATE_VERIFY_FAILED error when curator access elasticsearch,"<p>Im trying to setup elasticsearch-curator (version 5.6.0) to delete indices in elasticsearch (version 7.3.1).</p>

<p>Theirs versions should be compatible (<a href=""https://www.elastic.co/guide/en/elasticsearch/client/curator/current/version-compatibility.html"" rel=""nofollow noreferrer"">https://www.elastic.co/guide/en/elasticsearch/client/curator/current/version-compatibility.html</a>).</p>

<p>Elasticseach is protected by SSL using self-signed certificate, so I need to turn off certificate verification.</p>

<p>This is my curator.yml conf:</p>

<pre><code>client:
  hosts:
    - 127.0.0.1
  port: 9201
  url_prefix:
  use_ssl: True
  certificate: /opt/elastic-stack/curator/security/ca.crt
  client_cert:
  client_key:
  ssl_no_validate: True
  http_auth: curator:************
  timeout: 30
  master_only: False

logging:
  loglevel: INFO
  logfile: /var/log/elastic-stack/curator/curator.log
  logformat: default
  blacklist: ['elasticsearch', 'urllib3']
</code></pre>

<p>When I run</p>

<pre><code>curator --config /opt/elastic-stack/curator/curator.yml  /opt/elastic-stack/curator/actions.yml
</code></pre>

<p>Even though <strong>ssl_no_validate is set to True</strong>, I am getting:</p>

<pre><code>/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/curator/utils.py:53: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  return yaml.load(read_file(path))
/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/elasticsearch/connection/http_urllib3.py:175: UserWarning: Connecting to 127.0.0.1 using SSL with verify_certs=False is insecure.
  % host
Traceback (most recent call last):
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 672, in urlopen
    chunked=chunked,
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 376, in _make_request
    self._validate_conn(conn)
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 994, in _validate_conn
    conn.connect()
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/urllib3/connection.py"", line 394, in connect
    ssl_context=context,
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/urllib3/util/ssl_.py"", line 383, in ssl_wrap_socket
    return context.wrap_socket(sock)
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib64/python3.6/ssl.py"", line 407, in wrap_socket
    _context=self, _session=session)
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib64/python3.6/ssl.py"", line 814, in __init__
    self.do_handshake()
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib64/python3.6/ssl.py"", line 1068, in do_handshake
    self._sslobj.do_handshake()
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib64/python3.6/ssl.py"", line 689, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/elasticsearch/connection/http_urllib3.py"", line 217, in perform_request
    method, url, body, retries=Retry(False), headers=request_headers, **kw
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 720, in urlopen
    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/urllib3/util/retry.py"", line 376, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/urllib3/packages/six.py"", line 734, in reraise
    raise value.with_traceback(tb)
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 672, in urlopen
    chunked=chunked,
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 376, in _make_request
    self._validate_conn(conn)
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 994, in _validate_conn
    conn.connect()
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/urllib3/connection.py"", line 394, in connect
    ssl_context=context,
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/urllib3/util/ssl_.py"", line 383, in ssl_wrap_socket
    return context.wrap_socket(sock)
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib64/python3.6/ssl.py"", line 407, in wrap_socket
    _context=self, _session=session)
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib64/python3.6/ssl.py"", line 814, in __init__
    self.do_handshake()
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib64/python3.6/ssl.py"", line 1068, in do_handshake
    self._sslobj.do_handshake()
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib64/python3.6/ssl.py"", line 689, in do_handshake
    self._sslobj.do_handshake()
urllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/curator/utils.py"", line 899, in get_client
    check_version(client)
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/curator/utils.py"", line 685, in check_version
    version_number = get_version(client)
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/curator/utils.py"", line 658, in get_version
    version = client.info()['version']['number']
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/elasticsearch/client/utils.py"", line 84, in _wrapped
    return func(*args, params=params, **kwargs)
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/elasticsearch/client/__init__.py"", line 245, in info
    return self.transport.perform_request(""GET"", ""/"", params=params)
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/elasticsearch/transport.py"", line 353, in perform_request
    timeout=timeout,
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/elasticsearch/connection/http_urllib3.py"", line 226, in perform_request
    raise SSLError(""N/A"", str(e), e)
elasticsearch.exceptions.SSLError: ConnectionError([SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777)) caused by: SSLError([SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/bin//curator"", line 11, in &lt;module&gt;
    sys.exit(cli())
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/click/core.py"", line 722, in __call__
    return self.main(*args, **kwargs)
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/click/core.py"", line 697, in main
    rv = self.invoke(ctx)
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/click/core.py"", line 895, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/click/core.py"", line 535, in invoke
    return callback(*args, **kwargs)
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/curator/cli.py"", line 213, in cli
    run(config, action_file, dry_run)
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/curator/cli.py"", line 160, in run
    client = get_client(**client_args)
  File ""/app/python36/python/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/curator/utils.py"", line 906, in get_client
    'Error: {0}'.format(e)
elasticsearch.exceptions.ElasticsearchException: Unable to create client connection to Elasticsearch.  Error: ConnectionError([SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777)) caused by: SSLError([SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777))
</code></pre>

<p>I got successful response, when I run</p>

<pre><code>curl -k -u curator:******** https://127.0.0.1:9201
</code></pre>

<p>Also kibana and logstash communicates with elasticsearch properly.</p>

<p>Does anyone knows, what might be the problem or how to get more info?</p>

<p><strong>Edit 1:</strong></p>

<p>unfortunately I dont have permissions to use yum, when I installed elasticsearch-curator-5.8.1-1.x86_64.rpm to my home dir using </p>

<pre><code>cd {{ python_installation_dest }} &amp;&amp; rpm2cpio ../elasticsearch-curator-5.8.1-1.x86_64.rpm| cpio -idmB 
</code></pre>

<p>and then run curator, I got: </p>

<pre><code>Fatal Python error: initfsencoding: Unable to get the locale encoding Traceback (most recent call last): File ""/opt/python/3.7.4/lib/python3.7/encodings/__init__.py"", line 31, in &lt;module&gt; zipimport.ZipImportError: can't decompress data; zlib not available 
</code></pre>

<p>I dont use pythin 3.7.4, how can I change path to python?</p>
",0,1570640575,python;elasticsearch;elastic-stack;urllib3;elasticsearch-curator,True,2729,2,1586185033,https://stackoverflow.com/questions/58309029/ssl-certificate-verify-failed-error-when-curator-access-elasticsearch
60943717,How to make a put request using urllib3,"<p>I am trying to make a put request in AWS lambda. Requests does not comes by default in the python lambda runtime (I know it can be installed using pip and uploading a zip file but this is not what I want.), and requests from botocore is going to get retired soon, so I the only thing I have left is urllib3.</p>

<p>This is how I would do normally using the requests module:</p>

<pre><code>import requests
response_body = {'Status': 'SUCCESS',
    'Reason': 'whatever'}
requests.put(url, data=json.dumps(response_body))
</code></pre>

<p>How can I do the same using urllib3?</p>
",3,1585630717,python;http;aws-lambda;urllib3,True,5219,1,1585631928,https://stackoverflow.com/questions/60943717/how-to-make-a-put-request-using-urllib3
16339387,How can I see the actual data Requests sends over the wire?,"<p>(This is a follow-up question to <a href=""https://stackoverflow.com/questions/16337511/log-all-requests-from-the-python-requests-module"">this one</a>)</p>

<p>How can I tell <code>urllib3</code> to log the <strong>FULL</strong> request, including, but not limited to:</p>

<ul>
<li>URL</li>
<li>query parameters</li>
<li>headers</li>
<li>body</li>
<li><em>and</em> anything else which is sent inside the request (I am not sure there is anything else, but if there is something else, I also want to see it)</li>
</ul>

<p>I am having trouble connecting to LinkedIn with OAuth (a similar implementation works with Google and Facebook), and I would like to see <strong>exactly</strong> what requests are being sent. I suspect the <code>auth_token</code> is not being provided, but I need to confirm this. For that, I need <code>urllib3</code> to show the full requests, since they are over HTTPS and I can not analyze network traffic to see them (end-to-end encryption).</p>
",1,1367501550,python;oauth;python-requests;urllib3;rauth,True,618,2,1585052751,https://stackoverflow.com/questions/16339387/how-can-i-see-the-actual-data-requests-sends-over-the-wire
25189895,Error when trying to use urllib3 &amp; json to get Rotten Tomatoes data (Python),"<p>As an introduction to APIs, I'm trying to figure out how to access data in python using the Rotten Tomatoes API. This is also my first time dealing with json.</p>

<p>I'm using Python 3.4 and have confirmed that json and urllib3 have been installed.</p>

<p>Here is my code:</p>

<pre><code>import urllib3
import json

url = 'http://api.rottentomatoes.com/api/public/v1.0/lists/movies/box_office.json?limit=16&amp;country=us&amp;apikey=API-KEY';

http = urllib3.PoolManager()
request = http.request('GET', url)
print (json.load(request));
request.release_conn()
</code></pre>

<p>Here is the error I get:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Users\admnasst1\Documents\Personal\Python\RotTomTest.py"", line 16, in &lt;module&gt;
    print (str(json.load(request)));
  File ""C:\Python34\lib\json\__init__.py"", line 268, in load
    parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)
  File ""C:\Python34\lib\json\__init__.py"", line 312, in loads
    s.__class__.__name__))
TypeError: the JSON object must be str, not 'bytes'
</code></pre>

<p>Since I'm trying out so many new things (API, urllib3, json), I'm not exactly sure what's going on. I've tried doing a few other versions of the above code, and I keep getting the same error, so i think I must be missing something basic... Can any of you spot it?</p>
",8,1407437139,python;json;urllib3;rotten-tomatoes,True,11309,1,1583506657,https://stackoverflow.com/questions/25189895/error-when-trying-to-use-urllib3-json-to-get-rotten-tomatoes-data-python
60290799,Login into facebook using oauth in python?,"<p>I got the following link for a app
<code>https://www.facebook.com/dialog/oauth?client_id=SOME_NUMER_I_HAVE&amp;redirect_uri=https://someSITE.com/fblogin&amp;state=SOME_VALID_STATE</code>
I want to send the name and password, and get the cookie.</p>

<p>Does anyone know what the request would look like? 
(Or a library that takes care of this)</p>

<p>My setup, I'm trying to crawl a website, i need to login to this website to do so.</p>
",0,1582067658,python;urllib3,False,82,1,1582070373,https://stackoverflow.com/questions/60290799/login-into-facebook-using-oauth-in-python
31321854,Python warnings filter not catching InsecurePlatformWarning,"<p>Since this message </p>

<blockquote>
  <p>lib/python2.7/site-packages/requests/packages/urllib3/util/ssl_.py:90: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see <a href=""https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning"" rel=""nofollow"">https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning</a>.
    InsecurePlatformWarning</p>
</blockquote>

<p>has been flooding my logs (yes, I know it is important, I will fix the real problem eventually! I promise!), I want to add a filter to have it only show up once.
So I added this:</p>

<pre><code>warnings.simplefilter(action='once', category=InsecurePlatformWarning, append=True)
</code></pre>

<p>but it doesn't filter anything.
When I do this:</p>

<pre><code>warnings.simplefilter(action='once', append=True)
</code></pre>

<p>It does that for all the warnings, which is not what I want, but it shows the code is indeed being executed and the filter itself does work.</p>

<p>What am I doing wrong in terms of Category?
I don't want to disable warnings. I just want to make this specific warning only show up once for now.</p>

<p>Thanks!</p>
",1,1436455575,python;python-2.7;ssl;urllib3,True,365,1,1581972434,https://stackoverflow.com/questions/31321854/python-warnings-filter-not-catching-insecureplatformwarning
60223385,wget.download displays an output &quot;-1 / unknown&quot; in python,"<p>I was trying to get a file from FTP server using <code>wget</code>.</p>

<p>So I wrote a following code:</p>

<pre><code>import wget
filename = wget.download('ftp://un:pass@server/dir/file') #this line throws output
#fileprocessing
</code></pre>

<p>If I run this, it works as intended but it also shows this output ""-1 / unknown"".</p>

<p>Is there any way to suppress it?</p>

<p>Console:</p>

<pre><code>&gt;&gt;&gt; import wget
&gt;&gt;&gt; filename = wget.download('ftp://un:pass@server/dir/file')
-1 / unknown&gt;&gt;&gt;
</code></pre>

<p>Found Line 526 in <a href=""https://bitbucket.org/techtonik/python-wget/src/default/wget.py"" rel=""nofollow noreferrer"">wget.py</a> is displaying the output. </p>

<p>Digged even further and found the output is from Line 272 in <code>requests.py</code> under <code>urllib</code> package.</p>

<p>And the <a href=""https://docs.python.org/2/_sources/library/urllib.rst.txt"" rel=""nofollow noreferrer"">python docs</a> says,</p>

<blockquote>
  <p>The third argument may be <code>-1</code> on older FTP servers which do not
  return a file size in response to a retrieval request.</p>
</blockquote>

<p>Any way to stop displaying this output?</p>

<p>Versions:</p>

<p><a href=""https://www.python.org/downloads/release/python-372/"" rel=""nofollow noreferrer"">Python - 3.7.2</a></p>

<p><a href=""https://pypi.org/project/wget/"" rel=""nofollow noreferrer"">wget - 3.2</a> - <a href=""https://bitbucket.org/techtonik/python-wget/src/default/"" rel=""nofollow noreferrer"">homepage</a></p>

<p><code>urllib</code> is one of the built-in module in <code>Python 3.7.2</code>.</p>
",1,1581672204,python;ftp;wget;urllib3,False,391,0,1581914957,https://stackoverflow.com/questions/60223385/wget-download-displays-an-output-1-unknown-in-python
60183467,Python 3.x Downloading images from image-net code,"<p>I am using the following code (from Sentdex's OPENCV haar cascade training tutorial):</p>

<pre><code>import urllib.request
import cv2
import numpy as np
import os

def store_raw_images():
    neg_images_link = 'http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n00017222'   
    neg_image_urls = urllib.request.urlopen(neg_images_link).read().decode()
    pic_num = 1

    if not os.path.exists('neg'):
        os.makedirs('neg')

    for i in neg_image_urls.split('\n'):
        try:
            print(i)
            urllib.request.urlretrieve(i, ""neg/""+str(pic_num)+"".jpg"")
            img = cv2.imread(""neg/""+str(pic_num)+"".jpg"",cv2.IMREAD_GRAYSCALE)
            # should be larger than samples / pos pic (so we can place our image on it)
            resized_image = cv2.resize(img, (100, 100))
            cv2.imwrite(""neg/""+str(pic_num)+"".jpg"",resized_image)
            pic_num += 1

        except Exception as e:
            print(str(e))  
</code></pre>

<p>To download a bunch of images from IMAGENET so that I can train the Haar classifier. It basically should go to the link (which I had to change as it was broken) and download every picture from that link in order.</p>

<p>It doesn't run and doesn't give me an error (library-related errors on an ubuntu server and it tries installing different versions of Python on Windows 10).
I believe that I have the correct libraries on my windows - here's my current pip freeze output</p>

<pre><code>numpy==1.18.1
opencv-python==4.2.0.32
urllib3==1.25.8
</code></pre>
",0,1581493757,python;python-3.x;urllib2;urllib3,False,221,0,1581497131,https://stackoverflow.com/questions/60183467/python-3-x-downloading-images-from-image-net-code
54172666,AttributeError: module &#39;urllib3&#39; has no attribute &#39;urlopen&#39; in python,"<p>I am trying to send temperature data over onto one of my website currently online. This code consists of measuring the temperature through a sensor(ds18b20), sending that data onto a mysql databse entitled temp_pi and specifically onto a table intitled TAB_CLASSROOM and lastly sending that data onto a webpage of mine. Everything in this code runs except for the <code>sendDataToServer()</code> part. I specify the error right before this particular line. I have the PHP set up on my website for this to work.</p>

<pre><code>import os
import glob
import time
import MySQLdb
import datetime
import mysql.connector
from mysql.connector import Error

#define db and cur

db = MySQLdb.connect(host = ""127.0.0.1"", user = ""root"", passwd = ""xB7O4fXmuMpF6M0u"", db = ""temp_pi"")
cur = db.cursor()

#connection to the database
try:
    connection = mysql.connector.connect(host='127.0.0.1',
                             database='temp_pi',
                             user='root',
                             password='xB7O4fXmuMpF6M0u')

    if connection.is_connected():
       db_Info = connection.get_server_info()
       print(""Connected to MySQL database... MySQL Server version on "",db_Info)
       cursor = connection.cursor()
       cursor.execute(""select database();"")
       record = cursor.fetchone()
       print (""Your connected to - "", record)

except Error as e :
    print (""Error while connecting to MySQL"", e)

#obtaining the temperature through the ds18b20 sensor            
os.system('modprobe w1-gpio')
os.system('modprobe w1-therm')

base_dir = '/sys/bus/w1/devices/'
device_folder = glob.glob(base_dir + '28*')[0]
device_file = device_folder + '/w1_slave'

def read_temp_raw():
    f = open(device_file, 'r')
    lines = f.readlines()
    f.close()
    return lines

def read_temp():
    lines = read_temp_raw()
    while lines[0].strip()[-3:] != 'YES':
        time.sleep(0.2)
        lines = read_temp_raw()
    equals_pos = lines[1].find('t=')
    if equals_pos != -1:
        temp_string = lines[1][equals_pos+2:]
        temp_c = float(temp_string) / 1000.0
        temp_f = temp_c * 9.0 / 5.0 + 32.0
        return temp_c  
#Defining sendDataToServer() and trying to send this data towards my website
def sendDataToServer():
    global temperature

    threading.Timer(600,sendDataToServer).start()
    print(""Mesuring..."")
    read_temp()
    temperature = read_temp()
    print(temperature)
    temp= read_temp()
    urllib3.urlopen(""http://francoouesttemp.tech/weather/add_data.php?temp=""+temp).read()
#insertion of data into the mysql database
while True:
        print(""putting temperature data into temp_pi database"")
        i = datetime.datetime.now()
        year = str(i.year)
        month = str(i.month)
        day = str(i.day)
        date = day + ""-"" + month + ""-"" + year

        hour = str(i.hour)
        minute = str(i.minute)
        second = str(i.second)
        timestr = hour + "":"" + minute + "":"" + second

        valT = str(read_temp())

        try:
            cur.execute(""""""INSERT INTO TAB_CLASSROOM(temp_c,T_Date,T_Time) VALUES(%s,%s,%s)"""""",(valT,i,timestr))
            db.commit()
        except:
            db.rollback()

        time.sleep(5)

        #this is the part where my code tells me : NameError : name 'urllib3' is not defined ----- I want this part of the code to send the temperature, date and time over to my website.     
        sendDataToServer()

cur.close()  
db.close()
</code></pre>
",3,1547409495,python;urllib;urllib3,True,16570,2,1580563862,https://stackoverflow.com/questions/54172666/attributeerror-module-urllib3-has-no-attribute-urlopen-in-python
59919550,How to authorize proxy in urllib3?,"<p>I need to use urllib3 with proxy which requires authorization</p>

<pre><code>http = urllib3.ProxyManager('http://host:port', headers={ 'User-Agent': UA_CHROME} )
</code></pre>

<p>I have tried to pass proxy headers with login and password like this </p>

<pre><code>proxy_header={'login': login, 'password': pass}
</code></pre>

<p>and I used options in these answers <a href=""https://stackoverflow.com/questions/31151615/how-to-handle-proxies-in-urllib3"">How to handle proxies in urllib3</a> but it did not help.
How can I authorize my proxy in urllib3?</p>
",1,1580050896,python;python-3.x;urllib;urllib3,False,305,0,1580050896,https://stackoverflow.com/questions/59919550/how-to-authorize-proxy-in-urllib3
57611480,How to enable weak Cipher in python 3.7.4 with OpenSSL 1.1.1c?,"<p>The Server only supports weak ciphers. How can i connect  to the server via python?</p>

<p>The server has the following settings: (used sslyze 2-1-3)</p>

<ul>
<li>Certificate Information:
Content</li>
</ul>

<p>Signature Algorithm:               sha1</p>

<p>Public Key Algorithm:              RSA</p>

<p>Key Size:                          2048</p>

<p>Exponent:                          65537 (0x10001)</p>

<p>DNS Subject Alternative Names:     []</p>

<pre><code> Trust
   Hostname Validation:               OK - Certificate matches
   Android CA Store (9.0.0_r9):       FAILED - Certificate is NOT Trusted: self signed certificate in certificate chain
   Apple CA Store (iOS 12, macOS 10.14, watchOS 5, and tvOS 12):FAILED - Certificate is NOT Trusted: self signed certificate in certificate chain
   Java CA Store (jdk-12.0.1):        FAILED - Certificate is NOT Trusted: self signed certificate in certificate chain
   Mozilla CA Store (2019-03-14):     FAILED - Certificate is NOT Trusted: self signed certificate in certificate chain
   Windows CA Store (2019-05-27):     FAILED - Certificate is NOT Trusted: self signed certificate in certificate chain
   Symantec 2018 Deprecation:         OK - Not a Symantec-issued certificate

   Verified Chain:                    ERROR - Could not build verified chain(certificate untrusted?)
   Received Chain Contains Anchor:    ERROR - Could not build verified chain(certificate untrusted?)
   Received Chain Order:              OK - Order is valid
   Verified Chain contains SHA1:      ERROR - Could not build verified chain (certificate untrusted?)

 Extensions
   OCSP Must-Staple:                  NOT SUPPORTED - Extension not found
   Certificate Transparency:          NOT SUPPORTED - Extension not found

   OCSP Stapling
                                      NOT SUPPORTED - Server did not send back an OCSP response
</code></pre>

<ul>
<li>TLS 1.2 Session Resumption Support:</li>
</ul>

<p>With Session IDs:                  OK - Supported (5 successful, 0 failed, 0 errors, 5 total attempts).</p>

<p>With TLS Tickets:                  NOT SUPPORTED - TLS ticket not assigned</p>

<ul>
<li>SSLV3 Cipher Suites:</li>
</ul>

<p>Forward Secrecy                    OK - Supported
       RC4                                INSECURE - Supported</p>

<pre><code>Preferred:
    None - Server followed client cipher suite preference.

 Accepted:
    TLS_RSA_WITH_RC4_128_SHA                                         128 bits
    TLS_RSA_WITH_RC4_128_MD5                                         128 bits
    TLS_RSA_WITH_DES_CBC_SHA                                         56 bits
    TLS_RSA_WITH_3DES_EDE_CBC_SHA                                    112 bits
    TLS_RSA_EXPORT_WITH_RC4_40_MD5                                   40 bits
    TLS_RSA_EXPORT_WITH_DES40_CBC_SHA                                40 bits
    TLS_DHE_RSA_WITH_DES_CBC_SHA                                     56 bits
    TLS_DHE_RSA_WITH_3DES_EDE_CBC_SHA                                112 bits
</code></pre>

<ul>
<li>TLSV1 Cipher Suites:</li>
</ul>

<p>Forward Secrecy                    OK - Supported
       RC4                                INSECURE - Supported</p>

<pre><code> Preferred:
    None - Server followed client cipher suite preference.

 Accepted:
    TLS_RSA_WITH_RC4_128_SHA                                         128 bits
    TLS_RSA_WITH_RC4_128_MD5                                         128 bits
    TLS_RSA_WITH_DES_CBC_SHA                                         56 bits
    TLS_RSA_WITH_AES_256_CBC_SHA                                     256 bits
    TLS_RSA_WITH_AES_128_CBC_SHA                                     128 bits
    TLS_RSA_WITH_3DES_EDE_CBC_SHA                                    112 bits
    TLS_RSA_EXPORT_WITH_RC4_40_MD5                                   40 bits
    TLS_RSA_EXPORT_WITH_DES40_CBC_SHA                                40 bits
    TLS_DHE_RSA_WITH_DES_CBC_SHA                                     56 bits
    TLS_DHE_RSA_WITH_3DES_EDE_CBC_SHA                                112 bits
</code></pre>

<p>My Python code:</p>

<pre><code>class noSSLVerification(HttpAuthenticated):
    def u2handlers(self):
        # use handlers from superclass
        handlers = HttpAuthenticated.u2handlers(self)

        ctx = ssl._create_unverified_context()
        ctx.options &amp;= ~ssl.OP_ALL
        ctx.options &amp;= ~ssl.OP_NO_SSLv3
        ctx.options &amp;= ~ssl.OP_NO_COMPRESSION
        ctx.options &amp;= ~ssl.OP_CIPHER_SERVER_PREFERENCE

        # https://www.openssl.org/docs/manmaster/man1/ciphers.html#CIPHER-LIST-FORMAT
        # https://www.mkssoftware.com/docs/man1/openssl_ciphers.1.asp

        cipher = ""RC4-MD5:RC4-SHA:DES-CBC-SHA:DES-CBC3-SHA:EXP-RC4-MD5:EXP-DES-CBC-SHA:ADH-DES-CBC3-SHA:TLSv1.0:SSLv3""
        ctx.set_ciphers(cipher)

        handlers.append(HTTPSHandler(context=ctx))
        return handlers


url = ""https://...""
transport = noSSLVerification()
client = Client(url, transport=transport)
</code></pre>
",2,1566483620,python;python-3.x;encryption;urllib3,True,2404,1,1579363515,https://stackoverflow.com/questions/57611480/how-to-enable-weak-cipher-in-python-3-7-4-with-openssl-1-1-1c
58131573,urllib3.ProxyManager is giving authentication error,"<p>I am trying send a https request using urllib3.ProxyManager.
My code looks something like this</p>

<pre><code>default_headers = urllib3.util.make_headers(proxy_basic_auth='user:passwd')
http = urllib3.ProxyManager(proxyUrl, headers=default_headers, ca_certs=certifi.where())
http.request('GET', url)
</code></pre>

<p>I am getting below error -</p>

<pre><code>MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='clinicaltrials.gov', port=443): Max retries exceeded with url: /ct2/results?term=Lilly&amp;displayxml=true&amp;count=5000 (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 407 Proxy Authentication Required')))
</code></pre>

<p>Thanks,
Yatrik</p>
",1,1569576078,python;proxy;urllib3,True,2060,1,1578678608,https://stackoverflow.com/questions/58131573/urllib3-proxymanager-is-giving-authentication-error
59532239,Error when fetching information from API on phone,"<p>I'm trying to build my kivy app. It works good on my computer, but when it comes to phone this problem with urllib appears: </p>

<pre><code>    def Anime_Search(self):

        request = Request('https://kitsu.io/api/edge', headers={'User-Agent': 'Mozilla/5.0'})

        headers = {
            'Accept': 'application/vnd.api+json',
            'Content-Type': 'application/vnd.api+json'
        }

        url = 'https://kitsu.io/api/edge/anime?filter[text]=' + str(anime2)

        request = requests.request(""GET"", url=url, headers=headers)
        anime = request.json()

        poster = anime['data'][0]['attributes']['posterImage']['medium']

        r = requests.get(poster, stream=True, headers={'User-agent': 'Mozilla/5.0'})
        if r.status_code == 200:
            with open(""img.jpg"", 'wb') as f:
                r.raw.decode_content = True
                shutil.copyfileobj(r.raw, f)
</code></pre>

<pre><code>requests.exceptions.ConnectionError: HTTPSConnectionPool(host='kitsu.io', port=443): Max retries exceeded with url: /api/edge/anime?filter[text]=naruto (Caused by NewConnectionError('&lt;requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0xcc4c3810&gt;: Failed to establish a new connection: [Errno 7] No address associated with hostname'))
</code></pre>

<p>I would appriciate all help.</p>
",0,1577715212,python;request;kivy;buildozer;urllib3,True,599,1,1577717609,https://stackoverflow.com/questions/59532239/error-when-fetching-information-from-api-on-phone
59520421,Can&#39;t get the source code from a page when not typing the name manually,"<p><em>(first post on stackoverflow)</em></p>

<p>I'm trying to download the source code from that page <code>""https://nyaa.crystalyx.net/search?q=Kuzu+no+Honkai""</code> using <code>urllib3</code> on <code>python 3.7.1</code>.</p>

<p>I created the following function to save source code in a file:</p>

<pre><code>def get_source_code(url : str):

    print(url,len(url))
    os.system(""pause"")

    http = urllib3.PoolManager()
    r = http.request('GET', url)
    content = str(r.data)
    #print(content)

    #Saves the source_code in a file
    source_code = open(""source_code.txt"",""w+"")
    for letter in content:
        source_code.write(letter)
    source_code.close()

    #Saves the elements of the source code in a list of element splitted by ""\n"" then deletes the initial file
    source_code = open(""source_code.txt"",""r+"")
    content = (source_code.readline()).split(""\\n"")
    source_code.close()
    #os.system(""pause"")
    os.remove(""source_code.txt"")

    #Creates a new file containing the source_code correctly displayed
    source_code = open(""source_code.txt"",""w+"")
    for element in content:
        source_code.write(element + '\n')
    source_code.close()
</code></pre>

<p>Everything works well when I call my function like this:</p>

<p><code>get_source_code(""https://nyaa.crystalyx.net/search?q=Kuzu+no+Honkai"")</code></p>

<p><strong><em>(you can check the output here <a href=""https://pastebin.com/SBumCH3b"" rel=""nofollow noreferrer"">https://pastebin.com/SBumCH3b</a>)</em></strong></p>

<p>So I tried calling my function in a more user-friendly way by using <code>input()</code></p>

<pre><code>to_download = str(input(""Enter the name of the anime you wanna download: ""))
to_download = to_download.replace("" "",""+"")
to_download = str(""https://nyaa.crystalyx.net/search?q="") + str(to_download)
get_source_code(to_download)
</code></pre>

<p>This ends up giving me a very different and uncomplete source code inside my file
<strong><em>(you can check the output here <a href=""https://pastebin.com/bq0dqeZw"" rel=""nofollow noreferrer"">https://pastebin.com/bq0dqeZw</a>)</em></strong></p>

<p>I've already checked if the two strings given to <code>get_source_code()</code> are the same and have the same lenght</p>

<p>If anyone can help me it'd be cool.
Thanks.</p>
",0,1577630888,python;html;python-3.x;urllib;urllib3,True,53,1,1577631732,https://stackoverflow.com/questions/59520421/cant-get-the-source-code-from-a-page-when-not-typing-the-name-manually
59442096,python requests or urllib3 timings,"<p>Is it possible to somehow get the timings when making POST request with python requests or urllib3 library?</p>

<p>I mean, to get how long it took to establish the connection with the server, how long it took to send data and to receive data.</p>
",0,1576995878,python;python-3.x;python-requests;urllib3,False,415,0,1576995878,https://stackoverflow.com/questions/59442096/python-requests-or-urllib3-timings
42863240,How to get round the HTTP Error 403: Forbidden with urllib.request using Python 3,"<p>Hi not every time but sometimes when trying to gain access to the LSE code I am thrown the every annoying HTTP Error 403: Forbidden message.</p>

<p>Anyone know how I can overcome this issue only using standard python modules (so sadly no beautiful soup).</p>

<pre><code>import urllib.request

url = ""http://www.londonstockexchange.com/exchange/prices-and-markets/stocks/indices/ftse-indices.html""
infile = urllib.request.urlopen(url) # Open the URL
data = infile.read().decode('ISO-8859-1') # Read the content as string decoded with ISO-8859-1

print(data) # Print the data to the screen
</code></pre>

<p>However every now and then this is the error I am shown:</p>

<pre><code>Traceback (most recent call last):
  File ""/home/ubuntu/workspace/programming_practice/Assessment/Summative/removingThe403Error.py"", line 5, in &lt;module&gt;
    webpage = urlopen(req).read().decode('ISO-8859-1')
  File ""/usr/lib/python3.4/urllib/request.py"", line 161, in urlopen
    return opener.open(url, data, timeout)
  File ""/usr/lib/python3.4/urllib/request.py"", line 469, in open
    response = meth(req, response)
  File ""/usr/lib/python3.4/urllib/request.py"", line 579, in http_response
    'http', request, response, code, msg, hdrs)
  File ""/usr/lib/python3.4/urllib/request.py"", line 507, in error
    return self._call_chain(*args)
  File ""/usr/lib/python3.4/urllib/request.py"", line 441, in _call_chain
    result = func(*args)
  File ""/usr/lib/python3.4/urllib/request.py"", line 587, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden


Process exited with code: 1
</code></pre>

<p>Link to a list of all the modules that are okay: <a href=""https://docs.python.org/3.4/py-modindex.html"" rel=""noreferrer"">https://docs.python.org/3.4/py-modindex.html</a> </p>

<p>Many thanks in advance.</p>
",6,1489769998,python;python-3.x;urllib;http-status-code-403;urllib3,True,16860,2,1576494333,https://stackoverflow.com/questions/42863240/how-to-get-round-the-http-error-403-forbidden-with-urllib-request-using-python
59234952,No module named urllib3.exceptions,"<p>New to programming here. I was trying to follow a programming project i found online.</p>

<p>(For those that are curious, the link is here: <a href=""https://www.youtube.com/watch?v=fkVBAcvbrjU"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=fkVBAcvbrjU</a>)</p>

<p>I tried to run python smartmirror.py </p>

<p>However, when I run that command, i get this thrown back at me: </p>

<pre><code>Traceback (most recent call last):
  File ""smartmirror.py"", line 9, in &lt;module&gt;
    import requests
  File ""/home/pi/Smart-Mirror/requests/__init__.py"", line 60, in &lt;module&gt;
    from .packages.urllib3.exceptions import DependencyWarning
ImportError: No module named urllib3.exceptions
</code></pre>

<p>I ran <code>pip3 freeze</code> to see if I had urllib3 installed. Turns out that I had <code>urllib3==1.25.7</code> installed. I even ran <code>pip3 install --upgrade six</code> to ensure that I had the latest version.</p>

<p>However, afte this. I ran 'python smartmirror.py' and still encountered:</p>

<pre><code>Traceback (most recent call last):
      File ""smartmirror.py"", line 9, in &lt;module&gt;
        import requests
      File ""/home/pi/Smart-Mirror/requests/__init__.py"", line 60, in &lt;module&gt;
        from .packages.urllib3.exceptions import DependencyWarning
    ImportError: No module named urllib3.exceptions
</code></pre>

<p><strong>System</strong>
Raspberry Pi 3</p>

<p>Any help with this would be extremely great. So sorry if I have not asked a well documented question.</p>

<p>Thanks!</p>
",0,1575804832,python;raspberry-pi3;urllib3,False,1812,0,1575804832,https://stackoverflow.com/questions/59234952/no-module-named-urllib3-exceptions
59138809,connection pool life cycle of py2neo.Graph: would the connections be released when the instance is no longer referenced by programmer?,"<p>Recently I found a possible http connection leak issue in my code. I received ""Connection pool is full, discarding connection"" message in my log but there is few concurrent request in my code. </p>

<p>Actually I'm creating a <code>py2neo.Graph</code> instance every time it enters a API function, and I didn't make a close behavior when it leave the function.</p>

<p>Because there is no <code>close()</code> method for <code>py2neo.Graph</code> instance and there is no statement about the life cycle of <code>py2neo.Graph</code> instance in official documentation so I used to think that the instance would dead and automatically release its resources (like http connections in the pool) when it is no longer referenced by programmer's code (for python will execute deletion when the reference count of one object reaches zero). </p>

<p>So what is the actual behavior of the instance when its reference is released and what's the correct style of managing py2neo.Graph instance?</p>
",3,1575289174,python;neo4j;py2neo;urllib3,True,580,1,1575487555,https://stackoverflow.com/questions/59138809/connection-pool-life-cycle-of-py2neo-graph-would-the-connections-be-released-wh
58961480,How override respect_retry_after_header in urllib3 from requests?,"<p>When requesting this URL <a href=""http://www.trouverlesmots.com"" rel=""nofollow noreferrer"">http://www.trouverlesmots.com</a>, this <code>header</code> is received back:</p>

<pre class=""lang-py prettyprint-override""><code>{'headers': HTTPHeaderDict({'Date': 'Wed, 20 Nov 2019 18:40:39 GMT', 'Server': 'Apache/2.4.41 (Unix)', 'X-Powered-By': 'PHP/7.1.33', 'Expires': 'Wed, 11 Jan 1984 05:00:00 GMT', 'Cache-Control': 'no-cache, must-revalidate, max-age=0', 'Retry-After': '86400', 'Vary': 'User-Agent', 'Connection': 'close', 'Transfer-Encoding': 'chunked', 'Content-Type': 'text/html; charset=UTF-8'}), 'status': 503, 'version': 11, 'reason': 'Service Temporarily Unavailable', 'strict': 0, 'decode_content': False, 'retries': Retry(total=2, connect=None, read=None, redirect=None, status=None), 'enforce_content_length': False, 'auto_close': True, '_decoder': None, '_body': None, '_fp': &lt;http.client.HTTPResponse object at 0x7f2588117940&gt;, '_original_response': &lt;http.client.HTTPResponse object at 0x7f2588117940&gt;, '_fp_bytes_read': 7482, 'msg': None, '_request_url': None, '_pool': &lt;urllib3.connectionpool.HTTPConnectionPool object at 0x7f2588117e10&gt;, '_connection': None, 'chunked': True, 'chunk_left': None, 'length_remaining': None}
</code></pre>

<p>Two parameters are implied: </p>

<ul>
<li><code>status_code: 503</code> which implies a <code>retries</code> process</li>
<li><code>retry_after: 86400</code></li>
</ul>

<p><code>retry_after</code> is set to <code>86400</code> so my <code>requests.Session()</code> is pausing for one entire day.</p>

<p>Here is the piece of code commited:</p>

<pre class=""lang-py prettyprint-override""><code>    def sleep_for_retry(self, response=None):
        retry_after = self.get_retry_after(response)
        if retry_after:
            time.sleep(retry_after)  # stops here
            return True

        return False
</code></pre>

<p>From <code>urllib3.util.retry.py:277</code>.</p>

<p><code>respect_retry_after_header</code> may be changed to do not respect the <code>retry_after</code> parameter, in the <code>__init__</code> of the <code>Retry</code> object.</p>

<pre class=""lang-py prettyprint-override""><code>    def __init__(
        self,
        total=10,
        connect=None,
        read=None,
        redirect=None,
        status=None,
        method_whitelist=DEFAULT_METHOD_WHITELIST,
        status_forcelist=None,
        backoff_factor=0,
        raise_on_redirect=True,
        raise_on_status=True,
        history=None,
        respect_retry_after_header=True,
        remove_headers_on_redirect=DEFAULT_REDIRECT_HEADERS_BLACKLIST,
    )
</code></pre>

<p>From <code>urllib3.util.retry.py:174</code>.</p>

<p>Do you know how override that <code>respect_retry_after</code> parameter, from my <code>requests.Session()</code> ?</p>
",2,1574276100,python;python-requests;urllib;urllib3,True,1760,2,1574662259,https://stackoverflow.com/questions/58961480/how-override-respect-retry-after-header-in-urllib3-from-requests
58724052,urllib3: How to get response when MaxRetryError is thrown?,"<p>I'm using the Retry module from Python <code>urllib3</code> + <code>requests</code> for a case where a 3rd party API gives sporadic errors. One issue I have is that if the retries keep failing, I get a <code>exceptions.MaxRetryError</code> and never get to see what the response was. What if there was valuable debugging data coming from the server?</p>

<p>Is there a way to still get the response in cases that would throw <code>MaxRetryError</code>? </p>

<p>Here's my code below</p>

<pre><code>from requests.packages.urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

def req_with_retry(retries=3, backoff_factor=0.5, status_forcelist=(400, 404, 500, 502, 504,), method_whitelist=frozenset(['POST', 'HEAD', 'TRACE', 'GET', 'PUT', 'OPTIONS', 'DELETE']), session=None,):
    ''' 
    this returns a session that functions like the requests module but with retries built it for certain status codes
    '''
    session = session or requests.Session()
    retry = Retry(
        total=retries,
        read=retries,
        connect=retries,
        backoff_factor=backoff_factor,
        status_forcelist=status_forcelist,
        method_whitelist=method_whitelist
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session

r = req_with_retry().get(url=""https://www.google.com"")
</code></pre>
",4,1573020651,python;python-requests;urllib3,True,3632,1,1573815839,https://stackoverflow.com/questions/58724052/urllib3-how-to-get-response-when-maxretryerror-is-thrown
58594129,How to catch http error 404 with urllib3?,"<h2>What I'm trying to do</h2>

<p>I'm requesting a file from an API. If the file does't exist, I get a 404.</p>

<h2>What I tried</h2>

<p>I'm trying to handle this, using <a href=""https://urllib3.readthedocs.io/en/latest/"" rel=""nofollow noreferrer""><code>urllib3</code></a>.</p>

<p>I found a lot of great, but outdated (~10 years old), documentation how to do this with with <a href=""https://stackoverflow.com/questions/3193060/catch-specific-http-error-in-python/30327459""><code>urllib</code> and <code>urllib2</code></a>.</p>

<p>How does this work in <a href=""https://urllib3.readthedocs.io/en/latest/user-guide.html"" rel=""nofollow noreferrer""><code>urllib3</code></a>?</p>

<p>All I found in their docs was this</p>

<pre class=""lang-py prettyprint-override""><code>try:
    http.request('GET', 'nx.example.com', retries=False)
except urllib3.exceptions.NewConnectionError:
    print('Connection failed.')
</code></pre>
",0,1572277825,python;http;urllib3,False,1732,1,1573814603,https://stackoverflow.com/questions/58594129/how-to-catch-http-error-404-with-urllib3
58707232,Decorator that does not catch exceptions properly,"<p>I've experienced quite odd behaviour from decorator I wrote. It just has to catch errors occuring when a method tries to execute other methods relying on a connection to API.</p>

<p>Here is the decorator:</p>

<pre class=""lang-py prettyprint-override""><code>from functools import wraps
from requests.exceptions import ConnectionError, HTTPError
from urllib3.exceptions import NewConnectionError, MaxRetryError
from typing import Optional, Union

def retry_on_failure(retries: Optional[int]=5, retry_interval: int=15):
    def _retry_on_failure(fn):
        @wraps(fn)
        def wrapper(*args, **kwargs):
            i = 1
            while retries is None or i &lt;= retries:
                try:
                    print(f'This is try no. {i}')
                    return fn(*args, **kwargs)
                except (NewConnectionError, ConnectionError, HTTPError, MaxRetryError, TimeoutError):
                    print(f'Failed, retry in: {retry_interval}s')
                    i += 1
                    sleep(retry_interval)
        return wrapper
    return _retry_on_failure
</code></pre>

<p>And here is supposed method:</p>

<pre class=""lang-py prettyprint-override""><code>@retry_on_failure()
def some_func():
    some_other_func_throwing_exc_when_api_not_available()

</code></pre>

<p>The problem is that I still get following exceptions in a stacktrace:</p>

<pre><code>TimeoutError: [Errno 110] Connection timed out

urllib3.exceptions.NewConnectionError: &lt;urllib3.connection.HTTPConnection object at 0x7f27588a1b00&gt;: Failed to establish a new connection: [Errno 110] Connection timed out

urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='x.x.x.x', port=XXXX): Max retries exceeded with url: /some/api/ (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f27588a1b00&gt;: Failed to establish a new connection: [Errno 110] Connection timed out'))

requests.exceptions.ConnectionError: HTTPConnectionPool(host='x.x.x.x', port=XXXX): Max retries exceeded with url: /some/api/ (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f27588a1b00&gt;: Failed to establish a new connection: [Errno 110] Connection timed out'))
</code></pre>

<p>Just out of curiosity I tried catchin <code>Exception</code> and <code>BaseException</code>, but with no luck. </p>

<p>Does anyone know what might be a problem? Perhaps it's something very specific to <code>urllib3</code>?</p>

<p>Following code, after changing decorator to catch <code>KeyError</code>, works just fine...</p>

<pre class=""lang-py prettyprint-override""><code>def f1():
    raise KeyError

@retry_on_failure()
def f2():
    f1()

f2()
</code></pre>
",0,1572942453,python;python-3.x;urllib;python-decorators;urllib3,False,583,0,1572942740,https://stackoverflow.com/questions/58707232/decorator-that-does-not-catch-exceptions-properly
58412932,Store file retrieved by HTTP GET,"<p>I have an API I can retrieve files from. I do this using <code>urllib3</code> in <code>python</code>.</p>

<p>e.g.</p>

<pre class=""lang-py prettyprint-override""><code>url = ""https://example.com/api/files/13""
r = http.request('GET', url, headers=headers)
</code></pre>

<p>where the headers dict contain information for authentication. </p>

<p>When decoding all the data in the response, get the same result as I would expect. I verified this with the response in the network information terminal of my browser:</p>

<pre class=""lang-py prettyprint-override""><code>load = json.loads(r.data.decode('utf-8'))
data = load['data']
</code></pre>

<p><code>data</code> is at this stage a long string containing characters and digits and looks, depending on the file, similar to <code>dG0seHJheSxtZWRQaG90b24uU04yMDE2LUlSMDA3LkltUmlfRGV0X3BvcyxtZWRQaG90b24uU04yMDE2LUlSMDA3LkltUmlfRGV0X3ZlbCxtZWRQaG90b24uU04yMDE2LUlSMDA3LkltUmlfRGV0X2FjYyxtZWRQaG90b24uU04yMDE2LUlSMDA3LkltUmlfUm5nX3BvcyxtZWRQaG90b24uU04yMDE2LUlSMDA3LkltUmlfUm5nX3ZlbCxtZWRQaG90b...</code>, continuing after the dots. But it is just a csv file.</p>

<p>The only challenge I face now is to store the data to the disc. What I tried so far was:</p>

<pre class=""lang-py prettyprint-override""><code>with open(dst_file, 'wb') as out_file:
    out_file.write(data)
</code></pre>

<p>Instead of the <code>outfile.write(...)</code> I also tried <code>shutil.copyfileobj(r, out_file)</code>, but does not work either.</p>

<p>I guess it depends on the representation of the data I have it present. I guess having it as a string is not really beneficial. But how to store it?</p>

<p><strong>EDIT:</strong>
One thing that maybe have to be noted: The api url does not contain the file, so the call will be redirected. I tried it with another file from the web, here i did get the desired data immediately presented, when accessing <code>&gt;&gt; r.data</code>.</p>
",2,1571227612,python;python-3.x;file;urllib3,True,376,2,1572524463,https://stackoverflow.com/questions/58412932/store-file-retrieved-by-http-get
58595813,Suppress all InsecureRequestWarnings,"<p>When I try to make a request using:</p>

<pre><code>request_arguments = {
    ""method"": method,
    ""url"": self._build_url(self.url, action),
    ""headers"": self._get_headers(**kwargs),
    ""data"": kwargs[""data""] if ""data"" in kwargs else {},
    ""verify"": False
}

response = requests.request(**request_arguments)
</code></pre>

<p>I'm getting the following warning:</p>

<pre class=""lang-none prettyprint-override""><code> tests/end_to_end/test_integration_alert.py::test_mm_put
      /anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py:858:
        InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised.
        See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
        InsecureRequestWarning)
</code></pre>

<p>I tried to suppress it in the constructor via:</p>

<pre><code>urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
</code></pre>

<p>as described here:</p>

<p><a href=""https://stackoverflow.com/questions/27981545/suppress-insecurerequestwarning-unverified-https-request-is-being-made-in-pytho"">Suppress InsecureRequestWarning: Unverified HTTPS request is being made in Python2.6</a></p>

<p>Interestingly, I still get the warning the first time the code is called, but subsequent calls are correctly suppressed. (I hit the breakpoints in the correct order)</p>

<p>How can I suppress the initial warning as well?</p>
",2,1572284250,python;suppress-warnings;urllib3,False,838,0,1572285396,https://stackoverflow.com/questions/58595813/suppress-all-insecurerequestwarnings
58558511,What is the equivalent code from urllib2 to urllib3 in python,"<p>I am new to coding and am following an online course. The example in the course uses urllib2. For some reason I can't get urllib2 but i have got urllib3. The code they have written is for urllib2 as shown below:</p>

<pre class=""lang-py prettyprint-override""><code>webRequest = urllib2.Request(urlofFilename,headers=hdr)
</code></pre>

<p>When I write this out to do the same thing with urllib3 ie:</p>

<p><code>webRequest = urllib3.Request(urlofFilename,headers=hdr)</code></p>

<p>It gives me an error stating that the urllib3 module has no attribute 'Request'.</p>

<p>How then do i write the same bit of code but for urllib3?</p>

<p>Cheer</p>
",1,1572006303,python;urllib2;urllib3,True,1115,2,1572007886,https://stackoverflow.com/questions/58558511/what-is-the-equivalent-code-from-urllib2-to-urllib3-in-python
58509329,How to fix ‘ConnectionResetErrort’ in Python?,"<p>My script has been working fine, then all of a sudden it started throwing the following messages:</p>

<pre><code>ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host
</code></pre>

<p>During handling of the above exception, another exception occurred:</p>

<pre><code>urllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))
</code></pre>

<p>Some details:</p>

<p>Chrome: 77.0.3865.90
Webdriver: 77.0.3865.40
Selenium: 3.141.0
Python: 3.6.5</p>

<p>I've tried this: 
<a href=""https://stackoverflow.com/questions/51799251/urllib3-exceptions-protocolerror-connection-aborted-error10054-an-exist"">urllib3.exceptions.ProtocolError: (&#39;Connection aborted.&#39;, error(10054, &#39;An existing connection was forcibly closed by the remote host&#39;))</a></p>

<p>def advanced_search():</p>

<pre><code># some code here

xpath_expand = '//*[@id=""navigatorPanel-xcollapsed""]'
time.sleep(2)
driver.find_element_by_xpath(xpath_expand).click() 

# some code here
</code></pre>
",1,1571764528,python;selenium-webdriver;selenium-chromedriver;urllib3,False,505,0,1571767706,https://stackoverflow.com/questions/58509329/how-to-fix-connectionreseterrort-in-python
57857726,"Broken DAG: urllib3 1.25.3 (/home/ubuntu/.local/lib/python3.7/site-packages), Requirement.parse(&#39;urllib3&lt;1.25,&gt;=1.21&#39;), {&#39;sagemaker&#39;}","<p>I have created a DAG in Airflow with SageMakerOperators and I have not been able to make them work. The title is the error that appears in the airflow GUI. For solving it, I have made the following tries:</p>

<pre><code>sudo pip3 uninstall urllib3 &amp;&amp; sudo pip3 install urllib3==1.22 
sudo pip3 install urllib3==1.22 --upgrade
sudo pip3 install urllib3==1.22 -t /home/ubuntu/.local/lib/python3.7/site-packages -upgrade
</code></pre>

<p>But I am still getting the error in the GUI. Plus, in the console of the webserver I am getting:</p>

<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/.local/lib/python3.7/site-packages/urllib3-1.22.dist-info/METADATA'
</code></pre>

<p>The thing is that if I make <code>pip3 show urllib3</code> I get the version 1.22:
<a href=""https://i.stack.imgur.com/i5y8i.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i5y8i.png"" alt=""enter image description here""></a></p>

<p>However, it says dist-packages instead of site-packages. In addition, trying to go to <code>/home/ubuntu/.local/lib/python3.7/site-packages/urllib3-1.22.dist-info/</code> for trying to solve the metadata file not found error, the directory does not exists. 
<a href=""https://i.stack.imgur.com/44H4I.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/44H4I.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/2CnJl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2CnJl.png"" alt=""enter image description here""></a></p>

<p>I am totally lost at this point. How could I solve this problem?</p>
",0,1568045843,python;airflow;amazon-sagemaker;urllib3,True,438,1,1571189751,https://stackoverflow.com/questions/57857726/broken-dag-urllib3-1-25-3-home-ubuntu-local-lib-python3-7-site-packages-re
32986626,Python requests ImportError: cannot import name HeaderParsingError,"<p>OS: Mac OS X. When I'm trying to run the code below, I get the error: </p>

<blockquote>
  <p>ImportError: cannot import name HeaderParsingError</p>
</blockquote>

<p>I've attached traceback below the code.</p>

<p>I've tried to solve this issue for 20 min now, using Google and other stackoverflow. I have tried running: </p>

<blockquote>
  <p>pip install urllib3 --upgrade</p>
</blockquote>

<p>I've also tried reinstalling the requests package.</p>

<p>It did not help.</p>

<p><strong>This seems to be an issue with my requests or urllib3 package. Has anyone had a similar issue?</strong></p>

<p>The code:</p>

<pre><code>import requests
import json


def printResponse(r):
print '{} {}\n'.format(json.dumps(r.json(), sort_keys=True, indent=4,    separators=(',', ': ')), r)


r = requests.get('http://wikitest.orcsoftware.com/rest/api/content',
             params={'title': 'new page'},
             auth=('seb', '****'))
printResponse(r)
parentPage = r.json()['results'][0]
pageData = {'type': 'comment', 'container': parentPage,
        'body': {'storage': {'value': ""&lt;p&gt;A new comment&lt;/p&gt;"", 'representation': 'storage'}}}
r =    requests.post('http://localhost:8080/confluence/rest/api/content',
              data=json.dumps(pageData),
              auth=('admin', 'admin'),
              headers=({'Content-Type': 'application/json'}))
printResponse(r)
</code></pre>

<p>This is the traceback:</p>

<pre><code>Traceback (most recent call last):
  File ""/Users/sebastian/OneDrive/orc/restAPI/createSpace.py"", line 1, in &lt;module&gt;
    import requests
  File ""/Library/Python/2.7/site-packages/requests/__init__.py"", line 61, in &lt;module&gt;
    from . import utils
  File ""/Library/Python/2.7/site-packages/requests/utils.py"", line 25, in &lt;module&gt;
    from .compat import parse_http_list as _parse_list_header
  File ""/Library/Python/2.7/site-packages/requests/compat.py"", ine 7, in &lt;module&gt;
    from .packages import charade as chardet
  File ""/Library/Python/2.7/site-packages/requests/packages/__init__.py"", line 3, in &lt;module&gt;
    from . import urllib3
  File ""/Library/Python/2.7/site-packages/requests/packages/urllib3/__init__.py"", line 16, in &lt;module&gt;
    from .connectionpool import (
  File ""/Library/Python/2.7/site-packages/requests/packages/urllib3/connectionpool.py"", line 33, in &lt;module&gt;
    from .connection import (
  File ""/Library/Python/2.7/site-packages/requests/packages/urllib3/connection.py"", line 41, in &lt;module&gt;
    from .util import (
  File ""/Library/Python/2.7/site-packages/requests/packages/urllib3/util/__init__.py"", line 4, in &lt;module&gt;
    from .response import is_fp_closed
  File ""/Library/Python/2.7/site-packages/requests/packages/urllib3/util/response.py"", line 3, in &lt;module&gt;
    from ..exceptions import HeaderParsingError
ImportError: cannot import name HeaderParsingError
</code></pre>
",7,1444204254,python;python-2.7;python-requests;urllib3,True,29033,3,1570114800,https://stackoverflow.com/questions/32986626/python-requests-importerror-cannot-import-name-headerparsingerror
58098465,HTTPS connection closed after SSL handshake with no exception,"<p>I'm using a library (sentry.io observer) that should connect to a remote server via https and upload some data. I do not control the server, but I can see that no data is uploaded. I set the urllib logger level to debug and I see two log messages</p>

<pre><code>DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): &lt;server_url&gt;:443
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (2): &lt;server_url&gt;:443
</code></pre>

<p>but no exception is thrown. I used wireshark to sniff packets and I see two SSL handshakes are executed, but the FIN packet is sent right after server finishes the handshake. Packets exchange looks like:</p>

<pre><code>&lt; - client sends message
&gt; - server sends message

&lt; TCP handshake [syn, syn ack, ack]
&lt; Client hello
&gt; Server hello, certificate, server key exchange, server hello done
&lt; Client key exchange, change cipher spec, finished
&gt; New session ticket, change cipher spec, finished
&lt; TCP connection termination [fin ack, fin ack, ack]
</code></pre>

<p>This packet exchange is done twice, as urllib tries to connect to the remote server twice. The server certificate is valid, but the connection is cancelled by client. I set the library and urllib loggers to debug, but no error messages or anything that could help me narrow the issue down appears.</p>

<p>The issue only appears when requests are done from docker (based on centos 7), but when launching the app on ubuntu host it works fine, connection is established and data is uploaded. What could be the cause of the issue?</p>
",1,1569414710,python;ssl;sentry;urllib3,False,1603,0,1569416270,https://stackoverflow.com/questions/58098465/https-connection-closed-after-ssl-handshake-with-no-exception
57867838,Python &quot;SSL: UNSUPPORTED PROTOCOL&quot; error when using urllib3 on a https site,"<p>I'm using urllib3 to get the html content of a website via a socks proxy.
Unfortunately this does not work on any website. Running this on a few sites will give me an error.</p>

<pre><code>import urllib3
from urllib3.contrib.socks import SOCKSProxyManager
proxy = SOCKSProxyManager('socks5://myProxyIP:8080')
r = proxy.request('GET', ""https://urlofwebsite"", preload_content=False)
</code></pre>

<blockquote>
  <p>urllib3.exceptions.MaxRetryError:
  SOCKSHTTPSConnectionPool(host='urlofwebsite', port=443): Max retries
  exceeded with url: /index.php (Caused by SSLError(SSLError(1, u'[SSL:
  UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:727)'),))</p>
</blockquote>

<p>I assume this might be an issue with the SSL (or TLS) version the site is using but as I am not the owner of the server I have to edit my script to handle with this.</p>

<p>Is it possible to change the setting in urllib3 to accept this connection?</p>

<p>Thanks!</p>
",2,1568107688,python;ssl;https;urllib3,False,627,0,1568107688,https://stackoverflow.com/questions/57867838/python-ssl-unsupported-protocol-error-when-using-urllib3-on-a-https-site
57830397,Suddenly not working on Ubuntu Digital Ocean: requests error for only Google,"<p>It was working fine a week ago, but all of a sudden it stopped working.</p>

<p>Steps to reproduce:</p>

<pre><code>Python 2.7.12 (default, Nov 12 2018, 14:36:49)
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import requests
&gt;&gt;&gt; requests.get('http://yahoo.com')
&lt;Response [200]&gt;
&gt;&gt;&gt; requests.get('http://google.com')
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/lib/python2.7/dist-packages/requests/api.py"", line 67, in get
    return request('get', url, params=params, **kwargs)
  File ""/usr/lib/python2.7/dist-packages/requests/api.py"", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/usr/lib/python2.7/dist-packages/requests/sessions.py"", line 480, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/lib/python2.7/dist-packages/requests/sessions.py"", line 609, in send
    history = [resp for resp in gen] if allow_redirects else []
  File ""/usr/lib/python2.7/dist-packages/requests/sessions.py"", line 211, in resolve_redirects
    **adapter_kwargs
  File ""/usr/lib/python2.7/dist-packages/requests/sessions.py"", line 588, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/lib/python2.7/dist-packages/requests/adapters.py"", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='www.google.com', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('&lt;requests.packages.urllib3.connection.HTTPConnection object at 0x7fc5076e7950&gt;: Failed to establish a new connection: [Errno -5] No address associated with hostname',))
</code></pre>

<p>Expected output:</p>

<pre><code>&lt;Response [200]&gt;
</code></pre>

<p>What is weird is that it works for other websites, but not Google</p>
",1,1567826285,python;python-2.7;ubuntu;digital-ocean;urllib3,False,292,0,1567827676,https://stackoverflow.com/questions/57830397/suddenly-not-working-on-ubuntu-digital-ocean-requests-error-for-only-google
57795877,Statically built python code cannot resolve DNS names,"<p>I'm trying to deploy code to a server to which I don't have root access. So the solution I was thinking was to deploy using <a href=""https://www.pyinstaller.org/"" rel=""nofollow noreferrer"">pyinstaller</a> and <a href=""https://github.com/JonathonReinhart/staticx"" rel=""nofollow noreferrer"">staticx</a>.</p>

<p>My code runs in python 3.7, in a nutshell, does something like:</p>

<pre class=""lang-py prettyprint-override""><code>import requests
response = requests.get('http://example.com/api/action')
# Do something with the response
</code></pre>

<p>When I run it in my environment, even after ""building"" with <code>pyinstaller</code> and <code>staticx</code>, it works flawlessly.</p>

<p>However, when I try to deploy it (the server is running Red Hat Enterprise Linux Server release 7.4, which does not have python 3), I get:</p>

<pre><code>Traceback (most recent call last):
  File ""site-packages/urllib3/connection.py"", line 160, in _new_conn
  File ""site-packages/urllib3/util/connection.py"", line 57, in create_connection
  File ""socket.py"", line 748, in getaddrinfo
socket.gaierror: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""site-packages/urllib3/connectionpool.py"", line 603, in urlopen
  File ""site-packages/urllib3/connectionpool.py"", line 355, in _make_request
  File ""http/client.py"", line 1229, in request
  File ""http/client.py"", line 1275, in _send_request
  File ""http/client.py"", line 1224, in endheaders
  File ""http/client.py"", line 1016, in _send_output
  File ""http/client.py"", line 956, in send
  File ""site-packages/urllib3/connection.py"", line 183, in connect
  File ""site-packages/urllib3/connection.py"", line 169, in _new_conn
urllib3.exceptions.NewConnectionError: &lt;urllib3.connection.HTTPConnection object at 0x7f912ca96f28&gt;: Failed to establish a new connection: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""site-packages/requests/adapters.py"", line 449, in send
  File ""site-packages/urllib3/connectionpool.py"", line 641, in urlopen
  File ""site-packages/urllib3/util/retry.py"", line 399, in increment
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='someserver.com', port=80): Max retries exceeded with url: /api/action (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f912ca96f28&gt;: Failed to establish a new connection: [Errno -2] Name or service not known'))
</code></pre>

<p>Everything works if I use IP addresses instead of domains. Unfortunately, it's not a possibility because I have to hit an proxy that uses hosts. Also IP addresses may change in the future.</p>

<p>The server resolves names perfectly fine if I use <code>nslookup</code> or <code>ping</code>.</p>

<p>Any ideas why this could be happening? Any alternatives on how I could deploy my code in a way that it works in the remote server may also be a valid answer. Note however:</p>

<ul>
<li>I have no root access and requesting the installation of python3, any libs or any other package such as docker, etc would most likely be rejected by my customer</li>
<li>The server can resolve external names, such as <code>google.com</code> but does not have access to the outer internet (the code is ment to work on internal servers, with internal DNS such as <code>someserver.mycustomer.example</code>)</li>
</ul>
",5,1567632887,python;python-3.x;dns;urllib;urllib3,False,892,0,1567637543,https://stackoverflow.com/questions/57795877/statically-built-python-code-cannot-resolve-dns-names
57754958,which R package provides functionality similar to pythons urllib,"<p>My task is to port a snipped of python code which uses python <code>urllib</code> to R.
This is the entire python code:</p>

<pre><code>import urllib.parse
import urllib.request

url = ""https://www.uniprot.org/uploadlists/""

params = {
  ""from"": ""ACC+ID"",
  ""to"": ""ENSEMBL_ID"",
  ""format"": ""tab"",
  ""query"": ""P40925 P40926 O43175 Q9UM73 P97793""
}

data = urllib.parse.urlencode(params)
data = data.encode(""utf-8"")
req = urllib.request.Request(url, data)
with urllib.request.urlopen(req) as f:
  response = f.read()
print(response.decode(""utf-8""))
</code></pre>

<p>I am not to keen to use <code>paste</code> and <code>paste0</code> to build url's manually in R.
What I did find is the R <code>urltools</code> package.
But it does not help me with url-encoding the data as <code>urllib.parse.urlencode</code> does.</p>

<p>What would be the R package to use to re-implement the python code? Or should I use <code>reticulate</code>?</p>

<p>Thank you</p>
",2,1567418428,python;r;url;response;urllib3,True,844,1,1567420215,https://stackoverflow.com/questions/57754958/which-r-package-provides-functionality-similar-to-pythons-urllib
47243024,pandas read_csv on dynamic URL gives EmptyDataError: No columns to parse from file,"<p>I am learning Python from 'Python for Finance' and the packages used are outdated. Please help me out with the following error: 
It shows EmptyDataError: No columns to parse from file.         </p>

<pre><code>import pandas as pd
url1 = 'http://hopey.netfonds.no/posdump.php?'
url2 = 'date=%s%s%s&amp;paper=AAPL.O&amp;csv_format=csv'
url = url1 + url2

year = '2017'
month = '11'
days = ['9']

AAPL = pd.DataFrame()
for day in days:
    AAPL = AAPL.append(pd.read_csv(url % (year, month, day),
                                   index_col=0, header=0, parse_dates=True))

AAPL.columns = ['bid', 'bdepth', 'bdeptht','offer', 'odepth', 'odeptht']
</code></pre>

<p>Error:</p>

<pre><code>EmptyDataError: No columns to parse from file
</code></pre>
",0,1510436122,python;python-3.x;pandas;urllib3;pandas-datareader,True,1844,1,1567114377,https://stackoverflow.com/questions/47243024/pandas-read-csv-on-dynamic-url-gives-emptydataerror-no-columns-to-parse-from-fi
57670089,Why is python requests not terminating and why are these seperate logs printed?,"<p>I am running a job which makes many requests to retrieve data from an API. In order to make the requests, I am using the requests module and an iteration over this code:</p>

<pre class=""lang-py prettyprint-override""><code>logger.debug(""Some log message"")
response = requests.get(
    url=self._url,
    headers=self.headers,
    auth=self.auth,
)
logger.debug(""Some other log message"")
</code></pre>

<p>This usually produces the following logs:</p>

<pre><code>[...] Some log message
[2019-08-27 03:00:57,201 - DEBUG - connectionpool.py:393] https://my.url.com:port ""GET /some/important/endpoint?$skiptoken='12345' HTTP/1.1"" 401 0
[2019-08-27 03:00:57,601 - DEBUG - connectionpool.py:393] https://my.url.com:port ""GET /some/important/endpoint?$skiptoken='12345' HTTP/1.1"" 200 951999
[...] Some other log message
</code></pre>

<p>In very rare occasions however, the job never terminates and in the logs it says:</p>

<pre><code>[...] Some log message
[2019-08-27 03:00:57,201 - DEBUG - connectionpool.py:393] https://my.url.com:port ""GET /some/important/endpoint?$skiptoken='12345' HTTP/1.1"" 401 0
</code></pre>

<p>It never printed the remaining log messages and never returned. I am not able to reproduce the issue. I made the request which never returned manually but it gave me the desired response.</p>

<p><strong>Questions:</strong></p>

<ol>
<li><p>Why does <code>urllib3</code> always print a log with a status code 401 before printing a log with status code 200? Is this always the case or caused by an issue with the authentication or with the API server?</p></li>
<li><p>In the rare case of the second log snipped, is my assumption correct, that the application is stuck making a request which never returns? Or:</p>

<p>a) Could the <code>requests.get</code> throw an exception which results in the other log statements to never be printed and then is ""magically"" get caught somewhere in my code?</p>

<p>b) Is there a different possibility which I have not realised?</p></li>
</ol>

<hr>

<p><em>Additional Information:</em></p>

<ul>
<li><p>Python 2.7.13 (<em>We are already in the middle of upgrading to Python3,    but this needs to be solved before that is completed</em>)</p></li>
<li><p>requests 2.21.0</p></li>
<li><p>urllib3 1.24.3</p></li>
<li><p>auth is passed a <code>requests.auth.HTTPDigestAuth(username, password)</code></p></li>
<li><p>My code has no try/except block which is why I wrote ""magically"" in Question 2.a. This is because we would prefer the job to Fail ""loudly"".</p></li>
<li><p>I am iterating over a generator yielding urls in order to make multiple requests</p></li>
<li><p>The job is run by
Jenkins 2.95 on a schedule</p></li>
<li><p>When everything runs successfully it makes around 300 requests in about 5min</p></li>
<li><p>I am running two python scripts both running the same code but against different endpoints in one job but in parallel</p></li>
</ul>

<hr>

<p><strong>Update</strong></p>

<p>Answer to <strong>Q1</strong>:</p>

<p>This seems to be expected behaviour for HTTP Digest Auth.
See this <a href=""https://github.com/psf/requests/issues/5176"" rel=""nofollow noreferrer"">github issue</a> and <a href=""https://en.wikipedia.org/wiki/Digest_access_authentication"" rel=""nofollow noreferrer"">Wikipedia</a>.</p>
",0,1566892622,python;python-2.7;python-requests;urllib3,True,727,2,1567008988,https://stackoverflow.com/questions/57670089/why-is-python-requests-not-terminating-and-why-are-these-seperate-logs-printed
57638448,Failed to establish a new connection: [Errno 11001] getaddrinfo failed,"<p>The scraper should be downloading documents from a list of urls which was scraped earlier.</p>

<p>I don't have issue running it when I'm using my office network but when I run the scraper at home using my home's wifi, the scraper keeps giving the same error.</p>

<p>I tried some suggestions from another post here - by giving a timeout variable.
<a href=""https://stackoverflow.com/questions/46188295/python-httpconnectionpool-failed-to-establish-a-new-connection-errno-11004-ge"">Python HTTPConnectionPool Failed to establish a new connection: [Errno 11004] getaddrinfo failed</a></p>

<p>However it doesn't solve the problem.</p>

<p>I would appreciate some explanation along with a solution. I'm not well-versed with network issue. Thank you</p>

<pre><code>import requests
from bs4 import BeautifulSoup
import re
import pandas as pd
import os

doc_urls= [
'http://www.ha.org.hk/haho/ho/bssd/19d079Pa.htm',
'http://www.ha.org.hk/haho/ho/bssd/18S065Pg.htm',
'http://www.ha.org.hk/haho/ho/bssd/19d080Pa.htm',
'http://www.ha.org.hk/haho/ho/bssd/NTECT6AT003Pa.htm',
'http://www.ha.org.hk/haho/ho/bssd/19D093Pa.htm',
'http://www.ha.org.hk/haho/ho/bssd/19d098Pa.htm',
'http://www.ha.org.hk/haho/ho/bssd/19d103Pa.htm',
'http://www.ha.org.hk/haho/ho/bssd/18G044Pe.htm',
'http://www.ha.org.hk/haho/ho/bssd/19d104Pa.htm',
]

base_url = ""http://www.ha.org.hk""

for doc in doc_urls:
    with requests.Session() as session:
        r = session.get(doc)
        # get all documents links
        docs = BeautifulSoup(r.text, ""html.parser"").select(""a[href]"")
        print('Visiting:',doc)
        for doc in docs:
            href = doc.attrs[""href""]
            name = doc.text
            print(f""&gt;&gt;&gt; Downloading file name: {name}, href: {href}"")
            # open document page
            r = session.get(href)
            # get file path
            file_path = re.search(""(?&lt;=window.open\\(')(.*)(?=',)"", r.text).group(0)
            file_name = file_path.split(""/"")[-1]
            # get file and save
            r = session.get(f""{base_url}/{file_path}"")
            with open('C:\\Users\\Desktop\\tender_documents\\' + file_name, 'wb') as f:
                f.write(r.content)
</code></pre>

<p>As mentioned, the scraper runs well on my office's network. It fails when I tried using my own wifi, and also my mother in law's wifi. My mother in law and I use the same wifi provider - if that helps.</p>
",1,1566653657,python;network-programming;urllib;httpconnection;urllib3,False,3775,0,1566653657,https://stackoverflow.com/questions/57638448/failed-to-establish-a-new-connection-errno-11001-getaddrinfo-failed
57566853,Python urllib request and urlopen time out for stats.nba.com,"<p>In my code I have the snippet below where x is the iterator for an external loop with season dates as the values and here MYPROPERUSERAGENT is a stand-in for my actual user agent:</p>

<pre><code>for i in teamID:        
    url = 'https://stats.nba.com/stats/commonteamroster?Season=' + x.split('-')[0] +'&amp;TeamID=16106127' + str(i)
    print(url)

    req = urllib.request.Request(
        url, 
        data=None, 
        headers={
            'User-Agent': 'MYPROPERUSERAGENT'
        }
    )

    f = urllib.request.urlopen(req)
    print(f.read().decode('utf-8'))
</code></pre>

<p>This is directly from the docs but I'm still getting the error...</p>

<pre><code>TimeoutError:[WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond
</code></pre>

<p>An example url, the first one I'm using is <a href=""https://stats.nba.com/stats/commonteamroster?Season=1997&amp;TeamID=1610612737"" rel=""nofollow noreferrer"">this</a>. I've tried countless other ways of doing this too and usually end up just doing it in R because of how much of a hassle it is, but if someone could provide a solution it would be much appreciated.</p>

<pre><code>https://stats.nba.com/stats/commonteamroster?Season=1997&amp;TeamID=1610612737
</code></pre>
",0,1566276418,python;web-scraping;urllib;urllib3,False,106,0,1566277758,https://stackoverflow.com/questions/57566853/python-urllib-request-and-urlopen-time-out-for-stats-nba-com
57450667,GET request with header attribute set to None raises TypeError with urllib3,"<p>How do you pass <code>None</code> as the value of a parameter in a GET request header with urllib3 without raising a TypeError?</p>

<p>I have tried using <code>requests</code> and it works there, but the package lacks some features of <code>urllib3</code> regarding proxy management, pooling, and simplicity, which sadly are necessities for this project.</p>

<p>My code currently looks like this:</p>

<pre><code>headers = {}
headers.setdefault(""Attribute"", None)

r = urllib3.PoolManager().request(""GET"", url=""someurl.xyz"", headers=headers)
</code></pre>

<p>This raises a <code>TypeError: expected string or bytes-like object</code>, because None seems to be an illegal header value.</p>

<p>Here's the relevant part of the log:</p>

<pre><code>  File ""/home/user/PycharmProjects/Project/src/main/classes/oof.py"", line 54, in set_fingerprint
    r = self.session.request('GET', url=experiment_url, headers=headers)
  File ""/home/user/anaconda3/lib/python3.7/site-packages/urllib3/request.py"", line 68, in request
    **urlopen_kw)
  File ""/home/user/anaconda3/lib/python3.7/site-packages/urllib3/request.py"", line 89, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
  File ""/home/user/anaconda3/lib/python3.7/site-packages/urllib3/poolmanager.py"", line 324, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File ""/home/user/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 600, in urlopen
    chunked=chunked)
  File ""/home/user/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File ""/home/user/anaconda3/lib/python3.7/http/client.py"", line 1229, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File ""/home/user/anaconda3/lib/python3.7/http/client.py"", line 1270, in _send_request
    self.putheader(hdr, value)
  File ""/home/user/anaconda3/lib/python3.7/http/client.py"", line 1206, in putheader
    if _is_illegal_header_value(values[i]):
TypeError: expected string or bytes-like object
</code></pre>

<p>Is there a way to circumvent this error with <code>urllib3</code>?</p>
",0,1565532656,python;python-requests;urllib3,True,653,1,1565532987,https://stackoverflow.com/questions/57450667/get-request-with-header-attribute-set-to-none-raises-typeerror-with-urllib3
57381498,Need help converting something from urllib2 to urllib3,"<p>I am trying to adapt the functions below from urllib2 to urllib3.  The problem I am running into is that urllib3 does not have the openurl attribute.  How would I rewrite these functions?</p>

<p>Thank you in advance.</p>

<p>def get_season_URLs(year) :</p>

<pre><code>if not year in VALID_YEARS :
    print (str(year) + "" is an invalid year"")
    return
base_url_schedule_1 = BASE_URL + ""/leagues/NBA_"" + str(year) + ""_games-""
url_extensions = YEAR_ONE_MONTHS + YEAR_TWO_MONTHS    
urls = []

for extension in url_extensions :
    schedule_url = get_schedule_URL(year, extension)
    doc = BeautifulSoup(urllib2.urlopen(schedule_url).read(), ""html.parser"")
    schedule_table = doc.find_all(""tbody"")[0]
    box_score_els = schedule_table.find_all(attrs={""data-stat"": ""box_score_text""})

    for el in box_score_els :
        urls.append(BASE_URL + el.find(""a"").get('href'))
return urls
</code></pre>

<p>def scrape_game(url, f) :</p>

<pre><code>html = urllib2.urlopen(url).read()
doc = BeautifulSoup(html, ""html.parser"")
if not len(doc.find_all(attrs={""data-label"":""All Games in Series""})) == 0 :
    return False
line = """"```
</code></pre>
",0,1565113676,python;urllib;urllib2;urllib3,False,318,1,1565442231,https://stackoverflow.com/questions/57381498/need-help-converting-something-from-urllib2-to-urllib3
57333549,Scraping a secure website requiring clicks on javascript links,"<p>I have a daily task at work to download some files from internal company website. The site requires a login. But the main url is something like:   </p>

<pre><code>https://abcd.com
</code></pre>

<p>But when I open that in the browser, it redirects to something like:</p>

<pre><code>https://abcdGW/ln-eng.aspx?lang=eng&amp;lnid=e69d5d-xxx-xxx-1111cef&amp;regl=en-US
</code></pre>

<p>My task normally is to open this site, login, click some links back and forth and download some files. This takes me 10 minutes everyday. But I wanna automate this using python. Using my basic knowledge I have written below code:</p>

<pre><code>import urllib3
from bs4 import BeautifulSoup
import requests
import http

url = ""https://abcd.com""
redirectURL = requests.get(url).url

jar = http.cookiejar.CookieJar(policy=None)
http = urllib3.PoolManager()
acc_pwd = {'datasouce': 'Data1', 'user':'xxxx', 'password':'xxxx'}

response = http.request('GET', redirectURL)
soup = BeautifulSoup(response.data)
r = requests.get(redirectURL, cookies=jar)
r = requests.post(redirectURL, cookies=jar, data=acc_pwd)

print (""RData %s"" % r.text)
</code></pre>

<p>This shows that I am able to successfully login. The next step is something where i am stuck. On the page after login I have some links on left side, one of those I need to click. When I inspect them in Chrome, I see them as:</p>

<pre><code>href=""javascript:__doPostBack('myAppControl$menu_itm_proj11','')""&gt;&lt;div class=""menu-cell""&gt;
    &lt;img class=""menu-image"" src=""images/LiteMenu/projects.png"" style=""border-width:0px;""&gt;&lt;span class=""menu-text""&gt;Projects&lt;/span&gt; &lt;/div&gt;&lt;/a&gt;
</code></pre>

<p>This is probably a javascript link. I need to click this, and then on new page another link, then another to download a file and back to the main page and do this all over again to download different files.</p>

<p>I would be grateful to anyone who can help or suggest.</p>

<p>Thanks to chris, I was able to complete this..</p>

<p>First using the request library I got the redirect url as:</p>

<pre><code>redirectURL = requests.get(url).url
</code></pre>

<p>After that I use scrapy and selenium for click links and downloading files..
By adding selenium to the browser as add-in/plugin, it was quite simple.</p>
",0,1564780686,python;web-scraping;beautifulsoup;urllib3,False,78,0,1565200355,https://stackoverflow.com/questions/57333549/scraping-a-secure-website-requiring-clicks-on-javascript-links
46843988,urllib3 how to find code and message of Http error,"<p>I am catching http errors using python, but I want to know the code of the error (e.g. 400, 403,..). Additionally I want to get the message of the error. However, I can't find those two attributes in the documentation. Can anyone help? Thank you.</p>

<pre><code>    try:
        """"""some code here""""""
    except urllib3.exceptions.HTTPError as error:
        """"""code based on error message and code""""""
</code></pre>
",4,1508483315,python;exception;urllib3,True,19495,3,1565163111,https://stackoverflow.com/questions/46843988/urllib3-how-to-find-code-and-message-of-http-error
57380403,Error reading a local html file with urllib3,"<p>I am trying to read a previous downloaded html file with urllib3.</p>

<p>I have tried with:</p>

<pre><code>import urllib3

url = 'file:///test.html/' 
http = urllib3.PoolManager()
request = http.request('GET',url)

print(request.data)
</code></pre>

<p>but I got this:</p>

<p><code>urllib3.exceptions.LocationValueError: No host specified.</code></p>

<p>It is not supposed that there should be a 'host' because I already downloaded the file and it is on my local file system.</p>

<p>I have tried changing the syntax of the url but it has not worked. What should be the correct syntax of the url?</p>
",0,1565108954,python;urllib3,False,316,0,1565108954,https://stackoverflow.com/questions/57380403/error-reading-a-local-html-file-with-urllib3
57230388,Upgrade with Python3.7.4 version in miniconda docker image,"<p>There is some security regression was discovered in python 3.7.3, which still allows an attacker to exploit.</p>

<p>An issue was discovered in urllib2 in Python. CRLF injection is possible if the attacker controls a url parameter, as demonstrated by the first argument to urllib.request.urlopen with \r\n (specifically in the query string after a ? character) followed by an HTTP header or a Redis command.</p>

<p>Below are the refrences for the issue: </p>

<pre><code>[https://bugs.python.org/issue30458][1]
[https://bugs.python.org/issue33529][1]
[https://bugs.python.org/issue35755][1]
https://bugs.python.org/issue35907
https://bugs.python.org/issue36742
https://bugs.python.org/issue37463
</code></pre>

<p>The solution is to use Python 3.7.4 , which is released 9th of july 2019. But conda distribution is not released yet for python 3.7.4. </p>

<p>Now the question is:- Is there any way to upgrade python with python-3.7.4 in miniconda docker image container?</p>

<p>Docker file is : -</p>

<pre><code>FROM continuumio/miniconda3:4.6.14 
</code></pre>

<p>I tried to put upgrade python command in docker file but it fails as Python 3.7.4 is not available in conda distribution</p>

<pre><code>repo.anaconda.com/pkgs/main/linux-64 
</code></pre>
",0,1564215419,python;docker;anaconda;urllib3;miniconda,False,1098,0,1564222129,https://stackoverflow.com/questions/57230388/upgrade-with-python3-7-4-version-in-miniconda-docker-image
57087868,Urllib3 custom SSL key encryption,"<p>In Python's urllib3 under <a href=""https://urllib3.readthedocs.io/en/latest/advanced-usage.html#client-certificates"" rel=""nofollow noreferrer"">Client Certificates</a> there is an option for <code>key_password</code>.</p>

<p>Currently, I have the key info in plaintext and I want to encrypt it before storing it on the disk.</p>

<p>Here is the implementation:</p>

<pre class=""lang-py prettyprint-override""><code>http = urllib3.PoolManager(
...     cert_file='/path/to/your/client_cert.pem',
...     cert_reqs='CERT_REQUIRED',
...     key_file='/path/to/your/client.key',
...     key_password='keyfile_password')
</code></pre>

<p>However, I have not been able to find any documentation around what kind of encryption is supported for the key.</p>
",0,1563428974,python;python-requests;urllib3,True,353,1,1563431934,https://stackoverflow.com/questions/57087868/urllib3-custom-ssl-key-encryption
33827246,urllib3 - Failed to establish a new connection: [Errno 111],"<p>I am working in the code modifications on my local setup of openstack Designate.</p>

<p>Actually it was working fine before.</p>

<p>But now I am trying to execute the same command which was working fine before.</p>

<p>Unfortunately now for the same command, I am getting an error which is as follows:</p>

<pre><code>root@newds:~# designate record-list 5e18999d-1b4c-43f9-94e8-2bb2aab46aa0
ERROR: HTTPConnectionPool(host='127.0.0.1', port=9001): Max retries exceeded with url: /v1//domains/5e18999d-1b4c-43f9-94e8-2bb2aab46aa0/records (Caused by NewConnectionError('&lt;requests.packages.urllib3.connection.HTTPConnection object at 0x7f2077e2c210&gt;: Failed to establish a new connection: [Errno 111] Connection refused',))
</code></pre>

<p>Some one help me to sort this out.</p>
",6,1448024043,python;python-requests;openstack;urllib3,True,21183,2,1563262656,https://stackoverflow.com/questions/33827246/urllib3-failed-to-establish-a-new-connection-errno-111
57035467,Importing problem with urllib3 for webscraping exercise,"<p>I'm doing some webscraping exercises using Python and I've come up with errors in my code. Kindly need your help with analyzing the problem to my code.</p>

<p>I've already installed urllib3 using <code>pip install urllib3</code></p>

<pre><code>import urllib3 import urllib3.request

from bs4 import BeautifulSoup


def make_soup(url):
    thepage = url.request.urlopen(url)
    soup =BeautifulSoup (thepage, ""html.parser"")
    return soup

soup = make_soup (""https://edition.cnn.com/world"")
</code></pre>

<p>Here are the errors I'm receiving:
Errors on lines 10 and 6</p>

<blockquote>
  <p>AttributError: 'str' object has no attribute 'request'</p>
</blockquote>
",0,1563177401,python;python-requests;urllib3,True,223,1,1563178093,https://stackoverflow.com/questions/57035467/importing-problem-with-urllib3-for-webscraping-exercise
57011385,How to use urllib to download a partial HTML file?,"<p>I'm trying to download a bunch of HTML pages using urllib on Python 3.7, and then parse them using Beautifulsoup. However, the files are very large, so they take too long to download. </p>

<p>I only need data that exists within the first half of the documents, so I was wondering if there was a way to download only the first half of the pages? From my research, I found out that we can use something called range headers to download files partially (based on bytes), but this seems to only work in urllib2 for Python 2. I was having a hard time finding information about this for Python 3, which is why I wanted to as here.</p>

<p>Also, I'm using urllib instead of requests module because it seems that urllib is much faster. If anyone can suggest an even faster module (maybe urllib3?), I'd appreciate it!. </p>
",2,1562951544,python;web-scraping;python-requests;urllib;urllib3,False,179,0,1562951544,https://stackoverflow.com/questions/57011385/how-to-use-urllib-to-download-a-partial-html-file
56135465,Python: Reading Parquet files stored on s3 using petastorm generates connection warnings,"<p>I have a Tensorflow model that I would like to feed with parquet files stored on s3. I'm using <code>petastorm</code> to query these files from s3 and the result of the query is stored as a Tensorflow dataset thanks to <code>petastorm.tf_utils.make_petastorm_dataset</code>.</p>

<p>Here's the code I used (mainly inspired from this thread <a href=""https://stackoverflow.com/questions/51732446/tensorflow-dataset-api-input-pipeline-with-parquet-files"">Tensorflow Dataset API: input pipeline with parquet files</a>):</p>

<pre class=""lang-py prettyprint-override""><code>import s3fs
from pyarrow.filesystem import S3FSWrapper
from petastorm.reader import Reader
from petastorm.tf_utils import make_petastorm_dataset

dataset_url = ""analytics.xxx.xxx"" #s3 bucket name

fs = s3fs.S3FileSystem()
wrapped_fs = S3FSWrapper(fs)

with Reader(pyarrow_filesystem=wrapped_fs, dataset_path=dataset_url) as reader:
    dataset = make_petastorm_dataset(reader)
</code></pre>

<p>This works pretty well, except that it generates 20+ lines of connection warnings:</p>

<pre><code>W0514 18:56:42.779965 140231344908032 connectionpool.py:274] Connection pool is full, discarding connection: s3.eu-west-1.amazonaws.com
W0514 18:56:42.782773 140231311337216 connectionpool.py:274] Connection pool is full, discarding connection: s3.eu-west-1.amazonaws.com
W0514 18:56:42.854569 140232468973312 connectionpool.py:274] Connection pool is full, discarding connection: s3.eu-west-1.amazonaws.com
W0514 18:56:42.868761 140231328122624 connectionpool.py:274] Connection pool is full, discarding connection: s3.eu-west-1.amazonaws.com
W0514 18:56:42.885518 140230816429824 connectionpool.py:274] Connection pool is full, discarding connection: s3.eu-west-1.amazonaws.com
...
</code></pre>

<p>According to this thread <a href=""https://stackoverflow.com/questions/53765366/urllib3-connectionpool-connection-pool-is-full-discarding-connection"">urllib3 connectionpool - Connection pool is full, discarding connection</a>, it's certainly related to <code>urllib3</code>, but I can't figure a way to get rid of these warnings.</p>

<p>Has anyone encountered this issue? </p>
",0,1557854094,python;tensorflow;urllib3;petastorm,True,1008,1,1562836925,https://stackoverflow.com/questions/56135465/python-reading-parquet-files-stored-on-s3-using-petastorm-generates-connection
56959094,Reading a line that includes tabs,"<p>I was obtained a file from <em><a href=""https://www.clres.com/db/parses/oec/abaft.parse"" rel=""nofollow noreferrer"">https://www.clres.com/db/parses/oec/abaft.parse</a></em> using urllib3. It has tabs and then \r\n. In Python 2.7, I was using StringIO but this isn't available in Python 3.7. </p>

<p>I tried to use IO since StringIO has been eliminated.</p>

<pre><code>http = urllib3.PoolManager(timeout=10.0)
r = http.urlopen('GET', url, preload_content=False)
remote_file = r.data
memory_file = remote_file.decode('utf-8')
prep_sents = get_sentences(memory_file)
</code></pre>

<pre><code>def get_sentence(memory_file):
    sentence = []
    for line in memory_file:
        if not re.match(r'\s*\r?\n', line):
</code></pre>

<p>I expect to get a line, but instead I only get the first token in a line.</p>

<pre><code>1\tWith\twith\t_\tIN\t_\t0\tROOT\t_\t_\t_\t_\t_\t_\r\n
</code></pre>
",1,1562698846,python;io;urllib3;stringio,True,92,2,1562702310,https://stackoverflow.com/questions/56959094/reading-a-line-that-includes-tabs
56654406,"Given an html paragraph and a link, is there a way to retrieve the text before and the text after the link inside the paragraph in Python?","<p>I am using urllib3 to get the html of some pages. </p>

<p>I want to retrieve the text from the paragraph where the link is, with the text before and after the link stored separately.</p>

<p><strong>For example:</strong></p>

<pre><code>import urllib3
from bs4 import BeautifulSoup

http = urllib3.PoolManager()
r = http.request('get', ""https://www.snopes.com/fact-check/michael-novenche/"")
body = r.data
soup = BeautifulSoup(body, 'lxml')
for a in soup.findAll('a'):
    if a.has_attr('href'):
        if (a['href'] == ""http://web.archive.org/web/20040330161553/http://newyork.local.ie/content/31666.shtml/albany/news/newsletters/general""):
            link_text = a
            link_para = a.find_parent(""p"")
            print(link_text)
            print(link_para)

</code></pre>

<p><strong>Paragraph</strong></p>

<pre><code>&lt;p&gt;The message quoted above about Michael Novenche, a two-year-old boy 
undergoing chemotherapy to treat a brain tumor, was real, but keeping up with 
all the changes in his condition proved a challenge.  The message quoted above 
stated that Michael had a large tumor in his brain, was operated upon to 
remove part of the tumor, and needed prayers to help him through chemotherapy 
to a full recovery.  An &lt;nobr&gt;October 2000&lt;/nobr&gt; article in &lt;a 
href=""http://web.archive.org/web/20040330161553/http://newyork.local.ie/conten
t/31666.shtml/albany/news/newsletters/general"" 
onmouseout=""window.status='';return true"" onmouseover=""window.status='The
Local Albany Weekly';return true"" target=""_blank""&gt;&lt;i&gt;The Local Albany 
Weekly&lt;/i&gt;&lt;/a&gt; didn’t mention anything about little Michael’s medical 
condition but said that his family was “in need of funds to help pay for the
 transportation to the hospital and other costs not covered by their 
insurance.”  A June 2000 message posted to the &lt;a 
href=""http://www.ecunet.org/whatisecupage.html"" 
onmouseout=""window.status='';return true"" 
onmouseover=""window.status='Ecunet';return true"" target=""_blank""&gt;Ecunet&lt;/a&gt; 
mailing list indicated that Michael had just turned &lt;nobr&gt;3 years&lt;/nobr&gt; old, 
mentioned that his tumor appeared to be shrinking, and provided a mailing 
address for him:&lt;/p&gt;
</code></pre>

<p><strong>Link</strong></p>

<pre><code>&lt;a href=""http://web.archive.org/web/20040330161553/http://newyork.local.ie/conten
t/31666.shtml/albany/news/newsletters/general""
onmouseout=""window.status='';return true"" onmouseover=""window.status='The 
Local Albany Weekly';return true"" target=""_blank""&gt;&lt;i&gt;The Local Albany 
Weekly&lt;/i&gt;&lt;/a&gt;
</code></pre>

<p><strong>Text to be retrieved (2 parts)</strong></p>

<pre><code>The message quoted above about Michael Novenche, a two-year-old boy 
undergoing chemotherapy ... was operated upon to 
remove part of the tumor, and needed prayers to help him through chemotherapy 
to a full recovery.  An October 2000 article in
</code></pre>

<pre><code>didn’t mention anything about little Michael’s medical 
condition but said that his family was ... turned 3 years old, 
mentioned that his tumor appeared to be shrinking, and provided a mailing 
address for him:
</code></pre>

<p>I cant simply get_text() then use split as the link text might be repeated. </p>

<p>I thought I might just add a counter to see how many times the link text is repeated, use split(), then use a loop to get the parts I want.</p>

<p>I would appreciate a better, less messy method though.</p>
",0,1560879577,python;html;beautifulsoup;urllib3,True,154,4,1560984091,https://stackoverflow.com/questions/56654406/given-an-html-paragraph-and-a-link-is-there-a-way-to-retrieve-the-text-before-a
56234533,Max retries exceeded and connection refused errors when running Rasa Stack,"<p>Rasa version: Core = 0.14.4, Core-sdk = 0.14.0, NLU = 0.15.0</p>

<p>Python version: 3.6.7</p>

<p>Operating system (windows, osx, ...): Ubuntu 18.04.2 LTS</p>

<p>Issue: I have been using the Rasa Stack starter-pack (<a href=""https://github.com/RasaHQ/starter-pack-rasa-stack"" rel=""nofollow noreferrer"">https://github.com/RasaHQ/starter-pack-rasa-stack</a>) for which the files within it within it remain unchanged apart from the Makefile which I changed the instances of python to python3. I attempted to get Rasa Stack working as a server using the RestInput channel as described in <a href=""https://rasa.com/docs/core/connectors/#rest-channels"" rel=""nofollow noreferrer"">https://rasa.com/docs/core/connectors/#rest-channels</a>. I used the example to connect the rest input channel using the run script:</p>

<pre><code>python3 -m rasa_core.run -d models/current/dialogue -u models/current/nlu --port 5002 --credential credentials.yml
</code></pre>

<p>This initally worked but my attempt to post a message using the following code:</p>

<pre><code>import requests

URL = 'http://localhost:5010/webhooks/rest/webhook/'
PARAMS = {'sender': ""ola"", 'message': ""how are you""}

r = requests.get(URL)

data = r.json()
print(data)
</code></pre>

<p>resulted in a error to do with Max retries exceeded with url and connection refusal. The issue is since then, I have not been able to get the server up and running, with or without the commandline interface. When I try to connect the rest input channel as above, I get the following error:</p>

<pre><code>r/lib/python3.6/runpy.py:125: RuntimeWarning: 'rasa_core.run' found in sys.modules after import of package 'rasa_core', but prior to execution of 'rasa_core.run'; this may result in unpredictable behaviour
  warn(RuntimeWarning(msg))
2019-05-21 08:38:27 INFO     root  - Rasa process starting
2019-05-21 08:38:45 INFO     rasa_nlu.components  - Added 'SpacyNLP' to component cache. Key 'SpacyNLP-en'.
Traceback (most recent call last):
  File ""/home/thales/.local/lib/python3.6/site-packages/urllib3/connection.py"", line 160, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File ""/home/thales/.local/lib/python3.6/site-packages/urllib3/util/connection.py"", line 80, in create_connection
    raise err
  File ""/home/thales/.local/lib/python3.6/site-packages/urllib3/util/connection.py"", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/thales/.local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 603, in urlopen
    chunked=chunked)
  File ""/home/thales/.local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 355, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File ""/usr/lib/python3.6/http/client.py"", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File ""/usr/lib/python3.6/http/client.py"", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File ""/usr/lib/python3.6/http/client.py"", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File ""/usr/lib/python3.6/http/client.py"", line 1026, in _send_output
    self.send(msg)
  File ""/usr/lib/python3.6/http/client.py"", line 964, in send
    self.connect()
  File ""/home/thales/.local/lib/python3.6/site-packages/urllib3/connection.py"", line 183, in connect
    conn = self._new_conn()
  File ""/home/thales/.local/lib/python3.6/site-packages/urllib3/connection.py"", line 169, in _new_conn
    self, ""Failed to establish a new connection: %s"" % e)
urllib3.exceptions.NewConnectionError: &lt;urllib3.connection.HTTPConnection object at 0x7f58af883ba8&gt;: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/thales/.local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send
    timeout=timeout
  File ""/home/thales/.local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 641, in urlopen
    _stacktrace=sys.exc_info()[2])
  File ""/home/thales/.local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 399, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=5002): Max retries exceeded with url: /webhooks/rest/webhook/ (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f58af883ba8&gt;: Failed to establish a new connection: [Errno 111] Connection refused',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.6/dist-packages/rasa_core/run.py"", line 204, in &lt;module&gt;
    endpoints=_endpoints)
  File ""/usr/local/lib/python3.6/dist-packages/rasa_core/run.py"", line 157, in load_agent
    from rasa_core import agent
  File ""/usr/local/lib/python3.6/dist-packages/rasa_core/agent.py"", line 14, in &lt;module&gt;
    from rasa_core.channels import UserMessage, OutputChannel, InputChannel
  File ""/usr/local/lib/python3.6/dist-packages/rasa_core/channels/__init__.py"", line 23, in &lt;module&gt;
    from rasa_core.channels.telegram import TelegramInput  # nopep8
  File ""/usr/local/lib/python3.6/dist-packages/rasa_core/channels/telegram.py"", line 4, in &lt;module&gt;
    from telegram import (
  File ""/home/thales/.local/lib/python3.6/site-packages/telegram/__init__.py"", line 46, in &lt;module&gt;
    from .files.file import File
  File ""/home/thales/.local/lib/python3.6/site-packages/telegram/files/file.py"", line 23, in &lt;module&gt;
    from future.backports.urllib import parse as urllib_parse
  File ""/home/thales/.local/lib/python3.6/site-packages/future/backports/__init__.py"", line 14, in &lt;module&gt;
    import_top_level_modules()
  File ""/home/thales/.local/lib/python3.6/site-packages/future/standard_library/__init__.py"", line 810, in import_top_level_modules
    with exclude_local_folder_imports(*TOP_LEVEL_MODULES):
  File ""/home/thales/.local/lib/python3.6/site-packages/future/standard_library/__init__.py"", line 781, in __enter__
    module = __import__(m, level=0)
  File ""/home/thales/ola/starter-pack-rasa-stack/test.py"", line 6, in &lt;module&gt;
    r = requests.post(URL, data = PARAMS)
  File ""/home/thales/.local/lib/python3.6/site-packages/requests/api.py"", line 116, in post
    return request('post', url, data=data, json=json, **kwargs)
  File ""/home/thales/.local/lib/python3.6/site-packages/requests/api.py"", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/home/thales/.local/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request
    resp = self.send(prep, **send_kwargs)
  File ""/home/thales/.local/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send
    r = adapter.send(request, **kwargs)
  File ""/home/thales/.local/lib/python3.6/site-packages/requests/adapters.py"", line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=5002): Max retries exceeded with url: /webhooks/rest/webhook/ (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f58af883ba8&gt;: Failed to establish a new connection: [Errno 111] Connection refused',))
</code></pre>

<p>The part of the error mentioning the port seems to depend on the port that was in my post message python script. The port in the command line arguments seems to be ignored. My attempt to debug the run.py file finds it breaks down at line 157 ""from rasa_core import agent"" which makes little sense as that import works fine on it's own in a python terminal.</p>

<p>The ""make cmdline"" command bring up the same error as does the ""make train-core"" command.</p>

<p>I should note I can still create the action server and local servers for other things as well.</p>

<p>Content of configuration file (config.yml):</p>

<pre class=""lang-sh prettyprint-override""><code>language: ""en""
pipeline: spacy_sklearn
</code></pre>
",1,1558428543,python;python-requests;urllib3;rasa-nlu;rasa-core,False,2545,1,1560900354,https://stackoverflow.com/questions/56234533/max-retries-exceeded-and-connection-refused-errors-when-running-rasa-stack
56611812,How to accelerate download speed of a file using Python?,"<p>I am trying to download a file located on the web through <code>write</code> function with the <code>wb</code> mode. Files are being downloaded too slowly when it is compared to the speed of downloading through a web browser (as I have a high-speed internet connection). How can I accelerate the download speed? Is there a better way of handling file download? </p>

<p>Here is the way I use:</p>

<pre><code>resp = session.get(download_url)
with open(package_name + '.apk', 'wb+') as local_file:
    local_file.write(resp.content)
</code></pre>

<p>I have experimented that the download speeds of <code>requests</code> and <code>urllib3</code> libraries are almost the same. Here is the experimental result to download a <code>15 MB</code> file:</p>

<ul>
<li><code>requests</code>: <code>0:02:00.689587</code></li>
<li><code>urllib3</code>: <code>0:02:05.833442</code></li>
</ul>

<p>p.s. My Python version is <code>3.7.0</code>, and my OS is <code>Windows 10 version 1903</code>.</p>

<p>p.s. I have investigated the <a href=""https://stackoverflow.com/questions/21196161/python-downloading-is-extremely-slow"">reportedly similar question</a>, but the answers/comments did not work.</p>
",2,1560613126,python;python-3.x;python-requests;urllib3,True,3786,1,1560886297,https://stackoverflow.com/questions/56611812/how-to-accelerate-download-speed-of-a-file-using-python
56285807,Overcoming SSL: CERTIFICATE_VERIFY_FAILED on Windows 10 when using urllib3,"<p>This simple code in Python displays Ascii art and is taken from the following quora response: <a href=""https://qr.ae/TWNB4S"" rel=""nofollow noreferrer"">https://qr.ae/TWNB4S</a>. I have Python version 3.7.3 installed on my Windows 10 laptop via independent installer from python.org and also via anaconda suite. The code runs fine on Jupyter notebook launched via anaconda. But when I run the same code via pycharm IDE I get SSL related errors.</p>

<p>I searched online for this Error but could not find a way to resolve this on my Windows 10 python 3.7.3 installation. But it works fine on my Anaconda python 3.7.3 version. So is there a way to overcome this Error?
Code:</p>

<pre><code>import ascii
output =  ascii.loadFromUrl(""https://i.dailymail.co.uk/i/pix/2015/09/01/18/2BE1F43200000578-0-image-a-17_1441127603656.jpg"")
print(output)
</code></pre>

<p>The Errors when I run the above code via Pycharm IDE:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Users\billn\AppData\Local\Programs\Python\Python37\lib\site-packages\urllib3\connectionpool.py"", line 603, in urlopen
    chunked=chunked)
  File ""C:\Users\billn\AppData\Local\Programs\Python\Python37\lib\site-packages\urllib3\connectionpool.py"", line 344, in _make_request
    self._validate_conn(conn)
  File ""C:\Users\billn\AppData\Local\Programs\Python\Python37\lib\site-packages\urllib3\connectionpool.py"", line 843, in _validate_conn
    conn.connect()
  File ""C:\Users\billn\AppData\Local\Programs\Python\Python37\lib\site-packages\urllib3\connection.py"", line 350, in connect
    ssl_context=context)
  File ""C:\Users\billn\AppData\Local\Programs\Python\Python37\lib\site-packages\urllib3\util\ssl_.py"", line 355, in ssl_wrap_socket
    return context.wrap_socket(sock, server_hostname=server_hostname)
  File ""C:\Users\billn\AppData\Local\Programs\Python\Python37\lib\ssl.py"", line 412, in wrap_socket
    session=session
  File ""C:\Users\billn\AppData\Local\Programs\Python\Python37\lib\ssl.py"", line 853, in _create
    self.do_handshake()
  File ""C:\Users\billn\AppData\Local\Programs\Python\Python37\lib\ssl.py"", line 1117, in do_handshake
    self._sslobj.do_handshake()
***ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056)***

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/billn/Google Drive/Books/UDEMY - Complete Python 3 Bootcamp - Jose Portilla/gg.py"", line 3, in &lt;module&gt;
    output = ascii.loadFromUrl(""https://i.dailymail.co.uk/i/pix/2015/09/01/18/2BE1F43200000578-0-image-a-17_1441127603656.jpg"")
  File ""C:\Users\billn\AppData\Local\Programs\Python\Python37\lib\site-packages\ascii\__init__.py"", line 17, in loadFromUrl
    fd = http.request('GET', URL)
  File ""C:\Users\billn\AppData\Local\Programs\Python\Python37\lib\site-packages\urllib3\request.py"", line 68, in request
    **urlopen_kw)
  File ""C:\Users\billn\AppData\Local\Programs\Python\Python37\lib\site-packages\urllib3\request.py"", line 89, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
  File ""C:\Users\billn\AppData\Local\Programs\Python\Python37\lib\site-packages\urllib3\poolmanager.py"", line 326, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File ""C:\Users\billn\AppData\Local\Programs\Python\Python37\lib\site-packages\urllib3\connectionpool.py"", line 670, in urlopen
    **response_kw)
  File ""C:\Users\billn\AppData\Local\Programs\Python\Python37\lib\site-packages\urllib3\connectionpool.py"", line 670, in urlopen
    **response_kw)
  File ""C:\Users\billn\AppData\Local\Programs\Python\Python37\lib\site-packages\urllib3\connectionpool.py"", line 670, in urlopen
    **response_kw)
  File ""C:\Users\billn\AppData\Local\Programs\Python\Python37\lib\site-packages\urllib3\connectionpool.py"", line 641, in urlopen
    _stacktrace=sys.exc_info()[2])
  File ""C:\Users\billn\AppData\Local\Programs\Python\Python37\lib\site-packages\urllib3\util\retry.py"", line 399, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='i.dailymail.co.uk', port=443): Max retries exceeded with url: /i/pix/2015/09/01/18/2BE1F43200000578-0-image-a-17_1441127603656.jpg (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056)')))
</code></pre>
",0,1558671362,python;python-3.x;ssl-certificate;urllib3,False,4200,1,1558674283,https://stackoverflow.com/questions/56285807/overcoming-ssl-certificate-verify-failed-on-windows-10-when-using-urllib3
56197657,urllib.request.urlopen connection error max URL retries reached: how to scrape website bypassing this?,"<p>I am trying to scrape all of the pages on <a href=""http://www.signalpeptide.de/index.php?sess=&amp;m=listspdb_bacteria&amp;s=details&amp;id=810&amp;listname="" rel=""nofollow noreferrer"">this</a> site. I wrote this code:</p>

<pre><code>from bs4 import BeautifulSoup
import requests
import pandas as pd
import urllib
from urllib.request import urlopen

output = open('signalpeptide.txt', 'a')
for each_page in range(1,220000):
    if each_page%1000 == 0: #this is because of download limit
            time.sleep(5) #this is because of download limit

    url = 'http://www.signalpeptide.de/index.php?sess=&amp;m=listspdb_bacteria&amp;s=details&amp;id=' + str(each_page) + '&amp;listname='    
    page = urllib.request.urlopen(url)
    soup = BeautifulSoup(page, 'html.parser')
    tabs = soup.find_all('table')
    pd_list = pd.read_html(str(tabs[0]))
    temp_list = []
    for i in range(22):
        temp_list.append(str(pd_list[0][2][i]).strip())

    output.write(str(temp_list[1]).strip() + '\t' + str(temp_list[3]).strip() + '\t' + str(temp_list[7]).strip() + '\t' + str(temp_list[15]).strip() + '\t')
    pd_list2 =  pd.read_html(str(tabs[1]))
    output.write(str(pd_list2[0][0][1]) + '\t' + str(pd_list2[0][2][1]) + '\n')
</code></pre>

<p>My connection is being refused for trying the URL too many times (I know this because when I run the code with requests, instead or url.request.urlopen, the error says 'Max retries exceeded with url:':</p>

<pre><code>requests.exceptions.ConnectionError: HTTPConnectionPool(host='www.signalpeptide.de', port=80): Max retries exceeded with url: /index.php?sess=&amp;m=listspdb_bacteria&amp;s=details&amp;id=1000&amp;listname= (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x11ebc0e48&gt;: Failed to establish a new connection: [Errno 61] Connection refused'))
</code></pre>

<p>Other methods suggested <a href=""https://stackoverflow.com/questions/56137881/why-is-there-a-table-when-i-scrape-with-beautifulsoup-but-not-pandas/56138185?noredirect=1#comment98980852_56138185"">here</a> also didn't work, one of the forum users from that post suggest I make a different post for specifically this issue.</p>

<p>I have looked into scrapy, but I don't really understand how to make it link with the script above. Can anyone show me how to edit the above script, so that I avoid errors such as:</p>

<pre><code>urllib3.exceptions.NewConnectionError: &lt;urllib3.connection.VerifiedHTTPSConnection object at 0x1114f0898&gt;: Failed to establish a new connection: [Errno 61] Connection refused

urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.signalpeptide.de', port=443): Max retries exceeded with url: /index.php?sess=&amp;m=listspdb_bacteria&amp;s=details&amp;id=2&amp;listname= (Caused by NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x1114f0898&gt;: Failed to establish a new connection: [Errno 61] Connection refused'

ConnectionRefusedError: [Errno 61] Connection refused
</code></pre>

<p>I also tried using urllib3:</p>

<pre><code>from bs4 import BeautifulSoup
import requests
import pandas as pd
#import urllib
#from urllib.request import urlopen
import urllib3

http = urllib3.PoolManager()
output = open('signalpeptide.txt', 'a')
for each_page in range(1,220000):
    if each_page%1000 == 0: #this is because of download limit
            time.sleep(5) #this is because of download limit

    url = 'http://www.signalpeptide.de/index.php?sess=&amp;m=listspdb_bacteria&amp;s=details&amp;id=' + str(each_page) + '&amp;listname='    
    page = http.request('GET',url)
    soup = BeautifulSoup(page, 'html.parser')
</code></pre>

<p>with the error:</p>

<pre><code>    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='www.signalpeptide.de', port=80): Max retries exceeded with url: /index.php?sess=&amp;m=listspdb_bacteria&amp;s=details&amp;id=1&amp;listname= (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x11ce6f5f8&gt;: Failed to establish a new connection: [Errno 61] Connection refused'))
</code></pre>

<p>Note that I think the first time you run this script it will run, it was just when I was running it a few times while I was testing it/writing it, that now I have it written and i know it works. Then it ran for the first 400 entries and then I got the errors above, and now it won't let me run it at all.</p>

<p>If anyone has an idea for how to edit this script to get around the max number of URL retries, particularly be aware that I am already getting the connection refused error, I would appreciate it.</p>
",0,1558171645,python;beautifulsoup;urllib;urllib3,False,818,0,1558304736,https://stackoverflow.com/questions/56197657/urllib-request-urlopen-connection-error-max-url-retries-reached-how-to-scrape-w
56204481,How to fix python scrapy errors related to poolmanager https connection to get raw data from a web page,"<p>I'm using the sracpy library for the first time to scrape a website using selenium. I don't get any error using the request lib but I get errors, given in the code snipet block below, using urllib3 with beautifulsoup, the aim is to get raw data instead of HTML script containing first 200 chars. To understand my point, please refere to the code pasted for you. Thank you.  </p>

<p>I tried requests lib in Python with scrapy to extract data from a target website. It's working fine but next I'm intended to carry out the similir job using urllib3 &amp; beautifulsoup to extract raw data instead of HTML script of the first 200 chars. I hope it make sense, if not, please ask me. Looking forward.</p>

<pre><code>import requests
import urllib3
from bs4 import BeautifulSoup

# Extracting web data using requests urllib3 &amp; BeautifulSoap

print ""Retrieved the following data (Raw Form) using 'urllib3' lib \n""
http = urllib3.PoolManager()
r = http.request('GET', 'https://authoraditiagarwal.com')
soup = BeautifulSoup(r.data, 'lxml')
print soup.title
print soup.title.text
</code></pre>

<p>Error:</p>

<pre><code>File ""C:\Python27\lib\site-packages\urllib3\util\retry.py"", line 399, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
MaxRetryError: HTTPSConnectionPool(host='authoraditiagarwal.com', port=443): 
Max retries exceeded with url: / (Caused by SSLError(SSLError(""bad handshake: 
Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')],)"",),))
</code></pre>
",0,1558235915,python;beautifulsoup;urllib3,False,555,0,1558304257,https://stackoverflow.com/questions/56204481/how-to-fix-python-scrapy-errors-related-to-poolmanager-https-connection-to-get-r
56182990,Cartopy is unable to download data behind proxy despite proxies being set in os.environ,"<p>I am using Python 3.5 and cartopy 0.17.</p>

<pre><code>import cartopy.crs as ccrs
import matplotlib.pyplot as plt

ax = plt.axes(projection=ccrs.PlateCarree())
ax.coastlines()
plt.show()
</code></pre>

<p>When I try and execute this I am presented with a 502 Bad Gateway Error as the library is attempting to download Natural Earth datasets:</p>

<pre><code>Warning (from warnings module):
File ""/.local/lib/python3.5/site-packages/cartopy/io/__init__.py"", line 260
warnings.warn('Downloading: {}'.format(url), DownloadWarning)
DownloadWarning: Downloading: http://naciscdn.org/naturaleart/110m/physical/ne_110m_coastline.zip
Exception in Tkinter callback
Traceback (most recent call last):
File ""/usr/lib/python3.5/urllib/request.py"", line 1254, in do_open h.request(req.get_method(), req.selector, req.data, headers)
File ""/usr/lib/python3.5/http/client.py"", line 1107, in request self._send_request(method, url, body, headers)
File ""/usr/lib/python3.5/http/client.py"", line 1152, in _send_request self.endheaders(body)
File ""/usr/lib/python3.5/http/client.py"", line 1103, in endheaders self._send_output(message_body)
File ""/usr/lib/python3.5/http/client.py"", line 934, in _send_output self.send(msg)
File ""/usr/lib/python3.5/http/client.py"", line 877, in send self.connect()
File ""/usr/lib/python3.5/http/client.py"", line 1253, in connect super().connect()
File ""/usr/lib/python3.5/http/client.py"", line 853, in connect self._tunnel()
File ""/usr/lib/python3.5/http/client.py"", line 832, in _tunnel message.strip()))
OSError: Tunnel connection failed: 502 Bad Gateway

During handling of the above exception, another exception occurred:
File ""/.local/lib/python3.5/site-packages/cartopy/io/shapereader.py"", line 359, in natural_earth
return ne_downloader.path(format_dict)
File ""/usr/lib/python3.5/urllib/request.py"", line 1297, in https_open context=self._context, check_hostname=self._check_hostname)
File ""/usr/lib/python3.5/urllib/request.py"", line 1256, in do_open raise URLError(err)
urllib.error.URLError: &lt;urlopen error Tunnel connection failed: 502 Bad Gateway&gt;
</code></pre>

<p>The 502 code indicated a proxy error however I have set my proxies in os.environ using the following keys and I still get the same error:</p>

<pre><code>os.environ['HTTP_PROXY']= 'proxyaddress:port'
os.environ['HTTPS_PROXY']= 'proxyaddress:port'
</code></pre>

<p>How do I tell urllib3 via cartopy to use the proxy information in the Python environment? Is there something else which could be causing this error?</p>

<p>Note: I am following the tutorial located on the official documentation <a href=""https://scitools.org.uk/cartopy/docs/latest/matplotlib/intro.html"" rel=""nofollow noreferrer"">here</a></p>
",0,1558083526,python;python-3.x;urllib;urllib3;cartopy,False,1037,0,1558084944,https://stackoverflow.com/questions/56182990/cartopy-is-unable-to-download-data-behind-proxy-despite-proxies-being-set-in-os
55797834,Why is Urllib3 not working with PyInstaller,"<p>I have a problem when creating an exe file from my Python script using PyInstaller.</p>

<p>Script is working just fine when I run it from VS but once packed into exe file I get the following message when trying to run the app:</p>

<pre><code>Traceback (most recent call last):
  File ""final02.py"", line 11, in &lt;module&gt;
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_86\lib\site-packages\PyInstaller\loader\pyimod03_importers.py"", line 627, in exec_module
    exec(bytecode, module.__dict__)
  File ""selenium\webdriver\__init__.py"", line 18, in &lt;module&gt;
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_86\lib\site-packages\PyInstaller\loader\pyimod03_importers.py"", line 627, in exec_module
    exec(bytecode, module.__dict__)
  File ""selenium\webdriver\firefox\webdriver.py"", line 29, in &lt;module&gt;
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_86\lib\site-packages\PyInstaller\loader\pyimod03_importers.py"", line 627, in exec_module
    exec(bytecode, module.__dict__)
  File ""selenium\webdriver\remote\webdriver.py"", line 27, in &lt;module&gt;
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_86\lib\site-packages\PyInstaller\loader\pyimod03_importers.py"", line 627, in exec_module
    exec(bytecode, module.__dict__)
  File ""selenium\webdriver\remote\remote_connection.py"", line 24, in &lt;module&gt;
ModuleNotFoundError: No module named 'urllib3'
[16444] Failed to execute script final02
</code></pre>

<p>To pack the app I have used following command:</p>

<pre><code>pyinstaller.exe --onefile --icon=app.ico final02.py 
</code></pre>

<p>I have tried to include</p>

<pre><code> --hidden-import urllib3
</code></pre>

<p>but there I had no success.</p>

<p>Also it's worth mentioning that when running it form VS I have installed and successfully imported urllib3 module in my script where it works just fine.</p>
",1,1555951319,python;selenium;pyinstaller;urllib3,False,590,0,1556891262,https://stackoverflow.com/questions/55797834/why-is-urllib3-not-working-with-pyinstaller
55687639,Connect to Selenium webdriver URL through a proxy using Python,"<p>I'm using the Python selenium language binding to connect to a remote webdriver through a proxy. I'm not concerned about the proxy configuration in the destination web browser, just the webdriver connection.
e.g.</p>

<pre><code>driver = webdriver.Remote('http://mywebdriver:4444/wd/hub'...)
</code></pre>

<p>It seems the current implementation of the Selenium python module uses urllib3 for all underlying http requests:</p>

<p>selenium/webdriver/remote/remote_connection.py contains:</p>

<pre><code>http = urllib3.PoolManager(timeout=self._timeout)
resp = http.request(method, url, body=body, headers=headers)
</code></pre>

<p>however urllib3 does not seem to obey http_proxy or https_proxy environment variables. Instead it looks like you should use ProxyManager:</p>

<pre><code>http = urllib3.ProxyManager('http://myproxy:3128/')
</code></pre>

<p>I'd like to try and use standard unmodified libraries as far as possible but it looks like this would involve me running with a customised version of the Selenium python library in order to get proxy support.</p>

<p>Is there something I've missed here? Are there any other ways of setting proxy details?</p>
",3,1555325655,python;selenium;selenium-webdriver;urllib3,False,470,0,1556699502,https://stackoverflow.com/questions/55687639/connect-to-selenium-webdriver-url-through-a-proxy-using-python
55892054,Python Beautiful Soup and urllib.request - How to get past Steam age check,"<p>I'm trying to make a Steam parser that gets information from the week-long deals.</p>

<p>However, some items are blocked by the age check. I'm using urllib.request and Beautiful Soup 4 to get the information but as you may have guessed, I can't get to the M rated items. I tried searching up similar questions but none show me how to get past the age check using urllib.request</p>

<p>I want test to equal 'No description' only when the item actually has no description</p>

<p>Here's my code:</p>

<pre><code>import urllib.request

import bs4 as bs

source = urllib.request.urlopen('https://store.steampowered.com/search/?filter=weeklongdeals')
soup = bs.BeautifulSoup(source,'lxml')

searchResultContainer = soup.find('div',{'id':'search_result_container'})
containerHolder = searchResultContainer.findChildren()[1]

links = []
for a in containerHolder.findAll('a', href=True):
    links.append(a['href'])

x = 0
description = []
for link in links:
    source = urllib.request.urlopen(str(link))
    soup = bs.BeautifulSoup(source,'lxml')

    try: 
        test = soup.find('div',{'class':'game_description_snippet'}).get_text().strip()
        description.append(soup.find('div',{'class':'game_description_snippet'}).get_text().strip())
    except:
        test = 'No description'
        description.append('No description')
    finally:
        x += 1
        print(f'{x}: {test}')
</code></pre>
",1,1556467405,python;parsing;beautifulsoup;steam;urllib3,False,145,1,1556476746,https://stackoverflow.com/questions/55892054/python-beautiful-soup-and-urllib-request-how-to-get-past-steam-age-check
55807926,How to get Content-Type of URL using urllib3,"<p>I need to get the content-type of an internet resource not a local file. How can I get the MIME type from a resource behind an URL.</p>

<p>I have tried this :</p>

<pre><code>http = urllib3.PoolManager()
response = http.request(""GET"",url)
print(response.headers)
</code></pre>

<p>How can I get the Content-Type, can be done using urllib3 and how or if not what is the other way?</p>
",0,1556010887,python;python-3.x;urllib3,True,1394,1,1556011260,https://stackoverflow.com/questions/55807926/how-to-get-content-type-of-url-using-urllib3
55787377,BeautifulSoup can&#39;t find div with specific class,"<p><a href=""https://i.stack.imgur.com/qAoQe.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qAoQe.jpg"" alt=""Webpage HTML and my code""></a></p>

<p>So for some background I have been trying to learn web scraping to get some images for machine learning projects involving CNNs. I have been trying to scrape some images from a site (HTML code on the left, my code on the right) with no luck; my code ends up printing/returning an empty list. Is there something I am doing wrong?</p>

<p>For what it's worth, I tried finding other div tags that had an 'id' instead of a 'class' and that did work, but for some reason it can't find the ones I am looking for. </p>

<p>Edit: </p>

<pre><code>import requests
import urllib3
from bs4 import BeautifulSoup

http = urllib3.PoolManager()
url = 'https://www.grailed.com/shop/EkpEBRw4rw'

response = http.request('GET', url)
soup = BeautifulSoup(response.data, 'html.parser')

img_div = soup.findAll('div', {'class': ""listing-cover-photo ""})
print(img_div)
</code></pre>

<p>Edit 2:</p>

<pre><code>from bs4 import BeautifulSoup
from selenium import webdriver

url = 'https://www.grailed.com/shop/EkpEBRw4rw'
driver = webdriver.Chrome(executable_path='chromedriver.exe')
driver.get(url)
soup = BeautifulSoup(driver.page_source, 'html.parser')

listing = soup.select('.listing-cover-photo ')
for item in listing:
    print(item.select('img'))
</code></pre>

<p>OUTPUT:</p>

<pre><code>[&lt;img alt=""Off-White Off White Caravaggio Hoodie"" src=""https://process.fs.grailed.com/AJdAgnqCST4iPtnUxiGtTz/cache=expiry:max/rotate=deg:exif/resize=width:480,height:640,fit:crop/output=format:webp,quality:70/compress/https://cdn.fs.grailed.com/api/file/yX8vvvBsTaugadX0jssT""/&gt;]
(...a few more of these...)
[&lt;img alt=""Off-White Off-White Arrows Hoodie Black"" src=""https://process.fs.grailed.com/AJdAgnqCST4iPtnUxiGtTz/cache=expiry:max/rotate=deg:exif/resize=width:480,height:640,fit:crop/output=format:webp,quality:70/compress/https://cdn.fs.grailed.com/api/file/9CMvJoQIRaqgtK0u9ov0""/&gt;]
[]
[]
[]
[]
(...many more empty lists...)
</code></pre>
",2,1555886504,python;html;web-scraping;beautifulsoup;urllib3,True,2571,1,1555978095,https://stackoverflow.com/questions/55787377/beautifulsoup-cant-find-div-with-specific-class
55629679,k8s_api connection time out in Jenkins pipeline,"<p>I wanted to list the namespaces present in a cluster using Kubernetes API i.e. using the <code>list_namespace</code> method. </p>

<p>When I run the below-mentioned code from my Linux machine it works fine.</p>

<p>I created a Jenkins pipeline job and calling the python file as below its throwing Connection Timeout error.</p>

<p>If I do a <code>kubectl get ns</code> in same Jenkins file I am getting the output. </p>

<p>I thought of proxies are not allowing it. But could not find how to fix it. </p>

<p>My Python code: </p>

<pre><code>config.load_kube_config(os.environ['KUBE_CONFIG'])
v1_api = client.CoreV1Api()
api_instance = kubernetes.client.CoreV1Api()

try: 
    api_response = api_instance.list_namespace(limit=""1"")
    pprint(api_response)
except Exception as e:
    print(""Exception when calling CoreV1Api-&gt;list_namespace: %s\n"" % e)

sys.exit(""EXITING"")
</code></pre>

<p>Command I ran:</p>

<pre><code>export KUBE_CONFIG=${env.WORKSPACE}/.kube/kube_config
python36 listns.py 
</code></pre>

<p>Error:</p>

<blockquote>
  <p>WARNING Retrying (Retry(total=0, connect=None, read=None,
  redirect=None, status=None)) after connection broken by
  'NewConnectionError(': Failed to establish a new connection:
  [Errno 110] Connection timed out',)': /api/v1/namespaces?limit=1</p>
  
  <p>Exception when calling CoreV1Api->list_namespace:
  HTTPSConnectionPool(host='XXXXXXX', port=xxxx): Max retries exceeded
  with url: /api/v1/namespaces?limit=1 (Caused by
  NewConnectionError(': Failed to establish a new connection: [Errno 110]
  Connection timed out',))</p>
</blockquote>
",4,1554976524,python;python-3.x;kubernetes;jenkins-pipeline;urllib3,False,927,1,1555409479,https://stackoverflow.com/questions/55629679/k8s-api-connection-time-out-in-jenkins-pipeline
52860913,pip install urllib3 hanging on &quot;Caching due to etag&quot;,"<p>Pip install of urllib3 is hanging on ""Caching due to etag"". I'm building an AWS chalice project that doesn't let you specify --no-cache-dir, so I need to fix the issue without that command. Any ideas? </p>

<p>Using Python 3.6.5 and Pip 10.0.1 in a virtual environment.</p>

<pre><code>(partnerdb-virtualenv) C:\Windows\SysWOW64\partnerdb-project&gt;pip install urllib3 -vvv
Config variable 'Py_DEBUG' is unset, Python ABI tag may be incorrect
Config variable 'WITH_PYMALLOC' is unset, Python ABI tag may be incorrect
Created temporary directory: C:\Users\Matt\AppData\Local\Temp\pip-ephem-wheel-cache-v0e8ikpl
Created temporary directory: C:\Users\Matt\AppData\Local\Temp\pip-req-tracker-d58lw_h5
Created requirements tracker 'C:\\Users\\Matt\\AppData\\Local\\Temp\\pip-req-tracker-d58lw_h5'
Created temporary directory: C:\Users\Matt\AppData\Local\Temp\pip-install-6qpv92ms
Collecting urllib3
  1 location(s) to search for versions of urllib3:
  * https://pypi.org/simple/urllib3/
  Getting page https://pypi.org/simple/urllib3/
  Looking up ""https://pypi.org/simple/urllib3/"" in the cache
  Request header has ""max_age"" as 0, cache bypassed
  Starting new HTTPS connection (1): pypi.org:443
  https://pypi.org:443 ""GET /simple/urllib3/ HTTP/1.1"" 200 6330
  Updating cache with response from ""https://pypi.org/simple/urllib3/""
  Caching due to etag
</code></pre>

<p>Edit: Based on one of the comments, <code>curl -vvv https://files/pythonhosted.org/</code> returns:</p>

<pre><code>*   Trying 2a04:4e42:2d::319...
* TCP_NODELAY set
* Connected to files.pythonhosted.org (2a04:4e42:2d::319) port 443 (#0)
* schannel: SSL/TLS connection with files.pythonhosted.org port 443 (step 1/3)
* schannel: checking server certificate revocation
* schannel: sending initial handshake data: sending 187 bytes...
* schannel: sent initial handshake data: sent 187 bytes
* schannel: SSL/TLS connection with files.pythonhosted.org port 443 (step 2/3)
* schannel: failed to receive handshake, need more data
* schannel: SSL/TLS connection with files.pythonhosted.org port 443 (step 2/3)
* schannel: encrypted data got 4096
* schannel: encrypted data buffer: offset 4096 length 4096
* schannel: encrypted data length: 4018
* schannel: encrypted data buffer: offset 4018 length 4096
* schannel: received incomplete message, need more data
* schannel: SSL/TLS connection with files.pythonhosted.org port 443 (step 2/3)
* schannel: encrypted data got 1024
* schannel: encrypted data buffer: offset 5042 length 5042
* schannel: encrypted data length: 815
* schannel: encrypted data buffer: offset 815 length 5042
* schannel: received incomplete message, need more data
* schannel: SSL/TLS connection with files.pythonhosted.org port 443 (step 2/3)
* schannel: encrypted data got 1051
* schannel: encrypted data buffer: offset 1866 length 5042
* schannel: sending next handshake data: sending 93 bytes...
* schannel: SSL/TLS connection with files.pythonhosted.org port 443 (step 2/3)
* schannel: encrypted data got 258
* schannel: encrypted data buffer: offset 258 length 5042
* schannel: SSL/TLS handshake complete
* schannel: SSL/TLS connection with files.pythonhosted.org port 443 (step 3/3)
* schannel: stored credential handle in session cache
&gt; GET / HTTP/1.1
&gt; Host: files.pythonhosted.org
&gt; User-Agent: curl/7.55.1
&gt; Accept: */*
&gt;
* schannel: client wants to read 102400 bytes
* schannel: encdata_buffer resized 103424
* schannel: encrypted data buffer: offset 0 length 103424
* schannel: encrypted data got 2412
* schannel: encrypted data buffer: offset 2412 length 103424
* schannel: decrypted data length: 2383
* schannel: decrypted data added: 2383
* schannel: decrypted data cached: offset 2383 length 102400
* schannel: encrypted data buffer: offset 0 length 103424
* schannel: decrypted data buffer: offset 2383 length 102400
* schannel: schannel_recv cleanup
* schannel: decrypted data returned 2383
* schannel: decrypted data buffer: offset 0 length 102400
&lt; HTTP/1.1 200 OK
&lt; Content-Type: text/html
&lt; Server: nginx/1.13.9
&lt; Content-Length: 1822
&lt; Accept-Ranges: bytes
&lt; Date: Thu, 18 Oct 2018 18:18:01 GMT
&lt; Age: 0
&lt; Connection: keep-alive
&lt; X-Served-By: cache-iad2138-IAD, cache-pao17439-PAO
&lt; X-Cache: HIT, MISS
&lt; X-Cache-Hits: 1, 0
&lt; X-Timer: S1539886682.749014,VS0,VE70
&lt; Vary: Accept-Encoding
&lt; Strict-Transport-Security: max-age=31536000; includeSubDomains; preload
&lt; X-Frame-Options: deny
&lt; X-XSS-Protection: 1; mode=block
&lt; X-Content-Type-Options: nosniff
&lt; X-Permitted-Cross-Domain-Policies: none
&lt; X-Robots-Header: noindex
&lt;
&lt;html&gt; ... &lt;/html&gt;
* Connection #0 to host files.pythonhosted.org left intact
</code></pre>
",7,1539798879,python;pip;python-requests;urllib3;chalice,True,1992,2,1554998442,https://stackoverflow.com/questions/52860913/pip-install-urllib3-hanging-on-caching-due-to-etag
55494694,The contents of a DIV tag come empty when scraping with BeautifulSoup,"<p>I'm trying to scrape a webpage that contains a table of test results using Python and BeautifulSoup, At this point I don't mind if its just raw html/un parsed data. </p>

<p>There is a table of results all contained within a parent DIV tag called 'test-view-grid-area'.</p>

<p>I got the class of name of the DIV tag from inspecting the webpage within chrome, and when viewing source of webpage its definitely correct, but when I run the below code, my results come back as:</p>

<pre><code>[&lt;div class=""test-view-grid-area""&gt;&lt;/div&gt;]
</code></pre>

<p>So it appears to be finding the tag but not returning its contents? I am not sure what I need to do to get the contents of the DIV class returned.</p>

<pre><code>from bs4 import BeautifulSoup
import urllib3
http = urllib3.PoolManager()
url = '[url of server / webpage]')
response = http.request('GET', url, headers=headers)
soup = BeautifulSoup (response.data, 'html.parser')
grid_data = soup.find_all(""div"", class_=""test-view-grid-area"")
print(grid_data)
</code></pre>

<p>Edit: I've gotten a little further, I am now getting the following response directly from the script tag that returns a JSON string:</p>

<pre><code>[&lt;script class=""__allSuitesOfSelectedPlan"" defer=""defer"" type=""application/json""&gt;
{""selectedOutcome"":"""",""selectedTester"":{""displayName"" &lt;etc&gt;}&lt;/script&gt;]
</code></pre>

<p>So next now I am trying to figure out how to do some regex to create my search pattern for everything between {}, then run that pattern against my initial data scrape, and then load the json string into a object.</p>
",0,1554292951,python;web-scraping;beautifulsoup;urllib3,False,1038,0,1554302741,https://stackoverflow.com/questions/55494694/the-contents-of-a-div-tag-come-empty-when-scraping-with-beautifulsoup
55491389,Is there a way for finding details about the sent request from the Python urllib3 response object?,"<p>I'm migrating a Python tool that sends HTTP requests using requests module to another whose APIs use urllib3. Most of the migration is transparent, but I'm having trouble to get information about the sent request.</p>

<p>When using requests module, I just retrieve that information by getting the PreparedRequest object from the response. E.g.: </p>

<pre><code>import requests

... (response generated)

request_method = response.request.method
</code></pre>

<p>However, urllib3 response objects don't seem to have an attribute/method to get the request object. Is there a way to retrieve that information from the urllib3 response?</p>
",1,1554282829,python;http;urllib3,False,44,0,1554282829,https://stackoverflow.com/questions/55491389/is-there-a-way-for-finding-details-about-the-sent-request-from-the-python-urllib
55375372,Why it is possible to suppress InsecureRequestWarning from within the code with PyCharm and not from a shell,"<p>I am using <a href=""https://github.com/kubernetes-client/python"" rel=""nofollow noreferrer"">kuberentes-python client</a> to send queries to Kubernetes cluster.  </p>

<p>I am using only a bearer token without certificate to send the API requests. Therefore, the connection cannot be verified - insecure connection and I am getting warnings:  </p>

<pre><code>/usr/local/lib/python3.5/dist-packages/urllib3/connectionpool.py:857: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
</code></pre>

<p>I know how to suppress these messages and there is a <a href=""https://stackoverflow.com/questions/27981545/suppress-insecurerequestwarning-unverified-https-request-is-being-made-in-pytho"">way to do it</a>.  </p>

<p>I added in the code the following lines:  </p>

<pre><code>import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
</code></pre>

<p>It worked !  I don't get the warnings anymore in PyCharm.<br>
But when I ran the python code from the shell <code>python3 myapp.py</code>, the warning were still exist.  </p>

<p>I have a workaround for that: <code>export PYTHONWARNINGS=""ignore:Unverified HTTPS request""</code> but it works only if you run it from the shell before running the script and I preferred it to be from the code.<br>
My questions is <strong>why the warnings were suppressed with PyCharm but not from the shell</strong> ?</p>

<p>This is my code if you want to test it (just change the host and token):  </p>

<pre><code>from kubernetes import client
import os
from kubernetes.client import Configuration

SERVICE_TOKEN_FILENAME = ""/home/ubuntu/root-token""


class InClusterConfigLoader(object):

    def __init__(self, token_filename):
        self._token_filename = token_filename

    def load_and_set(self):
        self._load_config()
        self._set_config()

    def _load_config(self):
        self.host = ""https://10.0.62.0:8443""

        if not os.path.isfile(self._token_filename):
            print(""Service token file does not exists."")

        with open(self._token_filename) as f:
            self.token = f.read().rstrip('\n')
            if not self.token:
                print(""Token file exists but empty."")


    def _set_config(self):
        configuration = Configuration()
        configuration.host = self.host
        configuration.ssl_ca_cert = None
        configuration.verify_ssl = False
        configuration.api_key['authorization'] = ""bearer "" + self.token
        Configuration.set_default(configuration)

def load_incluster_config():
    InClusterConfigLoader(token_filename=SERVICE_TOKEN_FILENAME).load_and_set()

import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

load_incluster_config()

api = client.CoreV1Api()
service_accounts = api.list_service_account_for_all_namespaces()

print(len(service_accounts.items))
</code></pre>
",4,1553683708,python;kubernetes;pycharm;suppress-warnings;urllib3,False,2443,0,1553684086,https://stackoverflow.com/questions/55375372/why-it-is-possible-to-suppress-insecurerequestwarning-from-within-the-code-with
55354523,How to detect a disconnect in python-requests keep alive connection?,"<p>I am using python-requests to connect to a live feed.
After I connect, I use iter_lines to go over the data as it arrives.
I am trying to detect a disconnect when it happens, currently without success.</p>

<p>The way I detect disconnects currently, is by waiting a maximum of 30 seconds for a new line to arrive, and if it doesn't, I restart the connection. The problem is that I miss the data that arrives within these 30 seconds.</p>

<p>As a test, I tried turning off my wifi in the middle and seeing if anything (like Response.status_code) changes, but it does not.</p>

<p>Example code:</p>

<pre><code>r = requests.get(url, stream=True)

while True:
     time.sleep(1)
     print(r.status_code)
</code></pre>

<p>I expected the print to show 200 when connected and SOME_SORT_OF_ERROR when I turn the wifi off, but I keep on seeing 200.</p>

<p>Does anyone know of a way to detect the disconnect?</p>
",0,1553595117,python;python-requests;keep-alive;urllib3,False,1283,1,1553598229,https://stackoverflow.com/questions/55354523/how-to-detect-a-disconnect-in-python-requests-keep-alive-connection
55285021,beautiful soup vs selenium vs urllib,"<p>I am working on a web automation project. I need to be able to pull pages, assess data, and be able to interact with the page (e.g. login, enter values, and post to the site.) As a derivative of the logins, I think I will need something that will allow me to remain logged in given a credential (e.g. store the credential or cookies.)</p>

<p>I've already used UrlLib &amp; Requests libraries to pull files and the pages themselves.</p>

<p>I am trying to decide on the best Python library for the task.</p>

<p>Any suggestions would be highly appreciated.</p>

<p>thank you!</p>
",2,1553185484,python;beautifulsoup;urllib3;python-requests-html,True,2057,1,1553215934,https://stackoverflow.com/questions/55285021/beautiful-soup-vs-selenium-vs-urllib
55249775,python requests returns different status codes locally/production,"<p>all.
trying to access a foo.com using python requests library from the django application.</p>

<p>i'm using same versions of all dependencies locally and in production. django application is hosted on elastic beanstalk.
locally requests to foo.com returns status code 200 . while production request to the same page is always 403 . 
also , production requests to bar.com returns 200 . 
there is nothing in the aws EB logs.</p>

<p>where should i look for clues for this issue?</p>

<p>python3. requests 2.20.1.</p>

<p>****//**** 
production request/response  403
****//****</p>

<p>{'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36', 'accept-encoding': 'gzip, deflate, br', 'ACCEPT': 'text/html,application/json,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,<em>/</em>;q=0.8', 'Connection': 'keep-alive', 'accept-language': 'en-US,en;q=0.9', 'cache-control': 'max-age=0'}</p>

<p>{'Date': 'Tue, 19 Mar 2019 21:22:49 GMT', 'Set-Cookie': 'BIGipServer~blpart~prod_cag_int_pl=1729895340.36895.0000; path=/; Httponly; Secure, TS0146ef81=018311fb76135815ac1df572d8f2d1ea5aa4467189900a3c4bd271f978754ddcfc965e0f553f77c07b7d60700425d031c96db5abf8ed876c1f62e240755ba80603a146246f; Path=/', 'Transfer-Encoding': 'chunked'}</p>

<p>****//****
local request/response  200
****//****</p>

<p>{'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36', 'accept-encoding': 'gzip, deflate, br', 'ACCEPT': 'text/html,application/json,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,<em>/</em>;q=0.8', 'Connection': 'keep-alive', 'accept-language': 'en-US,en;q=0.9', 'cache-control': 'max-age=0'}</p>

<p>{'Date': 'Tue, 19 Mar 2019 21:21:42 GMT', 'Content-Type': 'application/json;charset=UTF-8', 'Content-Length': '164', 'Connection': 'keep-alive', 'Content-Encoding': 'gzip', 'Server': ‘foo’, 'Transaction-Guid': '32f6c127-065c-48ae-90b5-360e41a6fe68', 'Vary': 'Accept-Encoding, Accept-Encoding', 'X-Application-Context': 'application:PROD,dc2-pod-bootstrap:8015'}</p>
",0,1553028685,python;django;python-requests;urllib;urllib3,False,87,0,1553031904,https://stackoverflow.com/questions/55249775/python-requests-returns-different-status-codes-locally-production
55235461,Python script not working with django shell,"<p>i have written script to <code>create users</code> using csv file. i have to check the <code>sso username</code> is valid or not using an api request as well and its written in <code>_validate_user</code> function. To make the api request i'm using <code>urllib3</code>. Sample code provided below.</p>

<pre><code>import csv
import urllib3.request
from django.contrib.auth.models import User

def _validate_user(sso_username):
    http = urllib3.PoolManager()
    fl_name = '&lt;url&gt;' + sso_username
    site_data = http.request('GET', fl_name)
    _data = site_data.data
    print(_data)

FILENAME = r'./csv/usernames.csv'

with atomic():
    with open(FILENAME, 'rt',  encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        for i, row in enumerate(reader):
            user_name = row['username']
            if _validate_user(user_name):
                User.objects.get_or_create(
                    username=row['username'],
                    password='password',
                )
                print(""User added"")
            else:
                print(""Invaid user"")
</code></pre>

<p>when i run the code using <code>python3 manage.py shell</code> and input code line by line i'm not getting any error and everything is working as expected.</p>

<p>When i use <code>python3 manage.py shell &lt; utility_folder/load_users.py</code> to run the script i'm getting <code>NameError: name 'urllib3' is not defined at line number 6</code>. What am i missing here. I tried with <code>requests</code> module also didn't do much help.</p>

<pre><code>Django version is 1.11 and python 3.6
</code></pre>

<p>Please advice.</p>
",3,1552979831,python;django;nameerror;urllib3;django-shell,True,2418,1,1552985372,https://stackoverflow.com/questions/55235461/python-script-not-working-with-django-shell
53141697,"Do we need to import the requests lib to use requests if we have already imported urllib and urllib3? - urllibx, request confusion","<p>I need to use 'requests' and a function from <code>urllib3</code>. In the code<br>
<a href=""https://i.stack.imgur.com/HUEXk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HUEXk.png"" alt=""code importing requests and printing dir(urllib3)""></a><br>
you can see the <code>requests</code> library is being imported, but at the same time it is a module/package inside <code>urllib3</code>, which has already been imported too. </p>

<p>Doing some research I've found that Python comes with the <code>urllib</code> package, that comes with the <code>request</code> module. On the other hand, <code>requests</code> is a module inside <code>urllib3</code>, but it is a library on its own.</p>

<p><code>urllib</code> and <code>urllib2</code> are standard Python librares, but <code>urllib3</code> is a completely separated library with a confusing name. A portion of it has been included in the standard library and <code>requests</code> depends on it, but it is not a newer version of <code>urllib</code>/<code>urllib2</code>; the library that actually wants to improve is <code>httplib</code> (ref: <a href=""https://github.com/urllib3/urllib3/issues/1065"" rel=""nofollow noreferrer"">Github</a>). <blockquote>""Under the hood, <em>requests</em> uses <em>urllib3</em> to do most of the http heavy lifting. When used properly, it should be mostly the same unless you need more advanced configuration"" </blockquote> (ref: <a href=""https://stackoverflow.com/questions/36937110/what-is-the-practical-difference-between-these-two-ways-of-making-web-connection"">Stackexchange</a>):</p>

<p>I got to these conclusions but I'm still confused: if I have already imported <code>urllib</code>, do I still need to import <code>requests</code>? What if I had imported <code>urllib3</code>?  </p>

<p>Also, should <code>requests</code> be imported separately, as in the depicted code, or should it be import imported from one of the mentioned libraries?</p>
",-3,1541340558,python;python-3.x;python-requests;urllib;urllib3,True,2552,1,1551930553,https://stackoverflow.com/questions/53141697/do-we-need-to-import-the-requests-lib-to-use-requests-if-we-have-already-importe
55005713,View 404 response code not visible in Python test,"<p>I've created a test for my Python code where I do a PATCH operation using a non existant bucket. I expect a 404 response but instead get this:</p>

<pre><code>HTTPSConnectionPool(host='config.storage.cloud.com', port=443): Max retries exceeded with url: /v1/foo (Caused by NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x10ffe8c90&gt;: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',))
</code></pre>

<p>The response kinda makes sense but is there any way I can view the response code? I assume if there is it will be through urllib3 in some way.</p>
",0,1551798027,python;testing;http-status-code-404;urllib3,False,123,1,1551803384,https://stackoverflow.com/questions/55005713/view-404-response-code-not-visible-in-python-test
49615277,"Python Request module, getting SSLError even with verify=False","<p>When I tried to get the web page with Python Requests module for the first time on Elementary OS, I faced with SSLError. There is simple solution:</p>

<p>Python 3, trying:</p>

<pre><code>import requests
page = requests.get('https://api.github.com/events')
</code></pre>

<p>Getting SSLError:</p>

<pre><code>/usr/local/bin/python3 /home/led/PycharmProjects/urllib_p/urllib_p.py
Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/requests/adapters.py"", line 376, in send
    timeout=timeout
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 549, in urlopen
    conn = self._get_conn(timeout=pool_timeout)
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 251, in _get_conn
    return conn or self._new_conn()
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 764, in _new_conn
    raise SSLError(""Can't connect to HTTPS URL because the SSL ""
requests.packages.urllib3.exceptions.SSLError: Can't connect to HTTPS URL because the SSL module is not available.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/led/PycharmProjects/urllib_p/urllib_p.py"", line 5, in &lt;module&gt;
    page = requests.get('https://api.github.com/events')
  File ""/usr/lib/python3/dist-packages/requests/api.py"", line 67, in get
    return request('get', url, params=params, **kwargs)
  File ""/usr/lib/python3/dist-packages/requests/api.py"", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/usr/lib/python3/dist-packages/requests/sessions.py"", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/lib/python3/dist-packages/requests/sessions.py"", line 576, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/lib/python3/dist-packages/requests/adapters.py"", line 447, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: Can't connect to HTTPS URL because the SSL module is not available.
</code></pre>

<p>verify=False make no sense (same Error)</p>
",0,1522687883,python;ssl;https;python-requests;urllib3,False,919,2,1551454996,https://stackoverflow.com/questions/49615277/python-request-module-getting-sslerror-even-with-verify-false
54856587,ReadTimoutError with opening multiple webdrivers in selenium,"<p>My problem arises when multiple instances of the selenium webdriver get launched. I tried several things like changing the changing the method of requesting and going with and without headless, however the problem still remains. My program tries to parallelize the selenium webdriver and automate web interaction. Could somebody please help me to resolve this issue, either by handling the error or changing the code so that the error does not occur anymore. Thanks in advance. </p>

<pre><code>    if url:
        options = Options()
        # options.headless = True
        options.set_preference('dom.block_multiple_popups', False)
        options.set_preference('dom.popup_maximum', 100000000)
        driver = webdriver.Firefox(options=options)
        driver.set_page_load_timeout(30)

    pac = dict()

    try:
        # driver.get(url)
        # driver.execute_script('''window.location.href = '{0}';'''.format(url))
        driver.execute_script('''window.location.replace('{0}');'''.format(url))
        WebDriverWait(driver, 1000).until(lambda x: self.onload(pac, driver))
        pac['code'] = 200
    except ReadTimeoutError as ex:
        pac['code'] = 404
        print(""Exception has been thrown. "" + str(ex))

    return pac
</code></pre>

<p>urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='127.0.0.1', port=61322): Read timed out. (read timeout=)</p>
",0,1551042618,python;selenium;selenium-webdriver;urllib3,False,792,0,1551042618,https://stackoverflow.com/questions/54856587/readtimouterror-with-opening-multiple-webdrivers-in-selenium
54778215,Download files from a website using python,"<p>I am new to Python and I have a requirement to download multiple <code>csv</code>-files from a website authenticated using <code>username</code> and <code>password</code>.</p>

<p>I wrote the below piece of code to download a single file but unfortunately the contents in the downloaded file are not same as in the original file.</p>

<p>Could you please let me know what I am doing wrong here and how to achieve this.</p>

<pre><code>import requests
import shutil
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
url=""https:xxxxxxxxxxxxxxxxxxxx.aspx/20-02-2019 124316CampaignExport.csv"" 
r = requests.get(url, auth=('username', 'Password'), 
verify=False,stream=True)
r.raw.decode_content = True
with open(""D:/20-02-2019 124316CampaignExport.csv"", 'wb') as f:
    shutil.copyfileobj(r.raw, f) 
</code></pre>
",-1,1550632169,python;python-requests;shutil;urllib3,True,158,1,1550639318,https://stackoverflow.com/questions/54778215/download-files-from-a-website-using-python
54734382,Why does my web scraper not convert the URL&#39;S properly from a CSV for downloading?,"<p>I'm running a scraper that takes all url's from images that it can find from r/dankmemes on reddit and then converting it to a list, lastly it tries to download these files, but for some reason an error occures. Can someone please explain what I'm doing wrong, I'm new to python.</p>

<p>The trace back error goes back to (""line38""): <code>urllib.request.urlretrieve(image[0],'/Users/CENSORED/Desktop/Instagrammemes/image_' + str(img_count) + "".jpg"")</code></p>

<p>The Error Message:</p>

<pre><code>/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
Traceback (most recent call last):
  File ""/Users/CENSORED/Desktop/FirstImages/scraper.py"", line 38, in &lt;module&gt;
    urllib.request.urlretrieve(image[0],'/Users/CENSORED/Desktop/Instagrammemes/image_' + str(img_count) + "".jpg"")
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 247, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 222, in urlopen
    return opener.open(url, data, timeout)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 510, in open
    req = Request(fullurl, data)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 328, in __init__
    self.full_url = url
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 354, in full_url
    self._parse()
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 383, in _parse
    raise ValueError(""unknown url type: %r"" % self.full_url)
ValueError: unknown url type: 'h'
</code></pre>

<p>The code that I think is causing the problem:</p>

<pre><code>with open('/Users/CENSORED/Desktop/FirstImages/file.csv') as images :
    images = csv.reader(images)
    img_count = 1
    for image in images:
        image = url.strip('\'""')
        urllib.parse.quote(':')
        urllib.request.urlretrieve(image[0],'/Users/CENSORED/Desktop/Instagrammemes/image_' + str(img_count) + "".jpg"")
        img_count += 1
</code></pre>

<p>The text file: </p>

<pre><code>    ['https://a.thumbs.redditmedia.com/JkyImC_zyl4XzE_yW-G4KOUTTFB6MRHUR3eEHvrpq64.png', 
'https://www.redditstatic.com/desktop2x/img/renderTimingPixel.png', 'https://www.redditstatic.com/desktop2x/img/gold/badges/award-silver-cartoon.png',
 'https://www.redditstatic.com/desktop2x/img/renderTimingPixel.png', 
'https://www.redditstatic.com/desktop2x/img/renderTimingPixel.png', 
'https://preview.redd.it/i6sdyng7n3h21.jpg?
width=640&amp;crop=smart&amp;auto=webp&amp;s=1abb4b30f2b74f114f2743cf66bf3d0e7f618abf', 
'https://www.redditstatic.com/desktop2x/img/renderTimingPixel.png', 
'https://i.redd.it/m9q2841su3h21.jpg', 
'https://www.redditstatic.com/desktop2x/img/renderTimingPixel.png', 
'https://i.redd.it/tsp8qpamc3h21.png', 
'https://www.redditstatic.com/desktop2x/img/gold/badges/award-silver-cartoon.png',
 'https://www.redditstatic.com/desktop2x/img/renderTimingPixel.png', 
'https://external-preview.redd.it/Ho2XSQOhaHGN3LhkLnPAf2OTkXwtuBTKQ9FXgdumH-I.jpg?
width=640&amp;crop=smart&amp;auto=webp&amp;s=54356f6b63ea9f51953f6a42d6c77fa4bf47df44', 
'https://www.redditstatic.com/desktop2x/img/renderTimingPixel.png', 
'https://preview.redd.it/9j8389cno3h21.jpg?
width=640&amp;crop=smart&amp;auto=webp&amp;s=23c0ef3307b8b8ebdc7c4bcc3d16837ad58e460a', 
'https://www.redditstatic.com/desktop2x/img/renderTimingPixel.png', 
'https://preview.redd.it/up1ouzug13h21.jpg?
width=640&amp;crop=smart&amp;auto=webp&amp;s=584bb8c90056156c3d2483d6f4b1030f7bf4e27d', 'https://a.thumbs.redditmedia.com/JkyImC_zyl4XzE_yW-G4KOUTTFB6MRHUR3eEHvrpq64.png',
 'https://styles.redditmedia.com/t5_2zmfe/styles/image_widget_3xmxw4p2gqu01.png', 
'https://b.thumbs.redditmedia.com/aRUO-zIbXgMTDVJOcxKjY8P6rGkakMdyVXn4k1VN-Mk.png', 'https://b.thumbs.redditmedia.com/iL0Rq5QLIS6xVLwoYKL8na6ZaSa9tILrBbhBlMfjVdI.png', 'https://b.thumbs.redditmedia.com/9aAIqRjSQwF2C7Xohx1u2Q8nAUqmUsHqdYtAlhQZsgE.png', 
'https://b.thumbs.redditmedia.com/voAwqXNBDO4JwIODmO4HXXkUJbnVo_mL_bENHeagDNo.png']
</code></pre>
",0,1550414803,python;python-3.x;csv;beautifulsoup;urllib3,True,189,1,1550425523,https://stackoverflow.com/questions/54734382/why-does-my-web-scraper-not-convert-the-urls-properly-from-a-csv-for-downloadin
53130365,"Looking for a more efficient/pythonic way to sum tuples in a list, and compute an average","<p>I am trying to do some basic computations with data from the web. For this cause, I have found some code that extracts begin and end years for Rembrandt works. It saves it in a list</p>

<pre><code>date_list =[(work['datebegin'], work['dateend']) for work in `rembrandt2_parsed['records']]`
</code></pre>

<p>date_list is a list containing the tuples with begin and end years for some Rembrandt works in the Harvard Art Museum. For the sake of completeness, it looks like this:</p>

<pre><code>[(0, 0), (1648, 1648), (1637, 1647), (1626, 1636), (0, 0), (1638, 1638), (1635, 1635), (1634, 1634), (0, 0), (0, 0)]
</code></pre>

<p>Now I want to do some basic computations, <strong>I want to sum over this list of tuples, and compute the average of the years when they are not null</strong>. I came up with a solution:</p>

<pre><code>datebegin =0
date_end =0
count_begin =0
count_end =0

for x, y in date_list:
    if x !=0:
        datebegin +=x
        count_begin +=1
    if y != 0:
        date_end +=y
        count_end +=1

final_date_begin = datebegin/count_begin #value = year 1636
final_date_end = date_end/count_end #value = year 1639
</code></pre>

<p>But I think this can be done much more efficient/pythonic. In the first place because I seem to need a lot of code for such a simple task, and in the second place because I need to initialize 4(!) global vars if I do it in this way. <strong>Could someone enlighten me and show me a more efficient way to solve this?</strong> </p>
",0,1541240310,python;list;for-loop;tuples;urllib3,True,56,4,1545924747,https://stackoverflow.com/questions/53130365/looking-for-a-more-efficient-pythonic-way-to-sum-tuples-in-a-list-and-compute-a
53874593,"Incomplete parsing of Pinterest using urllib, requests and selenium","<p>I have tried parsing the following Pinterest page using urllib, requests, and chromedriver:</p>

<p><a href=""https://www.pinterest.com/pin/463237511669606028/"" rel=""nofollow noreferrer"">https://www.pinterest.com/pin/463237511669606028/</a></p>

<p>But it looks like some sections of the page are missing in my result. Specifically, I'm trying to parse the number of re-pins (below the comments), which I can't.</p>

<p>I have tried both of these options but userActivity class is not part of what I get:</p>

<pre><code>driver.get(""https://www.pinterest.com/pin/463237511669606028/"")
html = driver.page_source
soup = BeautifulSoup(html, features=""html.parser"") 
</code></pre>

<p>and </p>

<pre><code>req = urllib2.Request(""https://www.pinterest.com/pin/463237511669606028/"", 
headers={'User-Agent': ""PyBrowser""}) 
con = urllib2.urlopen(req)
content = con.read()
soup = BeautifulSoup(content,features=""html.parser"")
</code></pre>

<p>Any ideas?</p>
",0,1545332732,python;selenium;selenium-chromedriver;pinterest;urllib3,False,331,0,1545332732,https://stackoverflow.com/questions/53874593/incomplete-parsing-of-pinterest-using-urllib-requests-and-selenium
53870767,Turning off keep-alive,"<p>In older versions of Requests there was option to turn off keep alive. Something like this:</p>

<pre><code>s = requests.session()
s.config['keep_alive'] = False
</code></pre>

<p>I also find in other question that I can use header Connection: close  </p>

<pre><code>HTTP/1.1 defines the ""close"" connection option for the sender to signal 
that the connection will be closed after completion of the response. 
For example,
   Connection: close
in either the request or the response header fields indicates that the 
connection SHOULD NOT be considered `persistent' (section 8.1) after the 
current request/response is complete.
HTTP/1.1 applications that do not support persistent connections MUST 
include the ""close"" connection option in every message. 
</code></pre>

<p>If I understand this properly it doesn't mean that connection will be closed. It's only information that it shouldn't be considered as persistent one.</p>

<p>How to do it in recent versions? </p>
",1,1545316712,python;python-requests;keep-alive;urllib3,False,1944,0,1545316712,https://stackoverflow.com/questions/53870767/turning-off-keep-alive
53676622,"SSLError(&quot;bad handshake: Error([(&#39;SSL routines&#39;, &#39;tls_process_ske_dhe&#39;, &#39;dh key too small&#39; in Python","<p>I have seen a few links for this issue and most people want the server to be updated for security reasons.  I am looking to make an internal only tool and connect to a server that is not able to be modified.  My code is below and I am hopeful I can get clarity on how I can accept the small key and process the request. </p>

<p>Thank you all in advance</p>

<pre><code>import requests
from requests.auth import HTTPBasicAuth
import warnings
import urllib3

warnings.filterwarnings(""ignore"")
requests.packages.urllib3.disable_warnings()
requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += 'HIGH:!DH:!aNULL'
#requests.packages.urllib3.contrib.pyopenssl.DEFAULT_SSL_CIPHER_LIST += 'HIGH:!DH:!aNULL'

url = ""https://x.x.x.x/place/stuff""
userName = 'stuff'
passW = 'otherstuff'


dataR = requests.get(url,auth=HTTPBasicAuth(userName, passW),verify=False)
print(dataR.text)
</code></pre>
",1,1544215648,python;ssl;urllib3,True,4591,1,1544827770,https://stackoverflow.com/questions/53676622/sslerrorbad-handshake-errorssl-routines-tls-process-ske-dhe-dh-key
53735921,Webpage with a button to download with Python,"<p>I am learning Python and am attempting to automate a download of a file from a webpage. I have not tested it yet because I need to know if I am on the right track.  I expect to have the script perform what pressing the button on the webpage would do which is create a file and start a download of that file.</p>
<p>Below is the information from the page with the button:</p>
<blockquote>
<p>//input type=&quot;submit&quot; name=&quot;export&quot; value=&quot;Export&quot;//</p>
<p>//form name=&quot;export&quot; action=&quot;/cgi-bin/configuration.cgi&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot; id=&quot;export&quot;//</p>
</blockquote>
<pre><code>import urllib3
import requests

req = requests.post('&lt;IP Addy&gt;/cgi-bin/configuration.cgi', 'Export')
with open('file.data', 'wb') as file:
file.write(req.content)
</code></pre>
",0,1544587611,python;post;python-requests;urllib3,False,113,0,1544587611,https://stackoverflow.com/questions/53735921/webpage-with-a-button-to-download-with-python
53719469,Unshorten url in python 3,"<p>I am using this code for unshortening urls in python 3 , but the code returns the url as it is (shortened), so what should I do to get it unshortened?</p>

<pre><code>import requests
import http.client
import urllib.parse as urlparse   

def unshortenurl(url):
    parsed = urlparse.urlparse(url) 
    h = http.client.HTTPConnection(parsed.netloc) 
    h.request('HEAD', parsed.path) 
    response = h.getresponse() 
    if response.status/100 == 3 and response.getheader('Location'):
        return response.getheader('Location') 
    else: return url
</code></pre>
",-1,1544514145,python;python-3.x;url;urllib3;http.client,True,1127,1,1544514941,https://stackoverflow.com/questions/53719469/unshorten-url-in-python-3
53701062,Extract text only except the content of script tag from html with BeautifulSoup,"<p>I have html like this</p>

<pre><code>&lt;span class=""age""&gt;
    Ages 15
    &lt;span class=""loc"" id=""loc_loads1""&gt;
     &lt;/span&gt;
     &lt;script&gt;
        getCurrentLocationVal(""loc_loads1"",29.45218856,59.38139268,1);
     &lt;/script&gt;
&lt;/span&gt;
</code></pre>

<p>I am trying to extract <code>Age 15</code> using <code>BeautifulSoup</code></p>

<p>So i written python code as follows</p>

<p><strong>code:</strong></p>

<pre><code>from bs4 import BeautifulSoup as bs
import urllib3

URL = 'html file'

http = urllib3.PoolManager()

page = http.request('GET', URL)

soup = bs(page.data, 'html.parser')
age = soup.find(""span"", {""class"": ""age""})

print(age.text)
</code></pre>

<p><strong>output:</strong></p>

<pre><code>Age 15 getCurrentLocationVal(""loc_loads1"",29.45218856,59.38139268,1);
</code></pre>

<p>I want only <code>Age 15</code> not the function inside <code>script</code> tag. Is there any way to get only  text: <code>Age 15</code>? or any way to exclude the content of <code>script</code> tag?</p>

<blockquote>
  <p>PS: there are too many script tags and different URLS. I don't prefer
  replace text from the output.</p>
</blockquote>
",3,1544426151,python;python-3.x;beautifulsoup;urllib3,True,2032,2,1544428392,https://stackoverflow.com/questions/53701062/extract-text-only-except-the-content-of-script-tag-from-html-with-beautifulsoup
53662099,Choosing between Python HTTP clients urllib3 and requests,"<p>Python's built-in HTTP clients don't have many features, so even the Python docs recommend using requests. But there's also urllib3, which requests, itself uses, and they share some core developers, making me think they're more complementary than competing.</p>

<p>When would I use urllib3 instead of requests? What features does requests add on top of urllib3?</p>
",2,1544146735,python;python-requests;urllib3,False,501,1,1544150213,https://stackoverflow.com/questions/53662099/choosing-between-python-http-clients-urllib3-and-requests
53656247,slackclient OSError: [Errno 24] Too many open files,"<p>I add slackclient on my app to get regular notification.
The app is executed on AWS Linux and in a docker container (FROM python:3.6.6-stretch). </p>

<p>Whenever I tried to execute it over about 1000 loops but the process failed 
 just after 1000 loops and I got the following errors.</p>

<p>I already checked other cases on stackoverflow but it doesn't work for me. Hope that I can get some insight from the answer of yours. I personally want to know why this happens and solve the core problems, not just detour it.</p>

<p><strong>Error</strong></p>

<pre><code>Traceback (most recent call last):
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/urllib3/util/ssl_.py"", line 321, in ssl_wrap_socket
OSError: [Errno 24] Too many open files

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 343, in _make_request
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 839, in _validate_conn
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/urllib3/connection.py"", line 344, in connect
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/urllib3/util/ssl_.py"", line 323, in ssl_wrap_socket
urllib3.exceptions.SSLError: [Errno 24] Too many open files

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/urllib3/util/retry.py"", line 398, in increment
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='slack.com', port=443): Max retries exceeded with url: /api/chat.update (Caused by SSLError(OSError(24, 'Too many open files'),))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/scraping/notify.py"", line 50, in update
  File ""/home/scraping/notify.py"", line 70, in _api_call
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/slackclient/client.py"", line 184, in api_call
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/slackclient/server.py"", line 349, in api_call
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/slackclient/slackrequest.py"", line 84, in do
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/slackclient/slackrequest.py"", line 116, in post_http_request
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/requests/api.py"", line 116, in post
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/requests/api.py"", line 60, in request
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/requests/adapters.py"", line 514, in send
requests.exceptions.SSLError: HTTPSConnectionPool(host='slack.com', port=443): Max retries exceeded with url: /api/chat.update (Caused by SSLError(OSError(24, 'Too many open files'),))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/urllib3/connection.py"", line 159, in _new_conn
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/urllib3/util/connection.py"", line 57, in create_connection
  File ""/usr/local/lib/python3.6/socket.py"", line 745, in getaddrinfo
OSError: [Errno 24] Too many open files

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 343, in _make_request
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 839, in _validate_conn
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/urllib3/connection.py"", line 301, in connect
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/urllib3/connection.py"", line 168, in _new_conn
urllib3.exceptions.NewConnectionError: &lt;urllib3.connection.VerifiedHTTPSConnection object at 0x7ff2d9158048&gt;: Failed to establish a new connection: [Errno 24] Too many open files

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/urllib3/util/retry.py"", line 398, in increment
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='slack.com', port=443): Max retries exceeded with url: /api/chat.postMessage (Caused by NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x7ff2d9158048&gt;: Failed to establish a new connection: [Errno 24] Too many open files',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""main.py"", line 33, in &lt;module&gt;
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/fire/core.py"", line 127, in Fire
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/fire/core.py"", line 366, in _Fire
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/fire/core.py"", line 542, in _CallCallable
  File ""main.py"", line 29, in fasten
  File ""/home/scraping/tasks/apps_info.py"", line 199, in fasten
  File ""/home/scraping/pipe.py"", line 144, in _notify
  File ""/home/scraping/notify.py"", line 53, in update
  File ""/home/scraping/notify.py"", line 46, in resend
  File ""/home/scraping/notify.py"", line 70, in _api_call
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/slackclient/client.py"", line 184, in api_call
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/slackclient/server.py"", line 349, in api_call
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/slackclient/slackrequest.py"", line 84, in do
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/slackclient/slackrequest.py"", line 116, in post_http_request
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/requests/api.py"", line 116, in post
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/requests/api.py"", line 60, in request
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send
  File ""/root/.local/share/virtualenvs/home-oTyxYuqD/lib/python3.6/site-packages/requests/adapters.py"", line 516, in send
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='slack.com', port=443): Max retries exceeded with url: /api/chat.postMessage (Caused by NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x7ff2d9158048&gt;: Failed to establish a new connection: [Errno 24] Too many open files',))
</code></pre>

<p><strong>SlackClient Part</strong></p>

<pre><code>import errno
import time
import urllib3
from datetime import datetime
from typing import List, Text

import requests
from slackclient import SlackClient

import config
from .utils import get_utc_date


class SlackHelper:
    def __init__(self):
        self._date_format = ""%Y-%m-%d %H-%M-%S""
        self._sc = SlackClient(config.SLACK_TOKEN)
        self._channel = config.SLACK_CHANNEL
        self._user_name = config.SLACK_USERNAME
        self._icon_emoji = config.SLACK_MSG_EMOJI
        self._ts = None
        self._ch_encoded = None
        self._created_at = get_utc_date(_format=self._date_format)

    def send(self, *args, **kw):
        return self._api_call(*args, **kw)

    def resend(self, resend_cnt: int = 0, *args, **kw):
        try:
            self._ts = None
            kw['method'] = 'chat.postMessage'
        except (urllib3.exceptions.NewConnectionError,
                urllib3.exceptions.MaxRetryError,
                requests.exceptions.ConnectionError) as err:
            if resend_cnt &lt; 5:
                time.sleep(60)
                resend_cnt += 1
                self.resend(resend_cnt, *args, **kw)
            raise err
        return self._api_call(*args, **kw)

    def update(self, *args, **kw):
        try:
            return self._api_call(*args, **kw)
        except (requests.exceptions.ConnectionError,
                requests.exceptions.SSLError):
            self.resend(*args, **kw)
        except OSError as err:
            if err.errno == errno.EMFILE:
                return self.resend(*args, **kw)
            raise err

    def _api_call(self, title_prefix: Text, msg: Text,
                alarm: Text = 'good', method: Text = 'chat.postMessage'):
        attachments = self._get_attachments(title_prefix, msg, alarm)
        response = self._sc.api_call(
            method=method,
            channel=(
                self._ch_encoded if method == 'chat.update'
                else self._channel),
            attachments=[attachments],
            username=self._user_name,
            icon_emoji=self._icon_emoji,
            ts=self._ts if method == 'chat.update' else None)

        if self._check_retry(response):
            delay = int(response[""headers""][""Retry-After""])
            time.sleep(delay)
            return self._api_call(title_prefix, msg, alarm, method)
        if response[""ok""]:
            if not self._ts:
                self._ts = response.get('ts')
            self._ch_encoded = response.get('channel')
        return response.get('ok', True)

    @staticmethod
    def _check_retry(response):
        if response[""ok""] is False:
            if response[""headers""].get(""Retry-After"", False):
                delay = int(response[""headers""][""Retry-After""])
                if delay &lt; 60:
                    return True
        return False

    def _get_attachments(self, title_prefix, msg, alarm):
        now = get_utc_date(_format=self._date_format)
        return {
            ""title"": f""Created : {self._created_at}\n{title_prefix} : {now}"",
            ""text"": msg,
            ""mrkdwn_in"": [""text""],
            ""color"": f""{alarm}""
        }
</code></pre>

<p><strong>ENV</strong></p>

<pre><code>$ ulimit -a
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 15698
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 65535
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 15698
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
</code></pre>
",0,1544115363,python;linux;slack;urllib3;eoserror,False,1762,1,1544126066,https://stackoverflow.com/questions/53656247/slackclient-oserror-errno-24-too-many-open-files
53641068,"Connection pool is full, discarding connection with ThreadPoolExecutor and multiple headless browsers through Selenium and Python","<p>I'm writing some automation software using <code>selenium==3.141.0</code>, <code>python 3.6.7</code>, <code>chromedriver 2.44</code>.</p>

<p>Most of the the logic is ok to be executed by the single browser instance, but for some part i have to launch 10-20 instances to have a decent execution speed.</p>

<p>Once it comes to the part which is executed by <code>ThreadPoolExecutor</code>, browser interactions start throwing this error:</p>

<pre><code>WARNING|05/Dec/2018 17:33:11|connectionpool|_put_conn|274|Connection pool is full, discarding connection: 127.0.0.1
WARNING|05/Dec/2018 17:33:11|connectionpool|urlopen|662|Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))': /session/119df5b95710793a0421c13ec3a83847/url
WARNING|05/Dec/2018 17:33:11|connectionpool|urlopen|662|Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7fcee7ada048&gt;: Failed to establish a new connection: [Errno 111] Connection refused',)': /session/119df5b95710793a0421c13ec3a83847/url
</code></pre>

<p><strong>browser setup:</strong></p>

<pre><code>def init_chromedriver(cls):
    try:
        chrome_options = webdriver.ChromeOptions()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument(f""user-agent={Utils.get_random_browser_agent()}"")
        prefs = {""profile.managed_default_content_settings.images"": 2}
        chrome_options.add_experimental_option(""prefs"", prefs)

        driver = webdriver.Chrome(driver_paths['chrome'],
                                       chrome_options=chrome_options,
                                       service_args=['--verbose', f'--log-path={bundle_dir}/selenium/chromedriver.log'])
        driver.implicitly_wait(10)

        return driver
    except Exception as e:
        logger.error(e)
</code></pre>

<p><strong>relevant code:</strong></p>

<p><code>ProfileParser</code> instantiates a webdriver and execute a few page interactions. I suppose the interactions themselves are not relevant because everything works without <code>ThreadPoolExecutor</code>.
However, in short:</p>

<pre><code>class ProfileParser(object):
    def __init__(self, acc):
        self.driver = Utils.init_chromedriver()
    def __exit__(self, exc_type, exc_val, exc_tb):
        Utils.shutdown_chromedriver(self.driver)
        self.driver = None

    collect_user_info(post_url)
           self.driver.get(post_url)
           profile_url = self.driver.find_element_by_xpath('xpath_here')]').get_attribute('href')
</code></pre>

<p>While runs in <code>ThreadPoolExecutor</code>, the error above appears at this point <code>self.driver.find_element_by_xpath</code> or at <code>self.driver.get</code></p>

<p><strong>this is working:</strong></p>

<pre><code>with ProfileParser(acc) as pparser:
        pparser.collect_user_info(posts[0])
</code></pre>

<p><strong>these options are not working:</strong> (<code>connectionpool errors</code>)</p>

<pre><code>futures = []
#one worker, one future
with ThreadPoolExecutor(max_workers=1) as executor:
        with ProfileParser(acc) as pparser:
            futures.append(executor.submit(pparser.collect_user_info, posts[0]))

#10 workers, multiple futures
with ThreadPoolExecutor(max_workers=10) as executor:
    for p in posts:
        with ProfileParser(acc) as pparser:
            futures.append(executor.submit(pparser.collect_user_info, p))
</code></pre>

<p><strong>UPDATE:</strong></p>

<p>I found a temporal solution (which does not invalidate this initial question) - to instantiate a <code>webdriver</code> outside of <code>ProfileParser</code> class. Don't know why it works but the initial does not. I suppose the cause in some language specifics?
Thanks for answers, however it doesn't seem like the problem is with the <code>ThreadPoolExecutor</code> <code>max_workers</code> limit - as you see in one of the options i tried to submit a single instance and it is still didn't work.</p>

<p>current workaround:</p>

<pre><code>futures = []
with ThreadPoolExecutor(max_workers=10) as executor:
    for p in posts:
        driver = Utils.init_chromedriver()
        futures.append({
            'future': executor.submit(collect_user_info, driver, acc, p),
            'driver': driver
        })

for f in futures:
    f['future'].done()
    Utils.shutdown_chromedriver(f['driver'])
</code></pre>
",4,1544045490,python;selenium;threadpool;threadpoolexecutor;urllib3,True,18255,2,1544106845,https://stackoverflow.com/questions/53641068/connection-pool-is-full-discarding-connection-with-threadpoolexecutor-and-multi
53651862,requests does not retrieve html content from some websites,"<p>When trying to get the HTML content of a website, in this case, www.arrow.com, I get nothing, the web browser keeps waiting forever.</p>

<pre><code>import requests 

params = {'q': code}
url = ""https://www.arrow.com/en/products/search""
headers = {
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',
    'cache-control': ""no-cache"",
    'postman-token': ""564e5d76-282f-98f3-860b-d8e09e2e9073""
}
r = requests.get(url, headers=headers,params=params)
tree = html.fromstring(r.content)
</code></pre>

<p>The weird thing is that I can get the right content using Postman and accessing via web browser.</p>

<p>Postman uses this script when using HTTP:</p>

<pre><code>GET /en/products/search?q=cccccccc HTTP/1.1
Host: www.arrow.com
Cache-Control: no-cache
Postman-Token: c3821bb3-767b-b8c7-105a-84fd16291245
</code></pre>

<p>or with Python3:</p>

<pre><code>import http.client

conn = http.client.HTTPSConnection(""www.arrow.com"")

headers = {
    'cache-control': ""no-cache"",
    'postman-token': ""740c5681-3e67-b605-3040-964be3ea7296""
    }

conn.request(""GET"", ""/en/products/search?q=cccccccc"", headers=headers)

res = conn.getresponse()
data = res.read()


print(data.decode(""utf-8""))
</code></pre>

<p>Using the last one, I get also nothing.</p>
",1,1544100573,python;web-scraping;python-requests;urllib3,True,108,1,1544102082,https://stackoverflow.com/questions/53651862/requests-does-not-retrieve-html-content-from-some-websites
28099318,Seeing retry of a request sent using urllib3.PoolManager without retries configured,"<p>I have some python code that looks like the following:</p>

<pre><code>import urllib3
http = urllib3.PoolManager(cert_reqs='CERT_NONE')
...
full_url = 'https://[%s]:%d%s%s' % \
            (address, port, base_uri, relative_uri)
kwargs = {
    'headers': {
        'Host': '%s:%d' % (hostname, port)
    }
}

if data is not None:
    kwargs['body'] = json.dumps(data, indent=2, sort_keys=True)

# Directly use request_encode_url instead of request because requests
# will try to encode the body as 'multipart/form-data'.
response = http.request_encode_url('POST', full_url, **kwargs)
log.debug('Received response: HTTP status %d. Body: %s' %
          (response.status, repr(response.data)))
</code></pre>

<p>I have a log line that prints once prior to the code that issues the request, and the <code>log.debug('Received...')</code> line prints once. However, on the server side, I occasionally see two requests (they are both the same POST request that is sent by this code block), around 1-5 seconds apart. In such instances, the order of events is as follows:</p>

<ol>
<li>One request sent from python client</li>
<li>First request received</li>
<li>Second request received</li>
<li>First response sent with status 200 and an http entity indicating success</li>
<li>Second response sent with status 200 and http entity indicating failure</li>
<li>Python client receives the second reponse</li>
</ol>

<p>I tried to reproduce it reliably by sleeping in the server (guessing that there might be a timeout that causes a retry), but was unsuccessful. I believe the duplication is unlikely to be occurring on the server because it's just a basic Scala Spray server and haven't seen this with other clients. Looking at the source code for <code>PoolManager</code>, I can't find anywhere where retries would be included. There is a mechanism for retries  specified with an optional parameter, but this optional parameter is not being used in the code above.</p>

<p>Does anyone have any ideas where this extra request might be coming from?</p>

<p>EDIT: @shazow gave a pointer about <code>retries</code> having a default of 3, but I changed the code as suggested and got the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""my_file.py"", line 23, in &lt;module&gt;
    response = http.request_encode_url('GET', full_url, **kwargs)
  File ""/usr/lib/python2.7/dist-packages/urllib3/request.py"", line 88, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File ""/usr/lib/python2.7/dist-packages/urllib3/poolmanager.py"", line 145, in urlopen
    conn = self.connection_from_host(u.host, port=u.port, scheme=u.scheme)
  File ""/usr/lib/python2.7/dist-packages/urllib3/poolmanager.py"", line 119, in connection_from_host
    pool = self._new_pool(scheme, host, port)
  File ""/usr/lib/python2.7/dist-packages/urllib3/poolmanager.py"", line 86, in _new_pool
    return pool_cls(host, port, **kwargs)
TypeError: __init__() got an unexpected keyword argument 'retries'`
</code></pre>

<p>Edit #2: The following change to <code>kwargs</code> seems to work for me:</p>

<pre><code>import urllib3
http = urllib3.PoolManager(cert_reqs='CERT_NONE')
...
full_url = 'https://[%s]:%d%s%s' % \
            (address, port, base_uri, relative_uri)
kwargs = {
    'headers': {
        'Host': '%s:%d' % (hostname, port)
    },
    'retries': 0
}

if data is not None:
    kwargs['body'] = json.dumps(data, indent=2, sort_keys=True)

# Directly use request_encode_url instead of request because requests
# will try to encode the body as 'multipart/form-data'.
response = http.request_encode_url('POST', full_url, **kwargs)
log.debug('Received response: HTTP status %d. Body: %s' %
          (response.status, repr(response.data)))
</code></pre>
",2,1421963368,python;urllib3,True,2712,1,1543573614,https://stackoverflow.com/questions/28099318/seeing-retry-of-a-request-sent-using-urllib3-poolmanager-without-retries-configu
53095452,unable to connect to snowflake,"<p>I am trying to connect to snowflake from python. It's very simple straight forward, but unfortunately I'm unable to succeed. Same piece of code works in other machines (when my friends tested). Not sure what dependencies I'm missing. Tried debugging very hard, even tried with pipenv (assuming python path must be conflict) but no luck. I kindly request you to help and resolve this issue. Summarising below steps what i have done.</p>

<pre><code>sudo -H pip install pipenv    #installed pipenv

mkdir -p test_vn    #Created a directory

cd test_vn/

pipenv install requests     #Installed requests module
</code></pre>

<p>Since, it's a virtual , i checked the path where packages are installed.</p>

<pre><code>/Users/james/.local/share/virtualenvs/test_vn-mReMX2sP/lib/python2.7/site-packages

-rw-r--r--   1 james  staff   126 Nov  1 09:24 easy_install.py
-rw-r--r--   1 james  staff   367 Nov  1 09:24 easy_install.pyc
drwxr-xr-x  11 james  staff   374 Nov  1 09:24 setuptools-40.5.0.dist-info
drwxr-xr-x  69 james  staff  2346 Nov  1 09:24 setuptools
drwxr-xr-x   8 james  staff   272 Nov  1 09:24 pkg_resources
drwxr-xr-x   8 james  staff   272 Nov  1 09:24 pip
drwxr-xr-x   9 james  staff   306 Nov  1 09:24 wheel-0.32.2.dist-info
drwxr-xr-x  19 james  staff   646 Nov  1 09:24 wheel
drwxr-xr-x   9 james  staff   306 Nov  1 09:24 pip-18.1.dist-info
drwxr-xr-x   8 james  staff   272 Nov  1 09:24 idna-2.7.dist-info
drwxr-xr-x  18 james  staff   612 Nov  1 09:24 idna
drwxr-xr-x   8 james  staff   272 Nov  1 09:24 urllib3-1.24.dist-info
drwxr-xr-x  25 james  staff   850 Nov  1 09:24 urllib3
drwxr-xr-x  10 james  staff   340 Nov  1 09:24 requests-2.20.0.dist-info
drwxr-xr-x  38 james  staff  1292 Nov  1 09:24 requests
drwxr-xr-x  10 james  staff   340 Nov  1 09:24 chardet-3.0.4.dist-info
drwxr-xr-x  81 james  staff  2754 Nov  1 09:24 chardet
drwxr-xr-x  10 james  staff   340 Nov  1 09:24 certifi-2018.10.15.dist-info
drwxr-xr-x   9 james  staff   306 Nov  1 09:24 certifi
</code></pre>

<p>now, installing <code>pyOpenSSL</code> and <code>snowflake-connector</code> as mentioned below</p>

<pre><code>pipenv install pyOpenSSL       #Installed pyOpenSSL
pipenv install snowflake-connector-python     #Installed snowflake-connector
</code></pre>

<p>Now <code>/Users/james/.local/share/virtualenvs/test_vn-mReMX2sP/lib/python2.7/site-packages</code> has below
<a href=""https://i.stack.imgur.com/Xjvml.png"" rel=""nofollow noreferrer"">site-packages1</a>
<a href=""https://i.stack.imgur.com/hTeV0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hTeV0.png"" alt=""site-packages2""></a></p>

<p>created a file <code>test.py</code> under <code>/Users/james/test_vn</code></p>

<pre><code>import os
print os.__file__
import snowflake.connector
import logging
logging.basicConfig(
    filename='snowflake_python_connector.log',
    level=logging.DEBUG)

# Gets the version
ctx = snowflake.connector.connect(
    user='user-name',
    password='pass-word',
    account='test_account'
    )
cs = ctx.cursor()
try:
    cs.execute(""SELECT current_version()"")
    one_row = cs.fetchone()
    print(one_row[0])
finally:
    cs.close()
ctx.close()
</code></pre>

<p>when i execute the script, 
(test_vn) bash-3.2$ <strong>pipenv run python test.py</strong> </p>

<pre><code>/Users/james/Library/Python/2.7/lib/python/site-packages/pipenv/vendor/requirementslib/models/utils.py:19: UserWarning: Module pathlib2 was already imported from /Users/james/Library/Python/2.7/lib/python/site-packages/pipenv/vendor/pathlib2/__init__.pyc, but /usr/local/lib/python2.7/site-packages/pathlib2-2.3.2-py2.7.egg is being added to sys.path
  from pkg_resources import Requirement
/Users/james/Library/Python/2.7/lib/python/site-packages/pipenv/vendor/requirementslib/models/utils.py:19: UserWarning: Module attr was already imported from /Users/james/Library/Python/2.7/lib/python/site-packages/pipenv/vendor/attr/__init__.pyc, but /usr/local/lib/python2.7/site-packages/attrs-18.2.0-py2.7.egg is being added to sys.path
  from pkg_resources import Requirement
/Users/james/Library/Python/2.7/lib/python/site-packages/pipenv/vendor/requirementslib/models/utils.py:19: UserWarning: Module scandir was already imported from /Users/james/Library/Python/2.7/lib/python/site-packages/pipenv/vendor/scandir.pyc, but /usr/local/lib/python2.7/site-packages/scandir-1.9.0-py2.7-macosx-10.12-x86_64.egg is being added to sys.path
  from pkg_resources import Requirement
/Users/james/.local/share/virtualenvs/test_vn-mReMX2sP/lib/python2.7/os.py
Traceback (most recent call last):
  File ""test.py"", line 13, in &lt;module&gt;
    account='test_account'
  File ""/Users/james/.local/share/virtualenvs/test_vn-mReMX2sP/lib/python2.7/site-packages/snowflake/connector/__init__.py"", line 32, in Connect
    return SnowflakeConnection(**kwargs)
  File ""/Users/james/.local/share/virtualenvs/test_vn-mReMX2sP/lib/python2.7/site-packages/snowflake/connector/connection.py"", line 157, in __init__
    self.connect(**kwargs)
  File ""/Users/james/.local/share/virtualenvs/test_vn-mReMX2sP/lib/python2.7/site-packages/snowflake/connector/connection.py"", line 397, in connect
    self.__open_connection()
  File ""/Users/james/.local/share/virtualenvs/test_vn-mReMX2sP/lib/python2.7/site-packages/snowflake/connector/connection.py"", line 595, in __open_connection
    self.__authenticate(auth_instance)
  File ""/Users/james/.local/share/virtualenvs/test_vn-mReMX2sP/lib/python2.7/site-packages/snowflake/connector/connection.py"", line 818, in __authenticate
    session_parameters=self._session_parameters,
  File ""/Users/james/.local/share/virtualenvs/test_vn-mReMX2sP/lib/python2.7/site-packages/snowflake/connector/auth.py"", line 214, in authenticate
    socket_timeout=self._rest._connection.login_timeout)
  File ""/Users/james/.local/share/virtualenvs/test_vn-mReMX2sP/lib/python2.7/site-packages/snowflake/connector/network.py"", line 538, in _post_request
    _include_retry_params=_include_retry_params)
  File ""/Users/james/.local/share/virtualenvs/test_vn-mReMX2sP/lib/python2.7/site-packages/snowflake/connector/network.py"", line 627, in fetch
    **kwargs)
  File ""/Users/james/.local/share/virtualenvs/test_vn-mReMX2sP/lib/python2.7/site-packages/snowflake/connector/network.py"", line 704, in _request_exec_wrapper
    raise e
ImportError: cannot import name dump_publickey
(test_vn) bash-3.2$ 
</code></pre>

<p><strong>snowflake_python_connector.log</strong></p>

<pre><code>INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 1.6.12, Python Version: 2.7.14, Platform: Darwin-16.7.0-x86_64-i386-64bit
DEBUG:snowflake.connector.connection:connect
DEBUG:snowflake.connector.connection:__config
DEBUG:snowflake.connector.converter:use_sfbinaryformat: False, use_numpy: False
DEBUG:snowflake.connector.connection:REST API object was created: test_account.snowflakecomputing.com:443, proxy=None:None, proxy_user=None
DEBUG:snowflake.connector.auth:authenticate
DEBUG:snowflake.connector.auth:assertion content: *********
DEBUG:snowflake.connector.auth:account=test_account, user=user-name, database=None, schema=None, warehouse=None, role=None, request_id=b085d6f3-42fc-40c1-b28a-5c78f36e1a88
DEBUG:snowflake.connector.auth:body['data']: {u'CLIENT_APP_VERSION': u'1.6.12', u'CLIENT_APP_ID': u'PythonConnector', u'CLIENT_ENVIRONMENT': {u'PYTHON_VERSION': u'2.7.14', u'APPLICATION': u'PythonConnector', u'OS_VERSION': 'Darwin-16.7.0-x86_64-i386-64bit', u'PYTHON_COMPILER': 'GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)', u'OS': 'Darwin', u'PYTHON_RUNTIME': 'CPython'}, u'SESSION_PARAMETERS': {u'CLIENT_SESSION_KEEP_ALIVE_HEARTBEAT_FREQUENCY': 900}, u'LOGIN_NAME': 'user-name', u'SVN_REVISION': None, u'ACCOUNT_NAME': 'test_account'}
DEBUG:botocore.vendored.requests.packages.urllib3.util.retry:Converted retries value: 1 -&gt; Retry(total=1, connect=None, read=None, redirect=None)
DEBUG:botocore.vendored.requests.packages.urllib3.util.retry:Converted retries value: 1 -&gt; Retry(total=1, connect=None, read=None, redirect=None)
DEBUG:snowflake.connector.network:Active requests sessions: 1, idle: 0
DEBUG:snowflake.connector.network:remaining request timeout: 120, retry cnt: 1
DEBUG:snowflake.connector.network:socket timeout: 60
INFO:botocore.vendored.requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): test_account.snowflakecomputing.com
DEBUG:snowflake.connector.network:Active requests sessions: 0, idle: 1
</code></pre>

<p>Not sure what's wrong here. Any path conflicts or any missing package dependencies ? Please help me.</p>

<p><a href=""https://docs.snowflake.net/manuals/user-guide/python-connector-install.html#step-1-install-the-connector"" rel=""nofollow noreferrer"">https://docs.snowflake.net/manuals/user-guide/python-connector-install.html#step-1-install-the-connector</a></p>

<p><strong>/Users/james/test_vn</strong></p>

<p><strong>pipenv run pip freeze</strong></p>

<pre><code>/Users/james/Library/Python/2.7/lib/python/site-packages/pipenv/vendor/requirementslib/models/utils.py:19: UserWarning: Module pathlib2 was already imported from /Users/james/Library/Python/2.7/lib/python/site-packages/pipenv/vendor/pathlib2/__init__.pyc, but /usr/local/lib/python2.7/site-packages/pathlib2-2.3.2-py2.7.egg is being added to sys.path
  from pkg_resources import Requirement
/Users/james/Library/Python/2.7/lib/python/site-packages/pipenv/vendor/requirementslib/models/utils.py:19: UserWarning: Module attr was already imported from /Users/james/Library/Python/2.7/lib/python/site-packages/pipenv/vendor/attr/__init__.pyc, but /usr/local/lib/python2.7/site-packages/attrs-18.2.0-py2.7.egg is being added to sys.path
  from pkg_resources import Requirement
/Users/james/Library/Python/2.7/lib/python/site-packages/pipenv/vendor/requirementslib/models/utils.py:19: UserWarning: Module scandir was already imported from /Users/james/Library/Python/2.7/lib/python/site-packages/pipenv/vendor/scandir.pyc, but /usr/local/lib/python2.7/site-packages/scandir-1.9.0-py2.7-macosx-10.12-x86_64.egg is being added to sys.path
  from pkg_resources import Requirement
altgraph==0.10.2
asn1crypto==0.24.0
azure-common==1.1.16
azure-nspkg==2.0.0
azure-storage==0.36.0
bdist-mpkg==0.5.0
bonjour-py==0.3
boto3==1.9.35
botocore==1.12.35
certifi==2018.10.15
cffi==1.11.5
chardet==3.0.4
cryptography==2.3.1
docutils==0.14
enum34==1.1.6
future==0.17.1
futures==3.2.0
idna==2.7
ijson==2.3
ipaddress==1.0.22
jmespath==0.9.3
macholib==1.5.1
matplotlib==1.3.1
modulegraph==0.10.4
numpy==1.8.0rc1
py2app==0.7.3
pyasn1==0.4.4
pyasn1-modules==0.2.2
pycparser==2.19
pycryptodomex==3.7.0
PyJWT==1.6.4
pyOpenSSL==0.13.1
pyparsing==2.0.1
python-dateutil==1.5
pytz==2013.7
requests==2.20.0
s3transfer==0.1.13
scipy==0.13.0b1
six==1.4.1
snowflake-connector-python==1.6.12
urllib3==1.24
xattr==0.6.4
zope.interface==4.1.1
</code></pre>

<p>And also Versions above (<code>pipenv run pip freeze</code>) and in <code>site-packages</code> don't match.</p>
",1,1541048648,python;python-requests;pyopenssl;urllib3;snowflake-cloud-data-platform,True,4002,2,1542945217,https://stackoverflow.com/questions/53095452/unable-to-connect-to-snowflake
53314141,Error &quot;Failed processing Browscap file&quot; python webscraping,"<p>I am trying to create a program which will access a specific https webpage on my university website in which my semester marks are displayed. I want to scrape the web page to check for changes to when my final marks come out and then send an email to myself (easier than having to check at random throughout the day). </p>

<p>I've never done web scraping before and the site requires login authentication so this problem is probably more complicated than I think but here is what I have so far: </p>

<pre><code>import certifi
from bs4 import BeautifulSoup
import urllib3 as u

url = ""https://upnet.up.ac.za/psc/pscsmpra/EMPLOYEE/SA/c/UP_SS_MENU.UP_SS_RESULTS_FL.GBL""
http = u.PoolManager(cert_reqs='CERT_REQUIRED',ca_certs=certifi.where())

try:
    r =    http.request('GET',url,timeout=u.Timeout(connect=1.0, read=2.0))
except u.exceptions.NewConnectionError: 
    print(""Connection Failed"")

print(r.status)
soup = BeautifulSoup(r.data,features=""html.parser"")

print(soup.prettify())
</code></pre>

<p>running this code returns the following: </p>

<blockquote>
  <p>200 </p>
  
  <p>Failed processing Browscap file. as it could be missing. Please
  contact your system adminstrator.</p>
</blockquote>

<p>If i run r.headers i get this:</p>

<blockquote>
  <p>HTTPHeaderDict({'Date': 'Thu, 15 Nov 2018 07:13:27 GMT', 'Server':
  'Apache', 'Content-Length': '99', 'X-Frame-Options': 'SAMEORIGIN',
  'Set-Cookie':
  'appcsmpr08-7007-PORTAL-PSJSESSIONID=5T8WMyIf2lC2ZSBTaf84UZBA-tV9BLAh!631984709;
  domain=.up.ac.za; path=/; HttpOnly,
  NSC_wt_dbnqvtqspe=ffffffff8adf14db45525d5f4f58455e445a4a422d6f;expires=Thu,
  15-Nov-2018 08:09:29 GMT;path=/;httponly,
  dtCookie=E2152DB7F41B75674D0AE5146876F0EB|UFNDU018MQ; Path=/;
  Domain=.up.ac.za', 'X-Clacks-Overhead': 'GNU Terry Pratchett',
  'Content-Type': 'text/plain; charset=UTF-8'}) PS
  C:\Users\Shaun\Documents\Personal_Projects\markAlert></p>
</blockquote>

<p>I assume that I'm able to connect but I'm uncertain of what is meant by the error above. A google search was no help. </p>

<p>Any help on how to fix this appreciated! </p>
",0,1542265875,python;web-scraping;beautifulsoup;urllib3,True,513,1,1542296084,https://stackoverflow.com/questions/53314141/error-failed-processing-browscap-file-python-webscraping
52209093,ImportError: No module named &#39;requests.packages.urllib3&#39;,"<p>I am testing an application and I am getting the above error ""ImportError: No module named 'requests.packages.urllib3'"" Below are the details.</p>

<pre><code>[root@lab ~]# python /opt/test/panda_API.py
Traceback (most recent call last):
  File ""/opt/test/panda_API.py"", line 8, in &lt;module&gt;
    import requests
  File ""/usr/lib/python2.6/site-packages/requests/__init__.py"", line 58, in &lt;module&gt;
    from . import utils
  File ""/usr/lib/python2.6/site-packages/requests/utils.py"", line 32, in &lt;module&gt;
    from .exceptions import InvalidURL
  File ""/usr/lib/python2.6/site-packages/requests/exceptions.py"", line 10, in &lt;module&gt;
    from .packages.urllib3.exceptions import HTTPError as BaseHTTPError
  File ""/usr/lib/python2.6/site-packages/requests/packages/__init__.py"", line 99, in load_module
    raise ImportError(""No module named '%s'"" % (name,))
ImportError: No module named 'requests.packages.urllib3'
</code></pre>

<p>In a bid to resolve that, I try to <code>pip install requests</code>. Then the following error pops up.</p>

<pre><code>[root@lab ~]# pip install requests
Traceback (most recent call last):
  File ""/usr/bin/pip"", line 5, in &lt;module&gt;
    from pkg_resources import load_entry_point
  File ""/usr/lib/python2.6/site-packages/pkg_resources/__init__.py"", line 954, in &lt;module&gt;
    class Environment:
  File ""/usr/lib/python2.6/site-packages/pkg_resources/__init__.py"", line 958, in Environment
    self, search_path=None, platform=get_supported_platform(),
  File ""/usr/lib/python2.6/site-packages/pkg_resources/__init__.py"", line 188, in get_supported_platform
    plat = get_build_platform()
  File ""/usr/lib/python2.6/site-packages/pkg_resources/__init__.py"", line 388, in get_build_platform
    from sysconfig import get_platform
ImportError: No module named sysconfig
</code></pre>

<p>I have searched for solution here on stack overflow and other platforms, the closest is the one in this page on stack overflow. (<a href=""https://stackoverflow.com/questions/50742538/importerror-no-module-named-sysconfig-cant-get-pip-working/52207390?noredirect=1#comment91364742_52207390"">ImportError: No module named sysconfig--can&#39;t get pip working</a>). </p>

<p>I have tried the steps proposed in the above link, yet the same error ""ImportError: No module named sysconfig"", keeps repeating.</p>

<p>Running ""pip -V"", generated the same error.
Running the recommended command to check the version of setuptools, generated the following error --------</p>

<pre><code>    [root@lab pkg_resources]# python2.6 -c ""import setuptools; print(setuptools.__version__)""
`Traceback (most recent call last):
    File ""&lt;string&gt;"", line 1, in &lt;module&gt;
    File ""/usr/lib/python2.6/site-packages/setuptools/__init__.py"", line 14, in &lt;module     &gt;
      import setuptools.version
    File ""/usr/lib/python2.6/site-packages/setuptools/version.py"", line 1, in &lt;module&gt;
      import pkg_resources
    File ""/usr/lib/python2.6/site-packages/pkg_resources/__init__.py"", line 954, in &lt;mo     dule&gt;
      class Environment:
    File ""/usr/lib/python2.6/site-packages/pkg_resources/__init__.py"", line 958, in Env     ironment
      self, search_path=None, platform=get_supported_platform(),
    File ""/usr/lib/python2.6/site-packages/pkg_resources/__init__.py"", line 188, in get     _supported_platform
      plat = get_build_platform()
    File ""/usr/lib/python2.6/site-packages/pkg_resources/__init__.py"", line 388, in get     _build_platform
      from sysconfig import get_platform
  ImportError: No module named sysconfig
</code></pre>

<p>I have checked /usr/lib/python2.6/site-packages, and I do not have <code>pkg_resources.py</code>. Instead, what I have is the directory <code>pkg_resources</code>.</p>

<p>It appears that I have a more complicated case. Please I need help please!</p>

<p>Notes:</p>

<p>OS: <code>CentOS release 6.10 (Final)</code>, and Python Version: <code>Python 2.6.6</code></p>
",-1,1536253649,python;urllib3,True,8493,2,1541677054,https://stackoverflow.com/questions/52209093/importerror-no-module-named-requests-packages-urllib3
48435769,ModuleNotFoundError: No module named &#39;urllib3.exceptions&#39;; &#39;urllib3&#39; is not a package,"<p>I'm using the following code to get data from a REST API:</p>

<pre><code>import requests
import json

key = ""my service key""

api = ""http://api.data.go.kr/openapi/pblprfr-event-info-std?serviceKey="", key, ""&amp;s_page=1&amp;s_list=100&amp;type=json""


r = requests.get(api)

data = json.loads(r.text)

print(data[""행사명""]) 
</code></pre>

<p>This code produces the following error:</p>

<blockquote>
  <p>File ""sel2.py"", line 1, in &lt;module&gt;</p>

<pre><code>import requests
</code></pre>
  
  <p>File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/requests/<strong>init</strong>.py"", line 46, in &lt;module&gt;</p>

<pre><code>from .exceptions import RequestsDependencyWarning
</code></pre>
  
  <p>File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/requests/exceptions.py"", line 9, in &lt;module&gt;</p>

<pre><code>from urllib3.exceptions import HTTPError as BaseHTTPError
</code></pre>
  
  <p>ModuleNotFoundError: No module named 'urllib3.exceptions'; 'urllib3' is not a package</p>
</blockquote>

<p>Any ideas what the problem can be?</p>
",6,1516855021,python;python-requests;urllib3,True,9829,1,1541676748,https://stackoverflow.com/questions/48435769/modulenotfounderror-no-module-named-urllib3-exceptions-urllib3-is-not-a-pa
45187032,How to use python script to auto login portal?,"<p>I have the following code in order to help me auto login the portal but i fond that i able to print the content but the web portal does not pop up :</p>

<pre><code>import pandas as pd
import html5lib
import time
import requests
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
import webbrowser


with requests.Session() as c:
   proxies = {""http://proxy-udd.com""}

   url = 'https://ji.devtools.com/login'
   USERNAME = 'shiji'
   PASSWORD = 'Tan@9'

   c.get(url,verify= False)
   csrftoken = ''

   login_data = dict(proxies,atl_token = csrftoken, os_username=USERNAME, os_password=PASSWORD, next='/')

   c.post(url, data=login_data, headers={""referer"" : ""https://ji.devtools.com/login""})
   page = c.get('https://ji.devtools.com/')
   print (page.content)
</code></pre>
",0,1500458060,python;request;webbrowser-control;urllib3,False,2650,2,1541156990,https://stackoverflow.com/questions/45187032/how-to-use-python-script-to-auto-login-portal
53088272,python urllib certificate verify failed,"<p>I have the follwing script:</p>

<pre><code>from currency_converter import CurrencyConverter

test = CurrencyConverter('http://www.ecb.europa.eu/stats/eurofxref/eurofxref.zip')
</code></pre>

<p>I try to run it on machine with windows 10 and python 3.6.7 and always get this error:</p>

<pre><code>urllib.error.URLError: &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:847)&gt;
</code></pre>

<p>Running it on other windows 10 machines with python 3.6 works fine. I tried it on this machine with python 3.7 and get the same error.  What could be wrong with the python installation ?</p>
",1,1541004004,python;python-3.6;urllib3,True,314,1,1541023744,https://stackoverflow.com/questions/53088272/python-urllib-certificate-verify-failed
50545515,Cloud Vision API Client threw an OS Error &quot;too many open files&quot;,"<p>I have met an Error of ""Too many open files"" when I run label detection via Cloud Vision API Client with Python.<br>
When I asked this probrem on GitHub before this post, the maintainer gave me an advice that the problem is general Python issue rather than API.<br>
After this advice, I have not understood yet why Python threw ""too many open files"".<br>
I did logging and it showed that urllib3 had raised such errors, although I did not import that package explicitly.<br>
What I wrong? Please help me.<br>
My Environment is</p>

<ul>
<li>Ubuntu 16.04.3 LTS (GNU/Linux 4.4.0-112-generic x86_64)  </li>
<li>Python 3.5.2  </li>
<li>google-cloud-vision (0.31.1)  </li>
</ul>

<p>The error logs:  </p>

<pre><code>[2018-05-25 20:18:46,573] {label_detection.py:60} DEBUG - success open decile_data/image/src/00000814.jpg
[2018-05-25 20:18:46,573] {label_detection.py:62} DEBUG - success convert image to types.Image
[2018-05-25 20:18:46,657] {requests.py:117} DEBUG - Making request: POST https://accounts.google.com/o/oauth2/token
[2018-05-25 20:18:46,657] {connectionpool.py:824} DEBUG - Starting new HTTPS connection (1): accounts.google.com
[2018-05-25 20:18:46,775] {connectionpool.py:396} DEBUG - https://accounts.google.com:443 ""POST /o/oauth2/token HTTP/1.1"" 200 None
[2018-05-25 20:18:47,803] {label_detection.py:60} DEBUG - success open decile_data/image/src/00000815.jpg
[2018-05-25 20:18:47,803] {label_detection.py:62} DEBUG - success convert image to types.Image
[2018-05-25 20:18:47,896] {requests.py:117} DEBUG - Making request: POST https://accounts.google.com/o/oauth2/token
[2018-05-25 20:18:47,896] {connectionpool.py:824} DEBUG - Starting new HTTPS connection (1): accounts.google.com
[2018-05-25 20:18:47,902] {_plugin_wrapping.py:81} ERROR - AuthMetadataPluginCallback ""&lt;google.auth.transport.grpc.AuthMetadataPlugin object at 0x7fcd94eb7dd8&gt;"" raised exception!
Traceback (most recent call last):
  File ""/home/ishiyama/tensorflow/lib/python3.5/site-packages/urllib3/util/ssl_.py"", line 313, in ssl_wrap_socket
OSError: [Errno 24] Too many open files

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/ishiyama/tensorflow/lib/python3.5/site-packages/urllib3/connectionpool.py"", line 601, in urlopen
  File ""/home/ishiyama/tensorflow/lib/python3.5/site-packages/urllib3/connectionpool.py"", line 346, in _make_request
  File ""/home/ishiyama/tensorflow/lib/python3.5/site-packages/urllib3/connectionpool.py"", line 850, in _validate_conn
  File ""/home/ishiyama/tensorflow/lib/python3.5/site-packages/urllib3/connection.py"", line 326, in connect
  File ""/home/ishiyama/tensorflow/lib/python3.5/site-packages/urllib3/util/ssl_.py"", line 315, in ssl_wrap_socket
urllib3.exceptions.SSLError: [Errno 24] Too many open files

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/ishiyama/tensorflow/lib/python3.5/site-packages/requests/adapters.py"", line 440, in send
  File ""/home/ishiyama/tensorflow/lib/python3.5/site-packages/urllib3/connectionpool.py"", line 639, in urlopen
  File ""/home/ishiyama/tensorflow/lib/python3.5/site-packages/urllib3/util/retry.py"", line 388, in increment
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='accounts.google.com', port=443): Max retries exceeded with url: /o/oauth2/token (Caused by SSLError(OSError(24, 'Too many open files'),))
</code></pre>

<p>The script exported above errors is following:</p>

<pre><code># -*- coding: utf-8 -*-
"""""" Detecting labels of images using Google Cloud Vision. """"""

import argparse
import csv
from datetime import datetime
import os
import logging
from pathlib import Path
import sys
from google.cloud import vision
from google.cloud.vision import types


logger= logging.getLogger(__name__)


def get_commandline_args():
    parser = argparse.ArgumentParser(
        description='Detecting labels of images using Google Cloud Vision.')

    parser.add_argument('--image-dir',
                        type=str,
                        required=True,
                        help='Directory in which images are saved.')
    parser.add_argument('--output-path',
                        type=str,
                        required=True,
                        help='Path of output file. This is saved as CSV.')
    parser.add_argument('--max-results',
                        type=int,
                        required=False,
                        default=5,
                        help=('Maximum number of resulting labels.'
                              ' Default is 5.'))
    parser.add_argument('--debug',
                        type=bool,
                        required=False,
                        default=False,
                        help=('Whether running to debug.'
                              ' If True, this scripts will run on 3 files.'
                              ' Default is False.'))
    return parser.parse_args()


def load_image(path):
    """""" load image to be capable with Google Cloud Vision Clienet API.

    Args:
        path (str): a path of an image.

    Returns:
        img : an object which is google.cloud.vision.types.Image.

    Raise:
        IOError is raised when 'open' is failed to load the image.
    """"""
    with open(path, 'rb') as f:
        content = f.read()
    logger.debug('success open {}'.format(path))
    img = types.Image(content=content)
    logger.debug('success convert image to types.Image')

    return img


def detect_labels_of_image(path, max_results):
    _path = Path(path)
    client = vision.ImageAnnotatorClient()
    image = load_image(path=str(_path))
    execution_time = datetime.now()
    response = client.label_detection(image=image, max_results=max_results)
    labels = response.label_annotations
    for label in labels:
        record = (str(_path), _path.name, label.description,
                  label.score, execution_time.strftime('%Y-%m-%d %H:%M:%S'))
        yield record


def main():
    args = get_commandline_args()

    file_handler = logging.FileHandler(filename='label_detection.log')
    logging.basicConfig(
        level=logging.DEBUG,
        format='[%(asctime)s] {%(filename)s:%(lineno)s} %(levelname)s - %(message)s',
        handlers=[file_handler]
    )

    image_dir = args.image_dir

    with open(args.output_path, 'w') as fout:

        writer = csv.writer(fout, lineterminator='\n')
        header = ['path', 'filename', 'label', 'score', 'executed_at']
        writer.writerow(header)

        image_file_lists = os.listdir(image_dir)
        image_file_lists.sort()
        if args.debug:
            image_file_lists = image_file_lists[:3]

        for filename in image_file_lists:
            path = os.path.join(image_dir, filename)
            try:
                results = detect_labels_of_image(path, args.max_results)
            except Exception as e:
                logger.warning(e)
                logger.warning('skiped processing {} due to above exception.'.format(path))
            for record in results:
                writer.writerow(record)


if __name__ == '__main__':
    main()
</code></pre>
",7,1527356796,python;ubuntu;google-cloud-vision;urllib3,True,2646,3,1540878858,https://stackoverflow.com/questions/50545515/cloud-vision-api-client-threw-an-os-error-too-many-open-files
52956615,Download file in python using urllib2 vs wget,"<p>I am trying to download an online file in python. I have seen solutions using urllib2 for python 2 and wget. If my purpose is just to download the file, is there any advantage of using urllib2 instead of wget. To me using wget package seems simpler. However, most of the online solutions I see are using urllib2 and urllib with python 3. I am more inclined towards wget as it works for both python 2 and python 3.</p>

<p>This question is different from the question marked as duplicate of this as I have asked for difference with respect to wget, while the other question does not address the relationship with respect to wget package.</p>
",1,1540323294,python;download;wget;urllib2;urllib3,True,593,1,1540755104,https://stackoverflow.com/questions/52956615/download-file-in-python-using-urllib2-vs-wget
53029248,Python urllib3 opening url,"<pre><code>import urllib2
data = []
req=urllib2.Request(""https://raw.githubusercontent.com/plotly/datasets/master/miserables.json"")
opener = urllib2.build_opener()
f = opener.open(req)
data = json.loads(f.read())
</code></pre>

<p>How to maintain the same functionality using urllib3?</p>
",-1,1540711250,python;urllib3,False,1123,1,1540713889,https://stackoverflow.com/questions/53029248/python-urllib3-opening-url
52960634,How to call two different API services (App engine) in Google Cloud Platform,"<p>I have two different App Engine services and I wanted to access the same from other different service. I just wanted to call the URL so that it can perform all the operations. When I ran in Jupyter notebook, below code worked. But when I am running in another app engine service, I am getting 502 bad gateway issue.</p>

<pre><code>gatopubsub=urllib3.connection_from_url(r""url"")
gatopubsub.request(method=""GET"",url=r""url"")
time.sleep(120)
dataflow=urllib3.connection_from_url(r""url"")
dataflow.request(method=""GET"",url=r""url"")
time.sleep(720)
bigquery_success=big_query()
</code></pre>

<p>I gave time.sleep because, each URL takes the specified seconds to complete
I tried to install google-appengine package. But I am getting HTTP issue which is the reason I tried URLLIB3
Please assist </p>

<p>Thank you</p>
",0,1540351358,python;google-app-engine;urllib3,False,262,1,1540440429,https://stackoverflow.com/questions/52960634/how-to-call-two-different-api-services-app-engine-in-google-cloud-platform
52621830,urllib.request &#39;Request&#39; object has no attribute &#39;getcode&#39; and read,"<p>I am getting error for <code>print</code> statement 
<strong>'Request' object has no attribute 'getcode' and read</strong></p>

<pre><code>sample = '[{{ ""t"": ""{0}"", ""to"": ""{1}"", ""evs"": ""{2}"", ""fds"": {3} }}]'

response = urllib.request.Request(REST_API_URL, sample.encode('utf-8'))

print(""Response: HTTP {0} {1}\n"".format(response.getcode(), response.read()))
</code></pre>
",-2,1538551448,python;python-3.x;urllib;urllib3,True,1309,2,1538552395,https://stackoverflow.com/questions/52621830/urllib-request-request-object-has-no-attribute-getcode-and-read
52507544,Inconsistent IOError exception on HTTPS POST requests using Python &#39;requests&#39; library,"<p>I keep getting the following exception when trying to do a HTTPS POST using <code>requests</code></p>

<p>This problem occurs sporadically and the same request when retried (using <code>backoff</code> module) goes through successfully. I don't know how to reproduce this but I can see this problem when I run a load of HTTPS POST requests.</p>

<pre><code>Traceback (most recent call last):
 File \""/usr/local/lib/python2.7/dist-packages/shared/util/http_util.py\"", line 71, in send_https_post_request
 response = session.post(url, cert=cert, data=data)
 File \""/usr/local/lib/python2.7/dist-packages/requests/sessions.py\"", line 522, in post
 return self.request('POST', url, data=data, json=json, **kwargs)
 File \""/usr/local/lib/python2.7/dist-packages/requests/sessions.py\"", line 475, in request
 resp = self.send(prep, **send_kwargs)
 File \""/usr/local/lib/python2.7/dist-packages/requests/sessions.py\"", line 596, in send
 r = adapter.send(request, **kwargs)
 File \""/usr/local/lib/python2.7/dist-packages/requests/adapters.py\"", line 423, in send
 timeout=timeout
 File \""/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/connectionpool.py\"", line 595, in urlopen
 chunked=chunked)
 File \""/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/connectionpool.py\"", line 352, in _make_request
 self._validate_conn(conn)
 File \""/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/connectionpool.py\"", line 831, in _validate_conn
 conn.connect()
 File \""/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/connection.py\"", line 289, in connect
 ssl_version=resolved_ssl_version)
 File \""/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/util/ssl_.py\"", line 306, in ssl_wrap_socket
 context.load_cert_chain(certfile, keyfile)
IOError: [Errno 2] No such file or directory
</code></pre>

<p><strong>Relevant Code:</strong></p>

<pre><code>@contextlib.contextmanager
def pem_bytes_as_cert_file(pem_cert_bytes):
    '''
     Given bytes, return a temporary file which can be used as the cert
    '''
    with tempfile.NamedTemporaryFile(delete=True, suffix='.pem') as t_pem:
        f_pem = open(t_pem.name, 'wb')
        f_pem.write(pem_cert_bytes)
        f_pem.close()
        yield t_pem.name

def send_https_post_request(session, url, data, pem_cert_in_bytes):
    with pem_bytes_as_cert_file(pem_cert_in_bytes) as cert:
        response = session.post(url, cert=cert, data=data)
        response.raise_for_status()
        response.close()
</code></pre>

<p>Could you please help me understand more about this issue?</p>
",0,1537915103,python;ssl;python-requests;urllib3,False,324,1,1537937365,https://stackoverflow.com/questions/52507544/inconsistent-ioerror-exception-on-https-post-requests-using-python-requests-li
52423024,Python Urllib download the video but it&#39;s not playable,"<p>I'm trying to download a video from s URL using Python's urllib package. My Python version is 3.6.</p>

<p>Here's what I have tried:
<strong>from views.py:</strong></p>

<pre><code>def post(self, request, *args, **kwargs):
    serializer = VideoConverterSerializer(data=self.request.data)
    validation = serializer.is_valid()
    print(serializer.errors)
    if validation is True:
        url = request.POST.get('video_url')
        try:
            r = urllib.request.urlopen(url)
            with open('my_video.mp4', 'wb') as f:
                f.write(r.read())
            rea_response = HttpResponse('my_video.mp4', content_type='video/mp4')
            rea_response['Content-Disposition'] = 'attachment; filename=my_video.mp4'
            return rea_response
        except TimeoutError:
            return HttpResponse(TimeoutError)
    else:
        return HttpResponse('Not a valid request')
</code></pre>

<p>Here's an example URL I'm trying with:</p>

<blockquote>
  <p><a href=""https://expirebox.com/files/386713962c5f8b7556bc77c4a6c2a576.mp4"" rel=""nofollow noreferrer"">https://expirebox.com/files/386713962c5f8b7556bc77c4a6c2a576.mp4</a></p>
</blockquote>

<p>The code above download the video file as <code>my_video.mp4</code> but the video is not playable. The actual size of the video is <code>~5.9 MB</code> but the size of download video is <code>11 KB</code> only, so definitely something wrong with the downloaded video.</p>

<p>What can be wrong here?</p>

<p>help me, please!</p>

<p>Thanks in advance!</p>
",0,1537439416,python;django;python-3.x;urllib;urllib3,False,1398,1,1537440202,https://stackoverflow.com/questions/52423024/python-urllib-download-the-video-but-its-not-playable
52345425,How to access country restricted proxy website through urllib.request,"<p>I am trying to build a selenium scraper that rotate its IP address after every n request, for this i need to access a free proxy website in order to construct a list of IP:port addresses, the problem is that i am executing my code from a country that bans proxy websites, thus the code can't access the proxy website and  is returning me this error:
 raise URLError(err) urllib.error.URLError: urlopen error [Errno 104] Connection reset by peer</p>

<p>Here is my part of code that try to access <a href=""https://www.sslproxies.org/"" rel=""nofollow noreferrer"">https://www.sslproxies.org/</a> website:</p>

<pre><code>ua = UserAgent() # From here we generate a random user agent
proxies = [] # Will contain proxies [ip, port]
proxies_req = Request('https://www.sslproxies.org/')
proxies_req.add_header('User-Agent', ua.random)
proxies_doc = urlopen(proxies_req).read().decode('utf8')
soup = BeautifulSoup(proxies_doc, 'html.parser')
proxies_table = soup.find(id='proxylisttable')
</code></pre>

<p>The error is occuring from the fifth line :</p>

<pre><code>proxies_doc = urlopen(proxies_req).read().decode('utf8')
</code></pre>

<p>any suggestions?</p>
",1,1537020272,python;python-requests;selenium-chromedriver;http-proxy;urllib3,False,439,0,1537020424,https://stackoverflow.com/questions/52345425/how-to-access-country-restricted-proxy-website-through-urllib-request
52241712,Proxy authentication using CA5 cert &amp; http certificate authentication,"<p>I am new to Python, trying to read &amp; parse logs of applications https url. My org has proxy setup. I am able to connect http url using basic_auth providing  &amp;  &amp; also by CA5 cert using below code 
    import urllib3</p>

<pre><code>from urllib3 import ProxyManager, make_headers
# default_headers = make_headers(proxy_basic_auth='&lt;username&gt;:&lt;password&gt;')
# http = ProxyManager(""http://&lt;proxy_server&gt;:&lt;port&gt;/"", headers=default_headers)
http = ProxyManager(""http://&lt;proxy_server&gt;:&lt;port&gt;/"",cert_reqs='CERT_REQUIRED',ca_certs='&lt;Windows path to .cer&gt;')
r = http.request('GET','http://google.com')
print(r.data)
</code></pre>

<p>.pem certificate got saved as .cer on windows system. 
This is working as excepted and returnign google data.
Now while trying to use same for HTTPS url using below, </p>

<pre><code>r = http.request('GET','https://google.com',verify='&lt;same as above i.e. Windows path to .cer&gt;')
</code></pre>

<p>It is failing with below errors, </p>

<pre><code>.
.
OSError: Tunnel connection failed: 502 notresolvable
.
.
.
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='google.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:749)'),))
</code></pre>

<p>As the log files are keeping updating, might need to stream it; Suggestion on this is highly appreciated.</p>

<p>Using <strong>Python 3.6.0 |Anaconda 4.3.1 (64-bit)</strong> on <strong>Windows 7</strong>. </p>
",1,1536472068,python;urllib3,False,187,0,1536474944,https://stackoverflow.com/questions/52241712/proxy-authentication-using-ca5-cert-http-certificate-authentication
3731379,example urllib3 and threading in python,"<p>I am trying to use urllib3 in simple thread to fetch several wiki pages. 
The script will</p>

<p>Create 1 connection for every thread (I don't understand why) and Hang forever.
Any tip, advice or simple example of urllib3 and threading</p>

<pre><code>import threadpool
from urllib3 import connection_from_url

HTTP_POOL = connection_from_url(url, timeout=10.0, maxsize=10, block=True)

def fetch(url, fiedls):
  kwargs={'retries':6}
  return HTTP_POOL.get_url(url, fields, **kwargs)

pool = threadpool.ThreadPool(5)
requests = threadpool.makeRequests(fetch, iterable)
[pool.putRequest(req) for req in requests]
</code></pre>

<p>@Lennart's script got this error:</p>

<pre><code>http://en.wikipedia.org/wiki/2010-11_Premier_LeagueTraceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/threadpool.py"", line 156, in run
 http://en.wikipedia.org/wiki/List_of_MythBusters_episodeshttp://en.wikipedia.org/wiki/List_of_Top_Gear_episodes http://en.wikipedia.org/wiki/List_of_Unicode_characters    result = request.callable(*request.args, **request.kwds)
  File ""crawler.py"", line 9, in fetch
    print url, conn.get_url(url)
AttributeError: 'HTTPConnectionPool' object has no attribute 'get_url'
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/threadpool.py"", line 156, in run
    result = request.callable(*request.args, **request.kwds)
  File ""crawler.py"", line 9, in fetch
    print url, conn.get_url(url)
AttributeError: 'HTTPConnectionPool' object has no attribute 'get_url'
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/threadpool.py"", line 156, in run
    result = request.callable(*request.args, **request.kwds)
  File ""crawler.py"", line 9, in fetch
    print url, conn.get_url(url)
AttributeError: 'HTTPConnectionPool' object has no attribute 'get_url'
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/threadpool.py"", line 156, in run
    result = request.callable(*request.args, **request.kwds)
  File ""crawler.py"", line 9, in fetch
    print url, conn.get_url(url)
AttributeError: 'HTTPConnectionPool' object has no attribute 'get_url'
</code></pre>

<p>After adding <code>import threadpool; import urllib3</code> and <code>tpool = threadpool.ThreadPool(4)</code> @user318904's code got this error:</p>

<pre><code>Traceback (most recent call last):
  File ""crawler.py"", line 21, in &lt;module&gt;
    tpool.map_async(fetch, urls)
AttributeError: ThreadPool instance has no attribute 'map_async'
</code></pre>
",2,1284676535,python;multithreading;http;urllib2;urllib3,True,12306,4,1536090020,https://stackoverflow.com/questions/3731379/example-urllib3-and-threading-in-python
52088331,urllib3 PoolManager.request blocks forever when run from inside a running Thread in Python 3,"<p>I have a fairly simple REST request that I want to make during some asynchronous processing that happens inside a Flask application.  I took the naive approach of doing this async processing by just starting a Thread and letting the work get done inside the Thread, e.g.:</p>

<pre><code>t = Thread(target=_do_stuff, daemon=True)
t.start()

def _do_stuff():
    # stuff happens here.
</code></pre>

<p>The problem I am having is that when I make a call to a <code>urllib3.PoolManager</code>'s <code>.request</code> method from somewhere inside the call stack of _do_stuff(), it eventually blocks forever when it gets to the line <code>conn.request(method, url, **httplib_request_kw)</code> in connectionpool.py.  If a timeout is specified, it is ignored.</p>

<p>If I run the same code but do it <em>outside</em> the thread (ie, run it synchronously), then it executes immediately and everything works as expected.</p>

<p>I'm sure there must be something I'm missing about threading in Python 3 or with respect to urllib3, but my questions are:</p>

<ol>
<li>What am I missing, and,</li>
<li>What are my options for doing this in a way that will work?</li>
</ol>
",1,1535595246,python;multithreading;urllib3,False,393,0,1535595246,https://stackoverflow.com/questions/52088331/urllib3-poolmanager-request-blocks-forever-when-run-from-inside-a-running-thread
51771656,"can&#39;t install urllib3, having proxy in between using pip?","<p>Basically I am trying to install urllib3 but I am countering below error:</p>

<pre><code> `C:\Python 3.7\Scripts&gt;pip3.7.exe install urllib3
 Collecting urllib3
   Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None))
  after connection broken by 'ConnectTimeoutError(&lt;pip._vendor.urllib3.connection
 .VerifiedHTTPSConnection object at 0x03285170&gt;, 'Connection to pypi.org timed ou
 t. (connect timeout=15)')': /simple/urllib3/
   Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None))
  after connection broken by      'ConnectTimeoutError(&lt;pip._vendor.urllib3.connection
 .VerifiedHTTPSConnection object at 0x00D3E2B0&gt;, 'Connection to pypi.org timed ou
 t. (connect timeout=15)')': /simple/urllib3/
   Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None))
  after connection broken by           'ConnectTimeoutError(&lt;pip._vendor.urllib3.connection
 .VerifiedHTTPSConnection object at 0x032AB610&gt;, 'Connection to pypi.org timed ou
 t. (connect timeout=15)')': /simple/urllib3/
  Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None))
  after connection broken by 'ConnectTimeoutError(&lt;pip._vendor.urllib3.connection
 .VerifiedHTTPSConnection object at 0x032AB670&gt;, 'Connection to pypi.org timed ou
 t. (connect timeout=15)')': /simple/urllib3/
   Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None))
  after connection broken by      'ConnectTimeoutError(&lt;pip._vendor.urllib3.connection
 .VerifiedHTTPSConnection object at 0x032AB5F0&gt;, 'Connection to pypi.org timed ou
 t. (connect timeout=15)')': /simple/urllib3/


pip3.7.exe install urllib3
   Could not find a version that satisfies the requirement urllib3 (from versions: )
 No matching distribution found for urllib3
</code></pre>

<p><strong>I Tried:</strong> <code>pip3.7.exe install urllib3</code> and <code>pip install urllib3</code> both are having the same issue</p>

<p>FYI: I am located behind a proxy.</p>

<p>All replies are much appreciated</p>
",1,1533830957,python;pip;urllib;urllib3,False,1049,1,1533832334,https://stackoverflow.com/questions/51771656/cant-install-urllib3-having-proxy-in-between-using-pip
26385507,click submit button &quot;OK&quot; on a webpage using python requests module,"<p>i have a device that can be managed using webpage interface.</p>

<p>the device address can be :</p>

<pre><code>http://10.18.25.25/restart
</code></pre>

<p>with a submit button in the webpage:</p>

<pre><code>&lt;form action=""/restart/restart"" method=""POST"" name=""restartForm""&gt;
                            &lt;input type=""submit"" value=""OK""&gt;
                        &lt;/form&gt;
</code></pre>

<p>i am trying to use python module requests to automate clicking on the button on that webpage.</p>

<pre><code>from urllib3 import requests
r = requests.get('http://10.18.25.25/restart', auth=('username', 'password'))
</code></pre>

<p>any ideas??</p>
",2,1413384512,python;html;urllib3,True,9583,2,1533660054,https://stackoverflow.com/questions/26385507/click-submit-button-ok-on-a-webpage-using-python-requests-module
51350121,HTTPResponse Error in urllib python,"<p>I'm trying to use pandas to read coinmarketcap API . 
<a href=""https://stackoverflow.com/a/40589031/7159086"">https://stackoverflow.com/a/40589031/7159086</a>
I used the above link to form a basic query using json_normalize but I am getting the below error - </p>

<pre><code>'HTTPResponse' object does not support indexing .
</code></pre>

<p>My Query :-</p>

<pre><code>from urllib.request import urlopen
import pandas as pd

pd.io.json.json_normalize(urlopen('https://api.coinmarketcap.com/v2/ticker/?limit=10'))
</code></pre>

<p>Output :-</p>

<pre><code>TypeError: 'HTTPResponse' object does not support indexing
</code></pre>

<p>I am running this code in Jupyter Notebook with pandas version <code>0.20.3</code> 
I saw this post also - <a href=""https://stackoverflow.com/questions/26222415/retreiving-data-from-a-website"">Retreiving data from a website</a></p>

<p>But I was not able to solve my issue still .
Please tell how to approach this problem . Further , I want <code>name</code> field as the index column which I am not able to get due to nested json.</p>
",1,1531671811,python;json;pandas;urllib3,True,254,1,1531673323,https://stackoverflow.com/questions/51350121/httpresponse-error-in-urllib-python
50939418,What is the purpose of mounting a Session object?,"<p>I have seen something like this in a few code snippets and in the <a href=""http://docs.python-requests.org/en/master/user/advanced/#transport-adapters"" rel=""noreferrer"">Requests documentation</a>:</p>

<pre><code>import requests
sess = requests.Session()
adapter = requests.adapters.HTTPAdapter(max_retries=20)
sess.mount('https://', adapter)
</code></pre>

<p>I am trying to get a better sense of what <code>.mount()</code> does here.  In this case, is it only to increase the number of allowed retries for all calls to <code>sess.request()</code>?  Is it emulating something like:</p>

<pre><code>for _ in range(max_retries):
    try:
        return sess.request(...)
    except:
        pass
</code></pre>

<p>or is there more going on?</p>

<p>I know that <code>requests.Session</code> instances are <a href=""https://github.com/requests/requests/blob/883caaf145fbe93bd0d208a6b864de9146087312/requests/sessions.py#L396"" rel=""noreferrer"">initialized</a> with adapters that have <code>max_retries=0</code>, so the above is just a hunch based on that.</p>

<p>It would just be helpful to know how specifically <code>.mount()</code> is altering the session object's behavior in this case.</p>
",21,1529462294,python;python-requests;urllib3,True,21550,1,1529474527,https://stackoverflow.com/questions/50939418/what-is-the-purpose-of-mounting-a-session-object
50827869,How do you send an API key to Datadog using urllib?,"<p>I currently have a program that allows me to create and post dashboards to Datadog programmatically. Using the API functions <a href=""https://docs.datadoghq.com/api/?lang=python#overview"" rel=""nofollow noreferrer"">here</a>, I was successfully able to create, update, and remove dashboards as I please. However, now I'd like to extract the skeleton of existing dashboards that I have already created from Datadog to see what has been added or removed. To do this, I need to figure out how to send the API key along with a request. I have no problem getting the higher level information about the boards, but I'd like to go a step further.</p>

<p>This is what I get by calling <code>api.ScreenBoard.get_all()</code></p>

<pre><code>{
    'screenboards': [{
        'read_only': &lt;boolean&gt;,
        'resource': &lt;resource-link&gt;,
        'description': &lt;description&gt;,
        'created': &lt;date&gt;,
        'title': &lt;text&gt;,
        'modified': &lt;date&gt;,
        'created_by': { ''' &lt;creator information&gt; ''' },
        'id': &lt;table-id&gt;
    }]
}
</code></pre>

<p>Now, the end goal is simply to pull JSON from the ""resource"" link given from this command. I've tried to use urllib and urllib2 to merge that link with the host site (like <code>https://www.foo.com/{resource-link}</code>), but I keep getting the following results:</p>

<pre><code>&lt;addinfourl at 0000000000 whose fp = &lt;socket._fileobject object at 0x000000000&gt;&gt;
</code></pre>

<p>OR</p>

<pre><code>{""errors"": [""API key required""]}
</code></pre>

<p>The code that triggered this error is:</p>

<pre><code>def getSkeleton(self):
    boards = self.getAll(); # utilizing the api.ScreenBoards.get_all() function
    boardList = boards['screenboards'];
    for x in boardList:
        url = self.target + x['resource']; # creating the JSON url
        data = urllib.urlopen(url).read();
        print data
</code></pre>

<p>As you can see, my ""data"" variable returns the error. So, all I need is to figure out how to send the API key along with my request to resolve the issue. If anyone knows how to perform this task, I would really appreciate it.</p>
",0,1528852720,python;urllib2;urllib;urllib3;datadog,True,3013,2,1528853611,https://stackoverflow.com/questions/50827869/how-do-you-send-an-api-key-to-datadog-using-urllib
50718604,install python setuptools on linux,"<p>I have machine without internet access, running on CentOS. So I want to install urllib3. I downloaded zip-archive from github, transferred it to my machine, unzip it, and try to execute <code>python setup.py install</code> . Then I have an error: <code>ImportError: No module named setuptools</code>. 
So I want to install setuptools on CentOS where there is no internet access. Also I doesn't have pip. 
How can I install setuptools? Or maybe install urllib3 right away?</p>
",2,1528282436,python;linux;centos;setuptools;urllib3,False,1043,0,1528282436,https://stackoverflow.com/questions/50718604/install-python-setuptools-on-linux
50450622,Urllib3 HTTP Error 502: Bad Gateway,"<p>I am trying to scrape zk.fm in order to download music, but it's giving me some trouble. I'm using urllib3 to generate a response, but this always yields a Bad Gateway error. Accessing the website through a browser works perfectly fine.</p>

<p>This is my code (with a random fake user-agent). I'm trying to access ""<a href=""http://zk.fm/mp3/search?keywords="" rel=""nofollow noreferrer"">http://zk.fm/mp3/search?keywords=</a>"" followed by some keywords which indicate the song name and artist, for example ""<a href=""http://zk.fm/mp3/search?keywords=childish+gambino+heartbeat"" rel=""nofollow noreferrer"">http://zk.fm/mp3/search?keywords=childish+gambino+heartbeat</a>"".</p>

<pre><code>from bs4 import BeautifulSoup
from random import choice
import urllib3 

desktop_agents = ['Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36',
             'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36',
             'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36',
             'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/602.2.14 (KHTML, like Gecko) Version/10.0.1 Safari/602.2.14',
             'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36',
             'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.98 Safari/537.36',
             'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.98 Safari/537.36',
             'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36',
             'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36',
             'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:50.0) Gecko/20100101 Firefox/50.0']
def random_headers():
    return {'User-Agent': choice(desktop_agents)}

ua = random_headers()
http = urllib3.PoolManager(10,headers=user_agent)
response = http.request('GET',""http://zk.fm/mp3/search? 
keywords=childish+gambino+heartbeat"")
soup = BeautifulSoup(response.data)
</code></pre>

<p>Is there a way to work around the 502 Error, or is it out of my control?</p>
",2,1526911431,python;urllib3;bad-gateway,True,3242,1,1526916301,https://stackoverflow.com/questions/50450622/urllib3-http-error-502-bad-gateway
50436896,Handle google recaptcha with python / requests form,"<p>I was trying to find a way to fill a form then get the captcha response and found this code on stackoverflaw:</p>

<pre><code>import urllib
import json


URIReCaptcha = 'https://www.google.com/recaptcha/api/siteverify'
recaptchaResponse = body.get('recaptchaResponse', None)
private_recaptcha = '6LdXXXXXXXXXXXXXXXXXXXXXXXX'
remote_ip = request.remote_addr
params = urllib.urlencode({
            'secret': private_recaptcha,
            'response': recaptchaResponse,
            'remote_ip': remote_ip,
})

# print params
data = urllib.urlopen(URIReCaptcha, params).read()
result = json.loads(data)
success = result.get('success', None)

if success == True:
    print 'reCaptcha passed'
else:
    print 'recaptcha failed'
</code></pre>

<p>I tried it, but it says 'body' is not defined.
So instead I tried 'requests.get' but I get tons of mistakes.</p>

<p>Any clue on how i could make this work??<br>
moreover, to validate my form I only need to pass the captcha-response to the form to make it work?? </p>

<p>Thank you all for your answers</p>

<p>EDIT: 
here is the link of the Post where i got it form: 
<a href=""https://stackoverflow.com/questions/46393162/how-to-validate-a-recaptcha-response-server-side-with-python"">How to validate a ReCaptcha response server side with Python?</a></p>
",1,1526833113,python;json;urllib3,False,2745,0,1526834399,https://stackoverflow.com/questions/50436896/handle-google-recaptcha-with-python-requests-form
50356432,Printing data from a static xml file,"<p>I am trying to find product names in an xml file i downloaded. I have figured out how to display every result using a while loop. My problem is, i want to only display the first 10 results. Also, i need be able to call each result individually.</p>

<p>For example: print(read_xml_code.start_tag_5) would print the 5th product in the XML file.
print(read_xml_code.start_tag_10) would print the 10th</p>

<p>here is my code so far:</p>

<pre><code># Define the Static webpage XML file
static_webpage_1 = 'StaticStock/acoustic_guitar.html'


def Find_static_webpage_product_name():
    # Open and read the contents of the first XML file
    read_xml_code = open(static_webpage_1, encoding=""utf8"").read()
    # Find and print the static page title.
    start_tag = '&lt;title&gt;&lt;![CDATA['
    end_tag = ']]&gt;&lt;/title&gt;'
    end_position = 0
    starting_position = read_xml_code.find(start_tag, end_position)
    end_position = read_xml_code.find(end_tag, starting_position)
    while starting_position != -1 and end_position!= -1:
        print(read_xml_code[starting_position + len(start_tag) : end_position]+ '\n')
        starting_position = read_xml_code.find(start_tag, end_position)
        end_position = read_xml_code.find(end_tag, starting_position)

#call function
Find_static_webpage_product_name()
</code></pre>
",-1,1526406610,python;urllib3,False,68,1,1526409621,https://stackoverflow.com/questions/50356432/printing-data-from-a-static-xml-file
50196650,Downloading a pdf file from a url that redirects 2 times fails even when using requests.history,"<p>I have been struggling with this for a whole day. I compile a list of URLs that i get by using urllib3 on a webpage(also using BeautifulSoup). The URLs basically point to pdf files that i wanted to automate the download of. What's great is that the pdf links have a beautiful pattern. So i easily use regex to make a list of pdfs that i want to download and ignore the rest. But then that's where the problem starts. The URLs follow the pattern <strong><a href=""http://www.ti.com/lit/sboa263"" rel=""nofollow noreferrer"">http://www.ti.com/lit/sboa263</a></strong></p>

<p>NOTE: The last part changes for other files and NO pdf file extension.</p>

<p>But if you put this link in your browser you can clearly see it change from that to <a href=""http://www.ti.com/general/docs/lit/getliterature.tsp?baseLiteratureNumber=sboa263"" rel=""nofollow noreferrer"">http://www.ti.com/general/docs/lit/getliterature.tsp?baseLiteratureNumber=sboa263</a> and that eventually changes to <a href=""http://www.ti.com/lit/an/sboa263/sboa263.pdf"" rel=""nofollow noreferrer"">http://www.ti.com/lit/an/sboa263/sboa263.pdf</a><br>
Now i understand you can tell me that ""Great, follow this pattern then"". But i don't want to because IMO this is not the right way to solve this automation. I want to be able to download the pdf from the first link itself.  </p>

<p>I have tried <pre><code>response = requests.get(url,allow_redirects=True)</pre></code> which only takes me to the result of the first redirect NOT the last file.
Even <code>response.history</code> takes me only to the first redirect.</p>

<p>And when i anyway try to download the file there i get a corrupted pdf that wont open. However when i manually pass the final URL just to test the correctness of my file write, i get the actual pdf in perfect order.
I don't understand why <strong>requests</strong> is not able to get to the final URL.
My full code is below for your reference -</p>

<pre><code>from bs4 import BeautifulSoup
import urllib3
import re
http = urllib3.PoolManager()
url = 'http://www.ti.com/analog-circuit/circuit-cookbook.html'
response = http.request('GET', url)
soup = BeautifulSoup(response.data)
find = re.compile(""http://www.ti.com/lit/"")
download_links = []
for link in soup.find_all('a'):
    match = re.match(find, link.get('href'))
    if match:
        #print(link.get('href'))
        download_links.append(link.get('href'))
</code></pre>

<p>To get to the redirected URL i use -</p>

<pre><code>import requests
response = requests.get(download_links[45])
if response.history:
    print (""Request was redirected"")
    for resp in response.history:
        final_url = resp.url
response = requests.get(final_url)
</code></pre>

<p>For downloading the file i used the code below -</p>

<pre><code>with open('C:/Users/Dell/Desktop/test.pdf', 'wb') as f:
    f.write(response.content)
</code></pre>

<p>Also i would actually just like to pass a folder name and all files should get downloaded with the name of the last part of the URL itself. I am yet to figure out how to do that. I tried <strong>shutils</strong> but it didn't work. If you can help me with that part too it would be great.</p>

<p>EDIT: I passed the first two URLs to Postman and i got HTML, whereas passing the third URL downloads the pdf. In the HTML that i get i can clearly see that one of the Meta properties lists the final pdf URL. Here is a part of the Postman result that's relevant - </p>

<pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
    &lt;head&gt;
        &lt;meta http-equiv=""Content-Type"" content=""text/html; charset=utf-8""&gt;
        &lt;meta content=""IE=8;IE=9;IE=edge"" http-equiv=""x-ua-compatible""&gt;
        &lt;meta content='width=device-width, initial-scale=1.0' name='viewport'&gt;
        &lt;META HTTP-EQUIV=""Pragma"" CONTENT=""no-cache""&gt;
        &lt;META HTTP-EQUIV=""Expires"" CONTENT=""-1""&gt;
        &lt;META HTTP-EQUIV=""Refresh"" CONTENT=""1; URL=http://www.ti.com/lit/an/sboa263/sboa263.pdf""&gt;
</code></pre>

<p>The parts below also show the final URL, but i think you get the idea. Can we possibly make use of this information?</p>
",1,1525585902,python;redirect;download;python-requests;urllib3,True,1652,1,1526119112,https://stackoverflow.com/questions/50196650/downloading-a-pdf-file-from-a-url-that-redirects-2-times-fails-even-when-using-r
50223940,How can I post image and text MultipartForm on python3?,"<p>I'm porting code from NodeJS to python3.
I want to post image binary data and text.
I red urllib3 User Guide but I can't do it.
How can I do it ? Thank you.</p>

<p>NodeJS</p>

<pre><code>filePath = ""xxx.jpeg""
text = ""xxx""
return chakram.request(""POST"", ""http://xxx"",
     { ""multipart"" : [
           { ""body"" : fs.createReadStream(filePath),
             ""Content-Type"" : ""image/jpeg"",
             ""Content-Disposition"" : ""name='file'; filename='"" + filePath + ""'""
           },
           { ""body"" : JSON.stringify(this.RequestData(text)),
             ""Content-Type"" : ""text""
           }
       ],
       ""headers"" : {
           ""Authorization"" : ""xxx""
       }
     })
</code></pre>

<p>My Wrong Python Code with <code>requests</code>:</p>

<pre><code>filePath = ""xxx.jpeg""
text = ""xxx""
headers = {
    ""Authorization"" : ""xxx""
}

binary_data = None
with open(file_path, 'rb') as fp:
    binary_data = fp.read()

request_body = self.create_request_body(text)

files = {
    ""file"": (filePath, binary_data, 'image/jpeg'),
    """": ("""", request_body, ""xxx"")
}
resp = requests.post(""http://xxx"", files=files, headers=headers)
</code></pre>

<p>I got 500 error. </p>
",1,1525737292,python;python-3.x;urllib;urllib3,False,71,0,1525748483,https://stackoverflow.com/questions/50223940/how-can-i-post-image-and-text-multipartform-on-python3
50187537,How to print a csv content from remote url using python 3.x?,"<p>I want to print csv content from remote url, but I get this:</p>

<blockquote>
  <p>Error                                     Traceback (most recent call
  last)  in ()
  ----> 1 for row in cr:
        2     print(row)</p>
  
  <p>Error: iterator should return strings, not int (did you open the file
  in text mode?)</p>
</blockquote>

<p>My code is:</p>

<pre><code>import csv
import urllib3

medals_url = ""http://winterolympicsmedals.com/medals.csv""
http = urllib3.PoolManager()
r = http.request(""GET"", medals_url)
r.status
response = r.data
cr = csv.reader(response)
for row in cr:
    print(row)
</code></pre>

<p>Thanks in advance.</p>
",1,1525510201,python;python-3.x;csv;urllib3,True,2732,2,1525515596,https://stackoverflow.com/questions/50187537/how-to-print-a-csv-content-from-remote-url-using-python-3-x
50180123,urllib.requests.urlopen() causes CERTIFICATE_VERIFY_FAILED on MacOS,"<p>I am trying to familiarize myself with the urllib module in Python 3. 
The code below is not working on my computer but works when I test it on repl.it for some reason. </p>

<p>Please let me know what I am doing wrong, I am running python 3.6.5 if that helps. </p>

<pre><code>    import urllib.request 
    req = urllib.request.urlopen('http://www.pythonprogramming.net') 
    print (req.read())
</code></pre>

<p>The error I get is</p>

<blockquote>
  <p>urllib.error.URLError: urlopen error [SSL: CERTIFICATE_VERIFY_FAILED]
  certificate verify failed (_ssl.c:833)</p>
</blockquote>

<p>I am using a Mac</p>
",0,1525454289,python;python-3.x;urllib3,False,309,0,1525462197,https://stackoverflow.com/questions/50180123/urllib-requests-urlopen-causes-certificate-verify-failed-on-macos
50175821,How to use sklearn pickle object created in first request across all other requests,"<p>I have the following use case In my project- Python.
We have a prediction system in place for every user request through API (We are using IIS, need to look at flask capabilities).</p>

<p>The size of a pickle file is huge, like 700MB, I don't want to load it for every request (since it is consuming more RAM for every request).</p>

<p>Can we load the pickle file into flask cache and use it for all the requests? 
  (or) 
Can we share the data among all the requests, so that we don't need to load it again and again?</p>

<p>Please help with the sample implementation of sharing pickle data across all the requests</p>
",0,1525439507,python;python-3.x;flask;flask-restful;urllib3,True,54,1,1525442533,https://stackoverflow.com/questions/50175821/how-to-use-sklearn-pickle-object-created-in-first-request-across-all-other-reque
50110646,Correct way to parse json in python,"<p>I was given a task to read json from a url and then parse it in python. Each field will be assigned to a variable to store the value. The value will then be stored into postgres table.</p>

<p><strong>Example of Json</strong></p>

<pre><code>{
 ""forecasts"": 
[
 {
   ""class"": ""fod_long_range_daily"",
   ""expire_time_gmt"": 1525126617,
   ""night"": {
     ""fcst_valid"": 1525158000,
     ""fcst_valid_local"": ""2018-05-01T19:00:00+1200"",
     ""golf_category"": """"
   },
   ""day"": {
     ""uv_warning"": 0,
     ""uv_desc"": ""Moderate"",
     ""golf_index"": 10,
     ""golf_category"": ""Excellent""
   }
 }
]
}
</code></pre>

<p>I was told that this way i am able to parse json and read it into postgres. Can i know if I am doing the right way? Will there be performance issue ?</p>

<p><strong>Style 1:</strong></p>

<pre><code>import urllib3
import psycopg2
import json

conn = psycopg2.connect(host=""localhost"", database=""nzaus"", 
user=""admin"", password=""123321"")
print(""Database Connected"")
cur = conn.cursor()
rowcount = cur.rowcount

http = urllib3.PoolManager()
url = ""https://api.data.com/v1/geocode/-35.228208/174.095969/forecast/daily/15day.json?language=en-US&amp;units=m&amp;apiKey=1234""
try:
    response = http.request('GET', url)
    data = json.loads(response.data.decode('utf-8'))

    for item in data['forecasts']:
        class = None
        time = None
        fcst_valid = None
        golf_category = None

        result = []

        class = item['class']
        time  = item['expire_time_gmt']
        fcst_valid = item['night']['fcst_valid']
        golf_category = item['morning']['golf_category']
        result = [class,time,fcst_valid,golf_category]

        cur.execute(""""""INSERT into datatable 
                       VALUES
                       ( %s,
                         %s,
                         %s,
                         %s,
                       )"""""",(result))
        conn.commit()
        cur.close()
 except IOError as io:
    print(""cannot open"")
</code></pre>
",0,1525141556,python;json;postgresql;psycopg2;urllib3,False,132,0,1525141556,https://stackoverflow.com/questions/50110646/correct-way-to-parse-json-in-python
42133948,Python: pip is installed but not working in windows,"<p>I have installed python 3.6.0, you don't need to install pip manually if you are using python (>3.3). But When I am trying to access pip (pip --version), it throws me two errors which mainly relate to not finding the module.
Description is given below.</p>

<pre><code>C:\Users\sharma6&gt;pip --version
Traceback (most recent call last):
  File ""c:\python360\lib\site-packages\pip\_vendor\requests\packages\__init__.py"", line 27, in &lt;module&gt;
    from . import urllib3
  File ""c:\python360\lib\site-packages\pip\_vendor\requests\packages\urllib3\__init__.py"", line 8, in &lt;module&gt;
    from .connectionpool import (
  File ""c:\python360\lib\site-packages\pip\_vendor\requests\packages\urllib3\connectionpool.py"", line 7, in &lt;module&gt;
    from socket import error as SocketError, timeout as SocketTimeout
  File ""c:\python360\lib\socket.py"", line 49, in &lt;module&gt;
    import _socket
zipimport.ZipImportError: can't find module '_socket'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\python360\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\python360\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\python360\Scripts\pip.exe\__main__.py"", line 5, in &lt;module&gt;
  File ""c:\python360\lib\site-packages\pip\__init__.py"", line 21, in &lt;module&gt;
    from pip._vendor.requests.packages.urllib3.exceptions import DependencyWarning
  File ""c:\python360\lib\site-packages\pip\_vendor\requests\__init__.py"", line 62, in &lt;module&gt;
    from .packages.urllib3.exceptions import DependencyWarning
  File ""c:\python360\lib\site-packages\pip\_vendor\requests\packages\__init__.py"", line 29, in &lt;module&gt;
    import urllib3
ModuleNotFoundError: No module named 'urllib3'
</code></pre>

<p>Even when I import socket (>>>import socket)in python interpreter , It shows me ""Can not found the module"" error .</p>

<pre><code>&gt;&gt;&gt; import socket
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""C:\python352\lib\socket.py"", line 49, in &lt;module&gt;
    import _socket
zipimport.ZipImportError: can't find module '_socket
</code></pre>

<p>Could someone tell me about the errors and how to resolve them because I need pip to work properly.</p>
",0,1486635536,python;sockets;pip;urllib3,True,3723,2,1524400718,https://stackoverflow.com/questions/42133948/python-pip-is-installed-but-not-working-in-windows
49945083,Python HTTPS Post: can not find existing url,"<p>I am currently working on a project where i have to send json object to an embedded pc, which implements Json RPC.
The device is in my LAN, has an ip address and is reachable.</p>

<p>Like two weeks ago my code worked, but as I sat down today to go on it didn't work anymore.</p>

<p>I working with Visual Studio 2017.
So far I tried Python 3.4 (64 bit), Python 3.6 (64 and 32bit)</p>

<p>Here is what I tried:</p>

<pre><code>import json
import urllib3
import sys
import base64
import os
import io
import base64
import hashlib
from base64 import b64encode

role = ""admin""
passwd = ""*******""
service_url = ""https://192.168.0.65/base_service/""
http = urllib3.PoolManager( assert_hostname=False, ca_certs=""cert/myCertificate.crt"" )
passwd_b64 = str( b64encode( bytes( passwd, ""utf8"" ) ), ""utf8"" )
rpc_obj = {""jsonrpc"": ""2.0"",""id"":1,""method"":""get_auth_token"",""params"":{""user"": ""admin"", ""pwd"": passwd_b64}}

at = http.urlopen( ""POST"", service_url, body=json.dumps( rpc_obj ) )
</code></pre>

<p>This should generate an token used for other functions. but at the last line I get this error:</p>

<pre><code>urllib3.exceptions.MaxRetryError
    Nachricht = HTTPSConnectionPool(host='192.168.0.65', port=443): Max retries exceeded with url: /base_service/ (Caused by SSLError(FileNotFoundError(2, 'No such file or directory'),))
    Stapelüberwachung:
C:\Program Files (x86)\Entwicklung\Python\lib\site-packages\urllib3\util\retry.py:388 in ""Retry.increment""
C:\Program Files (x86)\Entwicklung\Python\lib\site-packages\urllib3\connectionpool.py:639 in ""HTTPConnectionPool.urlopen""
C:\Program Files (x86)\Entwicklung\Python\lib\site-packages\urllib3\connectionpool.py:668 in ""HTTPConnectionPool.urlopen""
C:\Program Files (x86)\Entwicklung\Python\lib\site-packages\urllib3\connectionpool.py:668 in ""HTTPConnectionPool.urlopen""
C:\Program Files (x86)\Entwicklung\Python\lib\site-packages\urllib3\connectionpool.py:668 in ""HTTPConnectionPool.urlopen""
C:\Program Files (x86)\Entwicklung\Python\lib\site-packages\urllib3\poolmanager.py:321 in ""PoolManager.urlopen""
C:\Users\UserXY\source\repos\containerController\containerController.py:20 in ""&lt;module&gt;""
</code></pre>

<p>I have no idea why I get this error. I also have a java programm, which does the same, but works perfectly.
Sadly the documentation of the device only has some python examples and I am not able to implement all functions in java.</p>

<p>Does anyone have an idea or hint?</p>
",0,1524238116,python;https;json-rpc;urllib3,True,163,1,1524239702,https://stackoverflow.com/questions/49945083/python-https-post-can-not-find-existing-url
43531481,IndexError: list index out of range [Python 3.x Web scraping],"<p>I am trying to grab geo locations using URLs from a csv by searching the twitter, tweets urls. The input file has more than 100K rows with bunch of columns. </p>

<p>I am using python 3.x anaconda with all the updated version and I am getting following error: </p>

<pre><code>Traceback (most recent call last):
  File ""__main__.py"", line 21, in &lt;module&gt;
    location = get_location(userid)
  File ""C:path\twitter_location.py"", line 22, in get_location
    location = html.select('.ProfileHeaderCard-locationText')[0].text.strip()
IndexError: list index out of range
</code></pre>

<p>The code below : </p>

<pre><code>#!/usr/env/bin python
import urllib.request
import urllib3
from bs4 import BeautifulSoup

def get_location(userid):
    '''
    Get location as string ('Paris', 'New york', ..) by scraping twitter profils page.
    Returns None if location can not be scrapped
    '''

    page_url = 'http://twitter.com/{0}'.format(userid)

    try:
        page = urllib.request.urlopen(page_url)
    except urllib.request.HTTPError:
        print ('ERROR: user {} not found'.format(userid))
        return None

    content = page.read()
    html = BeautifulSoup(content)
    location = html.select('.ProfileHeaderCard-locationText')[0].text.strip()

    if location.strip() == '':
        return None
    return location.strip()
</code></pre>

<p>I am looking for a quick fix so that I can execute the whole input files with more than 100k rows. </p>

<p><strong>Edit: I</strong>
As mentioned in the answer below, After including the <code>try</code> block the outputs have stopped grabbing geo location. </p>

<p>Before the inclusion of <code>try</code> block after certain count <code>list out of range</code> error.</p>

<p>After Including the <code>try</code> block the error is gone and so the coordinates. I am getting all <code>none</code> values. </p>

<p>Here is the <a href=""https://www.dropbox.com/sh/9o8ycus6tkewsz4/AACpX0hL1pei1du2uaxLON0ba?dl=0"" rel=""nofollow noreferrer"">DropBox</a> link with Input, Before &amp; After Output &amp; entire code bundle. </p>

<p><strong>Edit: II</strong></p>

<p>Entire code and inputs are in the dropbox I am searching for some help where we can eliminate the entire API thing and find an alternative to pull geo locations of twitter usernames. </p>

<p>Appreciate the help in fixing the problem. Thanks in advance. </p>
",-5,1492730663,python;web-scraping;beautifulsoup;urllib3;indexoutofrangeexception,True,1246,2,1523310080,https://stackoverflow.com/questions/43531481/indexerror-list-index-out-of-range-python-3-x-web-scraping
49728265,"pip not installing any package with warnings &quot; InsecurePlatformWarning, SNIMissingWarning&quot;","<p>Python version: 2.7.6
Pip version: 9.0.3</p>

<pre><code>C:\Python27\Scripts&gt;pip install pyOpenSSL
Collecting pyOpenSSL
</code></pre>

<h2>below is the error while installing any package</h2>

<blockquote>
  <p>c:\python27\lib\site-packages\pip_vendor\urllib3\util\ssl_.py:339: SNIMissingWarning: An HTTPS request has been made, b
  ut the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to pr
  esent an incorrect TLS certificate, which can cause validation failures. You can upgrade to a newer version of Python to
   solve this. For more information, see <a href=""https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings"" rel=""nofollow noreferrer"">https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings</a>
    SNIMissingWarning</p>
  
  <p>c:\python27\lib\site-packages\pip_vendor\urllib3\util\ssl_.py:137: InsecurePlatformWarning: A true SSLContext object is
   not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail.
  You can upgrade to a newer version of Python to solve this. For more information, see <a href=""https://urllib3.readthedocs.io/en/"" rel=""nofollow noreferrer"">https://urllib3.readthedocs.io/en/</a>
  latest/advanced-usage.html#ssl-warnings
    InsecurePlatformWarning
    Could not find a version that satisfies the requirement certify (from versions: )
  No matching distribution found for pyOpenSSL</p>
</blockquote>

<p>Tried installing <code>urllib3[secure] --upgrade</code>, but getting the below error along with InsecurePlatformWarning</p>

<blockquote>
  <p>Could not find a version that satisfies the requirement pyOpenSSL>=0.14; python_version &lt;= ""2.7"" and extra == ""secure""
   (from urllib3[secure]) (from versions: )
  No matching distribution found for pyOpenSSL>=0.14; python_version &lt;= ""2.7"" and extra == ""secure"" (from urllib3[secure])</p>
</blockquote>

<p>Tried installing the packages suggested in <a href=""https://stackoverflow.com/questions/29099404/ssl-insecureplatform-error-when-using-requests-package?noredirect=1&amp;lq=1"">SSL InsecurePlatform error when using Requests package</a> getting the same warning issues.</p>

<p>I cannot upgrade the python version due to some constraints. Any other way to resolve the issue?</p>
",0,1523261499,python;pip;pyopenssl;urllib3,True,3196,1,1523268656,https://stackoverflow.com/questions/49728265/pip-not-installing-any-package-with-warnings-insecureplatformwarning-snimissi
49678261,Strange PHP form post,"<p>So I'm writing a web crawler to batch download PDFs from my university's website, as I don't fancy downloading them one by one. </p>

<p>I've got most the code working, using the 'requests' module. The issue is, you have to be signed in to a university account to access the PDFs, so I've set up requests to use cookies to sign into my university account before downloading the PDFs, however the HTML form to sign in on the university page is rather peculiar.</p>

<p>I've abstracted the HTML which can be found here: </p>

<pre><code>&lt;form action=""/login"" method=""post""&gt;
    &lt;fieldset&gt;
        &lt;div&gt;
            &lt;label for=""username""&gt;Username:&lt;/label&gt;                          
            &lt;input id=""username"" name=""username"" type=""text"" value="""" /&gt;

            &lt;label for=""password""&gt;Password:&lt;/label&gt;
            &lt;input id=""password"" name=""password"" type=""password"" value=""""/&gt;

            &lt;input type=""hidden"" name=""lt"" value="""" /&gt;
            &lt;input type=""hidden"" name=""execution"" value=""*very_long_encrypted_code*"" /&gt;
            &lt;input type=""hidden"" name=""_eventId"" value=""submit"" /&gt;
            &lt;input type=""submit"" name=""submit"" value=""Login"" /&gt;
        &lt;/div&gt;
    &lt;/fieldset&gt;
&lt;/form&gt;
</code></pre>

<p>Firstly the <code>action</code> parameter in the form does not reference a PHP file which I don't understand. Is <code>action=""/login""</code> referencing the page itself, or <code>http://www.blahblah/login/login</code>? (the HTML is taken from the page <code>http://www.blahblah/login</code>.</p>

<p>Secondly, what's with all the 'hidden' inputs? I'm not sure how this page is taking the given login data and passing it to a PHP script.</p>

<p>This has led to the failure of the requests sign on in my python script:</p>

<pre><code>import requests
user = input(""User: "")
passw = input(""Password: "")
payload = {""username"" : user, ""password"" : passw}
s = requests.Session()
s.post(loginURL, data = payload)
r = s.get(url)
</code></pre>

<p>I would have thought this would take the login data and sign me into the page, but <code>r</code> is just assigned the original logon page. I'm assuming it's to do with the strange PHP interation in the HTML. Any ideas what I need to change?</p>

<p>EDIT: Thought I'd also mention there is no javascript on the page at all. Purely HTML &amp; CSS</p>
",2,1522949599,python;html;python-requests;urllib3,True,65,1,1522950479,https://stackoverflow.com/questions/49678261/strange-php-form-post
49096813,apt-get install unable to locate package python-urllib,"<pre><code>sudo apt-get install python-urllib    
Reading package lists... Done    
Building dependency tree          
Reading state information... Done    
E: Unable to locate package python-urllib
</code></pre>

<p>Help me please</p>
",0,1520176181,python;python-requests;urllib2;urllib;urllib3,False,3452,1,1522248078,https://stackoverflow.com/questions/49096813/apt-get-install-unable-to-locate-package-python-urllib
49474631,Python - Urllib3 and Requests connection error shows timeout or proxy,"<p>I'm trying to use the GET method using python-requests but I get an error with the socket connections. Used the urlopen with urllib too but got the same error.</p>

<p>I've been looking for solutions over the internet but nothing seems to work. I'm not using a proxy connection and I've tried keeping a timeout=5 but it doesn't help. (This is what the other solutions suggested)</p>

<p>The basic command - pip3 install ""(any_module_name)"" also doesn't work giving me the same errors. I've tried reinstalling pip3 but it didn't help.</p>

<pre><code>Python 3.5.2 (default, Nov 23 2017, 16:37:01) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import requests
&gt;&gt;&gt; res = requests.get('https://www.google.co.in',timeout=5)
Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/urllib3/connection.py"", line 137, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File ""/usr/lib/python3/dist-packages/urllib3/util/connection.py"", line 91, in create_connection
    raise err
  File ""/usr/lib/python3/dist-packages/urllib3/util/connection.py"", line 81, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 555, in urlopen
    self._prepare_proxy(conn)
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 753, in _prepare_proxy
    conn.connect()
  File ""/usr/lib/python3/dist-packages/urllib3/connection.py"", line 217, in connect
    conn = self._new_conn()
  File ""/usr/lib/python3/dist-packages/urllib3/connection.py"", line 142, in _new_conn
    (self.host, self.timeout))
requests.packages.urllib3.exceptions.ConnectTimeoutError: (&lt;requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7f807b2bcac8&gt;, 'Connection to 172.16.19.10 timed out. (connect timeout=5)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/requests/adapters.py"", line 376, in send
    timeout=timeout
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File ""/usr/lib/python3/dist-packages/urllib3/util/retry.py"", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.google.co.in', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(&lt;requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7f807b2bcac8&gt;, 'Connection to 172.16.19.10 timed out. (connect timeout=5)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/lib/python3/dist-packages/requests/api.py"", line 67, in get
    return request('get', url, params=params, **kwargs)
  File ""/usr/lib/python3/dist-packages/requests/api.py"", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/usr/lib/python3/dist-packages/requests/sessions.py"", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/lib/python3/dist-packages/requests/sessions.py"", line 576, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/lib/python3/dist-packages/requests/adapters.py"", line 432, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='www.google.co.in', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(&lt;requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7f807b2bcac8&gt;, 'Connection to 172.16.19.10 timed out. (connect timeout=5)'))
</code></pre>
",0,1521972290,python;python-3.x;python-requests;urllib3;socket-timeout-exception,False,4480,0,1521979191,https://stackoverflow.com/questions/49474631/python-urllib3-and-requests-connection-error-shows-timeout-or-proxy
49394007,Python 2.7 import requests issue,"<p>I've tried to install requests module (Debian, Python 2.7) by <code>pip install requests</code> command. Output was:</p>

<pre><code>Installing collected packages: urllib3, requests
Successfully installed requests-2.18.4 urllib3-1.22
</code></pre>

<p>Tried following the following <a href=""http://docs.python-requests.org/en/master/user/quickstart/"" rel=""nofollow noreferrer"">Quickstart tutorial</a> but I get the following error at <code>import requests</code></p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""requests/__init__.py"", line 76, in &lt;module&gt;
    check_compatibility(urllib3.__version__, chardet.__version__)
AttributeError: 'module' object has no attribute '__version__'
</code></pre>

<p>Any help?</p>
",0,1521579537,python;python-2.7;python-requests;urllib3,True,823,1,1521665828,https://stackoverflow.com/questions/49394007/python-2-7-import-requests-issue
20467107,Which urllib I should choose?,"<p>as we know, python has two built-in url lib:   </p>

<ul>
<li><code>urllib</code> </li>
<li><code>urllib2</code></li>
</ul>

<p>and a third-party lib:   </p>

<ul>
<li><code>urllib3</code></li>
</ul>

<p>if my requirement is only to request a API by GET method, assume it return a JSON string.<br>
which lib I should use? do they have some duplicated functions?<br>
if the <code>urllib</code> can implement my require, but after if my requirements get more and more complicated, the <code>urllib</code> can not fit my function, I should import another lib at that time, but I really want to import only one lib, because I think import all of them can make me confused, I think the method between them are totally different.   </p>

<p>so now I am confused which lib I should use, I prefer <code>urllib3</code>, I think it can fit my requirement all time, how do you think? </p>
",9,1386580983,python;urllib2;urllib;urllib3,True,23231,3,1520278654,https://stackoverflow.com/questions/20467107/which-urllib-i-should-choose
49096283,Python Keep-Alive proxy connection with urrlib3,"<p>I use the following method to maintain a Keep-Alive connection through a proxy to the target site. </p>

<pre><code>import http.client

proxies = {""https"": ""https://user:pass@host:port""}
url = urlparse(proxies[""https""])            
auth = '%s:%s' % (url.username, url.password)
headers['Proxy-Authorization'] = 'Basic '+base64.b64encode(auth.encode('utf-8')).decode()

conn = http.client.HTTPSConnection(proxy.hostname, proxy.port)
conn.set_tunnel(""target_site.com"",headers=p_headers)
conn.request(""POST"", ""/tapi/"", payload, headers)
</code></pre>

<p>However,  HTTPSConnection is not thread safe. I am trying to do connection through urllib3 so that I will be able to use multiprocessing. My code doesn't work. What is the wright way to do it?</p>

<pre><code>import urllib3
default_headers = urllib3.make_headers(proxy_basic_auth=""user:pass"")
conn=urllib3.HTTPSConnectionPool('https://target_site.com',maxsize=2,
         _proxy=""https://host:port/"", _proxy_headers=default_headers)
conn.request(""POST"", ""/tapi/"",fields=payload, headers=headers)
</code></pre>
",2,1520172341,python;proxy;python-requests;keep-alive;urllib3,False,617,0,1520180083,https://stackoverflow.com/questions/49096283/python-keep-alive-proxy-connection-with-urrlib3
48968918,urlib usage from Python 3,"<p>I get the following error:</p>

<pre><code>TypeError: POST data should be bytes, an iterable of bytes, or a file object. It cannot be of type str.
</code></pre>

<p>When making the call below</p>

<pre><code>import urllib.request, urllib.parse, urllib.error, urllib.request, 
urllib.error, urllib.parse
import json

chemcalcURL = 'http://www.chemcalc.org/chemcalc/em'

# Define a molecular formula string
mfRange = 'C0-100H0-100N0-10O0-10'
# target mass
mass = 300

# Define the parameters and send them to Chemcalc
# other options (mass tolerance, unsaturation, etc.
params = {'mfRange': mfRange,'monoisotopicMass': mass}


response = urllib.request.urlopen(chemcalcURL, urllib.parse.urlencode(params))

# Read the output and convert it from JSON into a Python dictionary
jsondata = response.read()
data = json.loads(jsondata)

print(data)
</code></pre>
",1,1519517234,python;python-3.x;urllib3,True,160,1,1519519288,https://stackoverflow.com/questions/48968918/urlib-usage-from-python-3
48947682,"Python, append curl results to list with urllib3","<p>I am trying to append the results from a curl request using urllib3 to a list. You can see from my below attempt that I am out of my depth already :) </p>

<pre><code>import urllib3
import json
import csv

http = urllib3.PoolManager()
r = http.request('GET', 'URL')
mails = json.loads(r.data.decode('utf-8'))
{'origin': '127.0.0.1'}
datas = list()
for mail in mails:
    datas.append(mail)
</code></pre>

<p>I fixed the syntax errors thanks for the answers. The results however appear to be incorrect.</p>

<p>What I expect one entry in the list to look like:</p>

<pre><code>{""success"":true,""data:[{""id"":20758,""company_id"":117376,""user_id"":1529193,""done"":false,""type"":""email"",""reference_type"":""none"",""reference_id"":null,""due_date"":""2017-09-01"",""due_time"":"""",""duration"":"""",""add_time"":""2017-01-13 
</code></pre>

<p>What I get is ONLY this, nothing else:</p>

<pre><code>success 
data
additional_data 
related_object
</code></pre>

<p>I am guessing it is something to do with the JSON hierarchy?</p>
",-1,1519388027,python;python-3.x;urllib3,True,219,3,1519389730,https://stackoverflow.com/questions/48947682/python-append-curl-results-to-list-with-urllib3
47653924,Installing dropbox using pip hangs,"<p>I want to install dropbox using '<strong>pip install dropbox</strong>'
but it constantly hangs when it comes to collect urllib3.
Any idea?</p>

<pre><code>(venv) E:\Python\DropBoxClient\webapp&gt;pip install dropbox
Collecting dropbox
  Using cached dropbox-8.5.0-py3-none-any.whl
Collecting requests&gt;=2.16.2 (from dropbox)
  Using cached requests-2.18.4-py2.py3-none-any.whl
Collecting six&gt;=1.3.0 (from dropbox)
  Using cached six-1.11.0-py2.py3-none-any.whl
Collecting idna&lt;2.7,&gt;=2.5 (from requests&gt;=2.16.2-&gt;dropbox)
  Using cached idna-2.6-py2.py3-none-any.whl
Collecting urllib3&lt;1.23,&gt;=1.21.1 (from requests&gt;=2.16.2-&gt;dropbox)
</code></pre>
",0,1512477775,python;pip;dropbox-api;urllib3,False,127,1,1518651550,https://stackoverflow.com/questions/47653924/installing-dropbox-using-pip-hangs
48186625,Submit button in webform does not post any params,"<p>I am trying to automate the submit form of <a href=""http://pantherdb.org/"" rel=""nofollow noreferrer"">PantherDB</a>. I have previously used python with requests to send the different parameters to the form. Usually the ""submit"" button in the forms also sends a Param during the Post. But in this website (<a href=""http://pantherdb.org/"" rel=""nofollow noreferrer"">PantherDB</a>) the ""submit"" button does not send any Param during the Post.</p>

<p>How can I tell python that it should ""click"" the submit button?</p>

<p>This is what I have in my code. </p>

<pre><code>url = ""http://www.pantherdb.org""
files = {""fileData"" : open(inputFile, ""r"")}

response = requests.post(url,  files = files, data = myParams)
</code></pre>
",0,1515584339,python;forms;python-requests;urllib3,False,54,1,1518399721,https://stackoverflow.com/questions/48186625/submit-button-in-webform-does-not-post-any-params
48516986,urlib3 https Not connected,"<p>I have local ubuntu server.
I try in local ubuntu server </p>

<pre><code>import certifi
import urllib3
http = urllib3.PoolManager(
    cert_reqs='CERT_REQUIRED',
    ca_certs=certifi.where())
http.request('GET', 'https://dormi.kongju.ac.kr/main/contents/food.php?mid=40&amp;k=2')
&lt;urllib3.response.HTTPResponse object at 0x7fab2c8c40f0&gt;
r = http.request('GET', 'https://dormi.kongju.ac.kr/main/contents/food.php?mid=40&amp;k=2')
print(r.data)
</code></pre>

<p><img src=""https://i.stack.imgur.com/Hbmh9.png"" alt=""enter image description here""></p>

<p>It's well-connected.</p>

<p>but not connect in digitalocean </p>

<p>I need help</p>
",0,1517300945,python;urllib3,False,50,0,1517304250,https://stackoverflow.com/questions/48516986/urlib3-https-not-connected
38700924,Python - urllib3 get text from docx using tika server,"<p>I am using <code>python3</code>, <code>urllib3</code> and <code>tika-server-1.13</code> in order to get text from different types of files. This is my python code:</p>

<pre><code>def get_text(self, input_file_path, text_output_path, content_type):
    global config

    headers = util.make_headers()
    mime_type = ContentType.get_mime_type(content_type)
    if mime_type != '':
        headers['Content-Type'] = mime_type

    with open(input_file_path, ""rb"") as input_file:
        fields = {
            'file': (os.path.basename(input_file_path), input_file.read(), mime_type)
        }

    retry_count = 0
    while retry_count &lt; int(config.get(""Tika"", ""RetriesCount"")):
        response = self.pool.request('PUT', '/tika', headers=headers, fields=fields)
        if response.status == 200:
            data = response.data.decode('utf-8')
            text = re.sub(""[\[][^\]]+[\]]"", """", data)
            final_text = re.sub(""(\n(\t\r )*\n)+"", ""\n\n"", text)
            with open(text_output_path, ""w+"") as output_file:
                output_file.write(final_text)
            break
        else:
            if retry_count == (int(config.get(""Tika"", ""RetriesCount"")) - 1):
                return False
            retry_count += 1
    return True
</code></pre>

<p>This code works for html files, but when i am trying to parse text from docx files it doesn't work.</p>

<p>I get back from the server Http error code <code>422: Unprocessable Entity</code> </p>

<p>Using the <code>tika-server</code> <a href=""https://wiki.apache.org/tika/TikaJAXRS"" rel=""nofollow"">documentation</a> I've tried using <code>curl</code> to check if it works with it:</p>

<pre><code>curl -X PUT --data-binary @test.docx http://localhost:9998/tika --header ""Content-type: application/vnd.openxmlformats-officedocument.wordprocessingml.document""
</code></pre>

<p>and it worked. </p>

<p>At the <a href=""https://wiki.apache.org/tika/TikaJAXRS"" rel=""nofollow"">tika server docs</a>:</p>

<blockquote>
  <p>422 Unprocessable Entity - Unsupported mime-type, encrypted document &amp; etc </p>
</blockquote>

<p>This is the correct mime-type(also checked it with tika's detect system), it's supported and the file is not encrypted.</p>

<p>I believe this is related to how I upload the file to the tika server, What am I doing wrong?</p>
",0,1470060248,python;python-3.x;urllib;apache-tika;urllib3,True,692,2,1516905577,https://stackoverflow.com/questions/38700924/python-urllib3-get-text-from-docx-using-tika-server
48161651,Dealing with Bad request,"<p>I’m getting: 'HTTP/1.1 400 Bad Request\r\n' and I don’t get why.  It looks like it authenticates and then there is a redirection and then it now doesn’t work. Why is this happening?  </p>

<p>I had thought it was the header and that it was missing content type, but even adding that produced the same outcome..</p>

<pre><code>headers = {
    'basic_auth': 'brofewfefwefewef:EKAXsWkdt5H6yJEmtexN',
    'Content-Type': 'application/json'
}
client = Client(ClientConfig(), headers=headers, refresh=True)


class FileDownloader(object):
    ...Line 152...
    def _get_http_pool(self, secure=True):
        if secure:
            _http = urllib3.PoolManager(cert_reqs=str('CERT_REQUIRED'),
                                        ca_certs=certifi.where())
        else:
            _http = urllib3.PoolManager()

        if self.headers:
            content_type = self.headers.get('Content-Type')
            if 'Content-Type' in self.headers:
                del self.headers['Content-Type']
            _headers = urllib3.util.make_headers(**self.headers)
            _http.headers.update(_headers)
            if content_type:
                _http.headers['content-type'] = content_type
        print(_http.headers)
        return _http
</code></pre>

<p><a href=""https://github.com/JMSwag/PyUpdater/blob/master/pyupdater/client/downloader.py"" rel=""nofollow noreferrer"">https://github.com/JMSwag/PyUpdater/blob/master/pyupdater/client/downloader.py</a>
Line 366, is where the download itself starts.  This is perplexing to say the least.</p>

<p>Error:</p>

<pre><code>DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.bitbucket.org
send: b'GET /2.0/repositories/Anexampleuser/repo/downloads/keys.gz HTTP/1.1\r\nHost: api.bitbucket.org\r\nAccept-Encoding: identity\r\nauthorization: Basic YnJvZmV3ZmVmd2VmZXdlZjpFS0FYc1drZHQ1SDZ5SkVtdGV4Tg==\r\n\r\n'
reply: 'HTTP/1.1 302 Found\r\n'
DEBUG:urllib3.connectionpool:https://api.bitbucket.org:443 ""GET/2.0/repositories/Anexampleuser/repo/downloads/keys.gz HTTP/1.1"" 302 0
DEBUG:urllib3.util.retry:Incremented Retry for (url='https://api.bitbucket.org/2.0/repositories/Anexampleuser/repo/downloads/keys.gz'): Retry(total=2, connect=None, read=None, redirect=None, status=None)
INFO:urllib3.poolmanager:Redirecting https://api.bitbucket.org/2.0/repositories/Anexampleuser/repo/downloads/keys.gz -&gt; https://bbuseruploads.s3.amazonaws.com/a0e395b6-0c54-4efb-9074-57ec4190020b/downloads/3fc0be6d-ca69-42d3-9711-fbb5cfd2bc38/keys.gz?Signature=ZQxeUTvYC3Q%2Fo1aaS1CSuzyit0Q%3D&amp;Expires=1515976464&amp;AWSAccessKeyId=AKIAIQWXW6WLXMB5QZAQ&amp;versionId=n.ymY11KRkq36Xozy25aChvfUT.YzTf5&amp;response-content-disposition=attachment%3B%20filename%3D%22keys.gz%22
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): bbuseruploads.s3.amazonaws.com
header: Server header: Vary header: Content-Type header: X-OAuth-Scopes header: Strict-Transport-Security header: Date header: Location header: X-Served-By header: ETag header: X-Static-Version header: X-Content-Type-Options header: X-Accepted-OAuth-Scopes header: X-Credential-Type header: X-Render-Time header: Connection header: X-Request-Count header: X-Frame-Options header: X-Version header: Content-Length send: b'GET /a0e395b6-0c54-4efb-9074-57ec4190020b/downloads/3fc0be6d-ca69-42d3-9711-fbb5cfd2bc38/keys.gz?Signature=ZQxeUTvYC3Q%2Fo1aaS1CSuzyit0Q%3D&amp;Expires=1515976464&amp;AWSAccessKeyId=AKIAIQWXW6WLXMB5QZAQ&amp;versionId=n.ymY11KRkq36Xozy25aChvfUT.YzTf5&amp;response-content-disposition=attachment%3B%20filename%3D%22keys.gz%22 HTTP/1.1\r\nHost: bbuseruploads.s3.amazonaws.com\r\nAccept-Encoding: identity\r\nauthorization: Basic YnJvZmV3ZmVmd2VmZXdlZjpFS0FYc1drZHQ1SDZ5SkVtdGV4Tg==\r\n\r\n'
reply: 'HTTP/1.1 400 Bad Request\r\n'
</code></pre>
",1,1515473752,python;urllib3,True,1546,3,1516318259,https://stackoverflow.com/questions/48161651/dealing-with-bad-request
48326015,Using urllib3 to fetch JSON data,"<p>I have written my own URL shortener in PHP and now am trying to develop a python frontend for it. My python script is as follows:</p>

<pre><code>import certifi
import urllib3
import json
manager = urllib3.PoolManager(
    cert_reqs='CERT_REQUIRED',
    ca_certs=certifi.where())
url = input(""Please enter a URL: "")
short = manager.request(""GET"", ""https://www.get-short.net/feed.php?i="" + url)
data = short.read(decode_content=True)
print(json.loads(data))
</code></pre>

<p>and, when I run it, I get the following output when I type in a URL    </p>

<pre><code>c:\Projects&gt;python short.py
Please enter a URL: https://docs.python.org/2/library/json.html
Traceback (most recent call last):
File ""short.py"", line 10, in &lt;module&gt;
    print(json.loads(data))
File ""C:\Program Files\Python36\lib\json\__init__.py"", line 354, in loads
    return _default_decoder.decode(s)
File ""C:\Program Files\Python36\lib\json\decoder.py"", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
File ""C:\Program Files\Python36\lib\json\decoder.py"", line 357, in raw_decode
    raise JSONDecodeError(""Expecting value"", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
</code></pre>

<p>I know the problem is either my code or my JSON output but what do I have to do to make it work</p>
",0,1516292980,python;json;urllib3,False,2081,1,1516293321,https://stackoverflow.com/questions/48326015/using-urllib3-to-fetch-json-data
48322094,Downloading a webpage using urllib3,"<p>I'm trying to write a program for an assignment that uses urllib3 to download a webpage and store it in a dictionary. (I'm using spyder 3.6)
The program is giving me an 'AttributeError' and I have no idea what I'm doing wrong. here is my code with step by step notes I wrote for the assignment.</p>

<pre><code>#Downloading a webpage
import urllib3
import sys
#these import statements allow us to use 'modules' aka 'libraries' ....
#code written by others that we can use

urlToRead = 'http://www.google.com'
#This value won't actually get used, because of the way the while loop
#below is set up. But while loops often need a dummy value like this to
#work right the first time

crawledWebLinks = {}
#Initialize an empty dictionary, in which (key, value) pairs will correspond to (short, url) eg
#(""Goolge"" , ""http://www.google.com"")

#Ok, there is a while loop coming up

#Here ends the set up

while urlToRead != ' ':
#This is a condition that dictates that the while loop will keep checking
#as long as this condition is true the loop will continue, if false it will stop
    try:
        urlToRead = input(""Please enter the next URL to crawl"")
    #the ""try"" prevents the program from crashing if there is an error
    #if there is an error the program will be sent to the except block
        if urlToRead == '':
            print (""OK, exiting loop"")
            break
        #if the user leaves the input blank it will break out of the loop
        shortName = input(""Please enter a short name for the URL "" + urlToRead)
        webFile = urllib3.urlopen(urlToRead).read()
        #This line above uses a ready a readymade function in the urllib3 module to
        #do something super - cool:
        #IT takes a url, goes to the website for the url, downloads the
        #contents (which are in the form of HTML) and returns them to be
        #stored in a string variable (here called webFile)
        crawledWebLinks[shortName] = webFile
        #this line above place a key value pair (shortname, HTML for that url)
        #in the dictionary
    except:
        #this bit of code - the indented lines following 'except:' will be
        #excecuted if the code in the try block (the indented following lines
        #the 'try:' above) throw and error
        #this is an example of something known as exeption-handling
        print (""*************\nUnexpected Error*****"", sys.exc_info()[0])
        #The snip 'sys.exc_info()[0]' return information about the last
        #error that occurred - 
        #this code is made available through the sys library that we imported above
        #Quite Magical :)
        stopOrProceed = input(""Hmm..stop or proceed? Enter 1 to stop, enter anything else to continue"")
        if stopOrProceed ==1 :
            print ('OK...Stopping\n')
            break
        #this break will break out of the nearest loop - in this case,
        #the while loop
    else:
        print (""Cool! Let's continue\n"")
        continue
        # this continue will skip out of the current iteration of this 
        #loop and move to the next i.e. the loop will reset to the start
print (crawledWebLinks.keys())
</code></pre>
",0,1516280712,python;urllib3,True,2140,1,1516282343,https://stackoverflow.com/questions/48322094/downloading-a-webpage-using-urllib3
48222953,How do I download file to local directory,"<p>How do I download a file in python to local directory C:\1\1.  I see lot of examples but most seem to be 5+ years old with out of date information.  Thanks.</p>

<pre><code> import urllib.request

url = ""http://download.thinkbroadband.com/10MB.zip""

file_name = url.split('/')[-1]
u = urllib.request.urlretrieve.urlopen(url)
f = open(file_name, 'wb')
meta = u.info()
file_size = int(meta.getheaders(""Content-Length"")[0])
print(""Downloading: %s Bytes: %s"" % (file_name, file_size))

file_size_dl = 0
block_sz = 8192
while True:
    buffer = u.read(block_sz)
    if not buffer:
        break

    file_size_dl += len(buffer)
    f.write(buffer)
    status = r""%10d  [%3.2f%%]"" % (file_size_dl, file_size_dl * 100. / file_size)
    status = status + chr(8)*(len(status)+1)
    print(status,)

f.close()
</code></pre>

<p>Error:  function has no attribute url open</p>

<pre><code>AttributeError: 'function' object has no attribute 'urlopen'
</code></pre>

<p>Can someone explain to me how i download a simple zip from a website this does not work for me.  Cheers.</p>
",0,1515748574,python;download;python-requests;urllib3,True,489,1,1515750501,https://stackoverflow.com/questions/48222953/how-do-i-download-file-to-local-directory
47984230,Wrapping urllib3.HTTPResponse in io.TextIOWrapper,"<p>I use AWS <code>boto3</code> library which returns me an instance of <code>urllib3.response.HTTPResponse</code>. That response is a subclass of <code>io.IOBase</code> and hence behaves as a binary file. Its <code>read()</code> method returns <code>bytes</code> instances.</p>

<p>Now, I need to decode <code>csv</code> data from a file received in such a way. I want my code to work on both <code>py2</code> and <code>py3</code> with minimal code overhead, so I use <code>backports.csv</code> which relies on <code>io.IOBase</code> objects as input rather than on py2's <code>file()</code> objects.</p>

<p>The first problem is that <code>HTTPResponse</code> yields <code>bytes</code> data for CSV file, and I have <code>csv.reader</code> which expects <code>str</code> data.</p>

<pre><code>&gt;&gt;&gt; import io
&gt;&gt;&gt; from backports import csv  # actually try..catch statement here
&gt;&gt;&gt; from mymodule import get_file

&gt;&gt;&gt; f = get_file()  # returns instance of urllib3.HTTPResponse
&gt;&gt;&gt; r = csv.reader(f)
&gt;&gt;&gt; list(r)
Error: iterator should return strings, not bytes (did you open the file in text mode?)
</code></pre>

<p>I tried to wrap <code>HTTPResponse</code> with <code>io.TextIOWrapper</code> and got error <code>'HTTPResponse' object has no attribute 'read1'</code>. This is expected becuase <code>TextIOWrapper</code> is intended to be used with <code>BufferedIOBase</code> objects, not <code>IOBase</code> objects. And it only happens on <code>python2</code>'s implementation of <code>TextIOWrapper</code> because it always expects underlying object to have <code>read1</code> (<a href=""https://github.com/python/cpython/blob/2.7/Modules/_io/textio.c#L1429"" rel=""nofollow noreferrer"">source</a>), while <code>python3</code>'s implementation checks for <code>read1</code> existence and falls back to <code>read</code> gracefully (<a href=""https://github.com/python/cpython/blob/3.6/Modules/_io/textio.c#L1495"" rel=""nofollow noreferrer"">source</a>).</p>

<pre><code>&gt;&gt;&gt; f = get_file()
&gt;&gt;&gt; tw = io.TextIOWrapper(f)
&gt;&gt;&gt; list(csv.reader(tw))
AttributeError: 'HTTPResponse' object has no attribute 'read1'
</code></pre>

<p>Then I tried to wrap <code>HTTPResponse</code> with <code>io.BufferedReader</code> and then with <code>io.TextIOWrapper</code>. And I got the following error:</p>

<pre><code>&gt;&gt;&gt; f = get_file()
&gt;&gt;&gt; br = io.BufferedReader(f)
&gt;&gt;&gt; tw = io.TextIOWrapper(br)
&gt;&gt;&gt; list(csv.reader(f))
ValueError: I/O operation on closed file.
</code></pre>

<p>After some investigation it turns out that the error only happens when the file doesn't end with <code>\n</code>. If it does end with <code>\n</code> then the problem does not happen and everything works fine.</p>

<p>There is some additional logic for closing underlying object in <code>HTTPResponse</code> (<a href=""https://github.com/shazow/urllib3/blob/master/urllib3/response.py#L385"" rel=""nofollow noreferrer"">source</a>) which is seemingly causing the problem.</p>

<p><strong>The question is:</strong> how can I write my code to</p>

<ul>
<li>work on both python2 and python3, preferably with no try..catch or version-dependent branching;</li>
<li>properly handle CSV files represented as <code>HTTPResponse</code> regardless of whether they end with <code>\n</code> or not?</li>
</ul>

<p>One possible solution would be to make a custom wrapper around <code>TextIOWrapper</code> which would make <code>read()</code> return <code>b''</code> when the object is closed instead of raising <code>ValueError</code>. But is there any better solution, without such hacks?</p>
",4,1514326225,python;python-3.x;urllib3,True,985,1,1515372422,https://stackoverflow.com/questions/47984230/wrapping-urllib3-httpresponse-in-io-textiowrapper
48129522,Python Website Scraper - Returning Google Page,"<p>I am trying to build my first website scraper and am very new to Python and programming in general. I am trying to practice scraping a website but my code does not work for some reason.  See code below.  When I run the code it returns the html for google.com not the County Assessors page.  </p>

<p>Is this an issue with my Python code or is there some code on the County Assessors page that is rerouting me to google?  How do I fix this issue?  Any help is much appreciated. Thanks.</p>

<pre><code>#IMPORT LIBRARIES
from urllib.request import urlopen
from bs4 import BeautifulSoup
import requests

#SCRAPER CODE
web_page = 'https://mcassessor.maricopa.gov/index.php'
page = urlopen(web_page)
soup = BeautifulSoup(page,'html.parser')
print (soup)
</code></pre>
",0,1515257134,python;html;web-scraping;python-requests;urllib3,True,74,1,1515257937,https://stackoverflow.com/questions/48129522/python-website-scraper-returning-google-page
48090641,Download from links using python,"<p>I would like to download data from this <a href=""http://current.hydro.gov.hk/en/download.php"" rel=""nofollow noreferrer"">website</a>.</p>

<p>I inspected the source code and found that it uses the following link format for downloading data.</p>

<pre><code>url = 'http://current.hydro.gov.hk/en/download_csv.php?start_dt={}%20{}:00&amp;end_dt={}%20{}:00&amp;mode=Surface'
url_filled = url.format(""2018-01-02"", ""00:00"", ""2018-01-02"", ""23:45"")
</code></pre>

<p>Then I tried to use request to download the CSV data.</p>

<pre><code>import requests
r = requests.get(url_filled)
</code></pre>

<p>But then I received error.</p>

<pre><code>  TimeoutError                              Traceback (most recent call last)
~\AppData\Local\Continuum\Anaconda3\lib\site-packages\requests\packages\urllib3\connection.py in _new_conn(self)
    140             conn = connection.create_connection(
--&gt; 141                 (self.host, self.port), self.timeout, **extra_kw)
    142 

~\AppData\Local\Continuum\Anaconda3\lib\site-packages\requests\packages\urllib3\util\connection.py in create_connection(address, timeout, source_address, socket_options)
     82     if err is not None:
---&gt; 83         raise err
     84 

~\AppData\Local\Continuum\Anaconda3\lib\site-packages\requests\packages\urllib3\util\connection.py in create_connection(address, timeout, source_address, socket_options)
     72                 sock.bind(source_address)
---&gt; 73             sock.connect(sa)
     74             return sock

TimeoutError: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond

During handling of the above exception, another exception occurred:

NewConnectionError                        Traceback (most recent call last)
~\AppData\Local\Continuum\Anaconda3\lib\site-packages\requests\packages\urllib3\connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    599                                                   body=body, headers=headers,
--&gt; 600                                                   chunked=chunked)
    601 

~\AppData\Local\Continuum\Anaconda3\lib\site-packages\requests\packages\urllib3\connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
    355         else:
--&gt; 356             conn.request(method, url, **httplib_request_kw)
    357 

~\AppData\Local\Continuum\Anaconda3\lib\http\client.py in request(self, method, url, body, headers)
   1106         """"""Send a complete request to the server.""""""
-&gt; 1107         self._send_request(method, url, body, headers)
   1108 

~\AppData\Local\Continuum\Anaconda3\lib\http\client.py in _send_request(self, method, url, body, headers)
   1151             body = _encode(body, 'body')
-&gt; 1152         self.endheaders(body)
   1153 

~\AppData\Local\Continuum\Anaconda3\lib\http\client.py in endheaders(self, message_body)
   1102             raise CannotSendHeader()
-&gt; 1103         self._send_output(message_body)
   1104 

~\AppData\Local\Continuum\Anaconda3\lib\http\client.py in _send_output(self, message_body)
    933 
--&gt; 934         self.send(msg)
    935         if message_body is not None:

~\AppData\Local\Continuum\Anaconda3\lib\http\client.py in send(self, data)
    876             if self.auto_open:
--&gt; 877                 self.connect()
    878             else:

~\AppData\Local\Continuum\Anaconda3\lib\site-packages\requests\packages\urllib3\connection.py in connect(self)
    165     def connect(self):
--&gt; 166         conn = self._new_conn()
    167         self._prepare_conn(conn)

~\AppData\Local\Continuum\Anaconda3\lib\site-packages\requests\packages\urllib3\connection.py in _new_conn(self)
    149             raise NewConnectionError(
--&gt; 150                 self, ""Failed to establish a new connection: %s"" % e)
    151 

NewConnectionError: &lt;requests.packages.urllib3.connection.HTTPConnection object at 0x00000000081F5D30&gt;: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond

During handling of the above exception, another exception occurred:

MaxRetryError                             Traceback (most recent call last)
~\AppData\Local\Continuum\Anaconda3\lib\site-packages\requests\adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    437                     retries=self.max_retries,
--&gt; 438                     timeout=timeout
    439                 )

~\AppData\Local\Continuum\Anaconda3\lib\site-packages\requests\packages\urllib3\connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    648             retries = retries.increment(method, url, error=e, _pool=self,
--&gt; 649                                         _stacktrace=sys.exc_info()[2])
    650             retries.sleep()

~\AppData\Local\Continuum\Anaconda3\lib\site-packages\requests\packages\urllib3\util\retry.py in increment(self, method, url, response, error, _pool, _stacktrace)
    387         if new_retry.is_exhausted():
--&gt; 388             raise MaxRetryError(_pool, url, error or ResponseError(cause))
    389 

MaxRetryError: HTTPConnectionPool(host='current.hydro.gov.hk', port=80): Max retries exceeded with url: /en/download_csv.php?start_dt=2018-01-02%2000:00:00&amp;end_dt=2018-01-02%2023:45:00&amp;mode=Surface (Caused by NewConnectionError('&lt;requests.packages.urllib3.connection.HTTPConnection object at 0x00000000081F5D30&gt;: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond',))

During handling of the above exception, another exception occurred:

ConnectionError                           Traceback (most recent call last)
&lt;ipython-input-39-ac5f4cccaa6a&gt; in &lt;module&gt;()
----&gt; 1 r = requests.get(url_filled)

~\AppData\Local\Continuum\Anaconda3\lib\site-packages\requests\api.py in get(url, params, **kwargs)
     70 
     71     kwargs.setdefault('allow_redirects', True)
---&gt; 72     return request('get', url, params=params, **kwargs)
     73 
     74 

~\AppData\Local\Continuum\Anaconda3\lib\site-packages\requests\api.py in request(method, url, **kwargs)
     56     # cases, and look like a memory leak in others.
     57     with sessions.Session() as session:
---&gt; 58         return session.request(method=method, url=url, **kwargs)
     59 
     60 

~\AppData\Local\Continuum\Anaconda3\lib\site-packages\requests\sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)
    516         }
    517         send_kwargs.update(settings)
--&gt; 518         resp = self.send(prep, **send_kwargs)
    519 
    520         return resp

~\AppData\Local\Continuum\Anaconda3\lib\site-packages\requests\sessions.py in send(self, request, **kwargs)
    637 
    638         # Send the request
--&gt; 639         r = adapter.send(request, **kwargs)
    640 
    641         # Total elapsed time of the request (approximately)

~\AppData\Local\Continuum\Anaconda3\lib\site-packages\requests\adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    500                 raise ProxyError(e, request=request)
    501 
--&gt; 502             raise ConnectionError(e, request=request)
    503 
    504         except ClosedPoolError as e:

ConnectionError: HTTPConnectionPool(host='current.hydro.gov.hk', port=80): Max retries exceeded with url: /en/download_csv.php?start_dt=2018-01-02%2000:00:00&amp;end_dt=2018-01-02%2023:45:00&amp;mode=Surface (Caused by NewConnectionError('&lt;requests.packages.urllib3.connection.HTTPConnection object at 0x00000000081F5D30&gt;: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond',))
</code></pre>

<p>When I try open the link in google chrome, it works. A dialog comes out and asks me the download location.</p>

<p>Anyone can help? Thanks</p>
",0,1515051175,python;request;urllib2;urllib;urllib3,True,659,2,1515053811,https://stackoverflow.com/questions/48090641/download-from-links-using-python
48062173,Do something on connection error apart from retry with python requests,"<p>I am using Python requests to make a post request.<br>
I am trying to do something like this as shown in below post:
<a href=""https://www.peterbe.com/plog/best-practice-with-retries-with-requests"" rel=""nofollow noreferrer"">Retry with requests</a></p>

<p>When there is connection error or response status code received is from status_forcelist, it should retry(which is working fine). What I want to do is after first try (before retrying), I want to do some other stuff. This can be possible if I can catch Exception and handle it to do other stuff. But it seems that requests is not raising any exception in case of connection error or response code is in status_forcelist unless retry count reaches to max configured. How can I achieve this?</p>

<p>Here is the code sample:</p>

<pre><code>def requests_retry_session(
    retries=3,
    backoff_factor=0.3,
    status_forcelist=(500, 502, 504),
    session=None,
):
    session = session or requests.Session()
    retry = Retry(
        total=retries,
        read=retries,
        connect=retries,
        backoff_factor=backoff_factor,
        status_forcelist=status_forcelist,
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session

def do_something_more():
    ## do something to tell user API failed and it will retry
    print(""I am doing something more..."")
</code></pre>

<p>Usage...</p>

<pre><code>t0 = time.time()
try:
    response = requests_retry_session().get(
        'http://localhost:9999',
    )
except Exception as x:
    # Catch exception when connection error or 500 on first attempt and do something more

    do_somthing_more() 
    print('It failed :(', x.__class__.__name__)
else:
    print('It eventually worked', response.status_code)
finally:
    t1 = time.time()
    print('Took', t1 - t0, 'seconds')
</code></pre>

<p>I know exception will be raised after max allowed attempts(defined in retries=3). All I want is some signal from requests or urllib3 to tell my main program that first attempt is failed and now it will start retrying. So that my program can do something more based on it. If not through exception, something else. </p>
",-1,1514900865,python;python-3.x;python-requests;urllib3,True,2893,1,1515013766,https://stackoverflow.com/questions/48062173/do-something-on-connection-error-apart-from-retry-with-python-requests
48054365,"ImportError: no module named urllib3, urllib3 is already installed","<p>Here is my code:</p>

<pre><code>from twilio.rest import Client

# Your Account SID from twilio.com/console
account_sid = ""sid""
# Your Auth Token from twilio.com/console
auth_token  = ""token""

client = Client(account_sid, auth_token)

message = client.messages.create(
    to=""number"", 
    from_=""number"",
    body=""new phone who dis"")

print(message.sid)
</code></pre>

<p>Here is the error message:</p>

<pre><code>Traceback (most recent call last):
  File ""/Users/mahika/Documents/sendtexttest.py"", line 1, in &lt;module&gt;
    from twilio.rest import Client
  File ""build/bdist.macosx-10.6-intel/egg/twilio/rest/__init__.py"", line 14, in &lt;module&gt;
  File ""build/bdist.macosx-10.6-intel/egg/twilio/http/http_client.py"", line 1, in &lt;module&gt;
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/requests-2.18.4-py2.7.egg/requests/__init__.py"", line 43, in &lt;module&gt;
import urllib3
ImportError: No module named urllib3
</code></pre>

<p>The problem is that urllib3 is already installed. What should I do?</p>
",1,1514853065,python;twilio;urllib3,True,10614,1,1514939565,https://stackoverflow.com/questions/48054365/importerror-no-module-named-urllib3-urllib3-is-already-installed
48068772,Using urllib gives SSL error,"<p>Running a request with urllib but continuously receive this error:</p>

<pre><code>Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 1318, in do_open
    encode_chunked=req.has_header('Transfer-encoding'))
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1026, in _send_output
    self.send(msg)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 964, in send
    self.connect()
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1400, in connect
    server_hostname=server_hostname)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py"", line 407, in wrap_socket
    _context=self, _session=session)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py"", line 814, in __init__
    self.do_handshake()
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py"", line 1068, in do_handshake
    self._sslobj.do_handshake()
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py"", line 689, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""&lt;pyshell#15&gt;"", line 1, in &lt;module&gt;
    products = amazon.search_n(1, Keywords='kindle', SearchIndex='All')
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/amazon/api.py"", line 288, in search_n
    return list(islice(items, n))
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/amazon/api.py"", line 544, in __iter__
    for page in self.iterate_pages():
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/amazon/api.py"", line 561, in iterate_pages
    yield self._query(ItemPage=self.current_page, **self.kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/amazon/api.py"", line 573, in _query
    response = self.api.ItemSearch(ResponseGroup=ResponseGroup, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/bottlenose/api.py"", line 274, in __call__
    {'api_url': api_url, 'cache_url': cache_url})
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/bottlenose/api.py"", line 235, in _call_api
    return urllib2.urlopen(api_request, timeout=self.Timeout)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 223, in urlopen
    return opener.open(url, data, timeout)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 526, in open
    response = self._open(req, data)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 544, in _open
    '_open', req)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 504, in _call_chain
    result = func(*args)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 1361, in https_open
    context=self._context, check_hostname=self._check_hostname)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 1320, in do_open
    raise URLError(err)
urllib.error.URLError: &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777)&gt;
</code></pre>

<p>Any ideas how this can be resolved?
Evidently the SSL verification is not working correctly, but I do not know how to fix the issue. </p>

<p>Using Python 3.6.1 on Mac Os</p>

<p>Thanks</p>
",0,1514932486,python;ssl;urllib2;urllib;urllib3,False,775,2,1514934276,https://stackoverflow.com/questions/48068772/using-urllib-gives-ssl-error
48018277,BeautifulSoup &amp; Selenium - not extracting full html from webpage,"<p>Using either sections of code below, neither pulls all the divs from the webpage. 
Both return a NoneType for content. This also happens for <code>{""id"": ""image_wrap""}</code> and others. Which shows it is not extracting the full HTML. (I have written data to .html documents to check as well)</p>

<p>However, other webpages such as <a href=""http://www.chictopia.com/photo/show/390693"" rel=""nofollow noreferrer"">http://www.chictopia.com/photo/show/390693</a> successfully download the full html. It is just certain URLs that do this, other works completely fine. It does not seem to be a javascript issue as the same happens when using selenium. </p>

<pre><code>import bs4
import urllib3

url = ""http://www.chictopia.com/photo/show/390695""
http = urllib3.PoolManager()
response = http.request('GET', url)
html = response.data
soup = bs4.BeautifulSoup(html, 'lxml')
content = soup.find(""div"", {""id"": ""image_wrap""})
print(content)
</code></pre>

<p>Below is using selenium to dynamically load the webpage.</p>

<pre><code>from selenium import webdriver
import bs4

url = ""http://www.chictopia.com/photo/show/390695""
browser = webdriver.Chrome()
browser.get(url)
html = browser.page_source
soup = bs4.BeautifulSoup(html, 'lxml')
content = soup.find(""div"", {""id"": ""image_wrap""})
print(content)
</code></pre>

<p>Why is this only happening for certain URLs?</p>
",0,1514528190,python;selenium;beautifulsoup;urllib3,False,185,0,1514528190,https://stackoverflow.com/questions/48018277/beautifulsoup-selenium-not-extracting-full-html-from-webpage
47896933,Using Python requests instead of urllib3,"<p>I have this code for sending queries to the TinEye API:</p>

<pre><code>from PIL import Image
from hashlib import sha1
import hmac, io, requests, string, urllib, urllib3

path = '/home/some/file.jpg'
limit = 10

TINEYE_API_URL = 'http://api.tineye.com/rest/search/'
TINEYE_PUBLIC_KEY = 'XXXXXXXXXXXXXXXXXXXXXXXXXX'
TINEYE_SECRET_KEY = 'XXXXXXXXXXXXXXXXXXXXXXXXXX'

img = Image.open(path)
img.thumbnail((300, 300), Image.ANTIALIAS)
image_data = io.BytesIO()
img.save(image_data, img.format == 'PNG' and 'PNG' or 'JPEG', optimize=1, quality=88)

t = int(time.time())
nonce = ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(10)) # 8+ random characters
boundary = '--%s--' % nonce
to_sign = TINEYE_SECRET_KEY + 'POST' + 'multipart/form-data; boundary=' + boundary + 'image_data' + str(t) + nonce + TINEYE_API_URL + 'limit=%s' % limit
data = { 'api_key': TINEYE_PUBLIC_KEY, 'date': t, 'nonce': nonce, 'api_sig': hmac.new(TINEYE_SECRET_KEY, to_sign, sha1).hexdigest(), 'limit': limit }
r = urllib3.connection_from_url(TINEYE_API_URL).request_encode_body('POST', TINEYE_API_URL+'?'+ urllib.urlencode(data), fields={'image_upload': ('image_data', image_data.getvalue())}, multipart_boundary=boundary, timeout=5)
print r.data
</code></pre>

<p>I usually prefer the Python requests library.  But I don't know  how to write the <code>urllib3.connection_from_url</code> part with the requests lib. Is that possible?</p>

<p>That's what I have, but it's not working:</p>

<pre><code>headers = { 'Content-Type': 'multipart/form-data;boundary=' + boundary }
r = requests.post(TINEYE_API_URL, data=data, files={ 'image_data': image_data.getvalue() }, headers=headers, timeout=5)
print r.text
</code></pre>
",0,1513727885,python;python-requests;urllib3,False,1174,0,1513761891,https://stackoverflow.com/questions/47896933/using-python-requests-instead-of-urllib3
45014859,regarding using urllib3 to replace urllib2,"<p>I was trying to use the following code segment. I am using Python 3, which has <code>urllib3</code> instead of urllib2. I would like to know how replace this part <code>fh = urllib2.urlopen('http://people.ku.edu/~gbohling/geostats/WGTutorial.zip')
    data = fh.read()</code> in <code>urllib3</code>. Thanks.</p>

<pre><code>clusterfile = 'ZoneA.dat'
if not os.path.isfile(clusterfile):
    fh = urllib2.urlopen('http://people.ku.edu/~gbohling/geostats/WGTutorial.zip')
    data = fh.read()
    fobj = StringIO.StringIO(data)
    myzip = zipfile.ZipFile(fobj,'r')
    myzip.extract(clusterfile)
    fobj.close()
    fh.close()
</code></pre>
",4,1499697251,python;urllib2;urllib;urllib3,True,12623,1,1513271784,https://stackoverflow.com/questions/45014859/regarding-using-urllib3-to-replace-urllib2
32578921,How can I use multithreading for video download?,"<p>I'm trying to download a video using python. </p>

<p>I'm trying to speed up download using multithreading, but I'm not able to come up with solutions in <code>requests</code> or <code>urllib2</code>.  </p>

<p>Also if anyone can give a code on how to solve it that would be really helpful.</p>

<p>Here is the code I was trying:</p>

<pre><code>import requests
http_proxy  = ""http://edcguest:edcguest@172.31.100.29:3128""
https_proxy = ""https://edcguest:edcguest@172.31.100.29:3128""
ftp_proxy   = ""ftp://edcguest:edcguest@172.31.100.29:3128""

proxyDict = { 
          ""http""  : http_proxy, 
          ""https"" : https_proxy, 
          ""ftp""   : ftp_proxy
        }

def download_file(url):
    resume_byte_pos = 0
    end_byte_pos = 432526330
    # NOTE the stream=True parameter
    resume_header = {'Range': 'bytes=%d-%d(resume_byte_pos,end_byte_pos)}
    #r = requests.get(url, stream=True, proxies=proxyDict)
    r = requests.get(url, stream=True,proxies=proxyDict,headers=resume_header)
    print r.headers


    with open('ab2.mp4', 'wb') as f:
         for chunk in r.iter_content(chunk_size=1024): 
               if chunk: # filter out keep-alive new chunks
                   f.write(chunk)
                   f.flush()

download_file('https://r2---sn-o3o-qxal.googlevideo.com/videoplayback?key=yt5&amp;sver=3&amp;signature=3D4D50B11C6206B737185B7A9887A72FE356C6DF.87458BB3BF357CEF131BEDF0C0ED3DC08F087646&amp;upn=8n6wa_1gM_o&amp;source=youtube&amp;requiressl=yes&amp;mime=video%2Fmp4&amp;ip=14.139.249.194&amp;expire=1442331973&amp;ratebypass=yes&amp;lmt=1441845538878516&amp;mm=31&amp;ipbits=0&amp;mn=sn-o3o-qxal&amp;pl=24&amp;sparams=dur%2Cid%2Cinitcwndbps%2Cip%2Cipbits%2Citag%2Clmt%2Cmime%2Cmm%2Cmn%2Cms%2Cmv%2Cpl%2Cratebypass%2Crequiressl%2Csource%2Cupn%2Cexpire&amp;fexp=9408710%2C9409069%2C9409170%2C9412773%2C9415365%2C9415485%2C9415942%2C9416023%2C9416126%2C9416333%2C9416729%2C9417707%2C9417710%2C9417818%2C9418153%2C9418162%2C9418200%2C9418245%2C9418448%2C9418986%2C9419773%2C9419788%2C9419837%2C9420348%2C9420777%2C9420798&amp;id=o-ALtyLWP7o7PqDhINh6FWp4v4FC8-3pQoZ0UH4COW6v5p&amp;mt=1442310331&amp;dur=8384.609&amp;mv=m&amp;initcwndbps=4191250&amp;ms=au&amp;itag=18&amp;cpn=zzjuCmNROtaupQMW&amp;ptk=Apple%252Bvid&amp;oid=ffsQQyXI443h2PgMzMjp-g&amp;ptchn=E_M8A5yxnLfW0KghEeajjw&amp;pltype=content&amp;c=WEB&amp;cver=html5')
</code></pre>
",1,1442297708,python;multithreading;python-requests;urllib2;urllib3,True,490,1,1512959249,https://stackoverflow.com/questions/32578921/how-can-i-use-multithreading-for-video-download
47644813,urllib3 overriding host IP address,"<p>I am trying to override the IP address for the destination host on the fly using urllib3, while I am passing client certificates. Here is my code:</p>

<pre><code>import urllib3
conn = urllib3.connection_from_url('https://MYHOST', ca_certs='ca_crt.pem', key_file='pr.pem', cert_file='crt.pem', cert_reqs='REQUIRED')
response = conn.request('GET', 'https://MYHOST/OBJ', headers={""HOST"": ""MYHOST""})
print(response.data)
</code></pre>

<p>I was thinking to use transport adapters, but I am not quite sure how to do it without using sessions.</p>

<p>Any thoughts or help?</p>
",0,1512438454,python;urllib;urllib3,False,2030,1,1512776858,https://stackoverflow.com/questions/47644813/urllib3-overriding-host-ip-address
47721121,Writing into text file on web server using urllib/urllib3,"<p>I am wondering if there is a way to write into .txt file on web server using <strong>urllib</strong> or <strong>urllib3</strong>. I tried using urllib3 <em>POST</em> but that doesnt do anything. Does any of these libraries have ability to write into files or do I have to use some other library?</p>
",0,1512762039,python;urllib;urllib3,True,225,1,1512762465,https://stackoverflow.com/questions/47721121/writing-into-text-file-on-web-server-using-urllib-urllib3
47672056,"boto3 add unix credentials, urllib2/3","<p>Can we add the unix user credentials to SQS client? </p>

<p>Situation : 
I have a user in Unix system (without 'sudo'/root privileges). And this Unix user's permissions does not allow me to connect to <a href=""http://amazonaws.com"" rel=""nofollow noreferrer"">amazonaws.com</a>.    </p>

<p>Can I pass the username and password of this Unix user to boto3 client (SQS) ? </p>

<p>OR, can we pass boto3 client receive message object in <strong>urllib2/3</strong>?</p>

<p>What I already tried : </p>

<pre><code>import boto3

# Create SQS client
sqs = boto3.client('sqs',region_name='eu-west-1')

queue_url='https://sqs.eu-west-1.amazonaws.com/XXXX'

# Receive message from SQS queue
response = sqs.receive_message(
    QueueUrl=queue_url,
    AttributeNames=[
        'SentTimestamp'
    ],
    MaxNumberOfMessages=10,
    MessageAttributeNames=[
        'All'
    ],
    VisibilityTimeout=0,
    WaitTimeSeconds=0
)
</code></pre>

<ul>
<li><p>I can access any URL using urllib2 to scrape it. And I am able to pass Unix credentials while doing it. </p></li>
<li><p>As a root user I am able to get messages from amazonSQS using boto3 as mentioned in the above code.  </p></li>
</ul>
",0,1512556177,python;amazon-web-services;urllib2;boto3;urllib3,True,107,1,1512575253,https://stackoverflow.com/questions/47672056/boto3-add-unix-credentials-urllib2-3
29416563,Google App Engine - SSL InsecurePlatformWarning,"<p>I'm using python's <a href=""http://docs.python-requests.org/en/latest/"" rel=""nofollow"">requests</a> library inside Google App Engine to send GET requests to a private server.  When I make the request I get this warning:</p>

<pre><code>requests/packages/urllib3/util/ssl_.py:79: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning
</code></pre>

<p>According to the <a href=""https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning"" rel=""nofollow"">documentation</a> that points to, I need to either upgrade past Python 2.7.x which GAE uses, or use pyopenssl.  Since I don't believe that I can force GAE to use Python 2.7.9 I tried to use pyopenssl.</p>

<p>Following the instruction on the page, I've downloaded the three libraries suggested into the lib directory of my app and where I use requests I try to inject pyopenssl into urllib3 with:</p>

<pre><code>import requests.packages.urllib3.contrib.pyopenssl
requests.packages.urllib3.contrib.pyopenssl.inject_into_urllib3()
</code></pre>

<p>This however, fails in the devserver and the production server with the following traceback:</p>

<pre><code>Traceback (most recent call last):


File ""/base/data/home/runtimes/python27/python27_lib/versions/1/google/appengine/runtime/wsgi.py"", line 240, in Handle
    handler = _config_handle.add_wsgi_middleware(self._LoadHandler())
  File ""/base/data/home/runtimes/python27/python27_lib/versions/1/google/appengine/runtime/wsgi.py"", line 299, in _LoadHandler
    handler, path, err = LoadObject(self._handler)
  File ""/base/data/home/runtimes/python27/python27_lib/versions/1/google/appengine/runtime/wsgi.py"", line 85, in LoadObject
    obj = __import__(path[0])
  File ""/base/data/home/apps/s~servicey1564/1.383321878696068897/main.py"", line 24, in &lt;module&gt;
    from API import setupautomatorAPI
  File ""/base/data/home/apps/s~servicey1564/1.383321878696068897/API.py"", line 12, in &lt;module&gt;
    from ServiceActivationTest import uploadSAT, getSATsForService
  File ""/base/data/home/apps/s~servicey1564/1.383321878696068897/ServiceActivationTest/__init__.py"", line 3, in &lt;module&gt;
    from requests.packages.urllib3.contrib import pyopenssl as pyopenssl
  File ""/base/data/home/apps/s~servicey1564/1.383321878696068897/lib/requests/packages/__init__.py"", line 95, in load_module
    raise ImportError(""No module named '%s'"" % (name,))
ImportError: No module named 'requests.packages.urllib3.contrib.pyopenssl'
</code></pre>

<p>This import statement works fine in the Python interpreter, and works if I take off the pyopenssl on the end.  pyopenssl is also the first .py file besides __init__.py files in that path.</p>

<p>Am I doing something wrong here?  Is there an easier way to fix the InsecurePlatformWarning?</p>

<p>UPDATE: After going to the sockets API page(Thanks shazow!) I found that part of my problem was that httplib was misbehaving because I lacked an environment variable.  This didn't get rid of the warning, but my certificate is being accepted now!</p>
",2,1427987940,python;google-app-engine;ssl;python-requests;urllib3,True,2611,3,1512470702,https://stackoverflow.com/questions/29416563/google-app-engine-ssl-insecureplatformwarning
47636534,Python - Unable to use pip and probably for the same reason unable to use library python-telegram-bot,"







<p>I have a rather bizarre problem that I havent been able to get around despite spending quite sometime Googling.</p>

<p><strong>First</strong> when I wanna search or install a package using <strong>pip</strong> in a virtualenv I get this:</p>

<pre><code>~$ ./virenv/bin/pip search telegram

Retrying (Retry(total=4, connect=None, read=None, redirect=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', NewConnectionError('&lt;pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7fee3a528e90&gt;: Failed to establish a new connection: [Errno 111] Connection refused',))': /pypi
...
Retrying (Retry(total=0, connect=None, read=None, redirect=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', NewConnectionError('&lt;pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7fee3a4ca2d0&gt;: Failed to establish a new connection: [Errno 111] Connection refused',))': /pypi


Exception:
Traceback (most recent call last):
 File ""~/virenv/local/lib/python2.7/site-packages/pip/basecommand.py"", line 215, in main
    status = self.run(options, args)
...
ProxyError: HTTPSConnectionPool(host='pypi.python.org', port=443): Max retries exceeded with url: /pypi (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('&lt;pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7fee3a4ca390&gt;: Failed to establish a new connection: [Errno 111] Connection refused',)))
</code></pre>

<p>I'm not behind a proxy and I'm able to run </p>

<pre><code>sudo -H pip search telegram
</code></pre>

<p>and it works seemlessly. 
My pip version:</p>

<pre><code>~$ ./virenv/bin/pip -V
pip 9.0.1 from ~/virenv/local/lib/python2.7/site-packages (python 2.7)
</code></pre>

<p>I examine patches folks had <a href=""https://github.com/shazow/urllib3/commit/1c30a1f3a4af9591f480a338f75221bdf5ca48da?diff=split"" rel=""nofollow noreferrer"">here</a> but the lines are already present in my pip's packages. Also replace </p>

<p><em>~/Test/virenvTest/lib/python2.7/site-packages/pip/_vendor/requests/packages/urllib3/connection.py</em>
with the one <a href=""https://github.com/shazow/urllib3/blob/master/urllib3/connection.py#L205"" rel=""nofollow noreferrer"">here</a> (since it seems to include additional lines) to no avail. And the modification to </p>

<p><em>python2.6/site-packages/pip/_vendor/requests/adapters.py</em></p>

<p>mentioned <a href=""https://github.com/pypa/pip/issues/1805"" rel=""nofollow noreferrer"">here</a> (by alexandrem) doesnt seem possible since the line has a different logic.</p>

<p>I know I was just groping around :(</p>

<p><strong>Second</strong> (another symptom) within Pycharm I can install packages and run the code without problem. But again when I try to run this <a href=""https://github.com/python-telegram-bot/python-telegram-bot/blob/master/examples/inlinebot.py"" rel=""nofollow noreferrer"">script</a> (that needs going to the Internet) from command line </p>

<pre><code>$ ~/virenv/bin/python ~/PycharmProjects/bot/test1.py

2017-12-04 16:59:30,278 - telegram.vendor.ptb_urllib3.urllib3.connectionpool - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', NewConnectionError('&lt;telegram.vendor.ptb_urllib3.urllib3.connection.VerifiedHTTPSConnection object at 0x7fa159cd1650&gt;: Failed to establish a new connection: [Errno 111] Connection refused',))': /botXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX/setWebhook
...
2017-12-04 16:59:30,280 - telegram.vendor.ptb_urllib3.urllib3.connectionpool - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', NewConnectionError('&lt;telegram.vendor.ptb_urllib3.urllib3.connection.VerifiedHTTPSConnection object at 0x7fa159cd18d0&gt;: Failed to establish a new connection: [Errno 111] Connection refused',))': /botXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX/setWebhook
2017-12-04 16:59:30,280 - telegram.ext.updater - ERROR - error in bootstrap phase; try=0 max_retries=0
Traceback (most recent call last):
  File ""~/virenv/local/lib/python2.7/site-packages/telegram/utils/request.py"", line 196, in _request_wrapper
raise NetworkError('urllib3 HTTPError {0}'.format(error))
...
NetworkError: urllib3 HTTPError HTTPSConnectionPool(host='api.telegram.org', port=443): Max retries exceeded with url: /botXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX/setWebhook (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('&lt;telegram.vendor.ptb_urllib3.urllib3.connection.VerifiedHTTPSConnection object at 0x7fa159cd1990&gt;: Failed to establish a new connection: [Errno 111] Connection refused',)))
2017-12-04 16:59:30,300 - telegram.ext.updater - ERROR - unhandled exception
...
NetworkError: urllib3 HTTPError HTTPSConnectionPool(host='api.telegram.org', port=443): Max retries exceeded with url: /botXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX/setWebhook (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('&lt;telegram.vendor.ptb_urllib3.urllib3.connection.VerifiedHTTPSConnection object at 0x7fa159cd1990&gt;: Failed to establish a new connection: [Errno 111] Connection refused',)))


2017-12-04 16:59:31,282 - telegram.ext.dispatcher - CRITICAL - stopping due to exception in another thread
</code></pre>

<p>It seems there's something wrong on both situations(I'm not sure since I'm a mediocre on the field) maybe it is python <strong><em>urllib3</em></strong> package but how should I fix it? Any help please!</p>
",1,1512400869,python;pip;urllib3;python-telegram-bot,False,1886,0,1512400869,https://stackoverflow.com/questions/47636534/python-unable-to-use-pip-and-probably-for-the-same-reason-unable-to-use-librar
47538226,Convert Code from 2.7 to 3.5,"<p>I'm very new to learning python, and was wondering if someone could convert a previously answered question so that it would work in Python 3.5. I would like to get the exact same info as the original question.</p>

<p><a href=""https://stackoverflow.com/questions/46024536/cannot-get-table-data-html"">Cannot get table data - HTML</a></p>

<p>Thanks in advance!</p>
",-4,1511892531,python;python-requests;urllib3,True,217,1,1511894130,https://stackoverflow.com/questions/47538226/convert-code-from-2-7-to-3-5
47397919,Python requests with HTTPAdapter is halting for hours,"<p>I have a special URL where my code is halting for hours (more than 3 hours). I can't seem to understand why it would do that.</p>

<p>The URL is <a href=""http://www.etudes.ccip.fr/maintenance_site.php"" rel=""noreferrer"">http://www.etudes.ccip.fr/maintenance_site.php</a>.</p>

<p>Direct requests.get() works instantaneously but whenever I have an HTTPAdapter, the code seems to sleep almost indefinitely</p>

<pre><code>import requests
from requests.adapters import HTTPAdapter    

url = 'http://www.etudes.ccip.fr/maintenance_site.php'
session = requests.Session()
session.mount('http://', HTTPAdapter(max_retries=2))
session.get(url, timeout=2)
</code></pre>
",17,1511199355,python;python-requests;urllib3,True,14481,3,1511464783,https://stackoverflow.com/questions/47397919/python-requests-with-httpadapter-is-halting-for-hours
47137518,Error while calculating distance matrix,"<p>I'm following <a href=""https://stackoverflow.com/questions/17267807/python-google-maps-driving-time"">this</a> post to calculate distance matrix
so this is the code snippet</p>

<pre><code>import urllib.request
from urllib.parse import quote  
import urllib.parse
import simplejson, urllib
import urllib.request
orig_coord = 19.12,19.45
dest_coord = 19.10,19.50
url = 'http://maps.googleapis.com/maps/api/distancematrix/json?origins=' + urllib.quote_plus(orig_coord)+ '&amp;destinations=' + urllib.quote_plus(dest_coord) + '&amp;mode=driving&amp;language=en-EN&amp;sensor=false'
#url = ""http://maps.googleapis.com/maps/api/distancematrix/json?origins={0}&amp;destinations={1}&amp;mode=driving&amp;language=en-EN&amp;sensor=false"".format(str(orig_coord),str(dest_coord))
#result= simplejson.load(urllib.urlopen(url))
result=urllib.request.urlopen(url)
driving_time = result['rows'][0]['elements'][0]['duration']['value']
</code></pre>

<p>I had tried these two method(other one with comments)
but it never worked though,
now with these code I get the error</p>

<pre><code>runfile('C:/Users/admin/.spyder-py3/temp.py', wdir='C:/Users/admin/.spyder-py3')
Traceback (most recent call last):

  File ""&lt;ipython-input-22-30ed5be6f1c7&gt;"", line 1, in &lt;module&gt;
    runfile('C:/Users/admin/.spyder-py3/temp.py', wdir='C:/Users/admin/.spyder-py3')

  File ""C:\Users\admin\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 866, in runfile
    execfile(filename, namespace)

  File ""C:\Users\admin\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/admin/.spyder-py3/temp.py"", line 16, in &lt;module&gt;
    url = 'http://maps.googleapis.com/maps/api/distancematrix/json?origins=' + urllib.quote_plus(orig_coord)+ '&amp;destinations=' + urllib.quote_plus(dest_coord) + '&amp;mode=driving&amp;language=en-EN&amp;sensor=false'

AttributeError: module 'urllib' has no attribute 'quote_plus'
</code></pre>
",1,1509972877,python;urllib3;google-distancematrix-api,True,352,1,1509973561,https://stackoverflow.com/questions/47137518/error-while-calculating-distance-matrix
46882276,Monkey patching _ssl_wrap_socket in Python requests library isn&#39;t executing,"<p>We are trying to add HTTPS support to a web server virtual host scanning tool. Said tool uses the python3 requests library, which uses urllib3 under the hood.</p>

<p>We need a way to provide our own SNI hostname so are attempting to monkey patch the <code>_ssl_wrap_socket</code> function of urllib3 to control <code>server_hostname</code> but aren't having much success.</p>

<p>Here is the full code:</p>

<pre><code>from urllib3.util import ssl_
_target_host = None
_orig_wrap_socket = ssl_.ssl_wrap_socket

def _ssl_wrap_socket(sock, keyfile=None, certfile=None, cert_reqs=None,
                     ca_certs=None, server_hostname=None,
                     ssl_version=None, ciphers=None, ssl_context=None,
                     ca_cert_dir=None):
    _orig_wrap_socket(sock, keyfile=keyfile, certfile=certfile,
                      cert_reqs=cert_reqs, ca_certs=ca_certs,
                      server_hostname=_target_host, ssl_version=ssl_version,
                      ciphers=ciphers, ssl_context=ssl_context,
                      ca_cert_dir=ca_cert_dir)

ssl_.ssl_wrap_socket = _ssl_wrap_socket
</code></pre>

<p>We then call <code>requests.get()</code> further down in the code. The full context can be found on <a href=""https://github.com/codingo/VHostScan/blob/master/lib/core/virtual_host_scanner.py"" rel=""nofollow noreferrer"">Github (here)</a>.</p>

<p>Unfortunately this isn't working as our code never appears to be reached, and we're not sure why. Is  there something obvious that we're missing or a better way to approach this issue?</p>

<h1>Further Explanation</h1>

<p>The following is the full class:</p>

<pre><code>import os
import random

import requests
import hashlib
import pandas as pd
import time
from lib.core.discovered_host import *
import urllib3

DEFAULT_USER_AGENT = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) '\
                     'AppleWebKit/537.36 (KHTML, like Gecko) '\
                     'Chrome/61.0.3163.100 Safari/537.36'

urllib3.disable_warnings()

from urllib3.util import ssl_



class virtual_host_scanner(object):
    """"""Virtual host scanning class

    Virtual host scanner has the following properties:

    Attributes:
        wordlist: location to a wordlist file to use with scans
        target: the target for scanning
        port: the port to scan. Defaults to 80
        ignore_http_codes: commad seperated list of http codes to ignore
        ignore_content_length: integer value of content length to ignore
        output: folder to write output file to
    """"""
    def __init__(self, target, wordlist, **kwargs):
        self.target = target
        self.wordlist = wordlist
        self.base_host = kwargs.get('base_host')
        self.rate_limit = int(kwargs.get('rate_limit', 0))
        self.port = int(kwargs.get('port', 80))
        self.real_port = int(kwargs.get('real_port', 80))
        self.ssl = kwargs.get('ssl', False)
        self.fuzzy_logic = kwargs.get('fuzzy_logic', False)
        self.unique_depth = int(kwargs.get('unique_depth', 1))
        self.ignore_http_codes = kwargs.get('ignore_http_codes', '404')
        self.first_hit = kwargs.get('first_hit')

        self.ignore_content_length = int(
            kwargs.get('ignore_content_length', 0)
        )

        self.add_waf_bypass_headers = kwargs.get(
            'add_waf_bypass_headers',
            False
        )

        # this can be made redundant in future with better exceptions
        self.completed_scan = False

        # this is maintained until likely-matches is refactored to use
        # new class
        self.results = []

        # store associated data for discovered hosts
        # in array for oN, oJ, etc'
        self.hosts = []

        # available user-agents
        self.user_agents = list(kwargs.get('user_agents')) \
            or [DEFAULT_USER_AGENT]

    @property
    def ignore_http_codes(self):
        return self._ignore_http_codes

    @ignore_http_codes.setter
    def ignore_http_codes(self, codes):
        self._ignore_http_codes = [
            int(code) for code in codes.replace(' ', '').split(',')
        ]

    _target_host = None
    _orig_wrap_socket = ssl_.ssl_wrap_socket

    def _ssl_wrap_socket(sock, keyfile=None, certfile=None, cert_reqs=None,
                         ca_certs=None, server_hostname=None,
                         ssl_version=None, ciphers=None, ssl_context=None,
                         ca_cert_dir=None):
        print('SHOULD BE PRINTED')
        _orig_wrap_socket(sock, keyfile=keyfile, certfile=certfile,
                          cert_reqs=cert_reqs, ca_certs=ca_certs,
                          server_hostname=_target_host, ssl_version=ssl_version,
                          ciphers=ciphers, ssl_context=ssl_context,
                          ca_cert_dir=ca_cert_dir)

    def scan(self):
        print('fdsa')
        ssl_.ssl_wrap_socket = self._ssl_wrap_socket

        if not self.base_host:
            self.base_host = self.target

        if not self.real_port:
            self.real_port = self.port

        for virtual_host in self.wordlist:
            hostname = virtual_host.replace('%s', self.base_host)

            if self.real_port == 80:
                host_header = hostname
            else:
                host_header = '{}:{}'.format(hostname, self.real_port)

            headers = {
                'User-Agent': random.choice(self.user_agents),
                'Host': host_header,
                'Accept': '*/*'
            }

            if self.add_waf_bypass_headers:
                headers.update({
                    'X-Originating-IP': '127.0.0.1',
                    'X-Forwarded-For': '127.0.0.1',
                    'X-Remote-IP': '127.0.0.1',
                    'X-Remote-Addr': '127.0.0.1'
                })

            dest_url = '{}://{}:{}/'.format(
                'https' if self.ssl else 'http',
                self.target,
                self.port
            )

            _target_host = hostname

            try:
                res = requests.get(dest_url, headers=headers, verify=False)
            except requests.exceptions.RequestException:
                continue

            if res.status_code in self.ignore_http_codes:
                continue

            response_length = int(res.headers.get('content-length', 0))
            if self.ignore_content_length and \
               self.ignore_content_length == response_length:
                continue

            # hash the page results to aid in identifing unique content
            page_hash = hashlib.sha256(res.text.encode('utf-8')).hexdigest()

            self.hosts.append(self.create_host(res, hostname, page_hash))

            # add url and hash into array for likely matches
            self.results.append(hostname + ',' + page_hash)

            if len(self.hosts) &gt;= 1 and self.first_hit:
                break

            # rate limit the connection, if the int is 0 it is ignored
            time.sleep(self.rate_limit)

        self.completed_scan = True

    def likely_matches(self):
        if self.completed_scan is False:
            print(""[!] Likely matches cannot be printed ""
                  ""as a scan has not yet been run."")
            return

        # segment results from previous scan into usable results
        segmented_data = {}
        for item in self.results:
            result = item.split("","")
            segmented_data[result[0]] = result[1]

        dataframe = pd.DataFrame([
            [key, value] for key, value in segmented_data.items()],
            columns=[""key_col"", ""val_col""]
        )

        segmented_data = dataframe.groupby(""val_col"").filter(
            lambda x: len(x) &lt;= self.unique_depth
        )

        return segmented_data[""key_col""].values.tolist()

    def create_host(self, response, hostname, page_hash):
        """"""
        Creates a host using the responce and the hash.
        Prints current result in real time.
        """"""
        output = '[#] Found: {} (code: {}, length: {}, hash: {})\n'.format(
            hostname,
            response.status_code,
            response.headers.get('content-length'),
            page_hash
        )

        host = discovered_host()
        host.hostname = hostname
        host.response_code = response.status_code
        host.hash = page_hash
        host.contnet = response.content

        for key, val in response.headers.items():
            output += '  {}: {}\n'.format(key, val)
            host.keys.append('{}: {}'.format(key, val))

        print(output)

        return host
</code></pre>

<p>In this case the following line is never being hit:</p>

<pre><code>print('SHOULD BE PRINTED')
</code></pre>

<p>This also results in the following log entry on the web server:</p>

<blockquote>
  <p>[Wed Oct 25 16:37:23.654321 2017] [ssl:error] [pid 1355] AH02032:
  Hostname  provided via SNI and hostname test.test provided via
  HTTP are different</p>
</blockquote>

<p>Which indicates the code was never run also.</p>
",3,1508735199,python;python-3.x;python-requests;vhosts;urllib3,True,2239,1,1509011847,https://stackoverflow.com/questions/46882276/monkey-patching-ssl-wrap-socket-in-python-requests-library-isnt-executing
41559464,Urllib3 socks5 proxy error: &#39;socks&#39; has no attribute &#39;create_connection&#39;,"<p>I need Robobrowser to use Tor, easiest way i thought would be like this: <a href=""https://stackoverflow.com/a/25005981/4237509"">Robobrowser with Sessions</a>.
But i encountered a strange problem with requests, or more specific urllib3:</p>

<blockquote>
  <p>AttributeError: module 'socks' has no attribute 'create_connection'</p>
</blockquote>

<p>As documented in <a href=""https://media.readthedocs.org/pdf/urllib3/latest/urllib3.pdf"" rel=""nofollow noreferrer"">Urllib3 Documentation</a> create_connection is an attribute from socks.</p>

<p>Urllib3 version: 1.19.1</p>

<p>Requests version: 2.12.4</p>

<p>PySocks version: 1.6.5</p>

<pre><code>import requests

session = requests.Session()
session.proxies = {'http':'socks5://127.0.0.1:9050'}
response = session.get('http://www.icanhazip.com', timeout=2)
print(response.text)
</code></pre>

<p>edit:</p>

<p>Stacktrace:</p>

<pre><code>    Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python3.5/dist-packages/requests/sessions.py"", line 501, in get
    return self.request('GET', url, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/requests/sessions.py"", line 488, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/requests/sessions.py"", line 609, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/requests/adapters.py"", line 423, in send
    timeout=timeout
  File ""/usr/local/lib/python3.5/dist-packages/requests/packages/urllib3/connectionpool.py"", line 594, in urlopen
    chunked=chunked)
  File ""/usr/local/lib/python3.5/dist-packages/requests/packages/urllib3/connectionpool.py"", line 361, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File ""/usr/lib/python3.5/http/client.py"", line 1106, in request
    self._send_request(method, url, body, headers)
  File ""/usr/lib/python3.5/http/client.py"", line 1151, in _send_request
    self.endheaders(body)
  File ""/usr/lib/python3.5/http/client.py"", line 1102, in endheaders
    self._send_output(message_body)
  File ""/usr/lib/python3.5/http/client.py"", line 934, in _send_output
    self.send(msg)
  File ""/usr/lib/python3.5/http/client.py"", line 877, in send
    self.connect()
  File ""/usr/local/lib/python3.5/dist-packages/requests/packages/urllib3/connection.py"", line 163, in connect
    conn = self._new_conn()
  File ""/usr/local/lib/python3.5/dist-packages/requests/packages/urllib3/contrib/socks.py"", line 79, in _new_conn
    conn = socks.create_connection(
AttributeError: module 'socks' has no attribute 'create_connection'
</code></pre>
",2,1484009184,python;python-requests;urllib3;robobrowser,False,2639,1,1507650081,https://stackoverflow.com/questions/41559464/urllib3-socks5-proxy-error-socks-has-no-attribute-create-connection
46626937,ImportError: No module named urllib3 while installing urllib3,"<p>I am working on macOS 10.12.6 (16G29).</p>

<p>Within the installation of cx_Oracle with pip, I am getting the following error:</p>

<pre><code> ImportError: No module named urllib3
</code></pre>

<p>Quite obviously, I go with urllib3 installation (<code>pip install urllib3</code>), but I’m getting the same error.</p>

<p>To tell the truth, that makes me literally crazy. I`m quite new to all this stuff, so any help will be highly appreciated! Any additional information from me on the topic will follow, if needed.</p>
",0,1507429011,python;pip;urllib3,False,462,0,1507540207,https://stackoverflow.com/questions/46626937/importerror-no-module-named-urllib3-while-installing-urllib3
46498233,Python Requests API requests.get() call with params is not working as intended (response ignores params),"<p>Here is my code:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>import requests

find_doctors_url = 'http://www.americhoice.com/find_doctor/Ver2/results_doc.jsp'

payload ={""specialty"":""ORSU"",""docproducts"":""HOBD,HLOP"",""planNameDropDoc"":""HOBD,HLOP"",""plan"":""uhcwa"",""zip"":""98122"",""zipradius"":""10"",""findButton"":""FIND DOCTOR"",""specialtyName"":""ORTHOPAEDIC SURGERY""}

response = requests.get(find_doctors_url,params=payload)
print(response.url)
print(response.content)</code></pre>
</div>
</div>
</p>

<p>when I print response.content, all I receive is:    </p>

<pre><code>&lt;!-- NEAADR0179 -Anil Kumar Vutikuri *** End--&gt;


&lt;!--BEGIN SETTING HEADERS TO NO CACHE--&gt;

&lt;!--END SETTING HEADERS TO NO CACHE--&gt;

&lt;!--SET SESSION VALUES FROM URL PARAMETERS--&gt;


&lt;!--END SET SESSION VALUES FROM URL PARAMETERS--&gt;
</code></pre>

<p>Which is the response you receive when you navigate to:
view-source:<a href=""http://www.americhoice.com/find_doctor/Ver2/results_doc.jsp"" rel=""nofollow noreferrer"">http://www.americhoice.com/find_doctor/Ver2/results_doc.jsp</a></p>

<p>However, I am seeking to return the complete html received when you navigate to the url generated by response.url</p>

<p>view-source:<a href=""http://www.americhoice.com/find_doctor/Ver2/results_doc.jsp?specialty=ORSU&amp;docproducts=HOBD%2CHLOP&amp;planNameDropDoc=HOBD%2CHLOP&amp;plan=uhcwa&amp;zip=98122&amp;zipradius=10&amp;findButton=FIND+DOCTOR&amp;specialtyName=ORTHOPAEDIC+SURGERY"" rel=""nofollow noreferrer"">http://www.americhoice.com/find_doctor/Ver2/results_doc.jsp?specialty=ORSU&amp;docproducts=HOBD%2CHLOP&amp;planNameDropDoc=HOBD%2CHLOP&amp;plan=uhcwa&amp;zip=98122&amp;zipradius=10&amp;findButton=FIND+DOCTOR&amp;specialtyName=ORTHOPAEDIC+SURGERY</a></p>

<p>The problems seems to be that the request is not sending the GET query params correctly</p>

<p>Things I have tried (unsuccessfully):
1)Requesting the complete URL(encoded) instead of using a params dictionary
2)Using the urllib3 library instead of the Requests library</p>
",0,1506729263,python;web-scraping;get;python-requests;urllib3,True,3720,1,1506734447,https://stackoverflow.com/questions/46498233/python-requests-api-requests-get-call-with-params-is-not-working-as-intended
46459433,pip install insecureplatformwarning snimissingwarning ubuntu 14.04 python 2.7.6,"<p>I'm getting insecure platform and sni missing warnings while running pip install. I've been trying to follow the instructions suggested in the error messages below without luck:</p>

<p><a href=""http://urllib3.readthedocs.io/en/latest/security.html#insecureplatformwarning"" rel=""nofollow noreferrer"">https://urllib3.readthedocs.io/en/latest/security.html#insecureplatformwarning</a>
<a href=""https://urllib3.readthedocs.io/en/latest/security.html#snimissingwarning"" rel=""nofollow noreferrer"">https://urllib3.readthedocs.io/en/latest/security.html#snimissingwarning</a>
<a href=""https://urllib3.readthedocs.io/en/latest/user-guide.html#ssl-py2"" rel=""nofollow noreferrer"">https://urllib3.readthedocs.io/en/latest/user-guide.html#ssl-py2</a></p>

<p>I wrote up a small Vagrant file &amp; Chef cookbook to demonstrate the issue and maybe get some help. The warnings show up before every install of a pip package. I've also tried various combos of pip install options and varying the version pip (8.1.2 vs 9.0.1). Any help appreciated.</p>

<p>Public GitHub Repo containing the code below: <a href=""https://github.com/marc-swingler/urllib_issue"" rel=""nofollow noreferrer"">https://github.com/marc-swingler/urllib_issue</a></p>

<p>Requirements: Vagrant and VirtualBox</p>

<p>To Run: vagrant up</p>

<p>UPDATE:: Found this thread <a href=""https://github.com/pypa/pip/issues/4098"" rel=""nofollow noreferrer"">https://github.com/pypa/pip/issues/4098</a></p>

<p>Turns out pip 9.0.1 doesn't play nice due to libs that come bundled with it. Also, install ndg-httpsclient rather than urllib3 and/or requests. The apt packages mentioned in the user-guide are not required, and no need to build from scratch, you can use wheels. Once ndg-httpsclient is installed the warnings go away and additional pip installs go smoothly. I also have a version of this where I bootstrap pip with the apt python-pip package that works. (I'll post it if anyone is interested.) Apt installs pip v1.5.4 initially. The script then updates to pip 8.1.2 and proceeds similarly to the code below. Just requires removing the ""--disable-pip-version-check"" option when upgrading pip.</p>

<p>Vagrantfile:</p>

<pre><code># -*- mode: ruby -*-
# vi: set ft=ruby :

VAGRANTFILE_API_VERSION = ""2""

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
    config.vm.box = ""ubuntu/trusty64""
    config.vm.network ""private_network"", type: ""dhcp""
    config.ssh.shell = ""bash -c 'BASH_ENV=/etc/profile exec bash'""
    config.vm.provision ""chef_solo"" do |chef|
            chef.add_recipe(""foo"")
    end
    config.vm.provider ""virtualbox"" do |v|
            v.memory = 4096
    end
end
</code></pre>

<p>cookbooks/foo/recipes/default.rb:</p>

<pre><code>pip_installer_path = '/tmp/get-pip.py'
remote_file ""download_pip_installer"" do
    path pip_installer_path
    source 'https://bootstrap.pypa.io/get-pip.py'
    owner 'root'
    group 'root'
    mode '0500'
    not_if 'which pip'
end
execute 'bootstrap_pip' do
    command ""python #{pip_installer_path}""
    not_if ""which pip""
end
cookbook_file 'delete_pip_installer' do
    path pip_installer_path
    action :delete
end

pip_packages = {
    'pip' =&gt; { 'version' =&gt; '8.1.2', 'extras' =&gt; nil },
    'ndg-httpsclient' =&gt; { 'version' =&gt; '0.4.3', 'extras' =&gt; nil },
    'botocore' =&gt; { 'version' =&gt; '1.7.18', 'extras' =&gt; nil },
    'pystache' =&gt; { 'version' =&gt; '0.5.4', 'extras' =&gt; nil }
}
pip_packages.each do |package_name, package_info|
    package_version = package_info['version']
    package_extras = package_info['extras']
    package_spec = package_name
    unless package_extras.nil? or package_extras.length &lt; 1
        package_spec = package_spec + '['
        package_extras.each do |package_extra|
            package_spec = package_spec + package_extra + ','
        end
        package_spec[-1] = ']'
    end
    package_spec = package_spec + '==' + package_version
    execute package_spec do
        command ""pip --disable-pip-version-check install -U #{package_spec}""
        not_if ""test #{package_version} = `pip --disable-pip-version-check list 2&gt;/dev/null | sed -rn 's/^#{package_name} \\(([0-9.]+)\\)/\\1/p'`""
    end
end
</code></pre>

<p>cookbooks/metadata.rb</p>

<pre><code>name             'foo'
maintainer       'foo'
maintainer_email 'foo@foo.com'
license          'foo'
description      'foo'
long_description 'foo'
version          '0.0.0'
</code></pre>
",1,1506561060,python;pip;urllib3,True,1442,1,1506611604,https://stackoverflow.com/questions/46459433/pip-install-insecureplatformwarning-snimissingwarning-ubuntu-14-04-python-2-7-6
45942557,Use ssl certificate of the server instead of root CA certificate while making HTTPS calls using urllib3,"<p>It is given in the documentation of <a href=""https://urllib3.readthedocs.io/en/latest/user-guide.html#certificate-verification"" rel=""nofollow noreferrer"">urllib3</a> that, </p>

<p><strong>In order to enable verification you will need a set of root certificates.</strong></p>

<p>but is there any way to make HTTPS calls by using the SSL certificate of the server itself instead of root certificates. </p>
",0,1504019243,python;ssl;urllib3,True,210,1,1504025610,https://stackoverflow.com/questions/45942557/use-ssl-certificate-of-the-server-instead-of-root-ca-certificate-while-making-ht
45888744,&#39;IncompleteRead&#39; import error in urllib3,"<p>I have recently installed urllib3, and I am getting an error that I can't find anything about online. The line causing the issue is: </p>

<pre><code>from .packages.six.moves.http_client import (IncompleteRead as httplib_IncompleteRead)
</code></pre>

<p>This causes:</p>

<pre><code>Import Error: cannot import name 'Incomplete Read'
</code></pre>

<p>When I run the exceptions.py file for urllib3 I get the following result:</p>

<pre><code>SystemError: Parent module '' not loaded, cannot perform relative import
</code></pre>

<p>What could possibly be the issue here?</p>
",0,1503691745,python;urllib3,False,662,0,1503691745,https://stackoverflow.com/questions/45888744/incompleteread-import-error-in-urllib3
45873213,Python index error in BeautifulSoup script,"<p>I am writing a small Python scraping script that pulls some price data from a website using urllib3 library and parses this in the Beautiful Soup Library so I can find the appropriate class that holds the data I am interested in before inserting this into a list of dictionaries for me to use later on in my application. </p>

<p>I have one item I iterate through in the postcode list, and then for each (Key) in this item I create a string to build my URL for my http.request that is stored in the variable priceurl.
priceurl is then parsed through BeautifulSoup and stored in the soup variable.</p>

<p>I then do a findAll on the html class I am interested in within the soup variable and store the results in the links variable.
Finally I attempt to write the data I am interested in back to the postcode list, dictionary key, dictionary key.</p>

<p>I am having a problem running the below code on Heroku where I get the following error:</p>

<p>2017-08-25T01:14:34.311553+00:00 app[web.1]:     postcode[0][each][""price""] = links[2]
2017-08-25T01:14:34.311553+00:00 app[web.1]: IndexError: list index out of range</p>

<p>As far as I am able to tell from using the interactive interpreter and checking the lenth of the list it isn't out of range (however it obviously is!!). I am left scratching my head!!! Help Please!!!</p>

<pre><code>realestateurl = ""https://www.realestate.com.au/neighbourhoods/""
postcode = [{3192: {""price"": ""100"", ""suburb"": ""cheltenham""}, 3195: {""price"": ""200"", ""suburb"": ""mentone""},
             3193: {""price"": ""300"",""suburb""""parkdale""}}]

for item in postcode:
    for each in item:
        priceurl = http.request(""GET"", realestateurl + item[each][""suburb""] + ""-"" + (str(each)) + ""-vic"",preload_content=False)
        soup = BeautifulSoup(priceurl)
        links = soup.findAll(""div"", {""class"": ""price strong""})
        postcode[0][each][""price""] = links[2]
</code></pre>
",0,1503625458,python;dictionary;indexing;beautifulsoup;urllib3,False,77,1,1503627068,https://stackoverflow.com/questions/45873213/python-index-error-in-beautifulsoup-script
45636839,urllib3 CERTIFICATE_VERIFY_FAILED on some sites with valid certificates,"<p>Making a GET request with urllib3 in python 3.5 is failing with the error CERTIFICATE_VERIFY_FAILED on some sites where the certificate is valid.</p>

<pre><code>import certifi
import urllib3
manager = urllib3.PoolManager(cert_reqs='CERT_REQUIRED', ca_certs=certifi.where())
http.request('GET', 'https://google.com')
</code></pre>

<p>Gives this error:</p>

<pre><code>Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/site-packages/urllib3/connectionpool.py"", line 601, in urlopen
    chunked=chunked)
  File ""/usr/local/lib/python3.5/site-packages/urllib3/connectionpool.py"", line 346, in _make_request
    self._validate_conn(conn)
  File ""/usr/local/lib/python3.5/site-packages/urllib3/connectionpool.py"", line 850, in _validate_conn
    conn.connect()
  File ""/usr/local/lib/python3.5/site-packages/urllib3/connection.py"", line 326, in connect
    ssl_context=context)
  File ""/usr/local/lib/python3.5/site-packages/urllib3/util/ssl_.py"", line 329, in ssl_wrap_socket
    return context.wrap_socket(sock, server_hostname=server_hostname)
  File ""/usr/local/lib/python3.5/ssl.py"", line 376, in wrap_socket
    _context=self)
  File ""/usr/local/lib/python3.5/ssl.py"", line 747, in __init__
    self.do_handshake()
  File ""/usr/local/lib/python3.5/ssl.py"", line 983, in do_handshake
    self._sslobj.do_handshake()
  File ""/usr/local/lib/python3.5/ssl.py"", line 628, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:646)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/site-packages/requests/adapters.py"", line 440, in send
    timeout=timeout
  File ""/usr/local/lib/python3.5/site-packages/urllib3/connectionpool.py"", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File ""/usr/local/lib/python3.5/site-packages/urllib3/util/retry.py"", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='google.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:646)'),))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python3.5/site-packages/requests/api.py"", line 72, in get
    return request('get', url, params=params, **kwargs)
  File ""/usr/local/lib/python3.5/site-packages/requests/api.py"", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/usr/local/lib/python3.5/site-packages/requests/sessions.py"", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/local/lib/python3.5/site-packages/requests/sessions.py"", line 618, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/local/lib/python3.5/site-packages/requests/adapters.py"", line 506, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='google.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:646)'),))
</code></pre>

<p>Yet the same request made with the openssl command line succeeds.</p>

<pre><code>openssl s_client -showcerts -connect google.com:443 -CAfile /usr/local/lib/python3.5/site-packages/certifi/cacert.pem
</code></pre>

<p>Urllib3 fails for some but not all domains.
For example the following succeeds.</p>

<pre><code>http.request('GET', 'https://bbc.com')
</code></pre>

<p>This is happening on a kubernetes pod running Debian GNU/Linux 8. Using the python3.5 docker image.</p>
",1,1502459274,python;ssl;kubernetes;urllib3,False,2319,2,1502702899,https://stackoverflow.com/questions/45636839/urllib3-certificate-verify-failed-on-some-sites-with-valid-certificates
45671174,Sending sms Using Twilio on Python,"<p>I typed in the provided sample code and obviously replaced my own info for the necessary spots:</p>

<pre><code>from twilio.rest import Client

account_sid = ""ACf2a49cff3675efb3344c603ef08748ef""
auth_token = ""myauthtoken""
client = Client(account_sid, auth_token)

message = client.messages.create(
    to=""+mynumber"", 
    from_=""+twilionumber"", 
    body=""Hi its Gitty!"")

print(message.sid)
</code></pre>

<p>And I received this error message:</p>

<pre><code>Traceback (most recent call last):
  File ""C:/Python27/Send a Text.py"", line 1, in &lt;module&gt;
    from twilio.rest import Client
  File ""C:\Python27\lib\site-packages\twilio-6.5.0-
py2.7.egg\twilio\rest\__init__.py"", line 13, in &lt;module&gt;
    from twilio.http.http_client import TwilioHttpClient
  File ""C:\Python27\lib\site-packages\twilio-6.5.0-
py2.7.egg\twilio\http\http_client.py"", line 1, in &lt;module&gt;
    from requests import Request, Session
  File ""C:\Python27\lib\site-packages\requests-2.18.3-
py2.7.egg\requests\__init__.py"", line 43, in &lt;module&gt;
    import urllib3
ImportError: No module named urllib3
</code></pre>

<p>I read somewhere else that I may need to re-install urllib3 or upgrade it, both of which I've done, and still no change to my error message. Help?</p>
",1,1502702157,python;twilio;importerror;urllib3,False,197,0,1502702157,https://stackoverflow.com/questions/45671174/sending-sms-using-twilio-on-python
45369131,Seeing ClosedPoolError when using urllib3.PoolManager,"<p>When using urllib3.PoolManager (python 2.7, urllib3 version: 1.21) I frequently encountered ClosedPoolError exceptions. After reading the code I'm inclined to believe that this is expected since the connection object returned may have been evicted by the pool's cache by the time it is used.</p>

<p>I have reproduced the error into the following script:</p>

<p><a href=""https://pastebin.com/aJggVHuv"" rel=""nofollow noreferrer"">https://pastebin.com/aJggVHuv</a></p>

<pre><code>#!/bin/bash

set -ex

TEST_DIR=/tmp/urllib3_bug
mkdir -p $TEST_DIR
cd $TEST_DIR
virtualenv .virtualenv
. .virtualenv/bin/activate
pip install urllib3==1.21

# Starts 3 threads with pool size = 2
echo 'from urllib3 import PoolManager
import threading
from threading import Thread

pool_manager = PoolManager(num_pools=2)


def start(url):
    print ""Getting"", url, ""on"", threading.current_thread().ident
    return pool_manager.urlopen(""GET"", url)


thread1 = Thread(target=start, args=[""http://localhost:9998""])
thread2 = Thread(target=start, args=[""http://localhost:9999""])
thread3 = Thread(target=start, args=[""http://localhost:10000""])

thread1.start()
import time
time.sleep(1)
thread2.start()
thread3.start()
' &gt; start.py

# Simulates preemption by sleeping the thread
echo '303a304,308
&gt;         import time
&gt;         import threading
&gt;         print ""Simulating preemption on"", threading.current_thread().ident
&gt;         time.sleep(3)
&gt;         print ""Waking up on"", threading.current_thread().ident' | patch $TEST_DIR/.virtualenv/local/lib/python2.7/site-packages/urllib3/poolmanager.py -
rm -f $TEST_DIR/.virtualenv/local/lib/python2.7/site-packages/urllib3/poolmanager.pyc


python start.py
</code></pre>

<p>I create a pool of size 2, then run 3 threads, each of which connecting to a different URL. The first thread should encounter a ClosedPoolError.</p>

<p>(The patch inserts sleep() to reliably reproduce a preemption on the first thread).</p>
",2,1501232298,python;multithreading;urllib3,False,328,0,1501232650,https://stackoverflow.com/questions/45369131/seeing-closedpoolerror-when-using-urllib3-poolmanager
45048857,A connection attempt failed because the connected party did not properly respond after a period of time or connected host has failed to respond 99,"<p>I've tried a lot way to read the content or extract background HTML from other website using Python 3.5.</p>

<p>I'm familiar with Using an HTTP PROXY, I develop my project on Windows Server 2008 r2, and it's the policy of the company. </p>

<p>Below is my simple code to be able to request connection to the other website.</p>

<pre><code> import requests
 r = requests.get(""https://stackoverflow.com"", 
             proxies={""http"": ""http://proxy.com:900""})
 print(r.text)
</code></pre>

<blockquote>
  <p>Its work when I change the url to google urls as below</p>
</blockquote>

<pre><code>r = requests.get(""https://www.google.com"", 
             proxies={""http"": ""http://proxy.com:900""})
</code></pre>

<p>When i use url from another website. </p>

<p>Then, I got this error on the output.</p>

<pre><code>  C:\Python35\python.exe C:/Users/mwirzonw/PycharmProjects/untitled/test.py
  Traceback (most recent call last):
  File ""C:\Python35\lib\site-packages\urllib3\connection.py"", line 141, in 
 _new_conn
 (self.host, self.port), self.timeout, **extra_kw)
 File ""C:\Python35\lib\site-packages\urllib3\util\connection.py"", line 83, 
 in create_connection
 raise err
 File ""C:\Python35\lib\site-packages\urllib3\util\connection.py"", line 73, 
 in create_connection
 sock.connect(sa)
 TimeoutError: [WinError 10060] A connection attempt failed because the 
 connected party did not properly respond after a period of time, or 
 established connection failed because connected host has failed to respond

 During handling of the above exception, another exception occurred:

 Traceback (most recent call last):
 File ""C:\Python35\lib\site-packages\urllib3\connectionpool.py"", line 600, 
 in urlopen
 chunked=chunked)
 File ""C:\Python35\lib\site-packages\urllib3\connectionpool.py"", line 345, 
 in _make_request
 self._validate_conn(conn)
 File ""C:\Python35\lib\site-packages\urllib3\connectionpool.py"", line 844, 
 in 
 _validate_conn
 conn.connect()
 File ""C:\Python35\lib\site-packages\urllib3\connection.py"", line 284, in 
 connect
 conn = self._new_conn()
 File ""C:\Python35\lib\site-packages\urllib3\connection.py"", line 150, in 
 _new_conn
 self, ""Failed to establish a new connection: %s"" % e)
 urllib3.exceptions.NewConnectionError: 
 &lt;urllib3.connection.VerifiedHTTPSConnection object at 0x0000000003694390&gt;: 
 Failed to establish a new connection: [WinError 10060] A connection attempt 
 failed because the connected party did not properly respond after a period 
 of time, or established connection failed because connected host has failed 
 to respond

 During handling of the above exception, another exception occurred:

 Traceback (most recent call last):
 File ""C:\Python35\lib\site-packages\requests\adapters.py"", line 440, in 
 send
 timeout=timeout
 File ""C:\Python35\lib\site-packages\urllib3\connectionpool.py"", line 649, 
 in urlopen
 _stacktrace=sys.exc_info()[2])
 File ""C:\Python35\lib\site-packages\urllib3\util\retry.py"", line 388, in 
 increment
 raise MaxRetryError(_pool, url, error or ResponseError(cause))
 urllib3.exceptions.MaxRetryError: 
 HTTPSConnectionPool(host='stackoverflow.com', port=443): Max retries 
 exceeded with url: / (Caused by 
 NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 
 0x0000000003694390&gt;: Failed to establish a new connection: [WinError 10060] 
 A 
 connection attempt failed because the connected party did not properly 
 respond 
 after a period of time, or established connection failed because connected 
 host has failed to respond',))

 During handling of the above exception, another exception occurred:

 Traceback (most recent call last):
  File ""C:/Users/mwirzonw/PycharmProjects/untitled/test.py"", line 4, in 
  &lt;module&gt;
  proxies={""http"": ""http://proxy.com:9""})
 File ""C:\Python35\lib\site-packages\requests\api.py"", line 72, in get
   return request('get', url, params=params, **kwargs)
  File ""C:\Python35\lib\site-packages\requests\api.py"", line 58, in request
   return session.request(method=method, url=url, **kwargs)
  File ""C:\Python35\lib\site-packages\requests\sessions.py"", line 502, in 
  request
  resp = self.send(prep, **send_kwargs)
  File ""C:\Python35\lib\site-packages\requests\sessions.py"", line 612, in 
  send
  r = adapter.send(request, **kwargs)
  File ""C:\Python35\lib\site-packages\requests\adapters.py"", line 504, in 
  send
   raise ConnectionError(e, request=request)
   requests.exceptions.ConnectionError: 
   HTTPSConnectionPool(host='stackoverflow.com', port=443): Max retries 
   exceeded with url: / (Caused by 
   NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 
   0x0000000003694390&gt;: Failed to establish a new connection: [WinError 
   10060] A connection attempt failed because the connected party did not 
   properly respond after a period of time, or established connection failed 
   because connected host has failed to respond',))

   Process finished with exit code 1
</code></pre>

<p>I really need help from who read this post. Because I'm new to python and I've tried a lot of methods.</p>

<blockquote>
  <p>I've tried using urllib2, urllib3, using request based on this post-<a href=""https://stackoverflow.com/questions/5620263/using-an-http-proxy-python"">Using an HTTP PROXY - Python</a></p>
</blockquote>

<p>But still get the same output. </p>

<p>Thank you....</p>
",3,1499836087,python;request;urllib2;urllib3,False,5071,0,1499836087,https://stackoverflow.com/questions/45048857/a-connection-attempt-failed-because-the-connected-party-did-not-properly-respond
44795614,How do I parse the HTML on a page to find the largest gif and videos thumbnail?,"<p>Using python. I'd like to pass in a url and then get the HTML from the URL. I figure I can use the urllib to do this. </p>

<p>The next part is slightly more complicated. I would like to parse the HTML to find the main (defined to be biggest in size) gif and video in the HTML, if it exists. I would then like to take a screenshot, or get a thumbnail of the gif or video should it exist. </p>

<p>This seems like quite an interesting problem, and I think it can definetly be solved. I'm still thinking about the problem but would like to hear anyone else's thoughts?</p>
",0,1498633166,python;video;web-scraping;gif;urllib3,False,45,0,1498633166,https://stackoverflow.com/questions/44795614/how-do-i-parse-the-html-on-a-page-to-find-the-largest-gif-and-videos-thumbnail
44378849,Bypassing the IncompleteRead exception,"<p>I am writing a Twitter stream listener in Python3 using Tweepy. I get this error after streaming for a while:</p>

<pre><code>urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))
</code></pre>

<p>How can I just bypass this, reconnect and keep going?</p>

<p>I have done:</p>

<pre><code>from requests.packages.urllib3.exceptions import ReadTimeoutError, IncompleteRead
</code></pre>

<p>And:</p>

<pre><code>while True:
    try:
        twitter_stream.filter(track=keywordlist, follow=userlist)

    except IncompleteRead:
        continue
</code></pre>

<p>But still getting the error.</p>
",1,1496702862,python;tweepy;urllib3,True,2999,1,1496703028,https://stackoverflow.com/questions/44378849/bypassing-the-incompleteread-exception
28867840,Why do I constantly see &quot;Resetting dropped connection&quot; when uploading data to my database?,"<p>I'm uploading hundreds of millions of items to my database via a REST API from a cloud server on Heroku to a database in AWS EC2. I'm using Python and I am constantly seeing the following INFO log message in the logs.</p>

<pre><code>[requests.packages.urllib3.connectionpool] [INFO] Resetting dropped connection: &lt;hostname&gt;
</code></pre>

<p>This ""resetting of the dropped connection"" seems to take many seconds (sometimes 30+ sec) before my code continues to execute again.</p>

<ul>
<li>Firstly what exactly is happening here and why?</li>
<li>Secondly is there a way to stop the connection from dropping so that I am able to upload data faster?</li>
</ul>

<p>Thanks for your help. 
Andrew.</p>
",45,1425515251,python;http;connection-pooling;python-requests;urllib3,True,41701,3,1495016098,https://stackoverflow.com/questions/28867840/why-do-i-constantly-see-resetting-dropped-connection-when-uploading-data-to-my
43924800,Read part of a website by limiting bytes,"<p>I am trying to read several websites, get the information that I need, and then move on. Though the python code hangs on some websites. I've noticed in real browsers that at random times, the website fails to completely load, maybe its waiting on some ads to load...?</p>

<p>The information that I need is within the first 50kb of the website. If I use a timeout, the entire response from the connection is lost in all of the modules that I have tried (urllib, urlib3, and pycurl). Also, in pycurl, set option RANGE does not seem do anything for the url. </p>

<p>Does anyone know how to save the content already received upon calling a timeout. Or, does someone know how to effectively limit the content to a certain number of bytes?</p>
",1,1494533816,python;python-3.x;urllib;pycurl;urllib3,True,200,1,1494608521,https://stackoverflow.com/questions/43924800/read-part-of-a-website-by-limiting-bytes
43473396,urllib.request.urlretrieve not working over HTTP websites,"<p>urllib.request.urlretrieve not working over HTTP websites, however the same is able to save url over HTTPS websites.</p>

<pre><code>import urllib.request

abc=""http://www.google.com""
urllib.request.urlretrieve(abc,""C:\\Users\\kj\\downloads\\abc.html"")
</code></pre>

<p>I'm getting the following error:</p>

<blockquote>
  <p>[WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond></p>
</blockquote>

<p>When i use ""<a href=""https://www.google.com"" rel=""nofollow noreferrer"">https://www.google.com</a>"", the code is working. I am currently working on a website which is not a secured website (http), and this code is not working on them.</p>

<p>I have read few threads on this portal, but could not find a solution.</p>

<p>i am using python 3.6.</p>

<p>Any suggestions?</p>
",0,1492521534,python;python-3.x;urllib2;urllib;urllib3,False,627,0,1492526227,https://stackoverflow.com/questions/43473396/urllib-request-urlretrieve-not-working-over-http-websites
43043861,ssl certificate verify apparently works with ssl client but not with urllib3,"<p>I am having a very similar problem to that discussed <a href=""https://stackoverflow.com/questions/10667960/python-requests-throwing-up-sslerror"">here</a></p>

<p>I am using python 3.4 with urllib3 library.</p>

<p>When I test the code below, i get:</p>

<pre><code>Traceback (most recent call last):
File ""/home/julimatt/zibawa3/zib3/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 600, in urlopen
    chunked=chunked)
  File ""/home/julimatt/zibawa3/zib3/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 345, in _make_request
    self._validate_conn(conn)
  File ""/home/julimatt/zibawa3/zib3/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 844, in _validate_conn
    conn.connect()
  File ""/home/julimatt/zibawa3/zib3/lib/python3.4/site-packages/requests/packages/urllib3/connection.py"", line 326, in connect
    ssl_context=context)
  File ""/home/julimatt/zibawa3/zib3/lib/python3.4/site-packages/requests/packages/urllib3/util/ssl_.py"", line 324, in ssl_wrap_socket
    return context.wrap_socket(sock, server_hostname=server_hostname)
  File ""/usr/lib/python3.4/ssl.py"", line 364, in wrap_socket
    _context=self)
  File ""/usr/lib/python3.4/ssl.py"", line 578, in __init__
    self.do_handshake()
  File ""/usr/lib/python3.4/ssl.py"", line 805, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:598)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/julimatt/zibawa3/zib3/lib/python3.4/site-packages/requests/adapters.py"", line 423, in send
    timeout=timeout
  File ""/home/julimatt/zibawa3/zib3/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 630, in urlopen
    raise SSLError(e)
requests.packages.urllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:598)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/julimatt/workspace2/zibawa/stack_configs/tests.py"", line 44, in test_bind_grafana
    result=getFromGrafanaApi(apiurl, data,'GET')
  File ""/home/julimatt/workspace2/zibawa/stack_configs/models.py"", line 317, in getFromGrafanaApi
    verify=ca_certs,
  File ""/home/julimatt/zibawa3/zib3/lib/python3.4/site-packages/requests/sessions.py"", line 609, in send
    r = adapter.send(request, **kwargs)
  File ""/home/julimatt/zibawa3/zib3/lib/python3.4/site-packages/requests/adapters.py"", line 497, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:598)
</code></pre>

<p>My code is:</p>

<pre><code>from requests import Request, Session
ca_certs='/path/to/letsencypt/fullchain.pem'  
url= 'https://myserver.com:3000/api/org'

username= settings.DASHBOARD['user']
password= settings.DASHBOARD['password']

headers = {'Accept': 'application/json',
               'Content-Type' : 'application/json',}

s = Session()
req = Request('GET',  url, data=data, headers=headers, auth=(username,password))

prepped = s.prepare_request(req)

resp = s.send(prepped,
verify=ca_certs,

)

print(resp.status_code)
return resp
</code></pre>

<p>If I test my code with 'verify=False' in the request, then it works fine but that is obviously not a secure solution.</p>

<p>I tried to test my ssl connection from a terminal on the same machine using:</p>

<pre><code>openssl s_client -connect myserver.com:3000 -CAfile /path/to/letsencypt/fullchain.pem
</code></pre>

<p>Then I get a succesful handshake.</p>

<p>So I cannot understand why I get this error.</p>

<p>Thanks in advance for any help you can provide.</p>
",0,1490610298,python;ssl;urllib;lets-encrypt;urllib3,False,688,1,1491909603,https://stackoverflow.com/questions/43043861/ssl-certificate-verify-apparently-works-with-ssl-client-but-not-with-urllib3
43309321,Python 3 import module error: No module named request,"<p>Hello I am using python 3.4 on a Mac, Here is what I did-</p>

<pre><code>import urllib.request
</code></pre>

<p>when I run the code it comes back with this error-</p>

<p>ImportError: No module named request</p>

<p>when I do pip3 freeze here are my modules-</p>

<pre><code>pip3 freeze
appdirs==1.4.3
beautifulsoup4==4.5.3
chardet2==2.0.3
html5lib==0.999999999
numpy==1.12.1
packaging==16.8
pandas==0.19.2
pyparsing==2.2.0
python-dateutil==2.6.0
pytz==2017.2
requests==2.13.0
six==1.10.0
urllib3==1.20
webencodings==0.5.1
</code></pre>

<p>What could be the issue? thanks</p>
",0,1491756558,python;geany;urllib3,False,2091,1,1491764076,https://stackoverflow.com/questions/43309321/python-3-import-module-error-no-module-named-request
43258485,How to use urllib in python 3 ? 4,"<p>I am using anaconda and python 3 - i wrote below but not working for some reason. I am very new to python please help ! thank you. </p>

<pre><code>import urllib.request 

x = urllib.request.urlopen('https://www.google.com')

print(x.read())
</code></pre>
",0,1491489888,python;python-3.x;urllib3,True,53,1,1491496470,https://stackoverflow.com/questions/43258485/how-to-use-urllib-in-python-3-4
42976413,"TypeError: tuple indices must be integers or slices, not str Python sentiment tweet","<p>I am trying to capture real live tweets. I am trying to access contents using the json library, and create new objects and append this into a list.</p>

<pre><code>from tweepy.streaming import StreamListener
from tweepy import OAuthHandler
from tweepy import Stream
import tweepy

import json
import urllib.parse
from urllib.request import urlopen
import json

# Variables that contains the user credentials to access Twitter API
consumer_key = """"
consumer_secret = ""C""
access_token = """"
access_token_secret = """"
sentDexAuth = ''


auth = OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
api = tweepy.API(auth)



def sentimentAnalysis(text):
    encoded_text = urllib.parse.quote(text)
    API_Call = 'http://sentdex.com/api/api.php?text=' + encoded_text + '&amp;auth=' + sentDexAuth
    output = urlopen(API_Call).read()

    return output

class StdOutListener(StreamListener):
    def __init___(self):
        self.tweet_data = []


def on_data(self, data):
    tweet = json.loads(data)
    for x in tweet.items():
        sentimentRating = sentimentAnalysis(x['text'])
        actualtweets = {
            'created_at' : x['created_at'],
            'id' : x['id'],
            'tweets': x['text'] + sentimentRating
        }
        self.tweet_data.append(actualtweets)


    with open('rawtweets2.json', 'w') as out:
        json.dump(self.tweet_data, out)

    print(tweet)
l = StdOutListener()
stream = Stream(auth, l)
keywords = ['iknow']
stream.filter(track=keywords)
</code></pre>

<p>I believe that i am accessing the json objects correctly however i am not sure of this error, i need it to be a string for my sentiment function to work, and 
iam getting a type error:</p>

<pre><code>sentimentRating = sentimentAnalysis(x['text'])
TypeError: tuple indices must be integers or slices, not str
</code></pre>
",0,1490272978,python;json;tweepy;sentiment-analysis;urllib3,True,2032,2,1490273385,https://stackoverflow.com/questions/42976413/typeerror-tuple-indices-must-be-integers-or-slices-not-str-python-sentiment-tw
39786546,conda missing requests.packages.urllib3.util.url,"<p>Somehow I broke my conda/urllib3 installation (Python 2.7 / Anaconda) but without admin rights to the machine I use have limited options to fix / reinstall everything the whole anaconda.</p>

<p>For any conda operations,(e.g. conda install launcher), I am getting the message:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Anaconda2\Scripts\conda-script.py"", line 3, in &lt;module
    import conda.cli
  File ""C:\Anaconda2\Lib\site-packages\conda\cli\__init__.py"", li
    from .main import main  # NOQA
  File ""C:\Anaconda2\Lib\site-packages\conda\cli\main.py"", line 4
    from ..base.context import context
  File ""C:\Anaconda2\Lib\site-packages\conda\base\context.py"", li
    from ..common.url import urlparse, path_to_url
  File ""C:\Anaconda2\Lib\site-packages\conda\common\url.py"", line
    from requests.packages.urllib3.util.url import parse_url
ImportError: No module named requests.packages.urllib3.util.url
</code></pre>

<p>For both conda and urllib3 i have tried a number of combinations using pip, e.g.</p>

<pre><code>pip install urllib3 
pip install urllib3 --upgrade
pip install urllib3 --upgrade --force-reinstall
pip install urllib3==1.7.1 --upgrade --force-reinstall
pip install H:\PyManPkgs\urllib3-1.18-py2.py3-none-any.whl --upgrade

pip install conda
pip install conda --upgrade --force-reinstall
</code></pre>

<p>etc.
All of these installs work fine, but the same conda/urllib3 issue persists.</p>

<p>thanks</p>
",2,1475222543,python;anaconda;conda;urllib3,True,3141,1,1489149168,https://stackoverflow.com/questions/39786546/conda-missing-requests-packages-urllib3-util-url
42518512,How to encode whitespaces as &#39;+&#39; in the url using urllib3?,"<p>I'm using urllib3 on Python 2.7. I need to send a request to a website that gives the desired response only when the spaces are separated by '+' and <strong>not</strong> by '%2b'. </p>

<pre><code>http = urllib3.PoolManager()
var = 'foo bar'
url = 'https://foo.com/release?q=' + var
webdata = http.request('GET', url)
</code></pre>

<p>How can I do this in urllib3?</p>
",-3,1488315113,python;url-encoding;urllib3,True,597,1,1488316108,https://stackoverflow.com/questions/42518512/how-to-encode-whitespaces-as-in-the-url-using-urllib3
42354641,Python Dropbox API Error,"<p>I'm getting the error ""ImportError: No module named packages.urllib3.poolmanager"" when I try to use the ""dropbox"" package from pip install, and I have listed my pip installed packages here as well. </p>

<pre><code>vagrant@vagrant-ubuntu-trusty-64:/var/www/my_site/dj_server$ python dropbox_backup.py
Traceback (most recent call last):
  File ""dropbox_backup.py"", line 1, in &lt;module&gt;
    import dropbox
  File ""/usr/local/lib/python2.7/dist-packages/dropbox/__init__.py"", line 3, in &lt;module&gt;
    from .dropbox import __version__, Dropbox, DropboxTeam, create_session
  File ""/usr/local/lib/python2.7/dist-packages/dropbox/dropbox.py"", line 34, in &lt;module&gt;
    from .session import pinned_session
  File ""/usr/local/lib/python2.7/dist-packages/dropbox/session.py"", line 7, in &lt;module&gt;
    from requests.packages.urllib3.poolmanager import PoolManager
ImportError: No module named packages.urllib3.poolmanager
vagrant@vagrant-ubuntu-trusty-64:/var/www/my_site/dj_server$
</code></pre>

<p>and here are my package info. </p>

<pre><code>vagrant@vagrant-ubuntu-trusty-64:/var/www/placesocial/dj_server$ pip freeze
Cheetah==2.4.4
Django==1.8.4
Landscape-Client==14.12
MySQL-python==1.2.5
PAM==0.4.2
Pillow==4.0.0
PyYAML==3.10
SecretStorage==2.0.0
Twisted-Core==13.2.0
Twisted-Names==13.2.0
Twisted-Web==13.2.0
apt-xapian-index==0.45
argparse==1.2.1
boto==2.45.0
chardet==2.0.1
cloud-init==0.7.5
colorama==0.2.5
configobj==4.7.2
configparser==3.5.0
contextlib2==0.5.4
cssselect==1.0.1
django-widget-tweaks==1.4.1
dropbox==7.1.1
elasticsearch==5.2.0
html5lib==0.999
httplib2==0.8
jsonpatch==1.3
jsonpointer==1.0
keyring==3.5
launchpadlib==1.10.2
lazr.restfulclient==0.13.3
lazr.uri==1.0.3
oauth==1.0.1
olefile==0.44
prettytable==0.7.2
pyOpenSSL==0.13
pycrypto==2.6.1
pycurl==7.19.3
pygeoip==0.3.2
pygobject==3.12.0
pyserial==2.6
python-apt==0.9.3.5ubuntu2
python-debian==0.1.21-nmu2ubuntu2
raven==5.32.0
requests==2.2.1
simplejson==3.3.1
six==1.5.2
ssh-import-id==3.21
typing==3.5.3.0
uWSGI==2.0.14
urllib3==1.7.1
wadllib==1.3.2
wheel==0.24.0
wsgiref==0.1.2
zope.interface==4.0.5
</code></pre>
",0,1487626517,python;dropbox;dropbox-api;urllib3,False,466,2,1487706078,https://stackoverflow.com/questions/42354641/python-dropbox-api-error
42118029,"for range to send post request by using requests.Session(), it alert &#39;module&#39; object has no attribute &#39;kqueue&#39;","<p><code>macOS 10.12.3</code> <code>python 2.7.13</code> <code>requests 2.13.0</code><br>
I use requests package to send post request.This request need to login before post data.So I use request.Session() and load a logined cookie.
Then I use this session to send post data in cycle mode.<br>
It is no error that I used to run this code in Windows and Linux.<br>
<strong>Simple Code:</strong></p>

<pre><code>s = request.Session()
s.cookies = cookieslib.LWPCookieJar('cookise')
s.cookies.load(ignore_discard=True)
for user_id in range(100,200):
    url = 'http://xxxx'
    data = { 'user': user_id, 'content': '123'}
    r = s.post(url, data)
    ...
</code></pre>

<p>But the program frequently (about every interval) crash, the error is<code>AttributeError: 'module' object has no attribute 'kqueue'</code></p>

<pre><code>Traceback (most recent call last):
  File ""/Users/gasxia/Dev/Projects/TgbookSpider/kfz_send_msg.py"", line 90, in send_msg
    r = requests.post(url, data)  # catch error if user isn't exist
  File ""/usr/local/lib/python2.7/site-packages/requests/sessions.py"", line 535, in post
    return self.request('POST', url, data=data, json=json, **kwargs)
  File ""/usr/local/lib/python2.7/site-packages/requests/sessions.py"", line 488, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/local/lib/python2.7/site-packages/requests/sessions.py"", line 609, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/local/lib/python2.7/site-packages/requests/adapters.py"", line 423, in send
    timeout=timeout
  File ""/usr/local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py"", line 588, in urlopen
    conn = self._get_conn(timeout=pool_timeout)
  File ""/usr/local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py"", line 241, in _get_conn
    if conn and is_connection_dropped(conn):
  File ""/usr/local/lib/python2.7/site-packages/requests/packages/urllib3/util/connection.py"", line 27, in is_connection_dropped
    return bool(wait_for_read(sock, timeout=0.0))
  File ""/usr/local/lib/python2.7/site-packages/requests/packages/urllib3/util/wait.py"", line 33, in wait_for_read
    return _wait_for_io_events(socks, EVENT_READ, timeout)
  File ""/usr/local/lib/python2.7/site-packages/requests/packages/urllib3/util/wait.py"", line 22, in _wait_for_io_events
    with DefaultSelector() as selector:
  File ""/usr/local/lib/python2.7/site-packages/requests/packages/urllib3/util/selectors.py"", line 431, in __init__
    self._kqueue = select.kqueue()
AttributeError: 'module' object has no attribute 'kqueue'
</code></pre>
",0,1486569961,python;macos;python-requests;urllib3;kqueue,True,526,1,1486625988,https://stackoverflow.com/questions/42118029/for-range-to-send-post-request-by-using-requests-session-it-alert-module-ob
42111534,Unable to import urllib3 in python 3.5,"<p>I am using python 3.5.2. </p>

<p>When I try to install <code>urllib3</code>, I am getting the below error:</p>

<pre><code>ImportError: No module named 'urllib3'
</code></pre>
",1,1486552293,python;python-3.x;urllib3,True,962,1,1486552625,https://stackoverflow.com/questions/42111534/unable-to-import-urllib3-in-python-3-5
41030812,Elasticsearch Python client SSLError on Mac OSX,"<p>Our devops recently turned on SSL on our in-house ElasticSearch servers, while our Ubuntu dev boxes are connecting to it fine, but it's causing <code>SSLError</code> on Mac dev boxes (running Django).</p>

<pre><code>SSLError at /search
ConnectionError(EOF occurred in violation of protocol (_ssl.c:590)) caused by: SSLError(EOF occurred in violation of protocol (_ssl.c:590))
</code></pre>

<p>What I know so far:</p>

<ul>
<li>Not limited to El Capitan, also breaks on earlier version</li>
<li>We can connect to other ES service over https, our devops told me our ES service has higher https requirements</li>
<li>Openssl v0.9.8 has handshake problem on the ES service</li>
<li>Openssl v1.0.1 works fine on the ES service</li>
</ul>

<p>There are many posts online around this problem but none helped.</p>

<p>I have tried:</p>

<ul>
<li>brew link --force openssl (but EL Capitan is stopping it), none of the solutions worked from <a href=""https://stackoverflow.com/questions/38670295/brew-refusing-to-link-openssl"">Homebrew refusing to link OpenSSL</a></li>
<li>adding <code>/usr/local/opt/openssl/lib</code> to <code>DYLD_LIBRARY_PATH</code></li>
<li>upgrading python from 2.7.10 to 2.7.12</li>
<li>rebuilding virtualenv</li>
</ul>

<p>None of the above worked...</p>

<p>Questions:</p>

<ul>
<li>Is it due to OpenSSL version?</li>
<li>Is it due to Python version due to _ssl.c?</li>
<li>How do I fix this?</li>
</ul>
",1,1481164360,python;macos;ssl;urllib3,True,146,1,1483930824,https://stackoverflow.com/questions/41030812/elasticsearch-python-client-sslerror-on-mac-osx
24136740,Python: Surpass SSL verification with urllib3,"<p>Connectiing to SSL server is throwing an error:</p>

<blockquote>
  <p>error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate
  verify failed</p>
</blockquote>

<p>I'm using this urllib3 python library for connecting to server, using this method:</p>

<pre><code>urllib3.connectionpool.connection_from_url(['remote_server_url'], maxsize=2)
</code></pre>

<p>How can i ignore SSL verification? Or is there another step for doing this?</p>

<p>Thanks.</p>
",0,1402390445,python;python-2.7;ssl;urllib;urllib3,True,5522,2,1482332335,https://stackoverflow.com/questions/24136740/python-surpass-ssl-verification-with-urllib3
40887629,Python 3.4 I want to write an intercepting proxy between browser and target application,"<p>I am using python 3.4. I want to write an intercepting Proxy between your browser and the target application. It should log the response into a file which could be analysed at later point of time. I need some guidance to start with or any relevant resources (preferred module with documentation). </p>
",0,1480506515,python;python-3.x;urllib3,True,174,1,1480506667,https://stackoverflow.com/questions/40887629/python-3-4-i-want-to-write-an-intercepting-proxy-between-browser-and-target-appl
40709692,Python Edit a header value with urllib,"<p>I am creating this request:</p>

<pre><code>url=""http://tn.ai/1244844""
response = self.http.request('GET', urllib.parse.quote(url, safe=':/?='), headers={""User-Agent"": ""Mozilla/5.0""})
</code></pre>

<p>Here I'm getting all headers:</p>

<pre><code>response.getheaders()
</code></pre>

<p>Result is:</p>

<pre><code>Content-Type: text/html; charset=UTF-8
Location: =?utf-8?b?aHR0cHM6Ly93d3cudGFzbmltbmV3cy5jb20vZmEvbmV3cy8xMzk1LzA4LzMwLzEyNDQ4NDQvw5nChsOawq/DmMKnw5nChy3DmMKow5jCrsOYwrTDm8KMLcOZ?=

Ø¨Ø§Ø±Ø²Ù-Ø¨Ø§-ÙØ§ÚØ§Ù-Ø±Ø§-Ù
Ø´Ú©Ù-Ú©Ø±Ø¯Ù-Ø§Ø³Øª
Server: Microsoft-IIS/8.5
Date: Sun, 20 Nov 2016 21:28:25 GMT
Content-Length: 253
</code></pre>

<p>Now, since i have some unknown values in <code>Location</code> value, i get this error in parsing header values:</p>

<pre><code>Failed to parse headers (url=http://www.tasnimnews.com:80/fa/news/1395/08/30/1244844/%D9%86%DA%AF%D8%A7%D9%87-%D8%A8%D8%AE%D8%B4%DB%8C-%D9%85%D8%A8%D8%A7%D8%B1%D8%B2%D9%87-%D8%A8%D8%A7-%D9%82%D8%A7%DA%86%D8%A7%D9%82-%D8%B1%D8%A7-%D9%85%D8%B4%DA%A9%D9%84-%DA%A9%D8%B1%D8%AF%D9%87-%D8%A7%D8%B3%D8%AA): [MissingHeaderBodySeparatorDefect()], unparsed data: 'Ø¨Ø§Ø±Ø²Ù\x87-Ø¨Ø§-Ù\x82Ø§Ú\x86Ø§Ù\x82-Ø±Ø§-Ù\x85Ø´Ú©Ù\x84-Ú©Ø±Ø¯Ù\x87-Ø§Ø³Øª\r\nServer: Microsoft-IIS/8.5\r\nDate: Sun, 20 Nov 2016 21:28:25 GMT\r\nContent-Length: 253\r\n\r\n'
Traceback (most recent call last):
  File ""C:\Python34\lib\site-packages\urllib3\connectionpool.py"", line 405, in _make_request
    assert_header_parsing(httplib_response.msg)
  File ""C:\Python34\lib\site-packages\urllib3\util\response.py"", line 59, in assert_header_parsing
    raise HeaderParsingError(defects=defects, unparsed_data=unparsed_data)
urllib3.exceptions.HeaderParsingError: [MissingHeaderBodySeparatorDefect()], unparsed data: 'Ø¨Ø§Ø±Ø²Ù\x87-Ø¨Ø§-Ù\x82Ø§Ú\x86Ø§Ù\x82-Ø±Ø§-Ù\x85Ø´Ú©Ù\x84-Ú©Ø±Ø¯Ù\x87-Ø§Ø³Øª\r\nServer: Microsoft-IIS/8.5\r\nDate: Sun, 20 Nov 2016 21:28:25 GMT\r\nContent-Length: 253\r\n\r\n'
</code></pre>

<p>What is the solution?</p>

<p>How can i prevent from this error?</p>

<p>Should i edit location value manually?</p>

<p>If yes, how?</p>
",2,1479677958,python;header;urllib3,False,281,0,1479678123,https://stackoverflow.com/questions/40709692/python-edit-a-header-value-with-urllib
40617396,import urllib3 works in terminal but not on IDLE,"<p>I am using Mac OSX 10.10.5, and Python version 3.5.2, and IDLE version 3.5.2.</p>

<p>I am extremely new to Python, and am trying to use the urllib3 module in IDLE. I have used the following code in the Terminal with success (the number 200 is returned):</p>

<pre><code>import urllib3
http = urllib3.PoolManager()
r = http.request('GET', 'http://httpbin.org/robots.txt')
r.status
</code></pre>

<p>But the same code does not work in IDLE. In IDLE I get the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""/Users/faculty/Documents/Python/Scraping_v1_d1.py"", line 1, in &lt;module&gt;
    import urllib3
ImportError: No module named 'urllib3'
</code></pre>

<p>I have also attempted to use other code such as the following in IDLE:</p>

<pre><code>import urllib3
htmlfile = urllib3.urlopen(""http://google.com"")
htmltext = htmlfile.read()
print (htmltext)
</code></pre>

<p>But I get the same error.</p>

<p>In my site-packages folder I have these pip and urllib3 folders:</p>

<p>1) pip</p>

<p>2) pip-9.0.1.dist-info</p>

<p>3) urllib3</p>

<p>4) urllib3-1.19.dist-info</p>

<p>I found one source that suggested that I try to do the following:</p>

<pre><code>import sys
sys.version
sys.path
</code></pre>

<p>This is the response in Terminal:</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>import sys
      sys.version
      '2.7.10 (default, Jul 14 2015, 19:46:27) \n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)]'
      sys.path
      ['', '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python27.zip', '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7', '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-darwin', '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-mac', '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-mac/lib-scriptpackages', '/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python', '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-tk', '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-old', '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload', '/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/PyObjC', '/Library/Python/2.7/site-packages']</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>When I type the same code into IDLE, nothing happens (this is all I get):</p>

<p>========= RESTART: /Users/faculty/Documents/Python/Scraping_v1_d1.py =========</p>

<p>I have searched the web and stackoverflow.com extensively, but can't locate a solution. Does anyone have any insight?</p>

<p>Thanks!</p>
",0,1479235451,python;macos;terminal;python-idle;urllib3,True,740,1,1479235866,https://stackoverflow.com/questions/40617396/import-urllib3-works-in-terminal-but-not-on-idle
40595634,Python | Http - can&#39;t get the correct mime type,"<p>I am building a web crawler using <code>urllib3</code>. Example code:</p>

<pre><code>from urllib3 import PoolManager

pool = PoolManager()
response = pool.request(""GET"", url)
mime_type = response.getheader(""content-type"")
</code></pre>

<p>I have stumbled upon few links to document files such as docx and epub and the mime type I'm getting from the server is <code>text/plain</code>.It is important to me to get the <strong>correct</strong> mime type.</p>

<p>Example to a problematic url:</p>

<p><a href=""http://lsa.mcgill.ca/pubdocs/files/advancedcommonlawobligations/523-gold_advancedcommonlawobligations_-2013.docx"" rel=""nofollow noreferrer"">http://lsa.mcgill.ca/pubdocs/files/advancedcommonlawobligations/523-gold_advancedcommonlawobligations_-2013.docx</a></p>

<p>Right now the logic of getting file's mime type is getting it from the server and if not available trying to get the file's extension.</p>

<p>How come <code>Firefox</code> is not getting confused by these kind of urls and let the user download the file right away? How does it know that this file is not plain text? How can i get the correct mime type?</p>
",2,1479149007,python;mime-types;urllib3,True,2752,2,1479150873,https://stackoverflow.com/questions/40595634/python-http-cant-get-the-correct-mime-type
40594817,Python | HTTP - How to check file size before downloading it,"<p>I am crawling the web using urllib3. Example code:</p>

<pre><code>from urllib3 import PoolManager

pool = PoolManager()
response = pool.request(""GET"", url)
</code></pre>

<p>The problem is that i may stumble upon url that is a download of a really large file and I am not interseted in downloading it.</p>

<p>I found this question - <a href=""https://stackoverflow.com/a/5985/4869599"">Link</a> - and it suggests using <code>urllib</code> and <code>urlopen</code>. I don't want to contact the server twice.</p>

<p>I want to limit the file size to 25MB. 
Is there a way i can do this with <code>urllib3</code>?</p>
",3,1479145743,python;http;urllib2;urllib;urllib3,True,4476,1,1479148698,https://stackoverflow.com/questions/40594817/python-http-how-to-check-file-size-before-downloading-it
35275985,Trying to install mysql-python: &quot;No module named urllib3&quot;,"<p>I'm trying to look around on this issue but so far nothing I've found has fixed it. I am not that familiar with Python and I'm working with someone else's script. I am on Windows 10 (64-bit).</p>

<p>Executing this command: <code>pip install mysql-python</code></p>

<p>And getting this error:</p>

<pre><code>Traceback (most recent call last):
File ""C:\Python27\Lib\runpy.py"", line 162, in _run_module_as_main
""__main__"", fname, loader, pkg_name)
File ""C:\Python27\Lib\runpy.py"", line 72, in _run_code
exec code in run_globals
File ""C:\Python27\Scripts\pip.exe\__main__.py"", line 5, in &lt;module&gt;
File ""C:\Python27\lib\site-packages\pip\__init__.py"", line 15, in &lt;module&gt;
from pip.vcs import git, mercurial, subversion, bazaar  # noqa
File ""C:\Python27\lib\site-packages\pip\vcs\mercurial.py"", line 9, in &lt;module&gt;
from pip.download import path_to_url
File ""C:\Python27\lib\site-packages\pip\download.py"", line 38, in &lt;module&gt;
from pip._vendor import requests, six
File ""C:\Python27\lib\site-packages\pip\_vendor\requests\__init__.py"", line 58, in &lt;module&gt;
from . import utils
File ""C:\Python27\lib\site-packages\pip\_vendor\requests\utils.py"", line 26, in &lt;module&gt;
from .compat import parse_http_list as _parse_list_header
File ""C:\Python27\lib\site-packages\pip\_vendor\requests\compat.py"", line 7, in &lt;module&gt;
from .packages import chardet
File ""C:\Python27\lib\site-packages\pip\_vendor\requests\packages\__init__.py"", line 29, in &lt;module&gt;
import urllib3
ImportError: No module named urllib3
</code></pre>
",0,1454953534,python;mysql;urllib;urllib3,False,875,2,1478846042,https://stackoverflow.com/questions/35275985/trying-to-install-mysql-python-no-module-named-urllib3
40421545,"Trying to scrape data, but can&#39;t scrape it all (Python)","<p>I'm trying to scrape the number of followers from this page <code>http://freelegalconsultancy.blogspot.co.uk/</code> but can't seem to pull it. I've tried using <code>urllib</code>, <code>urllib2</code>, <code>urllib3</code>, <code>selenium</code> and <code>beautiful soup</code>, but have had no luck pulling the followers. Here's what my code looks like currently:</p>

<pre><code>import urllib2

url = ""http://freelegalconsultancy.blogspot.co.uk/""

opener = urllib2.urlopen(url)

for item in opener:
    print item
</code></pre>

<p>How would I go about pulling the number of followers?</p>
",0,1478258988,python;selenium;urllib2;urllib;urllib3,True,95,1,1478260925,https://stackoverflow.com/questions/40421545/trying-to-scrape-data-but-cant-scrape-it-all-python
40054711,"Python: HTTPConnectionPool(host=&#39;%s&#39;, port=80):","<pre><code>import requests
import urllib3
from time import sleep
from sys import argv
script, filename = argv
http = urllib3.PoolManager()

datafile = open('datafile.txt','w')
crawl = """"

with open(filename) as f:
mylist = f.read().splitlines()

def crawlling(x):
    for i in mylist:
        domain = (""http://"" + ""%s"") % i
        crawl = http.request('GET','%s',preload_content=False) % domain
        for crawl in crawl.stream(32):
            print crawl
            sleep(10)
            crawl.release_conn()
            datafile.write(crawl.status)
            datafile.write('&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n')
            datafile.write(crawl.data)
            datafile.close()
return x


crawlling(crawl)

_______________________________________________________________________
Extract of domain.txt file:
fjarorojo.info
buscadordeproductos.com
</code></pre>

<p>I'm new to python so bear with me: I'm trying to trying get content from URL but it's throwing error. Further, it's working fine in browser.
Object of script is to get the data from domain.txt file and iterate over it and fetch the content and save it in file.</p>

<pre><code>Getting this error: 
  raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='%s',
port=80):     Max retries exceeded with url: / (Caused by 
NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 
0x7ff45e4f9cd0&gt;: Failed to establish a new connection: [Errno -2] Name or 
service not known',))
</code></pre>
",2,1476502899,python;web-scraping;python-requests;urllib3,True,3814,1,1476545251,https://stackoverflow.com/questions/40054711/python-httpconnectionpoolhost-s-port-80
39830446,For Loop doesn&#39;t spit out needed results,"<p>I got this piece of code to spit out the unique ""area number"" in the URL. However, the loop doesn't work. It spits out the same number, please see below:</p>

<pre><code>import urllib3
from bs4 import BeautifulSoup

http = urllib3.PoolManager()

url = open('MS Type 1 URL.txt',encoding='utf-8-sig')

links = []
for link in url:
    y = link.strip()
    links.append(y)

url.close()

print('Amount of Links: ', len(links))

for x in links:
    j = (x.find(""="") + 1)
    g = (x.find('&amp;housing'))
    print(link[j:g])
</code></pre>

<p>Results are:</p>

<p><a href=""http://millersamuel.com/aggy-data/home/query_report?area=38&amp;housing_type=3&amp;measure=4&amp;query_type=quarterly&amp;region=1&amp;year_end=2020&amp;year_start=1980"" rel=""nofollow"">http://millersamuel.com/aggy-data/home/query_report?area=38&amp;housing_type=3&amp;measure=4&amp;query_type=quarterly&amp;region=1&amp;year_end=2020&amp;year_start=1980</a>
23</p>

<p><a href=""http://millersamuel.com/aggy-data/home/query_report?area=23&amp;housing_type=1&amp;measure=4&amp;query_type=annual&amp;region=1&amp;year_end=2020&amp;year_start=1980"" rel=""nofollow"">http://millersamuel.com/aggy-data/home/query_report?area=23&amp;housing_type=1&amp;measure=4&amp;query_type=annual&amp;region=1&amp;year_end=2020&amp;year_start=1980</a>
23</p>

<p>As you can see it spits out the area number '23' which is only in one of this URL but not the '38' of the other URL.</p>
",0,1475493821,python;python-3.x;loops;beautifulsoup;urllib3,True,100,2,1475496090,https://stackoverflow.com/questions/39830446/for-loop-doesnt-spit-out-needed-results
39733482,Python urllib3 - Crawling webpages failed at a specific webpage only from google cloud vm,"<p>I have built a crawler script that download webpages using urllib3 and python3. I ran the crawler in a <code>Google Cloud Virtual Machine</code> instance and tried it with many different urls. I found a url that i couldn't download on the VM instance but succeeded to download on my personal Linux.</p>

<p>The code that i've tried on my computer and worked:</p>

<pre><code>&gt;&gt;&gt; import urllib3
&gt;&gt;&gt; pool = urllib3.PoolManager()
&gt;&gt;&gt; response = pool.request('GET', 'http://www.azlyrics.com/lyrics/drake/dowhatyoudo.html')
&gt;&gt;&gt; response.status
200
</code></pre>

<p>I have tried the exact same code on the VM instance using ssh and this exception occurred:</p>

<pre><code>Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/urllib3/connectionpool.py"", line 594, in urlopen
    chunked=chunked)
  File ""/usr/local/lib/python3.5/dist-packages/urllib3/connectionpool.py"", line 391, in _make_request
    six.raise_from(e, None)
  File ""&lt;string&gt;"", line 2, in raise_from
  File ""/usr/local/lib/python3.5/dist-packages/urllib3/connectionpool.py"", line 387, in _make_request
    httplib_response = conn.getresponse()
  File ""/usr/lib/python3.5/http/client.py"", line 1197, in getresponse
    response.begin()
  File ""/usr/lib/python3.5/http/client.py"", line 297, in begin
    version, status, reason = self._read_status()
  File ""/usr/lib/python3.5/http/client.py"", line 266, in _read_status
    raise RemoteDisconnected(""Remote end closed connection without""
http.client.RemoteDisconnected: Remote end closed connection without response
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python3.5/dist-packages/urllib3/request.py"", line 66, in request
    **urlopen_kw)
  File ""/usr/local/lib/python3.5/dist-packages/urllib3/request.py"", line 87, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
  File ""/usr/local/lib/python3.5/dist-packages/urllib3/poolmanager.py"", line 244, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/urllib3/connectionpool.py"", line 671, in urlopen
    release_conn=release_conn, **response_kw)
  File ""/usr/local/lib/python3.5/dist-packages/urllib3/connectionpool.py"", line 671, in urlopen
    release_conn=release_conn, **response_kw)
  File ""/usr/local/lib/python3.5/dist-packages/urllib3/connectionpool.py"", line 671, in urlopen
    release_conn=release_conn, **response_kw)
  File ""/usr/local/lib/python3.5/dist-packages/urllib3/connectionpool.py"", line 643, in urlopen
    _stacktrace=sys.exc_info()[2])
  File ""/usr/local/lib/python3.5/dist-packages/urllib3/util/retry.py"", line 303, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='www.azlyrics.com', port=80): Max retries exceeded with url: /lyrics/drake/dowhatyoudo.html (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote
 end closed connection without response',)))
</code></pre>

<p>I have checked the VM dashboard and I didn't find any firewall or other restrictions that I may have mistakenly defined.</p>

<p>What could it possibly be?</p>
",0,1475007053,python;python-3.x;google-cloud-platform;urllib3,False,409,0,1475007053,https://stackoverflow.com/questions/39733482/python-urllib3-crawling-webpages-failed-at-a-specific-webpage-only-from-google
39475652,Import error for urllib3,"<p>I'm using Python 2.7 and am trying pull data from an API using a python script and urllib3. I've installed urllib3 by copying the source code but from GitHub. But, I'm still getting the following error when running the script: </p>

<pre><code>ImportError: No module named urllib3
</code></pre>

<p>The script starts simply enough with: </p>

<pre><code>import urllib3 
http = urllib3.PoolManager() 
</code></pre>

<p>I've checked the urllib3 file, and it includes the <a href=""https://stackoverflow.com/questions/23361432/import-of-urllib3-util-failing-in-python-2-7"">util file cited in other responses</a> </p>
",-1,1473787135,python;python-2.7;urllib3,True,3937,2,1473787627,https://stackoverflow.com/questions/39475652/import-error-for-urllib3
37889960,Moving app to docker from host increases HTTP lag by 5 seconds,"<p>I have an app that I noticed was performing badly when making outgoing HTTP requests (an extra lag of 5 seconds). Through a lot of trials / tests, I realized that moving the app outside of docker into the host machine eliminated the weird HTTP lag. </p>

<p>I'm using alpine linux for the docker image, and ubuntu is hosting the parent machine. </p>

<p>Docker Info: </p>

<p>Docker version 1.11.2, build b9f10c9</p>

<pre><code>      ""NetworkSettings"": {
        ""Bridge"": """",
        ""SandboxID"": ""3ab81b8a66a99c6e9b1a1f49c5410d8260db37eee96c9231c0d83c1b40f84fa5"",
        ""HairpinMode"": false,
        ""LinkLocalIPv6Address"": """",
        ""LinkLocalIPv6PrefixLen"": 0,
        ""Ports"": {
            ""8084/tcp"": null
        },
        ""SandboxKey"": ""/var/run/docker/netns/3ab81b8a66a9"",
        ""SecondaryIPAddresses"": null,
        ""SecondaryIPv6Addresses"": null,

        ""EndpointID"": ""464acfb299941bbd301051ea05451823a7e527161185570c00f8569ce2afde88"",
        ""Gateway"": ""172.17.0.1"",
        ""GlobalIPv6Address"": """",
        ""GlobalIPv6PrefixLen"": 0,
        ""IPAddress"": ""172.17.0.3"",
        ""IPPrefixLen"": 16,
        ""IPv6Gateway"": """",
        ""MacAddress"": ""02:42:ac:11:00:03"",
        ""Networks"": {
            ""bridge"": {
                ""IPAMConfig"": null,
                ""Links"": null,
                ""Aliases"": null,
                ""NetworkID"": ""32ebc75bc4c98106c6775905906723405c58bc3de914283234a8e1273cba7193"",
                ""EndpointID"": ""464acfb299941bbd301051ea05451823a7e527161185570c00f8569ce2afde88"",

                ""Gateway"": ""172.17.0.1"",
                ""IPAddress"": ""172.17.0.3"",
                ""IPPrefixLen"": 16,
                ""IPv6Gateway"": """",
                ""GlobalIPv6Address"": """",
                ""GlobalIPv6PrefixLen"": 0,
                ""MacAddress"": ""02:42:ac:11:00:03""
            }
        }
    }
</code></pre>

<p>I ran the docker container without anything fancy: </p>

<pre><code>docker run -d test
</code></pre>

<p>My code (python) times how long it takes for the whole request cycle: </p>

<pre><code>now = datetime.now()
response = http.request('POST', url, body=request_body, headers=headers)

print(
    ""\nTotal Time: "",
    (datetime.now() - now).total_seconds()
)
</code></pre>

<p>The constant times are 5-6 seconds, where outside the container, its less than once second. </p>

<p>I did a drill to check DNS and a tcpdump from inside the container. The doesn't seem to be anything interesting to report from there. DNS looks fine, and the packet captures report 0.2 sec requests times. </p>

<p>The only interesting thing I found from the packet capture was that it took 5 seconds to see the beginning of the packet from when the http.request was called. </p>

<p>I'm convinced there is a docker networking misconfiguration here somewhere. Please let me know if there any more information I need to add. We are using docker for 20 other services without this problem. </p>

<p>Thanks!</p>
",1,1466195589,python;networking;docker;urllib3;alpine-linux,True,811,1,1473437689,https://stackoverflow.com/questions/37889960/moving-app-to-docker-from-host-increases-http-lag-by-5-seconds
39309299,Python Elasticsearch urllib3 exception,"<p>I have to use python-elasticsearch library on a machine where I could only execute programs.  I am trying to use elasticsearch module by appending sys.path as mentioned below. I am facing below issue. It looks like the problem related to what is mentioned here 
<a href=""https://github.com/elastic/elasticsearch-py/issues/253"" rel=""nofollow"">https://github.com/elastic/elasticsearch-py/issues/253</a> . But how do I resolve this when I dont have sudo access or any sort of upgrade access.</p>

<p>**Note :**I don't have sudo access on this machine so I cannot have venv, pip etc.</p>

<pre><code>    import sys
    sys.path.append('/tmp/elasticpy/elasticsearch-2.3.0')
    from elasticsearch import Elasticsearch
    Traceback (most recent call last):
      File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
      File ""/tmp/elasticpy/elasticsearch-2.3.0/elasticsearch/__init__.py"", line 17, in &lt;module&gt;
        from .client import Elasticsearch
      File ""/tmp/elasticpy/elasticsearch-2.3.0/elasticsearch/client/__init__.py"", line 5, in &lt;module&gt;
        from ..transport import Transport
      File ""/tmp/elasticpy/elasticsearch-2.3.0/elasticsearch/transport.py"", line 4, in &lt;module&gt;
        from .connection import Urllib3HttpConnection
      File ""/tmp/elasticpy/elasticsearch-2.3.0/elasticsearch/connection/__init__.py"", line 3, in &lt;module&gt;
        from .http_urllib3 import Urllib3HttpConnection
      File ""/tmp/elasticpy/elasticsearch-2.3.0/elasticsearch/connection/http_urllib3.py"", line 2, in &lt;module&gt;
        import urllib3
    ImportError: No module named urllib3
</code></pre>
",0,1472921871,python;python-2.7;elasticsearch;urllib3,False,1185,1,1473170773,https://stackoverflow.com/questions/39309299/python-elasticsearch-urllib3-exception
39288168,How to set proxy while using urllib3.PoolManager in python,"<p>I am currently using connection pool provided by urllib3 in python like the following,</p>

<pre><code>pool = urllib3.PoolManager(maxsize = 10)
resp = pool.request('GET', 'http://example.com')
content = resp.read()
resp.release_conn()
</code></pre>

<p>However, I don't know how to set proxy while using this connection pool. I tried to set proxy in the 'request' like <code>pool.request('GET', 'http://example.com', proxies={'http': '123.123.123.123:8888'}</code> but it didn't work.</p>

<p>Can someone tell me how to set the proxy while using connection pool</p>

<p>Thanks~</p>
",1,1472806243,python;proxy;connection-pooling;urllib3,True,7186,1,1473042158,https://stackoverflow.com/questions/39288168/how-to-set-proxy-while-using-urllib3-poolmanager-in-python
39289489,HTTPS request using DES-CBC-SHA and latest version of Python?,"<p>I'm trying to set up <code>https</code> connection from <code>Python</code> on <code>Windows</code> using <code>Requests</code> to Cisco ASA firewall which does not have <code>AES/3DES</code> license installed. The goal is to download <code>pcap</code> capture file from URL pattern <code>https://10.0.0.1/capture/TEST_CAPTURE/pcap</code> but it does not really matter for the question further and we'll consider URL to be <code>https://10.0.0.1</code>. </p>

<p>With default <code>Requests</code> setup a connection attempt  fails for both <code>Python 2</code> and <code>Python 3</code>:</p>

<pre><code>requests.get('https://10.0.0.1', verify = False, auth = ('username','password'))
...
requests.exceptions.SSLError: [SSL: SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure (_ssl.c:600)
</code></pre>

<p>This could be due to lack of cipher overlap between the server and the client. <code>CURL</code> with <code>-v</code> option was used to determine what cipher is used by the server:</p>

<pre><code>CURL -u username -v -l -k https://10.0.0.1
...
SSLv3, TLS handshake, Finished (20):
SSL connection using DES-CBC-SHA
...
</code></pre>

<p>The server is using <code>DES-CBC-SHA</code> cipher and I can't change that. So this cipher was added to a list of default ciphers for <code>urllib3</code>:</p>

<pre><code>requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += ':DES-CBC-SHA'
</code></pre>

<p>After that for <code>Python 3</code> version <code>3.4.3</code> connection succeeded but for <code>Python 2</code> version <code>2.7.12</code> it still failed. And I do need to use <code>Python 2</code>. I tried to install security dependencies: </p>

<pre><code>pip install pyopenssl ndg-httpsclient pyasn1
</code></pre>

<p>And to add <code>DES-CBC-SHA</code> cipher to <code>pyopenssl</code> list of default ciphers:</p>

<pre><code>requests.packages.urllib3.contrib.pyopenssl.DEFAULT_SSL_CIPHER_LIST += ':DES-CBC-SHA'
</code></pre>

<p>But connecton from <code>Python 2.7.12</code> still failed with the same error:</p>

<pre><code>requests.exceptions.SSLError: (""bad handshake: Error([('SSL routines', 'SSL23_GET_SERVER_HELLO', 'sslv3 alert handshake failure')],)"",)
</code></pre>

<p>It was noticed that <code>Python 3.4.3</code> for which the connection succeeds uses an older version of <code>openssl</code> than <code>Python 2.7.12</code> for which the connection fails:</p>

<pre><code>Python 3.4.3
&gt;&gt;&gt; import ssl
&gt;&gt;&gt; ssl.OPENSSL_VERSION
'OpenSSL 1.0.1l 15 Jan 2015'

Python 2.7.12
&gt;&gt;&gt; import ssl
&gt;&gt;&gt; ssl.OPENSSL_VERSION
'OpenSSL 1.0.2h  3 May 2016'
</code></pre>

<p>And after replacing <code>Python 2.7.12</code> with <code>Python 2.7.9</code> which has <code>'OpenSSL 1.0.1j 15 Oct 2014'</code>, the connection succeeded.</p>

<p>Is it possible to establish <code>https</code> connection using <code>DES-CBC-SHA</code> cipher with the latest version of <code>Python</code>?</p>
",4,1472809934,python;python-2.7;python-requests;pyopenssl;urllib3,False,624,0,1472869904,https://stackoverflow.com/questions/39289489/https-request-using-des-cbc-sha-and-latest-version-of-python
38971571,How to get image from dynamic url using urllib2?,"<p>I have generated a url from product code like,</p>

<pre><code>code: 2555-525
url : www.example.com/2555-525.png
</code></pre>

<p>But when fetching a url, it might be a different name format on server,like </p>

<pre><code>www.example.com/2555-525.png
www.example.com/2555-525_TEXT.png
www.example.com/2555-525_TEXT_TEXT.png
</code></pre>

<p>Sample code,</p>

<pre><code>urllib2.urlopen(URL).read()
</code></pre>

<p>could we pass the url like <code>www.example.com/2555-525*.png</code> ?</p>
",0,1471340690,python;urllib2;urllib3,True,164,1,1471347757,https://stackoverflow.com/questions/38971571/how-to-get-image-from-dynamic-url-using-urllib2
38938381,urllib3 segfault (core dumped),"<p>I'm getting a segfault (""Illegal operation (core dumped)"") for a python program that I've run every week without fault for ages. I'm also running Ubuntu on Nitrous. I recall dealing with these yonks ago when coding in C, and I haven't had to deal with them very much recently.</p>

<p>Importing the library urllib3 seems to be causing the problem. Does anyone know a fix?</p>

<p>Also, can someone advise or link to the best workflow for diagnosing these problems in future?</p>

<p>Thanks!</p>
",1,1471137602,python;linux;segmentation-fault;urllib3,False,270,1,1471153989,https://stackoverflow.com/questions/38938381/urllib3-segfault-core-dumped
38840970,Can&#39;t see infinite loop,"<p>I am trying to write a webcrawler but I am stuck because I cant see infinite loop somewhere in my code. </p>

<pre><code>class Crawler(object):
    def __init__(self, url, query, dir = os.path.dirname(__file__)):
        self.start_url = url
        self.start_parsed = urllib3.util.parse_url(url)
        self.query = re.compile(query, re.IGNORECASE)
        self.dir = dir
        self.__horizon = set()
        self.log = []

        self.__horizon.add(url)
        self.log.append(url)
        print(""initializing crawler...."")
        print(locals())

    def start(self, depth= 5, url = '/'):
        print(url, depth)
        self.log.append(url)
        if depth &gt; 0:
            pool = urllib3.PoolManager()
            data = pool.request(""GET"", self.start_url if url == '/' else url).data.decode('utf-8')

            valid_list = []
            self.add_horizon(parser_soup.get_links(data), valid_list)

            if re.search(self.query, parser_soup.get_text(data)):
                self.output(data)

            for u in valid_list:
                self.start(depth = (depth-1), url = u)

    def output(self, data):
        with open(os.path.join(self.dir, get_top_domain(self.start_parsed.host) + '.' + str(time.time()) + '.html'), 'w+') as f:
            f.write(data)

    def add_horizon(self, url_list, valid_list = []):
        for url in url_list:
            if get_top_domain(url) == get_top_domain(self.start_parsed.host)  \
                    and (not str(url) in self.log or not str(url) in self.__horizon):
                valid_list.append(str(url))

        self.__horizon.update(valid_list)
</code></pre>

<p>It runs forever. How should I ensure that I eliminate duplicate links?</p>
",2,1470708476,python;beautifulsoup;web-crawler;urllib3,True,116,2,1471130257,https://stackoverflow.com/questions/38840970/cant-see-infinite-loop
38938381,urllib3 segfault (core dumped),"<p>I'm getting a segfault (""Illegal operation (core dumped)"") for a python program that I've run every week without fault for ages. I'm also running Ubuntu on Nitrous. I recall dealing with these yonks ago when coding in C, and I haven't had to deal with them very much recently.</p>

<p>Importing the library urllib3 seems to be causing the problem. Does anyone know a fix?</p>

<p>Also, can someone advise or link to the best workflow for diagnosing these problems in future?</p>

<p>Thanks!</p>
",1,1471137602,python;linux;segmentation-fault;urllib3,False,270,1,1471153989,https://stackoverflow.com/questions/38938381/urllib3-segfault-core-dumped
38840970,Can&#39;t see infinite loop,"<p>I am trying to write a webcrawler but I am stuck because I cant see infinite loop somewhere in my code. </p>

<pre><code>class Crawler(object):
    def __init__(self, url, query, dir = os.path.dirname(__file__)):
        self.start_url = url
        self.start_parsed = urllib3.util.parse_url(url)
        self.query = re.compile(query, re.IGNORECASE)
        self.dir = dir
        self.__horizon = set()
        self.log = []

        self.__horizon.add(url)
        self.log.append(url)
        print(""initializing crawler...."")
        print(locals())

    def start(self, depth= 5, url = '/'):
        print(url, depth)
        self.log.append(url)
        if depth &gt; 0:
            pool = urllib3.PoolManager()
            data = pool.request(""GET"", self.start_url if url == '/' else url).data.decode('utf-8')

            valid_list = []
            self.add_horizon(parser_soup.get_links(data), valid_list)

            if re.search(self.query, parser_soup.get_text(data)):
                self.output(data)

            for u in valid_list:
                self.start(depth = (depth-1), url = u)

    def output(self, data):
        with open(os.path.join(self.dir, get_top_domain(self.start_parsed.host) + '.' + str(time.time()) + '.html'), 'w+') as f:
            f.write(data)

    def add_horizon(self, url_list, valid_list = []):
        for url in url_list:
            if get_top_domain(url) == get_top_domain(self.start_parsed.host)  \
                    and (not str(url) in self.log or not str(url) in self.__horizon):
                valid_list.append(str(url))

        self.__horizon.update(valid_list)
</code></pre>

<p>It runs forever. How should I ensure that I eliminate duplicate links?</p>
",2,1470708476,python;beautifulsoup;web-crawler;urllib3,True,116,2,1471130257,https://stackoverflow.com/questions/38840970/cant-see-infinite-loop
38901929,Python - urllib3 recieve 403 &#39;Forbidden&#39; while crawling websites,"<p>I am using <code>Python3</code> and <code>urllib3</code> for crawling and downloading websites. I crawled a list of 4000 different domains and at about 5 of them i got back <code>HttpErrorCode</code> -  <code>403 - 'Forbidden'</code>.</p>

<p>On my browser the website does exist and respond correctly. Probably these websites are suspecting me as a crawler and forbid me from getting the data.</p>

<p>This is my code:</p>

<pre><code>from urllib3 import PoolManager, util, Retry
import certifi as certifi
from urllib3.exceptions import MaxRetryError

manager = PoolManager(cert_reqs='CERT_REQUIRED',
                               ca_certs=certifi.where(),
                               num_pools=15,
                               maxsize=6,
                               timeout=40.0,
                               retries=Retry(connect=2, read=2, redirect=10))
url_to_download = ""https://www.uvision.co.il/""
headers = util.make_headers(accept_encoding='gzip, deflate',
                                keep_alive=True,
                                user_agent=""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0"")
headers['Accept-Language'] = ""en-US,en;q=0.5""
headers['Connection'] = 'keep-alive'
headers['Accept'] = ""text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8""
try:
    response = manager.request('GET',
                               url_to_download,
                               preload_content=False,
                               headers=headers)
except MaxRetryError as ex:
    raise FailedToDownload()
</code></pre>

<p>Example of websites that have rejected me:
<a href=""https://www.uvision.co.il/"" rel=""nofollow"">https://www.uvision.co.il/</a> and <a href=""http://www.medyummesut.net/"" rel=""nofollow"">http://www.medyummesut.net/</a>.</p>

<p>Another website that don't work and Throws <code>MaxRetryError</code> is:</p>

<p><a href=""http://www.nytimes.com/2015/10/28/world/asia/south-china-sea-uss-lassen-spratly-islands.html?hp&amp;action=click&amp;pgtype=Homepage&amp;module=first-column-region&amp;region=top-news&amp;WT.nav=top-news&amp;_r=1"" rel=""nofollow"">http://www.nytimes.com/2015/10/28/world/asia/south-china-sea-uss-lassen-spratly-islands.html?hp&amp;action=click&amp;pgtype=Homepage&amp;module=first-column-region&amp;region=top-news&amp;WT.nav=top-news&amp;_r=1</a></p>

<p>I've also tried to use the exact same headers that Firefox use and it didn't work either. Am i doing here something wrong?</p>
",1,1470933678,python;python-3.x;urllib;urllib3,True,1057,1,1470949346,https://stackoverflow.com/questions/38901929/python-urllib3-recieve-403-forbidden-while-crawling-websites
38498248,Slack connection error. Failed to establish a new connection: [Errno -2] Name or service not known,"<p>I have facing connection issue in slack api. I ran this code inside the vagrant Ubuntu 14.04.4 virtual machine. </p>

<pre><code>sc = SlackClient(token)
print sc.api_call(""api.test"")
</code></pre>

<p>I tried above example code from <a href=""http://python-slackclient.readthedocs.io/en/latest/?badge=latest"" rel=""nofollow"">slackclient</a> library and getting following error. </p>

<blockquote>
  <p>requests.exceptions.ConnectionError:
  HTTPSConnectionPool(host='slack.com', port=443): Max retries exceeded
  with url: /api/api.test (Caused by
  NewConnectionError(': Failed to establish a new connection:
  [Errno -2] Name or service not known',))</p>
</blockquote>

<p>How do I resolve this error?</p>
",0,1469087699,python;ubuntu-14.04;python-requests;slack-api;urllib3,False,2259,0,1469088270,https://stackoverflow.com/questions/38498248/slack-connection-error-failed-to-establish-a-new-connection-errno-2-name-or
38410661,Python3 urllib3 crawler - can&#39;t limit max connections to aa single domain,"<p>I am using python3 urllib3 in order to build a crawler to download multiple urls.</p>

<p>On my main activity i create 20 threads of that using <strong>the same (one) instance</strong> of my <code>Downloader</code> class which uses one instance of <code>PoolManager</code>:</p>

<pre><code>def __init__(self):
    self.manager = PoolManager(num_pools=20)
</code></pre>

<p>I've tried submitting the same url over and over again and i see at the log that it creates a lot of connections to the same domain. I've tried to limit number of pools (<code>num_pools=1</code>) and it still creating multiple connections to the same url. On the documentation i understood that the <code>PoolManager</code> creates a new connection if the other connections to the same domain are being used.</p>

<p>I want to limit the number of connection to a single domain. Using up to 2 different connections is what a normal browser use so it is safe. How can i do that?</p>
",0,1468668055,python;urllib;urllib3,True,678,1,1468678229,https://stackoverflow.com/questions/38410661/python3-urllib3-crawler-cant-limit-max-connections-to-aa-single-domain
37831713,TypeError when calling requests_oauthlib multiple times (error actually raised within urllib3),"<p>I'm writing a script to pull some data from the Twitter API. It's use of OAuth 1.1 means I'm using the <code>requests_oauthlib</code> helper library on top of <code>requests</code> to authenticate the session.</p>

<p>The first call to the API works, but then subsequent calls give a <code>TypeError</code> as follows:</p>

<pre><code>/Users/phil/code/Virtualenv/req_test/lib/python2.7/site-packages/requests/packages/urllib3/connection.pyc in __init__(self, *args, **kw)
124
125         # Superclass also sets self.source_address in Python 2.7+.
--&gt; 126         _HTTPConnection.__init__(self, *args, **kw)
127
128     def _new_conn(self):

TypeError: unbound method __init__() must be called with HTTPConnection instance as first argument (got VerifiedHTTPSConnection instance instead)
</code></pre>

<p>Looks like there's something persisting in the session as it's always on repeated use that the error comes. I've tried a clean virtualenv with latest versions installed via pip and no difference.</p>

<p>I'm using the context manager approach so thought that the session would be destroyed after each call, preventing this from happening:</p>

<pre><code>with ro.OAuth1Session(**self._auth) as s:
    response = s.get(url)
</code></pre>

<p>Any fix or pointers to understand what's causing the problem would be appreciated.</p>

<p><strong>Edit:</strong> I've tried a different approach, using the alternative way of building a session as described on the <code>requests</code> docs (<a href=""http://docs.python-requests.org/en/master/user/authentication/"" rel=""nofollow"">http://docs.python-requests.org/en/master/user/authentication/</a>) but same error is raised.</p>

<p><strong>Edit:</strong> Full stack in case it's useful:</p>

<p>/Users/phil/code/Virtualenv/req_test/lib/python2.7/site-packages/requests/sessions.pyc in get(self, url, **kwargs)
        485
        486         kwargs.setdefault('allow_redirects', True)
    --> 487         return self.request('GET', url, **kwargs)
        488
        489     def options(self, url, **kwargs):</p>

<pre><code>/Users/phil/code/Virtualenv/req_test/lib/python2.7/site-packages/requests/sessions.pyc in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)
    473         }
    474         send_kwargs.update(settings)
--&gt; 475         resp = self.send(prep, **send_kwargs)
    476
    477         return resp

/Users/phil/code/Virtualenv/req_test/lib/python2.7/site-packages/requests/sessions.pyc in send(self, request, **kwargs)
    583
    584         # Send the request
--&gt; 585         r = adapter.send(request, **kwargs)
    586
    587         # Total elapsed time of the request (approximately)

/Users/phil/code/Virtualenv/req_test/lib/python2.7/site-packages/requests/adapters.pyc in send(self, request, stream, timeout, verify, cert, proxies)
    401                     decode_content=False,
    402                     retries=self.max_retries,
--&gt; 403                     timeout=timeout
    404                 )
    405

/Users/phil/code/Virtualenv/req_test/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.pyc in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, **response_kw)
    564             # Request a connection from the queue.
    565             timeout_obj = self._get_timeout(timeout)
--&gt; 566             conn = self._get_conn(timeout=pool_timeout)
    567
    568             conn.timeout = timeout_obj.connect_timeout

/Users/phil/code/Virtualenv/req_test/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.pyc in _get_conn(self, timeout)
    254                 conn = None
    255
--&gt; 256         return conn or self._new_conn()
    257
    258     def _put_conn(self, conn):

/Users/phil/code/Virtualenv/req_test/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.pyc in _new_conn(self)
    800         conn = self.ConnectionCls(host=actual_host, port=actual_port,
    801                                   timeout=self.timeout.connect_timeout,
--&gt; 802                                   strict=self.strict, **self.conn_kw)
    803
    804         return self._prepare_conn(conn)

/Users/phil/code/Virtualenv/req_test/lib/python2.7/site-packages/requests/packages/urllib3/connection.pyc in __init__(self, host, port, key_file, cert_file, strict, timeout, **kw)
    208
    209         HTTPConnection.__init__(self, host, port, strict=strict,
--&gt; 210                                 timeout=timeout, **kw)
    211
    212         self.key_file = key_file

/Users/phil/code/Virtualenv/req_test/lib/python2.7/site-packages/requests/packages/urllib3/connection.pyc in __init__(self, *args, **kw)
    124
    125         # Superclass also sets self.source_address in Python 2.7+.
--&gt; 126         _HTTPConnection.__init__(self, *args, **kw)
    127
    128     def _new_conn(self):

TypeError: unbound method __init__() must be called with HTTPConnection instance as first argument (got VerifiedHTTPSConnection instance instead)
</code></pre>
",0,1465983671,python;python-requests;urllib3,True,176,2,1466262104,https://stackoverflow.com/questions/37831713/typeerror-when-calling-requests-oauthlib-multiple-times-error-actually-raised-w
37567054,Google Appengine: Requests Alternative,"<p>I have a non-GAE application/request-handler that uses the Python requests module in a to post an uploaded imaged via a POST request, as binary:</p>

<pre><code>headers = {""MyAuth"" : ""xyz""}
r = requests.post(base_uri, data=open('0.jpg')), headers=headers)
</code></pre>

<p>The user uploads an image, the uploaded image is saved locally, opened for reading, then sent to a remote classifier pipeline via post request - this returns some JSON regarding the image features, which can then be returned to the user.</p>

<p>I need to implement this behaviour in a GAE app, but know that GAE has no traditional file system,  so I will have to use <code>StringIO</code>:</p>

<pre><code>data = ... #some jpg =&gt; str
headers = {""MyAuth"" : ""xyz""}
r = requests.post(base_uri, data=StringIO.StringIO(data), headers=headers)
</code></pre>

<p>How could I completely replace the requests module in this example in a GAE friendly way?</p>

<p>Many thanks. </p>
",0,1464779656,python;google-app-engine;python-requests;urllib3,True,409,2,1464855431,https://stackoverflow.com/questions/37567054/google-appengine-requests-alternative
36016915,Suppress &#39;SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed&#39; errors in python,"<p>I am having trouble using pyVmomi with python 2.7.5. I get SSL certificate errors when trying to run the sample scripts from the SDK. I tried all the solutions mentioned on <a href=""https://stackoverflow.com/questions/27981545/suppress-insecurerequestwarning-unverified-https-request-is-being-made-in-pytho/"">this</a> post but none of them worked for me.</p>

<p>Below is the complete console output.</p>

<pre><code>/usr/lib/python2.7/site-packages/requests/packages/urllib3/util/ssl_.py:315: SNIMissingWarning: An HTTPS request has been made, but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate, which can cause validation failures. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#snimissingwarning. SNIMissingWarning /usr/lib/python2.7/site-packages/requests/packages/urllib3/util/ssl_.py:120: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning. InsecurePlatformWarning

Traceback (most recent call last):
  File ""hello_world_vcenter.py"", line 105, in &lt;module&gt;
    main()
  File ""hello_world_vcenter.py"", line 80, in main
    port=int(args.port))
  File ""/usr/lib/python2.7/site-packages/pyVim/connect.py"", line 663, in SmartConnect
    sslContext)
  File ""/usr/lib/python2.7/site-packages/pyVim/connect.py"", line 552, in __FindSupportedVersion
    sslContext)
  File ""/usr/lib/python2.7/site-packages/pyVim/connect.py"", line 472, in __GetServiceVersionDescription
    tree = __GetElementTreeFromUrl(url, sslContext)
  File ""/usr/lib/python2.7/site-packages/pyVim/connect.py"", line 440, in __GetElementTreeFromUrl
    sock = requests.get(url)
  File ""/usr/lib/python2.7/site-packages/requests/api.py"", line 67, in get
    return request('get', url, params=params, **kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/api.py"", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/sessions.py"", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/sessions.py"", line 576, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/adapters.py"", line 447, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: [Errno 1] _ssl.c:504: error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed
</code></pre>
",1,1458059343,python;python-2.7;suppress-warnings;urllib3;pyvmomi,False,754,1,1463657484,https://stackoverflow.com/questions/36016915/suppress-ssl-routinesssl3-get-server-certificatecertificate-verify-failed-er
37239354,How to encode latin characters with urllib3 request_encode_url?,"<p>i have a function to download a website html code with the urllib3 library. I'm using the request_encode_url function to pass arguments by GET and it works fine if i do not use special latin characters like 'ñ'. If i use 'ñ', the url is not properly encoded.</p>

<p>For instance, if i pass an argument like ""El señor"" this function converts it to ""El+seÃ±or"" instead of ""El+se%F1or"".</p>

<pre><code>z='El señor'
fields={'sec':'search','value': z}
http = urllib3.PoolManager() 
r = http.request_encode_url('GET', 'http://www.myurl.com/search.php',fields)
</code></pre>

<p>The expected url must be like:</p>

<pre><code>http://www.myurl.com/search.php?sec=search&amp;value=El+se%F1or
</code></pre>

<p>but if i use special characters i obtain next url:</p>

<pre><code>http://www.myurl.com/search.php?sec=search&amp;value=El+seÃ±or
</code></pre>

<p>Somebody can say me how can i pass arguments with special characters to encode a correct url?</p>

<p>I'm using Python 3.4</p>
",1,1463321876,python;python-3.x;url-encoding;urllib3,False,704,1,1463425391,https://stackoverflow.com/questions/37239354/how-to-encode-latin-characters-with-urllib3-request-encode-url
37240706,AttributeError: &#39;module&#39; object has no attribute &#39;urlopen&#39; in urllib3,"<p>I have checked other posts, but the solutions did not seem to work. I keep getting the AttributeError: 'module' object has no attribute 'urlopen' error. Any ideas why this wouldn't work be greatly appreciated. </p>

<pre><code>from lxml import html
import requests
import urllib3

page = requests.get('http://www.sfbos.org/index.aspx?page=18701')
tree = html.fromstring(page.content)

#This will create a list of buyers:
proposal_doc_date = tree.xpath('//ul[@title=""Date List""]/li/a/text()')
pdf_url = tree.xpath('//ul[@title=""Date List""]/li/a/@href')

print 'Proposal Date ', proposal_doc_date
print 'Proposal PDF ', pdf_url

def download_pdf(url_list):
    for i in url_list:
        response = urllib3.urlopen(i)
        file = open(proposal_doc_date[i], 'wb')
        file.write(response.read())
        file.close()
        print(""Completed"")

download_pdf(pdf_url)
</code></pre>
",0,1463329293,python;urllib3,True,3378,2,1463351000,https://stackoverflow.com/questions/37240706/attributeerror-module-object-has-no-attribute-urlopen-in-urllib3
37198328,Using certifi in urllib3,"<p>I am new to python programming. Can anybody please tell if there will be any problem in fetching data from the web if I don't use certifi or any sort of certificate verification while using urllib3? I am getting warnings regarding the authenticity but data is fetched regardless of that. I was just wondering, if there is any possibility of any sort of error from any site. Also, does , not using certifi speed up data fetching?</p>
",0,1463091171,python;certificate;ssl-certificate;urllib3,True,611,1,1463350650,https://stackoverflow.com/questions/37198328/using-certifi-in-urllib3
37135880,Python 3 urllib Vs requests performance,"<p>I'm using python 3.5 and I'm checking the performance of urllib module Vs requests module.
I wrote two clients in python the first one is using the urllib module and the second one is using the request module.
they both generate a binary data, which I send to a server which is based on flask and from the flask server I also return a binary data to the client.
I found that time took to send the data from the client to the server took same time for both modules (urllib, requests) but the time it took to return data from the server to the client is more then twice faster in urllib compare to request.
I'm working on localhost.<br>
<strong>my question is why?<br>
what I'm doing wrong with request module which make it to be slower?</strong>  </p>

<p><strong>this is the server code :</strong></p>

<pre><code>from flask import Flask, request
app = Flask(__name__)
from timeit import default_timer as timer
import os

@app.route('/onStringSend', methods=['GET', 'POST'])
def onStringSend():
    return data

if __name__ == '__main__':
    data_size = int(1e7)
    data = os.urandom(data_size)    
    app.run(host=""0.0.0.0"", port=8080)
</code></pre>

<p><strong>this is the client code based on urllib :</strong></p>

<pre><code>import urllib.request as urllib2
import urllib.parse
from timeit import default_timer as timer
import os

data_size = int(1e7)
num_of_runs = 20
url = 'http://127.0.0.1:8080/onStringSend'

def send_binary_data():
    data = os.urandom(data_size)
    headers = {'User-Agent': 'Mozilla/5.0 (compatible; Chrome/22.0.1229.94;  Windows NT)', 'Content-Length': '%d' % len(data), 'Content-Type':  'application/octet-stream'}
    req = urllib2.Request(url, data, headers)
    round_trip_time_msec = [0] * num_of_runs
    for i in range(0,num_of_runs):
        t1 = timer()
        resp = urllib.request.urlopen(req)
        response_data = resp.read()
        t2 = timer()
        round_trip_time_msec[i] = (t2 - t1) * 1000

    t_max = max(round_trip_time_msec)
    t_min = min(round_trip_time_msec)
    t_average = sum(round_trip_time_msec)/len(round_trip_time_msec)

    print('max round trip time [msec]: ', t_max)
    print('min round trip time [msec]: ', t_min)
    print('average round trip time [msec]: ', t_average)


send_binary_data()
</code></pre>

<p><strong>this is the client code based on requests :</strong></p>

<pre><code>import requests
import os
from timeit import default_timer as timer


url = 'http://127.0.0.1:8080/onStringSend'
data_size = int(1e7)
num_of_runs = 20


def send_binary_data():
    data = os.urandom(data_size)
    s = requests.Session()
    s.headers['User-Agent'] = 'Mozilla/5.0 (compatible; Chrome/22.0.1229.94;Windows NT)'
    s.headers['Content-Type'] = 'application/octet-stream'
    s.headers['Content-Length'] = '%d' % len(data)

    round_trip_time_msec = [0] * num_of_runs
    for i in range(0,num_of_runs):
        t1 = timer()
        response_data = s.post(url=url, data=data, stream=False, verify=False)
        t2 = timer()
        round_trip_time_msec[i] = (t2 - t1) * 1000

    t_max = max(round_trip_time_msec)
    t_min = min(round_trip_time_msec)
    t_average = sum(round_trip_time_msec)/len(round_trip_time_msec)

    print('max round trip time [msec]: ', t_max)
    print('min round trip time [msec]: ', t_min)
    print('average round trip time [msec]: ', t_average)

send_binary_data()
</code></pre>

<p>thanks very much</p>
",10,1462876182,python;performance;python-requests;urllib2;urllib3,True,25069,1,1462995733,https://stackoverflow.com/questions/37135880/python-3-urllib-vs-requests-performance
31535604,Is it precise to use `requests` to check if a domain name is registered?,"<p>I noticed that requesting an invalid url <code>requests.get(invalid_url)</code> throws the following exceptions:</p>

<pre><code>Traceback (most recent call last):
  File ""/usr/lib/python3.4/socket.py"", line 530, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
socket.gaierror: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 607, in urlopen
    raise MaxRetryError(self, url, e)
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='sparkandshine.me', port=80): Max retries exceeded with url: / (Caused by &lt;class 'socket.gaierror'&gt;: [Errno -2] Name or service not known)

During handling of the above exception, another exception occurred:
  File ""/usr/lib/python3/dist-packages/requests/adapters.py"", line 378, in send
    raise ConnectionError(e)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='sparkandshine.me', port=80): Max retries exceeded with url: / (Caused by &lt;class 'socket.gaierror'&gt;: [Errno -2] Name or service not known)
</code></pre>

<p><strong>Is it precise to determine whether a domain name is registered or not by catching those exceptions?</strong> Here is the source code:</p>

<p></p>

<pre><code>#!/usr/bin/env python3
import http
import urllib3
import requests

url = 'http://example.com'
try :
    r = requests.get(url)
except (http.client.HTTPException, urllib3.exceptions.MaxRetryError, requests.exceptions.ConnectionError):
    print(url) #this domain name is not registered?
</code></pre>
",2,1437471904,python;python-requests;urllib3;http.client,True,1821,2,1460407332,https://stackoverflow.com/questions/31535604/is-it-precise-to-use-requests-to-check-if-a-domain-name-is-registered
36049084,python - Multiprocessing and PoolManager: urllib3 Error: Can&#39;t pickle &lt;function PoolManager,"<p>I was trying to create a small program that could use a couple of threads to run a process in a class/function.</p>

<p>This Object has an instance of <code>urllib3.PoolManager(..)</code> and it seems to be a problem when launching the thread. See error after code.
It something hanging when the line <code>self.manager = urllib3.PoolManager(num_pools = 10)</code> that is to create an instance of PoolManager that will be used later on.</p>

<p>What is the issue here and what am I missing?</p>

<p><strong>Code</strong>:</p>

<pre><code>import time
import random
from multiprocessing import Process, Queue, current_process, freeze_support
import urllib3

class myObj:

    def __init__(self):
    #problem in line below
#==== &gt; self.manager = urllib3.PoolManager(num_pools = 10)
        x = 5 #something

    def double_id(self, val):
        return 2*val

# Functions referenced by tasks
def callMyObj(obj):
    myObj = obj[0]
    rev_val = myObj.double_id(random.random())
    return rev_val
    #return obj.double_id

#/////////////////////////////////////////////////////////////

# Function run by worker processes
def worker(input, output):
    for func, args in iter(input.get, 'STOP'):
        result = func(args)
        output.put(result)


#/////////////////////////////////////////////////////////////
if __name__ == '__main__':
    freeze_support()

    NUMBER_OF_PROCESSES = 4
    TASKS1 = [(callMyObj, (myObj(), 'double_id')) for i in range(10)]

    # Create queues
    task_queue = Queue()
    done_queue = Queue()

    #-Submit tasks, Start worker processes, Get Unordered results
    for task in TASKS1: task_queue.put(task)
    for i in range(NUMBER_OF_PROCESSES):
        Process(target=worker, args=(task_queue, done_queue)).start()
    for i in range(len(TASKS1)):
        print('&gt;&gt; done', done_queue.get())
    for i in range(NUMBER_OF_PROCESSES): task_queue.put('STOP') #stop childs
</code></pre>

<p><strong>Error</strong>:</p>

<blockquote>
  <p>File ""/usr/lib/python3.4/multiprocessing/reduction.py"", line 50, in
  dumps
      cls(buf, protocol).dump(obj)
  _pickle.PicklingError: Can't pickle . at 0x7f9752ae3840>: attribute
  lookup  on urllib3.poolmanager failed</p>
</blockquote>
",0,1458171802,python;multithreading;multiprocessing;python-multiprocessing;urllib3,False,1047,0,1458173346,https://stackoverflow.com/questions/36049084/python-multiprocessing-and-poolmanager-urllib3-error-cant-pickle-function
35997884,How to fix requests (urllib3) work with app engine?,"<p>I use requests library (or urllib3), trying to get xml page from web site. And my code is working, but when i use it in app engine it shows an error</p>

<pre><code>    Traceback (most recent call last):
  File ""C:\Program Files (x86)\Google\google_appengine\lib\webapp2-2.5.2\webapp2.py"", line 1535, in __call__
    rv = self.handle_exception(request, response, e)
  File ""C:\Program Files (x86)\Google\google_appengine\lib\webapp2-2.5.2\webapp2.py"", line 1529, in __call__
    rv = self.router.dispatch(request, response)
  File ""C:\Program Files (x86)\Google\google_appengine\lib\webapp2-2.5.2\webapp2.py"", line 1278, in default_dispatcher
    return route.handler_adapter(request, response)
  File ""C:\Program Files (x86)\Google\google_appengine\lib\webapp2-2.5.2\webapp2.py"", line 1102, in __call__
    return handler.dispatch()
  File ""C:\Program Files (x86)\Google\google_appengine\lib\webapp2-2.5.2\webapp2.py"", line 572, in dispatch
    return self.handle_exception(e, self.app.debug)
  File ""C:\Program Files (x86)\Google\google_appengine\lib\webapp2-2.5.2\webapp2.py"", line 570, in dispatch
    return method(*args, **kwargs)
  File ""C:\Users\Kirill\Desktop\Schedule\Schedule\main.py"", line 51, in get
    class_list = get_class_list()
  File ""C:\Users\Kirill\Desktop\Schedule\Schedule\get_class_urllib3.py"", line 6, in get_class_list
    r = http.request('POST', 'http://sgo.volganet.ru/lacc.asp?Function=GetClassListForSchool&amp;SchoolID=1460')
  File ""C:\Users\Kirill\Desktop\Schedule\Schedule\urllib3\request.py"", line 73, in request
    **urlopen_kw)
  File ""C:\Users\Kirill\Desktop\Schedule\Schedule\urllib3\request.py"", line 151, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File ""C:\Users\Kirill\Desktop\Schedule\Schedule\urllib3\poolmanager.py"", line 165, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File ""C:\Users\Kirill\Desktop\Schedule\Schedule\urllib3\connectionpool.py"", line 558, in urlopen
    body=body, headers=headers)
  File ""C:\Users\Kirill\Desktop\Schedule\Schedule\urllib3\connectionpool.py"", line 389, in _make_request
    assert_header_parsing(httplib_response.msg)
  File ""C:\Users\Kirill\Desktop\Schedule\Schedule\urllib3\util\response.py"", line 49, in assert_header_parsing
    type(headers)))

TypeError: expected httplib.Message, got &lt;type 'instance'&gt;.
</code></pre>

<p>So can you help me to fix this problem or suggest any module I can use in app angine to create post request to differen site.</p>
",3,1457988876,python;google-app-engine;python-requests;urllib3,True,496,1,1458001686,https://stackoverflow.com/questions/35997884/how-to-fix-requests-urllib3-work-with-app-engine
35800131,Python Requests - ephemeral port exhaustion,"<p>Is there anything I can do to the below code (I thought sessions would solve this?) to prevent new TCP connections being created with each GET request? I am hitting around 1000 requests a second and after around 10,000 request run out of sockets:</p>

<pre><code>def ReqOsrm(url_input):
    ul, qid = url_input
    conn_pool = HTTPConnectionPool(host='127.0.0.1', port=5005, maxsize=1)
    try:
        response = conn_pool.request('GET', ul)
        json_geocode = json.loads(response.data.decode('utf-8'))
        status = int(json_geocode['status'])
        if status == 200:
            tot_time_s = json_geocode['route_summary']['total_time']
            tot_dist_m = json_geocode['route_summary']['total_distance']
            used_from, used_to = json_geocode['via_points']
            out = [qid, status, tot_time_s, tot_dist_m, used_from[0], used_from[1], used_to[0], used_to[1]]
            return out
        else:
            print(""Done but no route: %d %s"" % (qid, req_url))
            return [qid, 999, 0, 0, 0, 0, 0, 0]
    except Exception as err:
        print(""%s: %d %s"" % (err, qid, req_url))
        return [qid, 999, 0, 0, 0, 0, 0, 0]

# run:
pool = Pool(int(cpu_count()))
calc_routes = pool.map(ReqOsrm, url_routes)
pool.close()
pool.join()
</code></pre>

<blockquote>
  <p>HTTPConnectionPool(host='127.0.0.1', port=5005): Max retries exceeded
  with url:
  /viaroute?loc=44.779708,4.2609877&amp;loc=44.648439,4.2811959&amp;alt=false&amp;geometry=false
  (Caused by NewConnectionError(': Failed to establish a new connection:
  [WinError 10048] Only one usage of each socket address
  (protocol/network address/port) is normally permitted',))</p>
</blockquote>

<hr>

<p>Eric - thank you a lot for the response I think it's exactly what I need. However, I can't quite modify it correctly. The code correctly returns 10,000 responses for the first few cycles however then it seems to break and returns less than 10,000 which leads me to think I implemented the Queue incorrectly?</p>

<p><a href=""https://i.stack.imgur.com/o2Yi6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/o2Yi6.png"" alt=""enter image description here""></a></p>

<pre><code>ghost = 'localhost'
gport = 8989

def CreateUrls(routes, ghost, gport):
    return [
        [""http://{0}:{1}/route?point={2}%2C{3}&amp;point={4}%2C{5}&amp;vehicle=car&amp;calc_points=false&amp;instructions=false"".format(
            ghost, gport, alat, alon, blat, blon),
            qid] for qid, alat, alon, blat, blon in routes]


def LoadRouteCSV(csv_loc):
    if not os.path.isfile(csv_loc):
        raise Exception(""Could not find CSV with addresses at: %s"" % csv_loc)
    else:
        return pd.read_csv(csv_loc, sep=',', header=None, iterator=True, chunksize=1000 * 10)

class Worker(Process):
    def __init__(self, qin, qout, *args, **kwargs):
        super(Worker, self).__init__(*args, **kwargs)
        self._qin = qin
        self._qout = qout

    def run(self):
        # Create threadsafe connection pool
        conn_pool = HTTPConnectionPool(host=ghost, port=gport, maxsize=10)

        class Consumer(threading.Thread):
            def __init__(self, qin, qout):
                threading.Thread.__init__(self)
                self.__qin = qin
                self.__qout = qout

            def run(self):
                while True:
                    msg = self.__qin.get()
                    ul, qid = msg
                    try:
                        response = conn_pool.request('GET', ul)
                        s = float(response.status)
                        if s == 200:
                            json_geocode = json.loads(response.data.decode('utf-8'))
                            tot_time_s = json_geocode['paths'][0]['time']
                            tot_dist_m = json_geocode['paths'][0]['distance']
                            out = [qid, s, tot_time_s, tot_dist_m]
                        elif s == 400:
                            print(""Done but no route for row: "", qid)
                            out = [qid, 999, 0, 0]
                        else:
                            print(""Done but unknown error for: "", s)
                            out = [qid, 999, 0, 0]
                    except Exception as err:
                        print(err)
                        out = [qid, 999, 0, 0]
                    self.__qout.put(out)
                    self.__qin.task_done()

        num_threads = 10
        [Consumer(self._qin, self._qout).start() for _ in range(num_threads)]

if __name__ == '__main__':
    try:
        with open(os.path.join(directory_loc, 'gh_output.csv'), 'w') as outfile:
            wr = csv.writer(outfile, delimiter=',', lineterminator='\n')
            for x in LoadRouteCSV(csv_loc=os.path.join(directory_loc, 'gh_input.csv')):
                routes = x.values.tolist()
                url_routes = CreateUrls(routes, ghost, gport)
                del routes

                stime = time.time()

                qout = Queue()
                qin = JoinableQueue()
                [qin.put(url_q) for url_q in url_routes]
                [Worker(qin, qout).start() for _ in range(cpu_count())]
                # Block until all urls in qin are processed
                qin.join()
                calc_routes = []
                while not qout.empty():
                    calc_routes.append(qout.get())

                # Time diagnostics
                dur = time.time() - stime
                print(""Calculated %d distances in %.2f seconds: %.0f per second"" % (len(calc_routes),
                                                                                    dur,
                                                                                    len(calc_routes) / dur))
                del url_routes
                wr.writerows(calc_routes)
                done_count += len(calc_routes)
                # Continually update progress in terms of millions
                print(""Saved %d calculations"" % done_count)
</code></pre>
",1,1457106282,python;sockets;tcp;python-requests;urllib3,True,1884,2,1457320564,https://stackoverflow.com/questions/35800131/python-requests-ephemeral-port-exhaustion
35830785,Get complete source code from a website generated on clicking a button,"<p>The webpage for which I want the source code contains some products.</p>

<p>When I simply load it, it gives me only the source code of the first few products.</p>

<p>What I need is the complete source code after clicking the <kbd>Show 140-250 items</kbd> which is generated several times and get the code when there is no something like <kbd>Show x - y items</kbd> anymore. The <code>id</code> of the button being generated is <code>see-more-products</code>.</p>

<p>I have tried <code>urllib</code>, <code>requests</code> modules, which don't do the job as expected. </p>

<p>Link to that particular page is in comments.</p>

<p>Any help is appreciated. Thanks :)</p>
",-2,1457288992,python;python-3.x;beautifulsoup;python-requests;urllib3,True,254,1,1457299545,https://stackoverflow.com/questions/35830785/get-complete-source-code-from-a-website-generated-on-clicking-a-button
35651845,Python3 program execution failing with error: &#39;socket.gaierror: [Errno -2] Name or service not known&#39;,"<p>I am running a python3 program using python virtualenv.
I am behind proxy and have exported the http_proxy/https_proxy before executing the below program. still running into issues. Please suggest.</p>

<pre><code>(geo_env)root@xyz-ntp-154:~/geo_env# cat search1.py
#!/usr/bin/env python3

from pygeocoder import Geocoder

if __name__ == '__main__':
    address = '207 N. Defiance St, Archbold, OH'
    print(Geocoder.geocode(address)[0].coordinates)
</code></pre>

<p>Here is the error:</p>

<pre><code>(geo_env)root@xyz-ntp-154:~/geo_env# python3 search1.py
Traceback (most recent call last):
File ""/root/geo_env/lib/python3.4/site-    packages/requests/packages/urllib3/connection.py"", line 137, in _new_conn
(self.host, self.port), self.timeout, **extra_kw)
File ""/root/geo_env/lib/python3.4/site-  packages/requests/packages/urllib3/util/connection.py"", line 67, in create_connection
for res in socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM):
File ""/usr/lib/python3.4/socket.py"", line 533, in getaddrinfo
for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
socket.gaierror: [Errno -2] Name or service not known
</code></pre>
",1,1456489931,python;urllib3,False,372,0,1456492997,https://stackoverflow.com/questions/35651845/python3-program-execution-failing-with-error-socket-gaierror-errno-2-name
35361816,Python urllib3 too many redirects,"<p>I learn Python now and I want to create simple tool to opening few sites. I have next code:</p>

<pre class=""lang-python prettyprint-override""><code>#!/usr/bin/python
import urllib3, ssl, certifi
from urllib3 import Retry, Timeout

def openurl(url, method = ""get""):
    retries = Retry(connect=500, read=2, redirect=500)
    http = urllib3.PoolManager(
        cert_reqs = 'CERT_REQUIRED',
        ca_certs = certifi.where(),
        retries = retries
    )
    con = urllib3.connection_from_url(url)
    r = con.request(method, '/trades');

openurl(""http://www.steamgifts.com"")
</code></pre>

<p>But on this site script returns <code>Caused by ResponseError('too many redirects',)</code></p>

<p>I try to fix this by <code>Retry(connect=500, read=2, redirect=500)</code> but I don't see changes.</p>
",2,1455277531,python;python-3.x;urllib3,True,3002,1,1455281396,https://stackoverflow.com/questions/35361816/python-urllib3-too-many-redirects
35090103,Python 3 - Urllib3 read internet radio metadata,"<p>How can I read information about playing song using urllib3? Which headers should I use? </p>

<pre><code>import urllib3 
http = urllib3.PoolManager()
response = http.request(""GET"", ""http://pool.cdn.lagardere.cz/fm-evropa2-128"", headers={
        'User-Agent': 'User-Agent: VLC/2.0.5 LibVLC/2.0.5',
        'Icy-MetaData': '1',
        'Range': 'bytes=0-',
    })
print(response.data)
</code></pre>

<p>I tried this. But it stucks at sending request. Can anyone help me? Thanks for answers.</p>
",3,1454087137,python;audio-streaming;shoutcast;urllib3,True,1204,1,1454088458,https://stackoverflow.com/questions/35090103/python-3-urllib3-read-internet-radio-metadata
35044413,Error Installing Pandas: ImportError: No module named &#39;urllib3&#39;,"<p>I have Python 2.7 and anaconda on my mac though I think some of the files related to these programs may have been deleted.  I'm trying to download pandas but I keep getting a long error message that ends in ""ImportError: No module named 'urllib3'"".</p>

<p>When I run these:</p>

<pre><code>conda update conda
conda update anaconda
</code></pre>

<p>I get the urllib3 error. I've run:</p>

<pre><code>sudo pip install urllib3 --upgrade
</code></pre>

<p>which results in:</p>

<p>The directory '/Users/erinwolpert/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.  The directory '/Users/erinwolpert/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag. <strong>Requirement already up-to-date: urllib3 in /usr/local/lib/python2.7/site-packages</strong>""</p>

<p>I don't understand why, if urllib3 is up to date, anaconda keeps saying that I don't have urllib3.</p>
",2,1453916553,python;python-2.7;anaconda;conda;urllib3,False,821,0,1453916553,https://stackoverflow.com/questions/35044413/error-installing-pandas-importerror-no-module-named-urllib3
35034556,Wait until page is fully loaded and then reading its content with urllib2/3,"<p>I am opening a webpage with urllib2 and reading its content and then passing it's content to BeautifulSoup and then scraping..<br/>
<strong>But what I want to first load the page fully or for a specfic time set and then read its content</strong>.</p>

<p>I have tried method like <strong><em>time.sleep(sec)</em></strong> but these are not working I am getting the same content either i read it instent or wait(sleep) for 10/15sec.
and when I am entering script line by line inot python shell then I am getting different result.</p>

<p>I am using urllib2 and python2.7. <strong>Also I tried to find a solution but everyone suggesting to use another module. Is this not possible with Urllib2 or urllib3?
Or Do I have to use another module like requests?</strong><br>
please suggest</p>
",0,1453889923,python;beautifulsoup;python-requests;urllib2;urllib3,False,1341,0,1453890629,https://stackoverflow.com/questions/35034556/wait-until-page-is-fully-loaded-and-then-reading-its-content-with-urllib2-3
34612605,Code keeps producing an empty string,"<p>I do not understand why the following code keeps producing an empty string. I am trying to get the code to extract the contents of the website to a ""txt"" file, but it just keeps producing an empty string. Is there an error in the code?</p>

<pre><code>import urllib3
import certifi


# Function: Convert information within html document to a text file
# Append information to the file
def html_to_text(source_html, target_file):

    http = urllib3.PoolManager(
        cert_reqs='CERT_REQUIRED',      # Force certificate check.
        ca_certs=certifi.where(),       # Path to the Certifi Bundle
        headers={'connection': 'keep-alive', 'user-agent': 'Mozilla/5.0', 'accept-encoding': 'gzip, deflate'},
    )

    r = http.urlopen('GET', source_html)
    print(source_html)
    response = r.read().decode('utf-8')
    # TODO: Find the problem that keeps making the code produce an empty string
    print(response)
    temp_file = open(target_file, 'w+')
    temp_file.write(response)


source_address = ""https://sg.finance.yahoo.com/lookup/all?s=*&amp;t=A&amp;m=SG&amp;r=&amp;b=0""
target_location = ""C:\\Users\\Admin\\PycharmProjects\\TheLastPuff\\Source\\yahoo_ticker_symbols.txt""

html_to_text(source_address, target_location)
</code></pre>
",-1,1451999567,python;python-3.x;urllib3,False,86,1,1452001967,https://stackoverflow.com/questions/34612605/code-keeps-producing-an-empty-string
34602991,List comes back as empty when retrieveing data from website ; Python,"<p>I am trying to parse data from a website by inserting the data into a list, but the list comes back empty. </p>

<pre><code>url =(""http://www.releasechimps.org/resources/publication/whos-there-md-  anderson"")
           http = urllib3.PoolManager()
r = http.request('Get',url)
soup = BeautifulSoup(r.data,""html.parser"")
#print(r.data)
loop = re.findall(r'&lt;td&gt;(.*?)&lt;/td&gt;',str(r.data))
#print(str(loop))
newLoop = str(loop)
#print(newLoop)
for x in range(1229):
    if ""\\n\\t\\t\\t\\t"" in loop[x]:
        loop[x] = loop[x].replace(""\\n\\t\\t\\t\\t"","""")
        list0_v2.append(str(loop[x]))
        print(loop[x])
print(str(list0_v2))
</code></pre>
",0,1451958684,python;list;parsing;urllib3,False,69,1,1451966926,https://stackoverflow.com/questions/34602991/list-comes-back-as-empty-when-retrieveing-data-from-website-python
34402510,Fetching a page which needs user interaction,"<p>In Python, I am trying to fetch pages from a specific website.
In this website, there are some parts in which the information is not completely accessible in the HTML page, and needs a bit of user interaction. To be more clear, there are some reviews, but the long reviews are shortened, and to see to whole review user must click on 'More' hyperlink. Is there any way to handle these hyperlinks in Python and fetch the whole reviews for all those cases?</p>

<p>Here is a snapshot of the 'More' hyperlink:</p>

<pre><code>&lt;span class=""bla bla"" onclick=""ta.util.cookie.setPIDCookie(123); ta.call('ta.servlet.Reviews.expandReviews',event,this,'review_331979201', '1', 123);""&gt; More &lt;/span&gt;
</code></pre>
",1,1450724193,python;web-crawler;fetch;urllib3,True,48,2,1450727110,https://stackoverflow.com/questions/34402510/fetching-a-page-which-needs-user-interaction
34313084,How to completly reset requests?,"<p>I'm using requests to make many http requests, and some time, i get timeouts. When i restart the python program, it goes fine. I tried to replicate the ""restart the program"" with exception handling, but it doesn't works. When i run that : </p>

<pre><code>import requests
session=requests.session()
while 1:
  try:
    session.get('..url..')
  except requests.Timeout:
    session=requests.session()
</code></pre>

<p>it doesn't do the same thing as restarting the program : i get stucked whith timeout, whereas when i restart the program, i don't get timeouts any more. What can i do ?</p>
",1,1450271774,python;http;python-requests;urllib3,False,2754,1,1450273671,https://stackoverflow.com/questions/34313084/how-to-completly-reset-requests
34075329,What is the urllib3 documentation telling me to do?,"<p>I am trying to get a Kilo OpenStack cloud deployed and I was getting this error:</p>

<blockquote>
  <p>Execution of '/usr/bin/openstack token issue --format value' returned
  1:
  /usr/lib/python2.7/site-packages/requests/packages/urllib3/util/ssl_.py:90:
  InsecurePlatformWarning: A true SSLContext object is not available.
  This prevents urllib3 from configuring SSL appropriately and may cause
  certain SSL connections to fail. For more information, see</p>
</blockquote>

<p>So I go to:
<a href=""https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning"" rel=""nofollow"">https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning</a></p>

<p>I find out that I do not have access to a python version better than 2.7.5 so I look at:</p>

<p><a href=""https://urllib3.readthedocs.org/en/latest/security.html#pyopenssl"" rel=""nofollow"">https://urllib3.readthedocs.org/en/latest/security.html#pyopenssl</a></p>

<p>So do what they recommend I do ...</p>

<pre><code># pip install pyopenssl ndg-httpsclient pyasn1
/usr/lib/python2.7/site-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:90: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.
  InsecurePlatformWarning
You are using pip version 7.1.0, however version 7.1.2 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
Requirement already satisfied (use --upgrade to upgrade): pyopenssl in /usr/lib64/python2.7/site-packages
Collecting ndg-httpsclient
/usr/lib/python2.7/site-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:90: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.
  InsecurePlatformWarning
  Downloading ndg_httpsclient-0.4.0.tar.gz
Requirement already satisfied (use --upgrade to upgrade): pyasn1 in /usr/lib/python2.7/site-packages
Installing collected packages: ndg-httpsclient
  Running setup.py install for ndg-httpsclient
Successfully installed ndg-httpsclient-0.4.0
</code></pre>

<p>The docs also say this:</p>

<blockquote>
  <p>Once the packages are installed, you can tell urllib3 to switch the
  ssl backend to PyOpenSSL with inject_into_urllib3():</p>
  
  <p>import urllib3.contrib.pyopenssl
  urllib3.contrib.pyopenssl.inject_into_urllib3() Now you can continue
  using urllib3 as you normally would.</p>
</blockquote>

<p>I do not understand what that means?  Is there some python source code I need to go patch?  </p>

<p>Update:</p>

<p>I did as suggested by Josep and this is what is happening:</p>

<pre><code># python
Python 2.7.5 (default, Jun 24 2015, 00:41:19) 
[GCC 4.8.3 20140911 (Red Hat 4.8.3-9)] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import urllib3
&gt;&gt;&gt; import pyopenssl
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ImportError: No module named pyopenssl
</code></pre>

<p>Yet I have pyopenssl installed ...</p>

<pre><code># pip install pyopenssl
Requirement already satisfied (use --upgrade to upgrade): pyopenssl in /usr/lib64/python2.7/site-packages
</code></pre>

<p>UPDATE:</p>

<p>Here's what happens if I do this ...</p>

<pre><code># python
Python 2.7.5 (default, Jun 24 2015, 00:41:19) 
[GCC 4.8.3 20140911 (Red Hat 4.8.3-9)] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import OpenSSL
&gt;&gt;&gt; import urllib3
&gt;&gt;&gt; import urllib3.contrib.pyopenssl as pyopenssl
&gt;&gt;&gt; pyopenssl.inject_into_urllib3()
&gt;&gt;&gt; 
</code></pre>
",-1,1449173711,python;ssl;openstack;urllib3;keystone,True,1578,1,1449254437,https://stackoverflow.com/questions/34075329/what-is-the-urllib3-documentation-telling-me-to-do
33785539,requests.get causing 104 error under Linux but fine in Windows,"<p>I have a python script that logs in to a website and scrapes a few pages.</p>

<p>On my Windows machine which I wrote the script on this works fine but on my Linux machine which I was planning to run it on I'm getting <code>requests.exceptions.ConnectionError: ('Connection aborted.', error(104, 'Connection reset by peer'))</code></p>

<p>Linux machine works fine with http requests but it's falling over when I attempt to open the log in page through https.</p>

<p>I followed the example here <a href=""https://stackoverflow.com/questions/10588644/how-can-i-see-the-entire-http-request-thats-being-sent-by-my-python-application"">How can I see the entire HTTP request that&#39;s being sent by my Python application?</a> and found that although I get debugging of the entire process under Windows the Linux machine only reports the first line <code>INFO:requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): www.example.com</code> and then I get the error.</p>

<p>I've stripped everything down and I'm still getting the error on Linux with a simple:</p>

<pre><code>import requests
requests.get('https://www.example.com/Login')
</code></pre>

<p>Both machines are running the same version of python running on Windows 8 and CentOS</p>

<p>Thanks in advance for any help</p>
",0,1447864720,python;https;python-requests;http-error;urllib3,False,466,0,1447864720,https://stackoverflow.com/questions/33785539/requests-get-causing-104-error-under-linux-but-fine-in-windows
33755444,urllib call seems to block in connectionpool in a very low frequency,"<p>I use 3rd party API which uses urllib (not sure which version), in some time, the call will be blocked and the last logging from service is ""Resetting dropped connection..."", which could be found from source code of connectionpool.py in urllib3:</p>

<p>connnectionpool.py</p>

<pre><code>    def _get_conn(self, timeout=None):
    """"""
    Get a connection. Will return a pooled connection if one is available.

    If no connections are available and :prop:`.block` is ``False``, then a
    fresh connection is returned.

    :param timeout:
        Seconds to wait before giving up and raising
        :class:`urllib3.exceptions.EmptyPoolError` if the pool is empty and
        :prop:`.block` is ``True``.
    """"""
    conn = None
    try:
        conn = self.pool.get(block=self.block, timeout=timeout)

    except AttributeError:  # self.pool is None
        raise ClosedPoolError(self, ""Pool is closed."")

    except Empty:
        if self.block:
            raise EmptyPoolError(self,
                                 ""Pool reached maximum size and no more ""
                                 ""connections are allowed."")
        pass  # Oh well, we'll create a new connection then

    # If this is a persistent connection, check if it got disconnected
    if conn and is_connection_dropped(conn):
        log.info(""Resetting dropped connection: %s"" % self.host)
        conn.close()
        if getattr(conn, 'auto_open', 1) == 0:
            # This is a proxied connection that has been mutated by
            # httplib._tunnel() and cannot be reused (since it would
            # attempt to bypass the proxy)
            conn = None

    return conn or self._new_conn()
</code></pre>

<p>Then I check conn.close, I guess it may be blocked in close function:</p>

<pre><code>    def close(self):
    """"""
    Close all pooled connections and disable the pool.
    """"""
    # Disable access to the pool
    old_pool, self.pool = self.pool, None

    try:
        while True:
            conn = old_pool.get(block=False)
            if conn:
                conn.close()

    except Empty:
        pass  # Done.
</code></pre>

<p>So it seems to mean old_pool doesn't get anything but block here. As old_pool is LifoQueue, and block=False means return if no data get, so it seems no problem here.</p>

<p>However, I met block in some time (even though in a very low frequency), I wonder any possible reason for that cause?</p>

<p>Thanks for your help</p>

<p>===============================</p>

<p>Actually I use <a href=""https://github.com/facebook/facebook-python-ads-sdk"" rel=""nofollow"">Facebook Ads SDK</a> to make Facebook API call, which gives the last log message before blocking happened: </p>

<pre><code>""logs/aam.log.bk.2015111802:[connectionpool.py:238 - _get_conn Wed, 18 Nov 2015 02:11:21;INFO] Resetting dropped connection: graph.facebook.com""
</code></pre>

<p>That's why I guess it's related with conn.close in urllib because no other block seems to exist both in Facebook Ads SDK and urllib3 (And another strange thing is that Facebook SDK seems to use requests other than urllib directly, so I don't know why it's related with urllib, please teach me if you know, I just ask one question to FB support team)</p>

<p>From Facebook SDK source code, it should be related with the following code in facebookads/api.py:</p>

<pre><code>    # Get request response and encapsulate it in a FacebookResponse
    if method in ('GET', 'DELETE'):
        response = self._session.requests.request(
            method,
            path,
            params=params,
            headers=headers,
            files=files,
        )
    else:
        response = self._session.requests.request(
            method,
            path,
            data=params,
            headers=headers,
            files=files,
        )
    fb_response = FacebookResponse(
        body=response.text,
        headers=response.headers,
        http_status=response.status_code,
        call={
            'method': method,
            'path': path,
            'params': params,
            'headers': headers,
            'files': files,
        },
    )
</code></pre>

<p>As the origin question described, it happens in a very low frequency, but it happens in different APIs which all make the above call in lower level.</p>

<p>Please tell me if other information needed, thanks for your help~</p>
",0,1447758661,python;urllib3,False,762,0,1447830506,https://stackoverflow.com/questions/33755444/urllib-call-seems-to-block-in-connectionpool-in-a-very-low-frequency
33682746,python requests/urllib3 connection pooling not catching HTTP errors,"<p>python requests (urllib3) with connection pooling is not catching http errors. Is this a bug? Or am I doing something wrong?</p>

<pre><code>#!/usr/bin/env python

import contextlib
import requests
import sys

connection_pool_size = 2
adapter = requests.adapters.HTTPAdapter(pool_connections=connection_pool_size,
                                        pool_maxsize=connection_pool_size)
r_session = requests.Session()
r_session.mount('http', adapter)

try:
    with contextlib.closing(r_session.get(sys.argv[1], timeout=30, allow_redirects=True)) as r:
        print 'success %r' % r
except requests.exceptions.HTTPError as e:
    print 'HTTPError %r' % e
except Exception as e:
    print 'Exception %r' % e
</code></pre>

<p>output:</p>

<pre><code>$ ./test.py https://github.com
success &lt;Response [200]&gt;
$ ./test.py https://github.com/sithlordyoyoma
success &lt;Response [404]&gt;
</code></pre>

<p>I was expecting HTTPError . Am I doing something wrong?</p>

<p>Closing with contextlib I got from this thread <a href=""https://stackoverflow.com/questions/1522636/should-i-call-close-after-urllib-urlopen/1522709#1522709"">should I call close() after urllib.urlopen()?</a>. As suggested by Alex Martelli.</p>

<p>actually running requests without connection also showing this behaviour</p>

<pre><code>#!/usr/bin/env python

import contextlib
import requests
import sys


try:
    with contextlib.closing(requests.get(sys.argv[1], timeout=30, allow_redirects=True)) as r:
        print 'success %r' % r
except requests.exceptions.HTTPError as e:
    print 'HTTPError %r' % e
except Exception as e:
    print 'Exception %r' % e
</code></pre>

<p>output:</p>

<pre><code>$ ./test.py https://github.com
success &lt;Response [200]&gt;
$ ./test.py https://github.com/sithlordyoyoma
success &lt;Response [404]&gt;
</code></pre>

<p>urllib2 does this correctly</p>

<pre><code>#!/usr/bin/env python

import contextlib
import urllib2
import sys


try:
    with contextlib.closing(urllib2.urlopen(sys.argv[1], timeout=30)) as r:
        print 'success %r' % r
except urllib2.HTTPError as e:
    print 'HTTPError %r' % e
except Exception as e:
    print 'Exception %r' % e
</code></pre>

<p>output:</p>

<pre><code>$ ./test.py https://github.com
success &lt;addinfourl at 4338734792 whose fp = &lt;socket._fileobject object at 0x1025a5c50&gt;&gt;
$ ./test.py https://github.com/sithlordyoyoma
HTTPError HTTPError()
</code></pre>
",2,1447367800,python;python-requests;urllib3,True,1459,1,1447368732,https://stackoverflow.com/questions/33682746/python-requests-urllib3-connection-pooling-not-catching-http-errors
33539809,Cannot recreate environment on heroku,"<p>Despite numerous pip freezes and attempts, I cannot seem to get the same version of urllib on heroku as my development environment. My requirements are upto date and code runs fine on my development machine, but I cannot get past the error on the production environment which must be down to the version of urllib.</p>

<pre><code>from urlparse import quote_plus
2015-11-05T08:24:38.133387+00:00 app[web.1]: ImportError: cannot import name quote_plus
</code></pre>

<p>Despite me trying the following import:</p>

<pre><code>try:
    from urllib.parse import quote_plus
except ImportError:
    from urlparse import quote_plus
</code></pre>

<p>and my <code>requirements.txt</code>:</p>

<pre><code>blinker==1.4
bson==0.4.1
Flask==0.10.1
Flask-Mail==0.9.1
flask-mongoengine==0.7.1
Flask-PyMongo==0.3.1
Flask-Sendmail==0.1
Flask-WTF==0.12
gunicorn==19.3.0
itsdangerous==0.24
Jinja2==2.8
MarkupSafe==0.23
mongoengine==0.10.0
pymongo==2.9
pytz==2015.7
requests==2.8.1
six==1.10.0
urllib3==1.12
Wand==0.4.1
Werkzeug==0.10.4
WTForms==2.0.2
</code></pre>
",2,1446712406,python;heroku;urllib;urllib3,True,309,1,1446714503,https://stackoverflow.com/questions/33539809/cannot-recreate-environment-on-heroku
33266539,HTTP GET site which uses TLS1.1 with urllib3,"<p>When I try to GET this site which uses the obsolete TLS1.1 with urllib3, this error is raised:</p>

<pre><code>&gt;&gt;&gt; import urllib3
&gt;&gt;&gt; http = urllib3.PoolManager()
&gt;&gt;&gt; r = http.request('GET', 'https://site/')

SSLError: [SSL: SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure (_ssl.c:646)
</code></pre>

<p>It seems that urllib3 is trying to handshake with SSL3, but as I mentioned, this site uses TLS1.1.</p>

<p>Do I have to specify that I wan't to use TLS1.1? How do I do it?</p>

<p>I've found this parameter called <a href=""http://urllib3.readthedocs.org/en/latest/helpers.html?highlight=ssl_version#module-urllib3.util.ssl_"" rel=""nofollow""><code>ssl_version</code></a> (it can be passed in the PoolManager constructor) which defaults to <code>PROTOCOL_SSLv23</code>. But I did not found something like <code>PROTOCOL_TLSv11</code> in <code>urllib3.util.ssl_</code>. In fact, the only var that starts with <code>PROTOCOL_</code> is the default option.</p>
",0,1445451781,python;python-3.x;ssl;urllib3,True,833,1,1445459526,https://stackoverflow.com/questions/33266539/http-get-site-which-uses-tls1-1-with-urllib3
32792469,InsecurePlatformWarning when building Docker image,"<p>I get this warning when building my Docker image:</p>

<pre><code>/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/util/ssl_.py:79: 
      InsecurePlatformWarning: A true SSLContext object is not available. 
      This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. 
      For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.
</code></pre>

<p>Several sources (like <a href=""https://stackoverflow.com/questions/29134512/insecureplatformwarning-a-true-sslcontext-object-is-not-available-this-prevent"">InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately</a>) say that <code>pip install pyopenssl ndg-httpsclient pyasn1</code> will fix this issue. But I get the warning as soon as pip attemps to install pyopenssl.</p>

<p>Here's my Dockerfile:</p>

<pre><code>FROM ubuntu:14.04

# Install packages
RUN apt-get update &amp;&amp; apt-get install -y \
    git \
    libmysqlclient-dev \
    mysql-server \
    nginx \
    python-dev \
    python-mysqldb \
    python-setuptools \
    supervisor \
    vim
RUN easy_install pip

# Handle urllib3 InsecurePlatformWarning
RUN apt-get install -y libffi-dev libssl-dev
RUN pip install pyopenssl ndg-httpsclient pyasn1

# ...more
</code></pre>
",8,1443228158,python;ubuntu;docker;pip;urllib3,True,1928,1,1444384961,https://stackoverflow.com/questions/32792469/insecureplatformwarning-when-building-docker-image
32718360,Python requests - how to add multiple own certificates,"<p>Is there a way to tell the requests lib to add multiple certificates like all .pem files from a specified folder?</p>

<pre><code>import requests, glob
CERTIFICATES = glob('/certs/')
url = '127.0.0.1:8080'
requests.get(url, cert=CERTIFICATES)
</code></pre>

<p>Seems to work only for a single certificate</p>

<p>I already search google and the python doc. The best tutorial I found was <a href=""http://docs.python-requests.org/en/latest/user/advanced/#ssl-cert-verification"" rel=""noreferrer"">the SSL certification section in the official documentation</a>.</p>
",6,1442928891,python;ssl;urllib3,True,4863,1,1442929188,https://stackoverflow.com/questions/32718360/python-requests-how-to-add-multiple-own-certificates
13573146,How to perform time limited response download with python requests?,"<p>When downloading a large file with python, I want to put a time limit not only for the connection process, but also for the download.</p>

<p>I am trying with the following python code:</p>

<pre><code>import requests

r = requests.get('http://ipv4.download.thinkbroadband.com/1GB.zip', timeout = 0.5, prefetch = False)

print r.headers['content-length']

print len(r.raw.read())
</code></pre>

<p>This does not work (the download is not time limited), as correctly noted in the docs: <a href=""https://requests.readthedocs.org/en/latest/user/quickstart/#timeouts"" rel=""noreferrer"">https://requests.readthedocs.org/en/latest/user/quickstart/#timeouts</a></p>

<p>This would be great if it was possible:</p>

<pre><code>r.raw.read(timeout = 10)
</code></pre>

<p>The question is, how to put a time limit to the download?</p>
",12,1353963907,python;python-requests;urllib3,True,11280,3,1442772678,https://stackoverflow.com/questions/13573146/how-to-perform-time-limited-response-download-with-python-requests
32654092,urllib3 define server_hostname ( for ssl check ) or ip where request,"<p>I want my program make himself the dns resolution for http requests.
It permits the user to make the requests only on IPv4 or IPv6 ( -4/-6 options ) or to randomize the destination to make a simple load balancing. To simplify my life, I use urllib3.</p>

<p>In http, it's easy. I make request on the ""ip url"" ( <a href=""http://X.X.X.X/.."" rel=""nofollow"">http://X.X.X.X/..</a>.) and I add the hostname in the request header.</p>

<p>But for security reasons, I need use https. I don't find how spécify the hostname to checked. I found the argument ""server_hostname"" for the ssl wrapper, but i didn't found how use it.</p>

<p>Some one know how give to urllib3/ssl the hostname to check ?</p>

<p>An other solution could be to specify to urllib3 the ip for an host.</p>

<p>If you have an idea ?</p>

<p>Or maybe the only solution is to make the https/ssl request myself ?</p>

<p>Regards</p>
",0,1442585256,python;ssl;urllib3,False,266,0,1442585256,https://stackoverflow.com/questions/32654092/urllib3-define-server-hostname-for-ssl-check-or-ip-where-request
32597790,InsecurePlatformWarning: A true SSLContext object is not available on iPython Notebook,"<p>I am trying to fetch some tweets using tweepy in my iPython notebook. I have the following code snippet for it - </p>

<pre><code>import tweepy
auth = tweepy.OAuthHandler(""abc"", ""xzy"")
auth.set_access_token(""asd"", ""zxc"")
api = tweepy.API(auth)
public_tweets = api.home_timeline()
</code></pre>

<p>I got the following error - </p>

<pre><code>/usr/local/lib/python2.7/site-packages/requests/packages/urllib3/util/ssl_.py:90: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.
</code></pre>

<p>InsecurePlatformWarning</p>

<p>I read <a href=""https://stackoverflow.com/questions/29134512/insecureplatformwarning-a-true-sslcontext-object-is-not-available-this-prevent"">this</a> SO question and update from 2.7.8 to 2.7.10. I am still getting this error. However when I run this piece of code on my terminal it works fine. So I am not understanding what is the issue with iPython.</p>
",1,1442363176,python;ipython;tweepy;urllib3,False,1408,0,1442363176,https://stackoverflow.com/questions/32597790/insecureplatformwarning-a-true-sslcontext-object-is-not-available-on-ipython-no
32315550,"upgrading urllib3 made cloud-init fail, apparently urllib3 and python requests have version compatibility issues?","<p>We upgraded our python-urllib3 from 1.5-7 to 10.10.4-1 and started getting errors in Cloud-init when it tried to get AWS metadata:</p>

<p><code>
  ""unexpected error [cannot concatenate 'str' and 'tuple' objects]"".
</code></p>

<p>I've seen several other StackOverFlow issues pointing to incompatibility issues between urllib3 and requests (<a href=""https://stackoverflow.com/questions/28081911/python-requests-timeout-value-error"">Python Requests Timeout Value error</a>), (<a href=""https://stackoverflow.com/questions/31768429/algolia-reindex-command-fails-with-exception-in-urllib3"">Algolia reindex command fails with exception in urllib3</a>).</p>

<p>What I can't find is any matrix of the versions that <strong>do</strong> work together.  The solutions in the other questions aren't available to us as the invoking code is buried in CloudInit.</p>

<p>We don't really have the option of going back to our previous version of urllib3 because other package (ElasticSearch Curator) apparently requires the newer version.</p>
",1,1441037558,python;python-requests;urllib3;cloud-init,True,430,1,1441101376,https://stackoverflow.com/questions/32315550/upgrading-urllib3-made-cloud-init-fail-apparently-urllib3-and-python-requests-h
32296255,How to pass data to urllib3 POST request method?,"<p>I want to use <code>urllib3</code> library for making POST request over <code>requests</code> library since it has connection pooling and retries etc. But I couldn't 
find any substitute of following <code>POST</code> request.</p>

<pre><code>import requests
result = requests.post(""http://myhost:8000/api/v1/edges"", json={'node_id1':""VLTTKeV-ixhcGgq53"", 'node_id2':""VLTTKeV-ixhcGgq51"", 'type': 1 })
</code></pre>

<p>This is working fine with <code>requests</code> library but I couldn't convert this into <code>urllib3</code> request.
I tried</p>

<pre><code>import json
import urllib3
urllib3.PoolManager().request(""POST"",""http://myhost:8000/api/v1/edges"", body=json.dumps(dict(json={'node_id1':""VLTTKeV-ixhcGgq53"", 'node_id2':""VLTTKeV-ixhcGgq51"", 'type': 1 })))
</code></pre>

<p>Problem is with passing raw json data with <code>json</code> as key in <code>POST</code> request. </p>
",5,1440935522,python;python-2.7;python-requests;urllib3,True,13942,1,1440962546,https://stackoverflow.com/questions/32296255/how-to-pass-data-to-urllib3-post-request-method
31945861,Why would inspect.getfile give me a file that&#39;s not there?,"<p>Or, Saltstack + docker-py <code>AttributeError: 'RecentlyUsedContainer' object has no attribute 'lock'</code></p>

<p>I have been digging into this issue to no avail. I'm trying to use SaltStack to manage my docker images/containers but ran into <a href=""https://github.com/saltstack/salt/issues/21896"" rel=""nofollow"">this problem</a>.</p>

<p>Initially I was using the salt state <code>docker.running</code> but that presented as the command does not exist. When I changed the state to <code>docker.running</code>, I got the traceback I posted over at that GitHub issue:</p>

<pre><code>      ID: scheduler
Function: docker.pulled
  Result: False
 Comment: An exception occurred in this state: Traceback (most recent call last):
            File ""/usr/lib/python2.7/dist-packages/salt/state.py"", line 1563, in call
              **cdata['kwargs'])
            File ""/usr/lib/python2.7/dist-packages/salt/states/dockerio.py"", line 271, in pulled
              returned = pull(name, tag=tag, insecure_registry=insecure_registry)
            File ""/usr/lib/python2.7/dist-packages/salt/modules/dockerio.py"", line 1599, in pull
              client = _get_client()
            File ""/usr/lib/python2.7/dist-packages/salt/modules/dockerio.py"", line 277, in _get_client
              client._version = client.version()['ApiVersion']
            File ""/usr/local/lib/python2.7/dist-packages/docker/client.py"", line 837, in version
              return self._result(self._get(url), json=True)
            File ""/usr/local/lib/python2.7/dist-packages/docker/clientbase.py"", line 86, in _get
              return self.get(url, **self._set_request_timeout(kwargs))
            File ""/usr/local/lib/python2.7/dist-packages/requests/sessions.py"", line 310, in get
              #: Stream response content default.
            File ""/usr/local/lib/python2.7/dist-packages/requests/sessions.py"", line 279, in request
            File ""/usr/local/lib/python2.7/dist-packages/requests/sessions.py"", line 374, in send
              url=request.url,
            File ""/usr/local/lib/python2.7/dist-packages/requests/adapters.py"", line 155, in send
              **proxy_kwargs)
            File ""/usr/local/lib/python2.7/dist-packages/docker/unixconn/unixconn.py"", line 74, in get_connection
              with self.pools.lock:
          AttributeError: 'RecentlyUsedContainer' object has no attribute 'lock'
 Started: 09:33:42.873628
Duration: 22.115 ms
</code></pre>

<p>After searching Google a bit more and coming up with nothing, I went ahead and started <a href=""http://blog.codinghorror.com/learn-to-read-the-source-luke/"" rel=""nofollow"">reading the source</a>.</p>

<p>After reading <code>unixconn.py</code> and realizing that <code>RecentlyUsedContainer</code> was coming from urllib3, I went and tracked down the source for that and discovered that there was a <code>_lock</code> attribute that was changed to <code>lock</code> a while ago. That seemed strange.</p>

<p>I looked closer at the imports and realized that <code>unixconn.py</code> was attempting to use requests' built-in urllib3 and <em>then</em> falling back to the stand alone urllib3. So I checked out the requests urllib3 and found that it did, indeed have the <code>_lock -&gt; lock</code> change. But it was newer than <em>my</em> version of requests. So I upgraded requests and tried again. Still no dice - same <code>AttributeError</code>.</p>

<p>Now things start to get weird.</p>

<p>In order to get information back to my salt master, I started mucking with the docker-py and urllib3 code on my salt minion. At first I raised exceptions with <code>urllib3.__file__</code> to make sure I was using the right file. But occasionally the file name that it would return was in a file and a folder that <em>did not exist</em>. Usually it was displaying <code>/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/_collections.pyc</code>, but when I would delete that file thinking that maybe the .pyc being cached was causing a problem it would still say that was the <code>__file__</code>, even though it didn't exist.</p>

<p>Then I discovered <code>inspect.getfile</code>. And I got the same bizarre behavior - I could delete the .pyc file and yet <code>inspect.getfile(self.pools)</code> would return the non-existent file.</p>

<p>To make life even better, I've added</p>

<pre><code>raise Exception('Pining for the Fjords')
</code></pre>

<p>to </p>

<pre><code>/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/_collections.py
</code></pre>

<p>At the end of the <code>RecentlyUsedContainer.__init__</code>. Yet that exception <strong>does not raise</strong>.</p>

<p>And I have just confirmed that something <em>is</em> in fact lying to me, because despite changing <code>unixconn.py</code></p>

<pre><code> def get_connection(self, url, proxies=None):                                
     import inspect                                                          
     r = RecentlyUsedContainer(10)                                           
     raise Exception(inspect.getfile(r.__class__) + '\n' + r.__doc__) 
</code></pre>

<p>which returns <code>/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/_collections.pyc</code>, when I go edit that .pyc and modify the <code>RecentlyUsedContainer</code>'s docstring I <strong>get the original docstring</strong>.</p>

<p>And finally, when I edit <code>/usr/lib/python2.7/dist-packages/urllib3/_collections.pyc</code> and change <em>it's</em> docstring, (or the same path but <code>_collections.py</code> instead)...</p>

<p>I still get the same docstring!</p>

<p>Why is the wrong code getting executed here, and how can I find out where it is so I can <em>fix</em> the problem?</p>
",2,1439306639,python;python-2.7;python-import;urllib3;dockerpy,True,333,1,1439383341,https://stackoverflow.com/questions/31945861/why-would-inspect-getfile-give-me-a-file-thats-not-there
25067580,Passing web data into Beautiful Soup - Empty list,"<p>I've rechecked my code and looked at comparable operations on opening a URL to pass web data into Beautiful Soup, for some reason my code just doesn't return anything although it's in correct form:</p>

<pre><code>&gt;&gt;&gt; from bs4 import BeautifulSoup

&gt;&gt;&gt; from urllib3 import poolmanager

&gt;&gt;&gt; connectBuilder = poolmanager.PoolManager()

&gt;&gt;&gt; content = connectBuilder.urlopen('GET', 'http://www.crummy.com/software/BeautifulSoup/')

&gt;&gt;&gt; content
&lt;urllib3.response.HTTPResponse object at 0x00000000032EC390&gt;

&gt;&gt;&gt; soup = BeautifulSoup(content)

&gt;&gt;&gt; soup.title
&gt;&gt;&gt; soup.title.name
Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
AttributeError: 'NoneType' object has no attribute 'name'
&gt;&gt;&gt; soup.p
&gt;&gt;&gt; soup.get_text()
''

&gt;&gt;&gt; content.data
a stream of data follows...
</code></pre>

<p>As shown, it's clear that urlopen() returns an HTTP response which is captured by the variable content, it makes sense that it can read the status of the response, but after it's passed into Beautiful Soup, the web data doesn't get converted into a Beautiful Soup object (variable soup).  You can see that I've tried to read a few tags and text, the get_text() returns an empty list, this is strange.</p>

<p>Strangely, when I access the web data via content.data, the data shows up but it's not useful since I can't use Beautiful Soup to parse it.  What is my problem?  Thanks.</p>
",7,1406835638,python;web-scraping;beautifulsoup;urllib3;web-content,True,14988,4,1437767109,https://stackoverflow.com/questions/25067580/passing-web-data-into-beautiful-soup-empty-list
30919825,Extracing CSS/JSS from a given URL,"<p>Currently trying to write a python script that can extract the CSS/JSS from a given URL. Had stumbled across urllib3 which helped me collect the HTML of a given URL with the help of their PoolManager() utility. With the short code below, I was able to extract the HTML of the given URL and later on store it in a file.</p>

<pre><code>import urllib3
http = urllib3.PoolManager()
x = http.request('GET','www.something.com')
x.data
</code></pre>

<p>I had gone through the documentation for urllib3 on their <a href=""https://urllib3.readthedocs.org/en/latest/"" rel=""nofollow"" title=""official page"">official page</a>. However, there wasn't too much on the various functions that was coming close to what I'm searching for. Now I need to somehow get the external resources of a particular URL and I'd like to know if it's possible using urllib3 or whether I need to search for something else that'll help me do the same (any suggestions are welcomed as well).</p>

<p>Thanks in advance everyone!</p>
",0,1434641768,python;extract;urllib3,False,74,1,1434641976,https://stackoverflow.com/questions/30919825/extracing-css-jss-from-a-given-url
30488027,Is join() enough for clean up python threads?,"<p>I've worked around with a multithreading <code>client</code> calls a domain with several HTTP requests once a minute. Basically each HTTP is wrapped in a single thread in the client. The <code>connection class</code> uses <code>urllib3</code> with a connection pool to manage sockets and reuse them. The <code>connection class</code> does not support <code>asyncio</code>.</p>

<p>But sometimes, usually after a long time running the <code>client</code> (~24hrs), some <code>OSError: Too many open files</code> will be raised in the <code>connection class</code> when sending the requests. And usually those exception was captured in threads with id greater than 100k... So I am thinking my way of manipulating threads is totally wrong.</p>

<p>The sample code shows as below. </p>

<pre><code>def run():
  timer_start = time.time()
  workers = []
  # create several tasks in workers
  # i.e. workers.append(threading.Thread(target=foo, ...)
  run_workers(workers)
  timer = Time(timer_start - time.time() + 60, run)
  timer.start()

def run_workers(workers):
  i = 0
  while i &lt; len(workers):
    if threading.active_count() &lt; 8:
      workers[i].start()
      i += 1
    else:
      time.sleep(0.1)
  i = 0
  while len(workers):
    if i &gt; len(workers):
      i = 0
    workers[i].join(timeout=0.1)
    if not workers[i].is_alive():
      del workers[i]
    i += 1

# A foo might do behavior like this
def foo(*args, **kwargs):
  res = pool.urlopen('POST', kwargs[url], kwargs[body], ...)
  if not (200 &lt;= res.status &lt; 300):
    #raise_error(...)    

#main()
run()
</code></pre>

<p>My main question will be is my method good enough for cleaning up finished threads in a long run client? I think python recycle used threads id but why would I still get a thread running with id > 100k? Does that mean those already finished threads still hanging there?</p>
",3,1432743645,python;multithreading;python-multithreading;urllib3,False,347,0,1432753657,https://stackoverflow.com/questions/30488027/is-join-enough-for-clean-up-python-threads
30363322,Issues using strip() on a variable,"<p>I'm trying to strip the characters <code>b,'()</code>.
The issue I'm having is that it says TypeError 'str' does not support the buffer interface. </p>

<p>Here are the relevant parts of code in this:</p>

<pre><code>import urllib3
def command_uptime():

    http = urllib3.PoolManager()
    r = http.request('GET', 'https://nightdev.com/hosted/uptime.php?channel=TrippedNW')
    rawData = r.data
    liveTime = bytes(rawData.strip(""b,\'()"", rawData))

    message = ""Tripped has been live for: "", liveTime
    send_message(CHAN, message)
</code></pre>
",1,1432173603,python;python-3.x;twitch;urllib3,True,1293,2,1432175220,https://stackoverflow.com/questions/30363322/issues-using-strip-on-a-variable
29810631,GET html data from multiple urls on website in one connection,"<p>I have a python script that takes in an input of a few urls. My script loops through each of these urls and prints out the htmltext from each page. Would the website see this as 3 seperate GET requests and therefore 3 ""hits"" to the site or would it see the socket connection and see it as 1 ""hit"" to the page?</p>

<p>I think it's the first option by checking the debug, if so, is it possible to GET data from multiple URLs on the same site but the site to only see this as 1 ""hit"" to the site? Can I utilise the keep-alive functionality to achieve this in urllib3?</p>

<p>My script is below:</p>

<pre><code>for u in url:
    opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))
    req = urllib2.Request(u)
    req.add_header('User-Agent','Mozilla/5.0')
    print urllib2.build_opener(urllib2.HTTPHandler(debuglevel=1)).open(req)
    resp = opener.open(req)
    htmltext = resp.read()
</code></pre>
",2,1429744461,python;python-2.7;urllib2;urllib3,True,586,1,1429747050,https://stackoverflow.com/questions/29810631/get-html-data-from-multiple-urls-on-website-in-one-connection
29474143,Requests.package.urllib3 error,"<p>I am a Python rookie and have been exploring the language.  Things are going well since I do have a fairly basic programming background and I can understand the general structure.  However, recently, I think that I did something to break urllib3 because my app was working before and now it fails and I can no longer import urllib3 through the interactive interpreter.</p>

<p>Here is the code:</p>

<pre><code>import json, requests, ssl

devID = ""xxxx"" &lt;--xxx is included for privacy/security purposes
varName = ""Current_T""
AToken = ""xxx"" &lt;--xxx is included for privacy/security purposes
spark_url = ""https://api.spark.io/v1/devices/%s/%s?access_token=%s"" % (devID,varName,AToken)
r = requests.get(spark_url)
data = r.json()
jsonData = ""result""
CurrentTemp = data[jsonData]
print(""Current temp is: %i"") % CurrentTemp
</code></pre>

<p>This pretty basic stuff and it worked perfectly until earlier this morning.  The error I now receive is the following:</p>

<pre><code>pi@raspberrypi ~/python-learning $ python ./spark-read-variable.py
Traceback (most recent call last):
  File ""./spark-read-variable.py"", line 1, in &lt;module&gt;
    import json, requests, ssl
  File ""/usr/local/lib/python2.7/dist-packages/requests/__init__.py"", line 58, in &lt;module&gt;
    from . import utils
  File ""/usr/local/lib/python2.7/dist-packages/requests/utils.py"", line 26, in &lt;module&gt;
    from .compat import parse_http_list as _parse_list_header
  File ""/usr/local/lib/python2.7/dist-packages/requests/compat.py"", line 42, in &lt;module&gt;
    from .packages.urllib3.packages.ordered_dict import OrderedDict
  File ""/usr/local/lib/python2.7/dist-packages/requests/packages/__init__.py"", line 95, in load_module
    raise ImportError(""No module named '%s'"" % (name,))
ImportError: No module named 'requests.packages.urllib3'
</code></pre>

<p>So far, I have tried the following to address the situation and nothing has fixed it.:</p>

<ol>
<li>Upgrade requests and urllib3</li>
<li>Remove and reinstall urllib3</li>
<li>Reboot Raspberry Pi &lt;- Probably a dumb idea, but I tried anyway</li>
</ol>

<p>The only thing that I can think of that could be causing this was that I tried to use pip to install stmtplib and email.utils and both failed.</p>

<p>One final data point is that if I try to import urllib3 through the interactive interpreter, I get the following:</p>

<pre><code>Python 2.7.3 (default, Mar 18 2014, 05:13:23)
[GCC 4.6.3] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import urllib3
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python2.7/dist-packages/urllib3/__init__.py"", line 10, in &lt;module&gt;
    from .connectionpool import (
  File ""/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py"", line 37, in &lt;module&gt;
    from .request import RequestMethods
  File ""/usr/local/lib/python2.7/dist-packages/urllib3/request.py"", line 6, in &lt;module&gt;
    from .filepost import encode_multipart_formdata
  File ""/usr/local/lib/python2.7/dist-packages/urllib3/filepost.py"", line 8, in &lt;module&gt;
    from .fields import RequestField
  File ""/usr/local/lib/python2.7/dist-packages/urllib3/fields.py"", line 1, in &lt;module&gt;
    import email.utils
  File ""email.py"", line 1, in &lt;module&gt;
    import smtplib
  File ""/usr/lib/python2.7/smtplib.py"", line 46, in &lt;module&gt;
    import email.utils
ImportError: No module named utils
</code></pre>

<p>Any ideas how I can fix this?</p>

<p>Thank you!</p>
",2,1428333621,python;python-2.7;urllib3,True,3821,1,1428334039,https://stackoverflow.com/questions/29474143/requests-package-urllib3-error
29216802,How to get Response history after few redirects in HTTPConnectionPool urllib3,"<p>I am using <code>HTTPConnectionPool</code> of <code>urllib3</code> for post requests. Here in the code <code>http://something.com/file.php</code> has <code>2</code> redirects. After the request is complete I am getting final response in the <code>headers</code> and <code>data</code>. How to see the history of redirects that was done by the server on the meantime?</p>

<pre><code>headers = {.....}
data = {...}
newPool = urllib3.HTTPConnectionPool(""something.com"",port=80,maxsize=5,headers=headers,retries =5,timeout=10)
r = newPool.request('POST', '/file.php',fields=data,redirect =True)
print r.ststus
print r.headers
print r.data
</code></pre>

<p>Please Give me some advice. Thanks in Advance.</p>
",2,1427132436,python;connection-pooling;urllib3,True,894,1,1427134804,https://stackoverflow.com/questions/29216802/how-to-get-response-history-after-few-redirects-in-httpconnectionpool-urllib3
29131873,"Python Requests, warning: urllib3.connectionpool:Connection pool is full","<p>I'm using the requests library in python 3 and despite my best efforts I can't get the following warning to disappear:</p>

<blockquote>
  <p>WARNING:requests.packages.urllib3.connectionpool:Connection pool is full, discarding connection: myorganization.zendesk.com</p>
</blockquote>

<p>I'm using requests in a multithreaded environment to get and post json files concurrently to a <strong>single host</strong>, definitely no subdomains. In this current set up I'm using just 20 threads.</p>

<p>I attempted to use a <code>Session</code> in order to get requests to reuse connections and thus get rid of the problem, but it hasn't worked. This is the code in my class constructor:</p>

<pre><code>self.session = requests.Session()
adapter = requests.adapters.HTTPAdapter(
    pool_connections=100, pool_maxsize=100)
self.session.mount('http://', adapter)
self.session.headers.update({'Connection':'Keep-Alive'})
self.session.auth = (self._user+""/token"", self._token)
</code></pre>

<p>According to advice from <a href=""https://bugs.launchpad.net/python-swiftclient/+bug/1295812"" rel=""nofollow noreferrer""> here </a> I shouldn't need to increase the pooled connections by that much considering the number of threads I'm using, but despite this I get this warning even when raising by 100.</p>

<p>This makes me think that connections are not being reused at all, or if they are, too many are being created for some reason. I've updated requests, so it is the most up to date version.</p>

<p>Does anyone have any ideas how I can get rid of this? I'm debugging some code and I think this is the blame for some requests not being made correctly.</p>

<p>Related:</p>

<p><a href=""https://stackoverflow.com/questions/18466079/can-i-change-the-connection-pool-size-for-pythons-requests-module"">Can I change the connection pool size for Python&#39;s &quot;requests&quot; module?</a></p>
",2,1426709895,python;multithreading;python-requests;urllib3,True,7686,1,1426716165,https://stackoverflow.com/questions/29131873/python-requests-warning-urllib3-connectionpoolconnection-pool-is-full
29061135,python urllib3 login + search,"<pre><code>import urllib3
import io
from bs4 import BeautifulSoup
import re
import cookielib

http = urllib3.PoolManager()
url = 'http://www.example.com'
headers = urllib3.util.make_headers(keep_alive=True,user_agent='Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.6) Gecko/20070725 Firefox/2.0.0.6')
r = http.urlopen('GET', url, preload_content=False)

# Params die dann am Post request übergeben werden
params = {
    'login': '/shop//index.php',
    'user': 'username',
    'pw': 'password'
  }
suche = {
    'id' : 'searchfield',
    'name' : 'suche',
    }

# Post Anfrage inkl params (login) Antwort in response.data
response = http.request('POST', url, params, headers)
suche = http.request('POST', site-to-search? , suche, headers)
html_suche = suche.data

print html_suche
</code></pre>

<p>I try to login with this code to a site and search after that.
With this code i get a answer that i am not loged in.</p>

<p>how can i combine that i first login and after that to search.
Thx.</p>
",1,1426426112,python;search;authentication;beautifulsoup;urllib3,True,4611,1,1426442820,https://stackoverflow.com/questions/29061135/python-urllib3-login-search
28714368,urllib3 set cookie onto the browser,"<p>In the scenario where the browser request the below python code to serve something from a webpage and the python program should be able set all the cookies issued by the host website, how to do the same using urllib3. also is urlib3 lib the right way to go?</p>

<p>The end goal is the cookie has to be set in the browser as issued by the host site</p>

<pre><code>#!/usr/bin/python
import urllib3
http_pool = urllib3.connection_from_url(""http://example.com"")
r = http_pool.get_url(""http://example.org/"", headers=""set the cookie as issued by the host site"")
</code></pre>
",1,1424852958,python;urllib3,False,565,0,1424853349,https://stackoverflow.com/questions/28714368/urllib3-set-cookie-onto-the-browser
28703071,How can search google using urllib3?,"<p>Im trying to retrieve a list of urls from a google search, here is my current implementation using urllib:</p>

<p>the query is searching last fm for two specific tags</p>

<pre><code>def searchTags(tag1, tag2):
    query = input( 'site:last.fm/music intitle:""tags for"" ""'+ tag1 + '"" ""'+ tag2 +'""' )
    query = urllib.urlencode( { 'q' : query } )
    response = urllib.urlopen( 'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&amp;' + query ).read()
    json = m_json.loads( response )
    results = json[ 'responseData' ][ 'results' ]
    print(results)
    for result in results:
        title = result['title']
        url = result['url']   # was URL in the original and that threw a name error exception
        print( title + '; ' + url )
</code></pre>
",0,1424801206,python;python-3.x;google-search;urllib3,False,445,0,1424801240,https://stackoverflow.com/questions/28703071/how-can-search-google-using-urllib3
28081911,Python Requests Timeout Value error,"<p>Good Evening,</p>

<p>I cannot get my https request to go through. I'm having to use SSLv3, so I'm specifying the protocol with:</p>

<pre><code>import requests
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager
import ssl

class MyAdapter(HTTPAdapter):
     def init_poolmanager(self, connections, maxsize, block=False):
        self.poolmanager = PoolManager(num_pools=connections,
                                        maxsize=maxsize,
                                        block=block,
                                        ssl_version=ssl.PROTOCOL_SSLv3)

username = 'username'
password = 'password'
email = 'email@example.com'
url = 'https://api.example.com/'
headers = {'Accept': 'application/json', 'content-type': 'application/json'}
params = {'emailaddress': email}
auth = (username, password)

s = requests.Session()
s.mount(url, MyAdapter())
r = s.get(url+'customer.svc/search', params=params, auth=auth, headers=headers)
</code></pre>

<p>When I run my get request I get the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/home/ubuntu/workspace/app/venv/lib/python2.7/site-packages/requests/sessions.py"", line 473, in get
    return self.request('GET', url, **kwargs)
  File ""/home/ubuntu/workspace/app/venv/lib/python2.7/site-packages/requests/sessions.py"", line 461, in request
    resp = self.send(prep, **send_kwargs)
  File ""/home/ubuntu/workspace/app/venv/lib/python2.7/site-packages/requests/sessions.py"", line 573, in send
    r = adapter.send(request, **kwargs)
  File ""/home/ubuntu/workspace/app/venv/lib/python2.7/site-packages/requests/adapters.py"", line 370, in send
    timeout=timeout
  File ""/home/ubuntu/workspace/app/venv/lib/python2.7/site-packages/urllib3/connectionpool.py"", line 517, in urlopen
    timeout_obj = self._get_timeout(timeout)
  File ""/home/ubuntu/workspace/app/venv/lib/python2.7/site-packages/urllib3/connectionpool.py"", line 283, in _get_timeout
    return Timeout.from_float(timeout)
  File ""/home/ubuntu/workspace/app/venv/lib/python2.7/site-packages/urllib3/util/timeout.py"", line 152, in from_float
    return Timeout(read=timeout, connect=timeout)
  File ""/home/ubuntu/workspace/app/venv/lib/python2.7/site-packages/urllib3/util/timeout.py"", line 95, in __init__
    self._connect = self._validate_timeout(connect, 'connect')
  File ""/home/ubuntu/workspace/app/venv/lib/python2.7/site-packages/urllib3/util/timeout.py"", line 125, in _validate_timeout
    ""int or float."" % (name, value))
ValueError: Timeout value connect was Timeout(connect=None, read=None, total=None), but it must be an int or float.
</code></pre>

<p>Any ideas? I can't figure it out.</p>

<p>Additional Context: I'm on an Amazon EC2 Ubuntu Instance, running requests 2.5.1 and python 2.7.6</p>
",0,1421905001,python;amazon-ec2;python-requests;urllib3,True,2306,1,1423768815,https://stackoverflow.com/questions/28081911/python-requests-timeout-value-error
28418860,Best way to constantly request http data?,"<p>Which is the best way to request constant data from a server in Python? I've tried with Urllib3 but for some reason after a while the python script stops. And I am also trying urllib2 (see below the code), but I notice there's a huge delay sometimes (that did not happen as frequently with urllib3) and the response is not every 0.5 seconds (sometimes it's every 6 seconds). What can I do to solve this?</p>

<pre><code>import socket
import urllib2
import time

# timeout in seconds
timeout = 10
socket.setdefaulttimeout(timeout)

while True:
    try:
        # this call to urllib2.urlopen now uses the default timeout
        # we have set in the socket module
        req = urllib2.Request('https://www.okcoin.com/api/v1/future_ticker.do?symbol=btc_usd&amp;contract_type=this_week')
        response = urllib2.urlopen(req)
        r = response.read()

        req2 = urllib2.Request('http://market.bitvc.com/futures/ticker_btc_week.js')
        response2 = urllib2.urlopen(req2)
        r2 = response2.read()
    except:
        continue

    print r + str(time.time())
    print r2 + str(time.time())
    time.sleep(0.5)
</code></pre>
",0,1423514149,python;httprequest;urllib2;urllib3,False,2036,1,1423516490,https://stackoverflow.com/questions/28418860/best-way-to-constantly-request-http-data
28157151,Easily upgrade Requests&#39;s vendored version of urllib3?,"<p>Python's Requests library <a href=""https://github.com/kennethreitz/requests/tree/master/requests/packages"" rel=""nofollow"">bundles the <code>chardet</code> and <code>urllib3</code> packages</a>.</p>

<p>This can mean that the bundled version will have bugs. I am wondering if there is a clean or easy way to get Requests to use a different version of urllib3. For example, could I <code>pip install requests urllib3</code> and then have requests use that version automatically?</p>

<p>I know that Requests <a href=""https://github.com/kennethreitz/requests/blob/e23bf10cf4ecc62f6c3dd6284043516fb833d9ce/docs/user/advanced.rst#ca-certificates"" rel=""nofollow"">automatically uses <code>certifi</code></a> instead of its bundled certificate, if <code>certifi</code> is installed, but I couldn't find documentation like this for <code>urllib3</code>.</p>

<p>Otherwise the options I see are:</p>

<ol>
<li>Get requests to merge the newest version of urllib3,</li>
<li>Fork my own version of requests and use that.</li>
</ol>
",2,1422299906,python;python-requests;urllib3,True,1652,1,1422333057,https://stackoverflow.com/questions/28157151/easily-upgrade-requestss-vendored-version-of-urllib3
27369998,urllib3 download a file using specified user agent,"<p>What is the correct way to update the user agent information in <code>urllib3</code>?</p>

<p>How can I check that the user agent information was indeed changed and is being used?</p>

<p>For example:</p>

<pre><code>user_agent = {'user-agent': 'Mozilla/5.0 (Windows NT 6.3; rv:36.0) Gecko/20100101 Firefox/36.0'}
http = urllib3.PoolManager(10, headers=user_agent)

r1 = http.request('GET', 'http://example.com/')
if r1.status is 200:
    with open('somefile','w+') as f:
        f.write(r1.data)
</code></pre>

<p>When I create a <code>PoolManager</code> at <code>http</code> I looked at it by <code>dir(http)</code> and saw that <code>http.headers</code> was empty by default and updated to the user agent info specified, but is it being used? Is there anyway to check without having to look at <code>apache</code> logs?</p>

<p>And actually checking <code>/var/log/apache2/access.log</code> after trying to update the user agent:</p>

<pre><code>&gt;&gt;&gt; import urllib3
&gt;&gt;&gt; user_agent = {'user-agent': 'Mozilla/5.0 (Windows NT 6.3; rv:36.0) Gecko/20100101 Firefox/36.0'}
&gt;&gt;&gt; http = urllib3.PoolManager(2, headers=user_agent)
&gt;&gt;&gt; r = http.request('GET','localhost')
&gt;&gt;&gt; with open('/var/log/apache2/access.log','r') as f:
...     last_line = f.readlines()[-1]
... 
&gt;&gt;&gt; last_line
'127.0.0.1 - - [08/Dec/2014:20:42:04 -0500] ""GET / HTTP/1.1"" 200 461 ""-"" ""-""\n'
</code></pre>
",5,1418088366,python;urllib3,True,7810,1,1418089370,https://stackoverflow.com/questions/27369998/urllib3-download-a-file-using-specified-user-agent
27006676,Using WorkerPool to multithread through a list of URLs,"<p>I'm trying to use multithreads to go through a txt file of urls and scrape the contents found at each url. This works for about 20 URLs (not consistent how many) but then consistently gets stuck on the last url in the file. It doesn't seem to be doing them in order.</p>

<p>I have no idea why it's getting stuck or where to start so thank you so much for your help. </p>

<pre><code>from bs4 import BeautifulSoup, SoupStrainer
import urllib3
import urllib2
import io
import os
import re
import workerpool
from urllib2 import Request, urlopen, URLError

NUM_SOCKETS = 3
NUM_WORKERS = 5

urlfile = open(""dailynewsurls.txt"",'r') # read one line at a time until end of file
http = urllib3.PoolManager(maxsize=NUM_SOCKETS)
workers = workerpool.WorkerPool(size=NUM_WORKERS)

class MyJob(workerpool.Job):
    def __init__(self, url):
       self.url = url

    def run(self):
        r = http.request('GET', self.url)
        req = urllib2.Request(url)
        try:
            page = urllib2.urlopen(req)
        except:
            print ""had to skip one""
            return            
        pagecontent = page.read() # get a file-like object at this url

#this tells it to soup the page that is at the url above
        soup = BeautifulSoup(pagecontent)

#this tells it to find the string in the first instance of each of the tags in the parenthesis
        title = soup.find_all('title')
        article = soup.find_all('article')


        try:
            title = str(title[0].get_text().encode('utf-8'))
        except:
            print ""had to skip one""
            return
        try:
            article = str(article[0].get_text().encode('utf-8'))
        except:
            print ""had to skip one""
            return

        try: 
    # make the file using the things above
            output_files_pathname = 'DailyNews/'  # path where output will go
            new_filename = title + "".txt""

# write each of the things defined into the text file
            outfile = open(output_files_pathname + new_filename,'w')
            outfile.write(title)
            outfile.write(""\n"")
            outfile.write(article)
            outfile.close()
            print ""%r added as a text file"" % title
            return

        except:
            print ""had to skip one""
            return

        return

for url in urlfile:  
    workers.put(MyJob(url))  

workers.shutdown()
workers.wait()

print ""All done.""
</code></pre>

<p>Here's an example list of the urls: </p>

<pre><code>http://www.nydailynews.com/entertainment/tv-movies/x-factor-season-2-episode-2-recap-oops-britney-spears-article-1.1159546
http://www.nydailynews.com/new-york/brooklyn/lois-mclohon-resurfaced-iconic-daily-news-coney-island-cheesecake-photo-brings-back-memories-50-year-long-romance-article-1.1160457
http://www.nydailynews.com/new-york/uptown/espaillat-linares-rivals-bitter-history-battle-state-senate-seat-article-1.1157994
http://www.nydailynews.com/sports/baseball/mlb-power-rankings-yankees-split-orioles-tumble-rankings-nationals-shut-stephen-strasburg-hang-top-spot-article-1.1155953
http://www.nydailynews.com/news/national/salon-sell-internet-online-communities-article-1.1150614
http://www.nydailynews.com/sports/more-sports/jiyai-shin-wins-women-british-open-dominating-fashion-record-nine-shot-victory-article-1.1160894
http://www.nydailynews.com/entertainment/music-arts/justin-bieber-offered-hockey-contract-bakersfield-condors-minor-league-team-article-1.1157991
http://www.nydailynews.com/sports/baseball/yankees/umpire-blown-call-9th-inning-dooms-yankees-5-4-loss-baltimore-orioles-camden-yards-article-1.1155141
http://www.nydailynews.com/entertainment/gossip/kellie-pickler-shaving-head-support-best-friend-cancer-fight-hair-article-1.1160938
http://www.nydailynews.com/new-york/secret-103-000-settlement-staffers-accused-assemblyman-vito-lopez-sexual-harassment-included-penalty-20k-involved-talked-details-article-1.1157849
http://www.nydailynews.com/entertainment/tv-movies/ricki-lake-fun-adds-substance-new-syndicated-daytime-show-article-1.1153301
http://www.nydailynews.com/sports/college/matt-barkley-loyalty-usc-trojans-contention-bcs-national-championship-article-1.1152969
http://www.nydailynews.com/sports/daily-news-sports-photos-day-farewell-andy-roddick-world-1-u-s-open-champ-retires-loss-juan-martin-del-potro-article-1.1152827
http://www.nydailynews.com/entertainment/gossip/britney-spears-made-move-relationship-fiance-jason-trawick-reveals-article-1.1152722
http://www.nydailynews.com/new-york/brooklyn/brooklyn-lupus-center-tayumika-zurita-leads-local-battle-disease-difficult-adversary-article-1.1153494
http://www.nydailynews.com/life-style/fashion/kate-middleton-prabal-gurung-dress-sells-hour-myhabit-site-sold-1-995-dress-599-article-1.1161583
http://www.nydailynews.com/news/politics/obama-romney-campaigns-vie-advantage-president-maintains-lead-article-1.1161540
http://www.nydailynews.com/life-style/free-cheap-new-york-city-tuesday-sept-11-article-1.1155950
http://www.nydailynews.com/news/world/dozens-storm-embassy-compound-tunis-article-1.1159663
http://www.nydailynews.com/opinion/send-egypt-message-article-1.1157828
http://www.nydailynews.com/sports/more-sports/witnesses-feel-sheryl-crow-lance-amstrong-activities-article-1.1152899
http://www.nydailynews.com/sports/baseball/yankees/hiroki-kuroda-replacing-cc-sabathia-yankees-ace-pitcher-real-possibility-playoffs-looming-article-1.1161812
http://www.nydailynews.com/life-style/eats/finland-hosts-pop-down-restaurant-belly-earth-262-feet-underground-article-1.1151523
http://www.nydailynews.com/sports/more-sports/mighty-quinn-sept-23-article-1.1165584
http://www.nydailynews.com/sports/more-sports/jerry-king-lawler-stable-condition-suffering-heart-attack-wwe-raw-broadcast-monday-night-article-1.1156915
http://www.nydailynews.com/news/politics/ambassador-chris-stevens-breathing-libyans-found-american-consulate-rescue-article-1.1161454
http://www.nydailynews.com/news/crime/swiss-banker-bradley-birkenfeld-104-million-reward-irs-blowing-whistle-thousands-tax-dodgers-article-1.1156736
http://www.nydailynews.com/sports/hockey/nhl-board-governors-votes-favor-lockout-league-players-association-fail-reach-agreement-cba-article-1.1159131
http://www.nydailynews.com/news/national/iphone-5-works-t-network-article-1.1165543
http://www.nydailynews.com/sports/baseball/yankees/yankees-broadcasters-michael-kay-ken-singleton-opportunity-important-statement-article-1.1165479
http://www.nydailynews.com/news/national/boss-year-michigan-car-dealer-retires-employees-1-000-year-service-article-1.1156763
http://www.nydailynews.com/entertainment/tv-movies/hero-denzel-washington-clint-eastwood-article-1.1165538
http://www.nydailynews.com/sports/football/giants/ny-giants-secondary-roasted-tony-romo-dallas-cowboys-offense-article-1.1153055
http://www.nydailynews.com/news/national/hide-and-seek-tragedy-3-year-old-suffocates-hiding-bean-bag-article-1.1160138
</code></pre>
",2,1416356112,python;multithreading;urllib3;worker-pool,True,514,1,1416369951,https://stackoverflow.com/questions/27006676/using-workerpool-to-multithread-through-a-list-of-urls
26759577,Python Requests library throws exceptions in logging,"<p>The Python <code>requests</code> library appears to have some rather strange quirks when it comes to its logging behaviour.
Using the latest Python 2.7.8, I have the following code:</p>

<pre><code>import requests
import logging

logging.basicConfig(
    filename='mylog.txt',
    format='%(asctime)-19.19s|%(task)-36s|%(levelname)s:%(name)s: %(message)s',
    level=eval('logging.%s' % 'DEBUG'))

logger = logging.getLogger(__name__)

logger.info('myprogram starting up...', extra={'task': ''})     # so far so good
...
(ommited code)
...
payload = {'id': 'abc', 'status': 'ok'}

# At this point the program continues but throws an exception.
requests.get('http://localhost:9100/notify', params=payload) 

print 'Task is complete! NotifyURL was hit! - Exiting'
</code></pre>

<p>My program seems to exit normally, however inside the log file it creates (mylog.txt) I always find the following exception:</p>

<pre><code>KeyError: 'task'
Logged from file connectionpool.py, line 362
</code></pre>

<p>If I remove this:
<code>requests.get('http://localhost:9100/notify', params=payload)</code>
then the exception is gone.</p>

<p>What exactly am I doing wrong here and how may I fix this?
I am using requests v2.4.3.</p>
",3,1415197584,python;python-2.7;logging;python-requests;urllib3,True,861,2,1415699682,https://stackoverflow.com/questions/26759577/python-requests-library-throws-exceptions-in-logging
26834235,issues with encoding in python3 and urllib3,"<p>I'm trying to write a python program which will help me to automatically get some news from different websites.
At the moment I'm using python3 with beautifulsoup4 and urllib3 to get the remote page and parse it.</p>

<p>the problem comes out when I'm trying to read text from this pages because they contain non ascii characters such as À à é ó...and so on...</p>

<p>I've tried to decode the page from utf-8 just after retrieving it to put it in a variable and then write it in a file without success... and even after reading different way to approach this problem I couldn't figure out a working solution.</p>

<p>I was wondering then if anyone of you is been in my same situation..</p>

<p>Here is my code</p>

<pre><code># -*- coding: utf-8 -*-
from bs4 import BeautifulSoup
import urllib3

http = urllib3.PoolManager()
req = http.request('GET', 'http://www.....')
page = req.data.decode('utf-8')
soup = BeautifulSoup(page)

elements = soup.find_all('div', class_='content')

fp = open('results.xml', 'a')

for element in elements:
  link  = element.find('a')
  descr = element.find('div', class_='description')

  v_link  = u'%s' % link.get('href')
  v_description = u'%s' % descr.text

  xml = ""&lt;news&gt;\n""
  xml = xml+ ""  &lt;description&gt;""+ v_description+ ""&lt;/description&gt;\n""
  xml = xml+ ""  &lt;page_link&gt;""+ v_link+ ""&lt;/page_link&gt;\n""
  xml = xml+ ""&lt;/news&gt;\n""

  fp.write(xml+ '\n')

#END FOR LOOP

fp.close()
</code></pre>
",0,1415573334,python;python-3.x;encoding;beautifulsoup;urllib3,False,2115,2,1415575929,https://stackoverflow.com/questions/26834235/issues-with-encoding-in-python3-and-urllib3
25674667,How to get the redirect history from urllib3?,"<p>I can't figure out how to get the next redirect url and response kind of like the redirect history in the Requests lib.</p>
",3,1409863382,python;urllib3,False,340,0,1409863382,https://stackoverflow.com/questions/25674667/how-to-get-the-redirect-history-from-urllib3
25121848,Error 10054 when connecting google service,"<p>I tried to connect to google api and got this error</p>

<pre><code>requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.google.com', port=443): Max retries exceeded with url: /speech-api/v1/recognize?xjerr=1&amp;client=chromium&amp;lang=ru-RU (Caused by &lt;class 'socket.error'&gt;: [Errno 10054] An existing connection was forcibly closed by the remote host)
</code></pre>

<p>this is my code, i used library urllib3,urllib2,requests but it did not help</p>

<pre><code>import requests

recording = 'C:/Users/sborovskiy/PycharmProjects/VoiceManager/com/work/voiceManager/output.flac'
url = 'https://www.google.com/speech-api/v1/recognize?xjerr=1&amp;client=chromium&amp;lang=ru-RU'
headers = {'Content-Type' : 'audio/x-flac; rate=16000', 'User-Agent': 'Netscape 6.0'}
files = {'output.flac' : open(recording, 'rb')}
response = requests.post(url = url, data = files, headers = headers)
</code></pre>

<p>internet not give a specific answer</p>
",0,1407164982,python;google-api;python-requests;urllib3,False,380,0,1407164982,https://stackoverflow.com/questions/25121848/error-10054-when-connecting-google-service
25101175,Have an example of gzipped attachment decoding with Python&#39;s urllib3 or requests?,"<p>I just need an example ...</p>

<p>I have working code with both urllib3 and requests to download a page with an attached gzipped file (of XML).</p>

<p>I don't see how to get this attachment with either package; all I get is the HTML page but not the attached data.</p>

<p>Anybody have an example showing this with either package? I could also try something else if it uses connection pooling, which is what I'm after.</p>

<p>Thanks for any examples whatsoever!</p>
",0,1407034175,python;urllib3,True,630,2,1407109533,https://stackoverflow.com/questions/25101175/have-an-example-of-gzipped-attachment-decoding-with-pythons-urllib3-or-requests
25043051,difference between urllibx and requests?,"<p>Can anyone explain the approach used by urllibx opener for heavy lifting in comparison to how request module ?</p>

<p>why the approach of using handler in urllibx goes with like HTTPSHandler , so for auth handler, 
but in case of requests you can just pass username password like:</p>

<p><code>r = requests.get('https://api.github.com', auth=('user', 'pass'))</code></p>

<p>Does both have any advantage over each other, I am also curious because requests also uses urllib3 in background, as far what I have learnt about it.</p>

<p>I was looking for a definitive answer for the clarity , couldn't find a clear difference anywhere w.r.t. use of opener and handler as given in python docs for urllib2 and comparison to requests module.</p>
",2,1406740684,python;urllib2;python-requests;urllib;urllib3,False,87,0,1406740684,https://stackoverflow.com/questions/25043051/difference-between-urllibx-and-requests
24935042,Connecting streams in Python,"<ol>
<li>I have python3.3</li>
<li>I have a Response object from urllib3 (which has a <code>stream()</code> method, which returns generator)</li>
<li>I have a file I want to write the data to</li>
</ol>

<p>What is the most idiomatic way of writing data from (1) into (2)? I can use list comprehension to do something like:</p>

<pre><code>with http.request_encode_url('GET', …, {'param1': value1}) as response:
    with open(path, 'wb') as fp:
        [fp.write(piece) for piece in response.stream(decode_content=True)]
</code></pre>

<p>but it just looks too manual for such a common operation</p>
",0,1406208360,python;python-3.x;urllib3,True,700,2,1406285297,https://stackoverflow.com/questions/24935042/connecting-streams-in-python
24820925,Which response goes with which request?,"<p>I'm using <code>Python</code> with <code>OpenERP 7</code>.
I fire requests to partners using <code>urllib3</code>. Some of these requests may be asynchronous.
So I've built a little <code>asyncore</code> server to wait for responses.
But the thing is, the server cannot know which response goes with which request.</p>

<p>In the content of my request, I have a tag named <em>TransactionID</em>.
So far, I tried to link responses with requests using this <em>TransactionID</em>.
But the responses are not the same from one partner to another.
So, what I've done is create a list of possible <em>TransactionID tag structures</em>.</p>

<p>This method works, but it's so ugly.
I was hopping for a better, cleaner solution, if someone knows how to achieve that.</p>

<p>Thanks !</p>

<p><strong>Edit:</strong></p>

<p>I've made a mistake by calling it asynchronous I think.
The partner gives a synchronous response. But it's just to confirm that my request is ok.
Later, the partner will send me a response on a specific url:port on my server. This is the response I'm talking about. Sorry if I've not given enough details.</p>
",0,1405673533,python;openerp-7;asyncore;urllib3,False,47,0,1405674523,https://stackoverflow.com/questions/24820925/which-response-goes-with-which-request
24647851,white spaces in url,"<p>I'm trying to access a url with spaces:</p>

<pre><code>url_string = ""http://api.company.com/SendMessageXml.ashx?SendXML=&lt;company&gt;&lt;User&gt;&lt;Username&gt;username&lt;/Username&gt;&lt;Password&gt;passweord&lt;/Password&gt;&lt;/User&gt;&lt;Content Type=sms&gt;..""
with urllib.request.urlopen(url_string) as url:
    s = url.read()
</code></pre>

<p>The problem is with the space in ""content type"" separates
the string to two different blocks.</p>

<p>How can I send this request?</p>
",1,1404891374,python;python-3.x;urllib3,True,5629,4,1404893106,https://stackoverflow.com/questions/24647851/white-spaces-in-url
12422228,Using python Requests library to consume from Twitter&#39;s user streams - how to detect disconnection?,"<p>I'm trying to use <a href=""http://docs.python-requests.org/en/latest/index.html"">Requests</a> to create a robust way of consuming from Twitter's user streams. So far, I've produced the following basic working example:</p>

<pre class=""lang-py prettyprint-override""><code>""""""
Example of connecting to the Twitter user stream using Requests.
""""""

import sys

import json

import requests

from oauth_hook import OAuthHook

def userstream(access_token, access_token_secret, consumer_key, consumer_secret):
    oauth_hook = OAuthHook(access_token=access_token, access_token_secret=access_token_secret, 
                           consumer_key=consumer_key, consumer_secret=consumer_secret, 
                           header_auth=True)

    hooks = dict(pre_request=oauth_hook)
    config = dict(verbose=sys.stderr)
    client = requests.session(hooks=hooks, config=config)

    data = dict(delimited=""length"")
    r = client.post(""https://userstream.twitter.com/2/user.json"", data=data, prefetch=False)

    # TODO detect disconnection somehow
    # https://github.com/kennethreitz/requests/pull/200/files#L13R169
    # Use a timeout? http://pguides.net/python-tutorial/python-timeout-a-function/
    for chunk in r.iter_lines(chunk_size=1):
        if chunk and not chunk.isdigit():
            yield json.loads(chunk)

if __name__ == ""__main__"":
    import pprint
    import settings
    for obj in userstream(access_token=settings.ACCESS_TOKEN, access_token_secret=settings.ACCESS_TOKEN_SECRET, consumer_key=settings.CONSUMER_KEY, consumer_secret=settings.CONSUMER_SECRET):
        pprint.pprint(obj)
</code></pre>

<p>However, I need to be able to handle disconnections gracefully. Currently, when the stream disconnects, the above just hangs, and there are no exceptions raised.</p>

<p>What would be the best way to achieve this? Is there a way to detect this through the urllib3 connection pool? Should I use a timeout?</p>
",7,1347616722,python;http;python-requests;urllib3,False,754,1,1403545404,https://stackoverflow.com/questions/12422228/using-python-requests-library-to-consume-from-twitters-user-streams-how-to-de
24259079,Python urllib3 urlopen does not use retries whith specific errors,"<p>Why <code>urlopen</code> from <code>urllib3</code> does not retry when it hits a <code>ReadTimeoutError</code> ? Am I missing something ? I can get around this by using my own retry code, but I would prefer to use the one from the module.</p>

<p>I've even tried to explicitely put <code>retries=3</code> as an argument of <code>HTTPSConnectionPool.request()</code> but it does not help...</p>

<p>Here's my code:</p>

<pre><code>conn = urllib3.HTTPConnectionPool(host, port=port)
return conn.request(method, api_url, data, retries=3, 
                    timeout=urllib3.Timeout(connect=2, read=3))
</code></pre>

<p>Thanks !</p>

<p>EDIT:</p>

<p>Now, the response with some debug info (it's a log from openERP):</p>

<pre><code>2014-06-18 08:36:42,791 5585 DEBUG test 
openerp.addons.myaddon.xml_request.xml_request: args prepared
2014-06-18 08:36:42,791 DEBUG Added an stderr logging handler to logger: 
urllib3
2014-06-18 08:36:42,791 5585 DEBUG test urllib3: Added an stderr logging 
handler to logger: urllib3
2014-06-18 08:36:42,792 INFO Starting new HTTPS connection (1): 
some.host.com
2014-06-18 08:36:42,792 5585 INFO test urllib3.connectionpool: Starting new 
HTTPS connection (1): some.host.com
2014-06-18 08:36:43,699 DEBUG Setting read timeout to 10
2014-06-18 08:36:43,699 5585 DEBUG test urllib3.connectionpool: Setting read 
timeout to 10
2014-06-18 08:36:53,722 5585 ERROR test openerp.osv.osv: Uncaught exception
Traceback (most recent call last):
  File ""/home/openerp/server/openerp/osv/osv.py"", line 132, in wrapper
    return f(self, dbname, *args, **kwargs)
  File ""/home/openerp/server/openerp/osv/osv.py"", line 199, in execute
    res = self.execute_cr(cr, uid, obj, method, *args, **kw)
  File ""/home/openerp/server/openerp/osv/osv.py"", line 187, in execute_cr
    return getattr(object, method)(cr, uid, *args, **kw)
  File ""/home/openerp/custom_addons/myaddon/xml_request/xml_request.py"", 
  line 260, in button_test
    self.request(cr, uid, ids[0], data, context=context)
  File ""/home/openerp/custom_addons/myaddon/xml_request/xml_request.py"", 
  line 234, in request
    config_rec.xml_port, 'POST', api_url, vals)
  File ""/home/openerp/custom_addons/myaddon/xml_request/xml_request.py"", 
  line 208, in connAndReq
    timeout=urllib3.Timeout(connect=5, read=10))
  File ""/usr/lib/python2.7/dist-packages/urllib3/request.py"", line 79, in 
  request
    **urlopen_kw)
  File ""/usr/lib/python2.7/dist-packages/urllib3/request.py"", line 142, in 
  request_encode_body
    **urlopen_kw)
  File ""/usr/lib/python2.7/dist-packages/urllib3/connectionpool.py"", line 
  574, in urlopen
    raise ReadTimeoutError(self, url, ""Read timed out."")
ReadTimeoutError: HTTPSConnectionPool(host=u'some.host.com', port=443): Read 
timed out.
</code></pre>

<p>As you can see, the <code>ReadTimeoutError</code> is thrown just after the expiration time, with no retries.</p>
",3,1402993696,python;urllib3,True,1600,1,1403220197,https://stackoverflow.com/questions/24259079/python-urllib3-urlopen-does-not-use-retries-whith-specific-errors
24160244,Why HTTPSConnectionPool doesn&#39;t work when PoolManager does?,"<p>I have tested a 'POST' request with both PoolManager and HTTPSConnectionPool. The first one works, the other throw me a :</p>

<pre><code>urllib3.exceptions.MaxRetryError: 
HTTPSConnectionPool(host='https://some.url.com', port=443): 
Max retries exceeded with url: /some-api (Caused by &lt;class 'socket.gaierror'&gt;:
[Errno -2] Name or service not known)
</code></pre>

<p>Here's my code for PoolManager:</p>

<pre><code>import urllib3

HOST = 'https://some.url.com'
PORT = 443
PATH = '/some-api'
xml_request = '&lt;some xml tree/&gt;'

manager = urllib3.PoolManager()
res = manager.request('POST', HOST+PATH, {'req':xml_request})
</code></pre>

<p>and for HTTPSConnectonPool:</p>

<pre><code>manager = urllib3.HTTPSConnectionPool(HOST, port=PORT)
res = manager.request('POST', PATH, {'req':xml_request})
</code></pre>
",5,1402481420,python;urllib3,True,6016,1,1402481794,https://stackoverflow.com/questions/24160244/why-httpsconnectionpool-doesnt-work-when-poolmanager-does
24152238,How to get content of url by python (must be login in site),"<p>I want to get content of webpage by python but it has login .<br>
so I must first login in to site then get my content.<br>
How can I do that?</p>
",-1,1402441343,python;curl;urllib2;urllib;urllib3,False,73,1,1402441823,https://stackoverflow.com/questions/24152238/how-to-get-content-of-url-by-python-must-be-login-in-site
23938059,Accessing Imgur API with Python 3.4.1 and Urllib3,"<p>I am trying to wrap my head around the Imgur API. I have found some good examples of how to send the authorization header to Imgur, however they all use urllib2, and I apparently, using pyhton 3.4.1 can only use urllib3.</p>

<p>So I have tried a couple of things and none of them seem to be working.</p>

<p>from <a href=""https://stackoverflow.com/questions/11335825/authentication-with-urllib3"">this post</a> I tried using the basic_auth header:</p>

<pre><code>http = urllib3.PoolManager()
header = urllib3.make_headers(basic_auth=""Client-ID"" + CLIENT_ID)
r = http.request('GET', 'https://api.imgur.com/3/gallery/r/pics', headers=header)
</code></pre>

<p>that gives me a 403 error.</p>

<p>from <a href=""https://stackoverflow.com/questions/13918823/403-error-while-getting-data-from-imgur-api-where-am-i-going-wrong"">this post</a> I tried this method instead:</p>

<pre><code>http = urllib3.PoolManager()
header= {""Content-Type"": ""text"", ""Authorization"": ""Client-ID"" + CLIENT_ID}
r = http.request('GET', 'https://api.imgur.com/3/gallery/r/pics', headers=header)
</code></pre>

<p>that also returns a 403.</p>

<p>Now however I have got a step closer by reading the urllib3 documents and tried sending the Authorization as a field instead.</p>

<pre><code>http = urllib3.PoolManager()
r = http.request('GET', 'https://api.imgur.com/3/gallery/r/pics', fields={""Authorization"": ""Client-ID "" + CLIENT_ID})
</code></pre>

<p>this however returns a 401.</p>

<p>so can some one help me to figure out basic anonymous interaction with the Imgur API using these, or other methods?</p>
",1,1401380523,python;python-3.x;imgur;urllib3,True,580,1,1401381904,https://stackoverflow.com/questions/23938059/accessing-imgur-api-with-python-3-4-1-and-urllib3
21602924,what is the proper way to set a global socket timeout in python?,"<p>I am using a number of different libraries, which each of which use different underlying ways to make socket connections, like urllib3, requests, and httplib. this is in python 2.7.5.</p>

<p>periodically, the program gets hung up in one of the libraries waiting for a network IO to complete. </p>

<p>I have tried socket.setdefaulttimeout(1.0) and socket._GLOBAL_DEFAULT_TIMEOUT = 1.0, and neither had any effect on my program.</p>

<p>what is the proper way to set a global socket timeout?</p>
",2,1391689481,python;sockets;python-requests;httplib;urllib3,True,6431,1,1391695165,https://stackoverflow.com/questions/21602924/what-is-the-proper-way-to-set-a-global-socket-timeout-in-python
21235416,"Python, Catch timeout during stream request","<p>I'm reading XML events with the requests library as stated in the code below. How do I raise a connection-lost error once the request is started? The Server is emulating a HTTP push / long polling -> <a href=""http://en.wikipedia.org/wiki/Push_technology#Long_polling"" rel=""nofollow"">http://en.wikipedia.org/wiki/Push_technology#Long_polling</a> and will not end by default.
If there is no new message after 10minutes, the while loop should be exited.  </p>

<pre><code>import requests
from time import time


if __name__ == '__main__':
    #: Set a default content-length
    content_length = 512
    try:
        requests_stream = requests.get('http://agent.mtconnect.org:80/sample?interval=0', stream=True, timeout=2)
        while True:
            start_time = time()
            #: Read three lines to determine the content-length         
            for line in requests_stream.iter_lines(3, decode_unicode=None):
                if line.startswith('Content-length'):
                    content_length = int(''.join(x for x in line if x.isdigit()))
                    #: pause the generator
                    break

            #: Continue the generator and read the exact amount of the body.        
            for xml in requests_stream.iter_content(content_length):
                print ""Received XML document with content length of %s in %s seconds"" % (len(xml), time() - start_time)
                break

    except requests.exceptions.RequestException as e:
        print('error: ', e)
</code></pre>

<p>The server push could be tested with curl via command line:</p>

<pre><code>curl http://agent.mtconnect.org:80/sample\?interval\=0
</code></pre>
",4,1390224691,python;python-requests;urllib3,False,2709,1,1390229564,https://stackoverflow.com/questions/21235416/python-catch-timeout-during-stream-request
20437753,Python Requests library can&#39;t handle redirects for HTTPS URLs when behind a proxy,"<p>I think I've discovered a problem with the Requests library's handling of redirects when using HTTPS.  As far as I can tell, this is only a problem when the server redirects the Requests client to another HTTPS resource.</p>

<p>I can assure you that the proxy I'm using supports HTTPS and the CONNECT method because I can use it with a browser just fine.  I'm using version 2.1.0 of the Requests library which is using 1.7.1 of the urllib3 library.</p>

<p>I watched the transactions in wireshark and I can see the first transaction for <a href=""https://www.paypal.com/"" rel=""nofollow"">https://www.paypal.com/</a> but I don't see anything for <a href=""https://www.paypal.com/home"" rel=""nofollow"">https://www.paypal.com/home</a>. I keep getting timeouts when debugging any deeper in the stack with my debugger so I don't know where to go from here.  I'm definitely not seeing the request for /home as a result of the redirect.  So it must be erroring out in the code before it gets sent to the proxy.</p>

<p>I want to know if this truly is a bug or if I am doing something wrong.  It is really easy to reproduce so long as you have access to a proxy that you can send traffic through.  See the code below:</p>

<pre class=""lang-py prettyprint-override""><code>import requests

proxiesDict = {
    'http': ""http://127.0.0.1:8080"",
    'https': ""http://127.0.0.1:8080""
}

# This fails with ""requests.exceptions.ProxyError: Cannot connect to proxy. Socket error: [Errno 111] Connection refused."" when it tries to follow the redirect to /home
r = requests.get(""https://www.paypal.com/"", proxies=proxiesDict)
# This succeeds.
r = requests.get(""https://www.paypal.com/home"", proxies=proxiesDict)
</code></pre>

<p>This also happens when using urllib3 directly. It is probably mainly a bug in urllib3, which Requests uses under the hood, but I'm using the higher level requests library.  See below:</p>

<pre><code>proxy = urllib3.proxy_from_url('http://127.0.0.1:8080/')

# This fails with the same error as above.
res = proxy.urlopen('GET', https://www.paypal.com/)
# This succeeds
res = proxy.urlopen('GET', https://www.paypal.com/home)
</code></pre>

<p>Here is the traceback when using Requests:</p>

<pre><code>Traceback (most recent call last):
  File ""tests/downloader_tests.py"", line 22, in test_proxy_https_request
    r = requests.get(""https://www.paypal.com/"", proxies=proxiesDict)
  File ""/usr/local/lib/python2.7/dist-packages/requests/api.py"", line 55, in get
    return request('get', url, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/requests/api.py"", line 44, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/requests/sessions.py"", line 382, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/requests/sessions.py"", line 505, in send
    history = [resp for resp in gen] if allow_redirects else []
  File ""/usr/local/lib/python2.7/dist-packages/requests/sessions.py"", line 167, in resolve_redirects
    allow_redirects=False,
  File ""/usr/local/lib/python2.7/dist-packages/requests/sessions.py"", line 485, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/requests/adapters.py"", line 375, in send
    raise ProxyError(e)
requests.exceptions.ProxyError: Cannot connect to proxy. Socket error: [Errno 111] Connection refused.
</code></pre>

<p><strong>Update:</strong></p>

<p>The problem only seems to happen with a 302 (Found) redirect not with the normal 301 redirects (Moved Permanently).  Also, I noticed that with the Chrome browser, Paypal doesn't return a redirect.  I do see the redirect when using Requests - even though I'm borrowing Chrome's User Agent for this experiment.  I'm looking for more URLs that return a 302 in order to get more data points.</p>

<p>I need this to work for all URLs or at least understand why I'm seeing this behavior.</p>
",4,1386391912,python;https;proxy;python-requests;urllib3,True,4301,1,1386608737,https://stackoverflow.com/questions/20437753/python-requests-library-cant-handle-redirects-for-https-urls-when-behind-a-prox
19757721,web crawler cannot exceed about 1MB/sec speed,"<p>I am building a webcrawler that gets 1-3 pages off a list of millions of domains, I am using Python with multi threading, i have tried multithreading with httplib, httplib2, urllib, urllib2, urllib3, requests, and curl(fastest of the bunch) as well as twisted, and scrapy but none of them are allowing me to use up more than about 10 mbits of bandwidth( I have 60 mbit speed), usually maxes out at around 100-300 threads and after that it causes failed requests.  I have also had this problem with php/curl.  I have a scraper that scrapes from google plus pages with urllib3 and the Threads module (Python) and that maxes out my 100mbit connection ( I believe this may be because it is re-using an open socket with the same host and google has a fast network response)</p>

<p>here is an example of one of my scripts using pycurl I am reading the urls from a csv file containing the urls.</p>

<pre><code>import pycurl
from threading import Thread
from Queue import Queue
import cStringIO


def get(readq,writeq):
    buf = cStringIO.StringIO()
    while True:
        url=readq.get()

        c = pycurl.Curl()
        c.setopt(pycurl.TIMEOUT, 15)
        c.setopt(pycurl.FOLLOWLOCATION, 1)
        c.setopt(pycurl.USERAGENT, 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:24.0) Gecko/20100101 Firefox/24.0')
        c.setopt(c.WRITEFUNCTION, buf.write)
        c.setopt(c.URL, url)
        try:
            c.perform()
            writeq.put(url+'  '+str(c.getinfo(pycurl.HTTP_CODE)))
        except:
            writeq.put('error  '+url)
print('hi')
readq=Queue()
writeq=Queue()

import csv
reader=csv.reader(open('alldataunq2.csv'))
sites = []
ct=0
for l in reader:
    if l[3] != '':
        readq.put('http://'+l[3])
        ct+=1
        if ct &gt; 100000:
            break

t=[]
for i in range(100):
    Thread(target=get,args=(readq,writeq)).start()

while True:
    print(writeq.get())
</code></pre>

<p>the bottleneck is definitely network IO as my processor/memory is barely being used.  Has anyone had success in writing a similar scraper that was able to use a full 100mbit connection or more?</p>

<p>any input on how I can increase the speed of my scraping code is greatly appreciated</p>
",2,1383510543,python;web-scraping;twisted;scrapy;urllib3,True,910,3,1383647127,https://stackoverflow.com/questions/19757721/web-crawler-cannot-exceed-about-1mb-sec-speed
19477214,urllib3 on python 2.7 SNI error on Google App Engine,"<p>I'm trying to download an HTTPS page from my site hosted on Google App Engine with SNI.
No matter what library I use, I get the following error:</p>

<pre><code>[Errno 8] _ssl.c:504: EOF occurred in violation of protocol
</code></pre>

<p>I've tried solving the error in many ways, including using the urllib3 openssl monkeypatch:</p>

<pre><code>from urllib3.contrib import pyopenssl
pyopenssl.inject_into_urllib3
</code></pre>

<p>But I always get the same error mentioned above.</p>

<p>Any ideas?</p>
",5,1382271412,python;google-app-engine;sni;urllib3,True,5868,1,1382272797,https://stackoverflow.com/questions/19477214/urllib3-on-python-2-7-sni-error-on-google-app-engine
19233001,How do I pass raw POST data into urllib3?,"<p>Trying to use urllib3 to post JSON-encoded data.
Just want my POST payload to be raw JSON string, with content type application/json.
I just cannot see how to do this.</p>

<p>The urllib3 documentation describes posting data in ""fields"", i.e. dicts with (key,value) pairs, like how HTML forms are URL-encoded with the URL. But I don't want to do that.</p>

<p>The closest I've been able to get is this (I just guessed where to put the data, as it's not documented anywhere that I can find):</p>

<pre><code>http = urllib3.PoolManager()
headers = urllib3.util.make_headers(basic_auth=key+"":"")
r = http.request_encode_body('POST', path, json.dumps(payload), headers=headers)
</code></pre>

<p>which causes this urllib3 error:</p>

<pre><code>File ""C:\Python27\lib\site-packages\urllib3-1.7.1-py2.7.egg\urllib3\filepost.py"", line 44, in iter_field_objects
yield RequestField.from_tuples(*field)
TypeError: from_tuples() takes exactly 3 arguments (2 given)
</code></pre>

<p>Thanks for any pointers!</p>
",5,1381174181,python;post;urllib3,True,15615,1,1381179175,https://stackoverflow.com/questions/19233001/how-do-i-pass-raw-post-data-into-urllib3
17662039,Can not send attachments via multidict as suggested in mailgun Api,"<p>I'm using python 2.7 and requests 
This is taken from mailgun's docs :</p>

<pre><code>def send_inline_image():
    return requests.post(
        ""https://api.mailgun.net/v2/samples.mailgun.org/messages"",
        auth=(""api"", ""key-3ax6xnjp29jd6fds4gc373sgvjxteol0""),
        files=MultiDict([(""inline"", open(""files/test.jpg""))]),
        data={""from"": ""Excited User &lt;me@samples.mailgun.org&gt;"",
              ""to"": ""sergeyo@profista.com"",
              ""subject"": ""Hello"",
              ""text"": ""Testing some Mailgun awesomness!"",
              ""html"": '&lt;html&gt;Inline image here: &lt;img src=""cid:test.jpg""&gt;&lt;/html&gt;'})
</code></pre>

<p>When trying to send the attchments as described I get the following error:</p>

<pre><code>    files=MultiDict([(""inline"", open(""embeddedImages/bg.png""))])
  File ""build\bdist.win32\egg\requests\api.py"", line 87, in post
  File ""build\bdist.win32\egg\requests\safe_mode.py"", line 37, in wrapped
  File ""build\bdist.win32\egg\requests\api.py"", line 42, in request
  File ""build\bdist.win32\egg\requests\sessions.py"", line 230, in request
  File ""build\bdist.win32\egg\requests\models.py"", line 507, in send
  File ""build\bdist.win32\egg\requests\models.py"", line 366, in _encode_files
  File ""build\bdist.win32\egg\requests\packages\urllib3\filepost.py"", line 80, i
n encode_multipart_formdata
TypeError: 'list' does not have the buffer interface
</code></pre>

<p>Any ideas?</p>

<p>Regards,
Omer.</p>
",2,1373915725,python;request;urllib3;mailgun,False,329,0,1373915725,https://stackoverflow.com/questions/17662039/can-not-send-attachments-via-multidict-as-suggested-in-mailgun-api
12429088,Retrieve JPG pictures from URL (Python),"<p>I'm trying to retrieve JPG images from http ULRs (to display them in a GUI) with the following Python code:</p>

<pre><code>import urllib3
from cStringIO import StringIO
from PIL import Image

conn = urllib3.connection_from_url('http://www.solarspace.co.uk/')
img_file = conn.urlopen('GET', 'http://www.solarspace.co.uk/PlanetPics/Neptune/NeptuneAlt1.jpg')
image = StringIO(img_file.read())
image.seek(0)
resized_image = Image.open(image)
</code></pre>

<p>However, this gives me this error message: ""IOError: cannot identify image file"".</p>

<p>The reason why I'm using urllib3 is because I need a persistent connection (to send multiple requests), which is not available with urllib/urllib2.</p>

<p>Thanks in advance.</p>
",1,1347642753,python;python-imaging-library;urllib3,True,10168,3,1373189321,https://stackoverflow.com/questions/12429088/retrieve-jpg-pictures-from-url-python
17257912,How to print raw html string using urllib3？,"<p>I use below statment to get html string:<br></p>

<pre><code>import urllib3

url ='http://urllib3.readthedocs.org/'
http_pool = urllib3.connection_from_url(url)
r = http_pool.urlopen('GET',url)

print (r.data)
</code></pre>

<p>But the output is :<br></p>

<pre><code>b'&lt;!DOCTYPE html PUBLIC ""-//W3C//DTD XHTML 1.0 Strict//EN"" ""b'\n&lt;!DOCTYPE html PUBLIC ""-//W3C//DTD XHTML 1.0 Transitional//EN""\n  ""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd""&gt;\n\n\n&lt;html xmlns=""http://www.w3.org/1999/xhtml""&gt;\n  &lt;head&gt;\n    &lt;meta http-equiv=""Content-Type"" content=""text/html; charset=utf-8"" /&gt;\n    \n\n   .......................................\n&lt;/script&gt;\n\n\n\n  &lt;/body&gt;\n&lt;/html&gt;''
</code></pre>

<p>How can I get a raw html string?</p>
",8,1371964562,python;urllib3,True,14707,1,1371966808,https://stackoverflow.com/questions/17257912/how-to-print-raw-html-string-using-urllib3
17188348,HTTP Request/Response from a file and back,"<p>What is the best way to parse HTTP request/response from a file to some kind of standard python objects and later dump it to a file?</p>

<p>What are the standard HTTP Request/Response objects in python. I need some kind of wrapper as output, just like <code>requests.Response</code> or <code>httplib.HTTPResponse</code> and also for request and a way to parse a file and load data to these wrappers/objects.</p>

<p>I don't know if httplib.HTTPResponse is some basic/standard class for that and why there is no HTTPRequest class and moreover i don't know how to convert them from plain text files to objects and the other way around. </p>
",2,1371635933,python;python-requests;httplib;urllib3,False,782,0,1371635933,https://stackoverflow.com/questions/17188348/http-request-response-from-a-file-and-back
17030590,"ValueError: need more than 1 value to unpack, PoolManager request","<p>The following code in <code>utils.py</code></p>

<pre><code>manager = PoolManager()
data = json.dumps(dict) #takes in a python dictionary of json
manager.request(""POST"", ""https://myurlthattakesjson"", data)
</code></pre>

<p>Gives me <code>ValueError: need more than 1 value to unpack</code> when the server is run. Does this most likely mean that the JSON is incorrect or something else?</p>
",1,1370889676,python;django;urllib3,True,1922,1,1370892127,https://stackoverflow.com/questions/17030590/valueerror-need-more-than-1-value-to-unpack-poolmanager-request
16577632,Convert string to JSON in Python?,"<p>I'm trying to convert a string, generated from an http request with urllib3.</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;pyshell#16&gt;"", line 1, in &lt;module&gt;
    data = json.load(data)
  File ""C:\Python27\Lib\json\__init__.py"", line 286, in load
    return loads(fp.read(),
AttributeError: 'str' object has no attribute 'read'

&gt;&gt;&gt; import urllib3
&gt;&gt;&gt; import json
&gt;&gt;&gt; request = #urllib3.request(method, url, fields=parameters)
&gt;&gt;&gt; data = request.data
</code></pre>

<p>Now... When trying the following, I get that error...</p>

<pre><code>&gt;&gt;&gt; json.load(data) # generates the error
&gt;&gt;&gt; json.load(request.read()) # generates the error
</code></pre>

<p>Running <code>type(data)</code> and <code>type(data.read())</code> both return <code>&lt;type 'str'&gt;</code></p>

<pre><code>data = '{""subscriber"":""0""}}\n'
</code></pre>
",16,1368667900,python;json;urllib3,True,58329,1,1368668384,https://stackoverflow.com/questions/16577632/convert-string-to-json-in-python
16573809,Multipart form encoding and posting with urllib3,"<p>I'm attempting to upload a <code>csv</code> file to <a href=""http://www.ipm.ucdavis.edu/calludt.cgi/DDMODEL?MODEL=CM&amp;CROP=apples"" rel=""nofollow"">this site</a>. However, I've encountered a few issues, and I <em>think</em> it stems from the incorrect <code>mimetype</code> (maybe).  </p>

<p>I'm attempting to manually post the file via <code>urllib2</code>, so my code looks as follows:</p>

<pre><code>import urllib
import urllib2
import mimetools, mimetypes
import os, stat
from cStringIO import StringIO

#============================
# Note: I found this recipe online. I can't remember where exactly though.. 
#=============================

class Callable:
    def __init__(self, anycallable):
        self.__call__ = anycallable

# Controls how sequences are uncoded. If true, elements may be given multiple values by
#  assigning a sequence.
doseq = 1

class MultipartPostHandler(urllib2.BaseHandler):
    handler_order = urllib2.HTTPHandler.handler_order - 10 # needs to run first

    def http_request(self, request):
        data = request.get_data()
        if data is not None and type(data) != str:
            v_files = []
            v_vars = []
            try:
                 for(key, value) in data.items():
                     if type(value) == file:
                         v_files.append((key, value))
                     else:
                         v_vars.append((key, value))
            except TypeError:
                systype, value, traceback = sys.exc_info()
                raise TypeError, ""not a valid non-string sequence or mapping object"", traceback

            if len(v_files) == 0:
                data = urllib.urlencode(v_vars, doseq)
            else:
                boundary, data = self.multipart_encode(v_vars, v_files)

                contenttype = 'multipart/form-data; boundary=%s' % boundary
                if(request.has_header('Content-Type')
                   and request.get_header('Content-Type').find('multipart/form-data') != 0):
                    print ""Replacing %s with %s"" % (request.get_header('content-type'), 'multipart/form-data')
                request.add_unredirected_header('Content-Type', contenttype)

            request.add_data(data)

        return request

    def multipart_encode(vars, files, boundary = None, buf = None):
        if boundary is None:
            boundary = mimetools.choose_boundary()
        if buf is None:
            buf = StringIO()
        for(key, value) in vars:
            buf.write('--%s\r\n' % boundary)
            buf.write('Content-Disposition: form-data; name=""%s""' % key)
            buf.write('\r\n\r\n' + value + '\r\n')
        for(key, fd) in files:
            file_size = os.fstat(fd.fileno())[stat.ST_SIZE]
            filename = fd.name.split('/')[-1]
            contenttype = mimetypes.guess_type(filename)[0] or 'application/octet-stream'
            buf.write('--%s\r\n' % boundary)
            buf.write('Content-Disposition: form-data; name=""%s""; filename=""%s""\r\n' % (key, filename))
            buf.write('Content-Type: %s\r\n' % contenttype)
            # buffer += 'Content-Length: %s\r\n' % file_size
            fd.seek(0)
            buf.write('\r\n' + fd.read() + '\r\n')
        buf.write('--' + boundary + '--\r\n\r\n')
        buf = buf.getvalue()
        return boundary, buf
    multipart_encode = Callable(multipart_encode)

    https_request = http_request

    import cookielib
    cookies = cookielib.CookieJar()
    opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookies),
            MultipartPostHandler)

    opener.addheaders = [(
            'User-agent', 
            'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.6) Gecko/20070725 Firefox/2.0.0.6'
        )]


    params = {""FILENAME"" : open(""weather_scrape.csv"", 'rb'),
            'CGIREF' : '/calludt.cgi/DDFILE1',
            'USE':'MODEL',
            'MODEL':'CM',
            'CROP':'APPLES',
            'METHOD': 'SS',
            'UNITS' : 'E',
            'LOWTHRESHOLD': '50',
            'UPTHRESHOLD': '88',
            'CUTOFF':'H',
            'COUNTY':'AL',
            'ACTIVE':'Y',
            'FROMMONTH':'3',
            'FROMDAY':'15',
            'FROMYEAR': '2013',
            'THRUMONTH':'5',
            'THRUDAY':'13',
            'THRUYEAR':'2013',
            'DATASOURCE' : 'FILE'
            }

    response = opener.open(""http://www.ipm.ucdavis.edu/WEATHER/textupload.cgi"", params)
</code></pre>

<p>Now, when I post this, all seems to be fine, until I click the submit button on the subsequent webpage that the first <code>POST</code> returns. I then get this error message: </p>

<pre><code>ERROR (bad data) in file 'weather.csv' at line 135.

Data record = [--192.168.117.2.1.4404.1368589639.796.1--]

Too few values found. Check delimiter specification.
</code></pre>

<p>Now, upon investigating the post request that gets made when I do the actions in browser, I notice that the <code>content-type</code> is very specific, namely: </p>

<pre><code>------WebKitFormBoundaryfBp6Jfhv7LlPZLKd
Content-Disposition: form-data; name=""FILENAME""; filename=""weather.csv""
Content-Type: application/vnd.ms-excel
</code></pre>

<p>I'm not entirely sure it the content-type is what's causing the error, but.. it's what I'm currently ruling out (as I don't know what is actually going wrong.) I don't see any way to set the content type via urllib2, so after some googling, I stumbled upon <code>urllib3.</code> </p>

<p><code>Urllib3</code> has a build in file posting capability, but I'm not entirely sure how to use it. </p>

<pre><code>Filepost

urllib3.filepost.encode_multipart_formdata(fields, boundary=None)
Encode a dictionary of fields using the multipart/form-data MIME format.

Parameters: 
fields –
Dictionary of fields or list of (key, value) or (key, value, MIME type) field tuples. The key is treated as the field name, and the value as the body of the form-data bytes. If the value is a tuple of two elements, then the first element is treated as the filename of the form-data section and a suitable MIME type is guessed based on the filename. If the value is a tuple of three elements, then the third element is treated as an explicit MIME type of the form-data section.
Field names and filenames must be unicode.
boundary – If not specified, then a random boundary will be generated using mimetools.choose_boundary().
urllib3.filepost.iter_fields(fields)
Iterate over fields.

Supports list of (k, v) tuples and dicts.
</code></pre>

<p>Using this library, I tried encoding the values as a decribes in the doc, but I'm getting errors. </p>

<p>I tried initially, just to test things out, as a <code>dict</code>. </p>

<pre><code>params = {""FILENAME"" : open(""weather.csv"", 'rb'),
            'CGIREF' : '/calludt.cgi/DDFILE1',
            'USE':'MODEL',
            'MODEL':'CM',
            'CROP':'APPLES',
            'METHOD': 'SS',
            'UNITS' : 'E',
            'LOWTHRESHOLD': '50',
            'UPTHRESHOLD': '88',
            'CUTOFF':'H',
            'COUNTY':'AL',
            'ACTIVE':'Y',
            'FROMMONTH':'3',
            'FROMDAY':'15',
            'FROMYEAR': '2013',
            'THRUMONTH':'5',
            'THRUDAY':'13',
            'THRUYEAR':'2013',
            'DATASOURCE' : 'FILE'
            }

    values = urllib3.filepost.encode_multipart_formdata(params)
</code></pre>

<p>however, this raises the following error:</p>

<pre><code>    values = urllib3.filepost.encode_multipart_formdata(params)
  File ""c:\python27\lib\site-packages\urllib3-dev-py2.7.egg\urllib3\filepost.py"", line 90, in encode_multipart_formdata
    body.write(data)
TypeError: 'file' does not have the buffer interface
</code></pre>

<p>Not sure what caused it, I tried passing in a list of tuples (key, value, mimetype), but that also throws an error: </p>

<pre><code>params = [
        (""FILENAME"" , open(""weather_scrape.csv""), 'application/vnd.ms-excel'),
        ('CGIREF' , '/calludt.cgi/DDFILE1'),
        ('USE','MODEL'),
        ('MODEL','CM'),
        ('CROP','APPLES'),
        ('METHOD', 'SS'),
        ('UNITS' , 'E'),
        ('LOWTHRESHOLD', '50'),
        ('UPTHRESHOLD', '88'),
        ('CUTOFF','H'),
        ('COUNTY','AL'),
        ('ACTIVE','Y'),
        ('FROMMONTH','3'),
        ('FROMDAY','15'),
        ('FROMYEAR', '2013'),
        ('THRUMONTH','5'),
        ('THRUDAY','13'),
        ('THRUYEAR','2013'),
        ('DATASOURCE' , 'FILE)')
        ]

    values = urllib3.filepost.encode_multipart_formdata(params)



&gt;&gt;ValueError: too many values to unpack
</code></pre>
",3,1368647506,python;urllib2;urllib3,True,11632,1,1368650538,https://stackoverflow.com/questions/16573809/multipart-form-encoding-and-posting-with-urllib3
16263265,"python web-client multipile set-cookie header, get raw set-cookie header","<p>i am using python-requests on python 2.7,
i am trying to authenticate against a web-server that returns multiple set-cookie headers in the response.
python-requests keeps only one of those cookies.</p>

<p>i couldn't find a python 'http' client that either handles this problem correctly,
or allows access to the raw header with the 'set-cookie' statements in order to manually deal with the problem.</p>

<p>i found a few statements in the internet that claim that this problem was solved in python3, however no further details or examples were provided.</p>

<p>would appreciate any assistance. </p>

<p>thanks</p>
",1,1367155066,python;urllib2;python-requests;httplib;urllib3,True,1670,1,1367166685,https://stackoverflow.com/questions/16263265/python-web-client-multipile-set-cookie-header-get-raw-set-cookie-header
15293522,How do I get HTTP VERB from Python Requests &#39;Response&#39; objects?,"<p>From looking at their source it appears that the <code>method</code> member attribute is what I want.</p>

<p><a href=""https://github.com/kennethreitz/requests/blob/master/requests/models.py"" rel=""nofollow"">https://github.com/kennethreitz/requests/blob/master/requests/models.py</a></p>

<p>To summarise by example, this is what I want:</p>

<pre><code>&gt;&gt;&gt; r = requests.get(""http://httpbin.org/get"")
&gt;&gt;&gt; print r.method
'GET'
</code></pre>

<p>However I can't figure out if there's a way to get it (without writing my own hacky wrapper)...</p>
",1,1362743972,python;wrapper;python-requests;urllib3,True,1925,1,1362744390,https://stackoverflow.com/questions/15293522/how-do-i-get-http-verb-from-python-requests-response-objects
14711153,How to authenticate with requests module using a trust store?,"<p>I am currently writing a python language plugin for a compiler I have written that automates http calls for a RESTful API. I have managed to get the login/authentication working using the socket and ssl modules, but this low-level approach seems to create potential problems with parsing the response in order to obtain the authentication token and secret. The requests module seems popular/efficient, however, I cannot seem to get it to function properly for my particular authentication needs. I am using a trust store in the form of a .pem file (containing just a public key) that I converted from my .jks file used to authenticate for the Java plugin. The server expects the username and password to be submitted in the request body in json format. Here is the code I have been trying to use:</p>

<pre><code>#Server and login data
...
host = 'localhost'
port = 8443
pem_file = ""C:\\Users\\aharasta\\pycert.pem""

#Digest password with MD5 algorithm
m = hashlib.md5()
m.update(password)
encrypted_password = m.hexdigest()

url = &lt;url&gt;
data = {'userid': user_name, 'password': encrypted_password}
json_data = json.dumps(data)
headers = {'Content-type': 'application/json', 'Accept': 'text/plain', 'Content \                     
          Length': len(json_data)} 

r = requests.post(url, headers = headers, data = json_data, cert = pem_file)
print(r)
</code></pre>

<p>Upon execution, this code will raise an SSL error stating ""certificate verify failed"". If I add the parameter <code>verify = False</code> or <code>verify = pem_file</code>, I will receive a 404 response from the server. I should also note that when I launch the server in debug mode and execute the request (with one of the verify parameters), it never makes it to the server's authentication methods, or any methods for that matter. Any insight or help on this matter would be <em>greatly</em> appreciated! </p>
",0,1360078557,python;authentication;python-requests;truststore;urllib3,True,10765,1,1360105001,https://stackoverflow.com/questions/14711153/how-to-authenticate-with-requests-module-using-a-trust-store
12322429,Using urllib3 or requests and Celery,"<p>We have a script that downloads documents from various sources periodically. I'm going to move this over to celery, but while doing so, I wanted to take advantage of connection pooling at the same time, but I wasn't sure how to go about it.</p>

<p>My current thought is to do something like this using Requests:</p>

<pre><code>import celery
import requests

s = requests.session()

@celery.task(retry=2)
def get_doc(url):
    doc = s.get(url)
    #do stuff with doc
</code></pre>

<p>But I'm concerned that the connections will stay open indefinitely.</p>

<p>I really only need the connections to stay open so long as I'm processing new documents.</p>

<p>So something like this possible:</p>

<pre><code>import celery
import requests


def get_all_docs()
    docs = Doc.objects.filter(some_filter=True)
    s = requests.session()
    for doc in docs: t=get_doc.delay(doc.url, s)

@celery.task(retry=2)
def get_doc(url):
    doc = s.get(url)
    #do stuff with doc
</code></pre>

<p>However, in this case, I'm not certain that the connection sessions will persist across instances, or if Requests will create new connections once the pickling / unpickling is complete.</p>

<p>Lastly, I could try the experimental support for task decorators on a class method, so something like this:</p>

<pre><code>import celery
import requests


class GetDoc(object):
    def __init__(self):
        self.s = requests.session()

@celery.task(retry=2)
def get_doc(url):
    doc = self.s.get(url)
    #do stuff with doc
</code></pre>

<p>The last one seems like this best approach, and I'm going to test this; however,  I was wondering if anyone here has already done something similar to this, or if not, one of you reading this might have a better approach than one of the above methods.</p>
",6,1347037229,python;django;celery;python-requests;urllib3,False,1462,0,1353828289,https://stackoverflow.com/questions/12322429/using-urllib3-or-requests-and-celery
13532163,Getting this weird &quot;invalid syntax&quot; error when using urllib3/requests,"<p>Following what others suggested in: <a href=""https://stackoverflow.com/questions/13424753/given-a-big-list-of-urls-what-is-a-way-to-check-which-are-active-inactive"">Given a big list of urls, what is a way to check which are active/inactive?</a></p>

<p>Getting the error when trying to install requests, using urllib3.</p>

<p>Trying to install requests.</p>

<pre><code>C:\Users\yao\Desktop\My Downloads\requests-develop\requests-develop&gt;setup.py install
Traceback (most recent call last):
  File ""C:\Users\yao\Desktop\My Downloads\requests-develop\requests-develop\setup.py"", line 6, in &lt;module&gt;
    import requests
  File ""C:\Users\yao\Desktop\My Downloads\requests-develop\requests-develop\requests\__init__.py"", line 52, in &lt;module&gt;
    from . import utils
  File ""C:\Users\yao\Desktop\My Downloads\requests-develop\requests-develop\requests\utils.py"", line 22, in &lt;module&gt;
    from .compat import parse_http_list as _parse_list_header
  File ""C:\Users\yao\Desktop\My Downloads\requests-develop\requests-develop\requests\compat.py"", line 95, in &lt;module&gt;
    from .packages import chardet
  File ""C:\Users\yao\Desktop\My Downloads\requests-develop\requests-develop\requests\packages\__init__.py"", line 3, in &lt;module&gt;
    from . import urllib3
  File ""C:\Users\yao\Desktop\My Downloads\requests-develop\requests-develop\requests\packages\urllib3\__init__.py"", line 16, in &lt;module&gt;
    from .connectionpool import (
  File ""C:\Users\yao\Desktop\My Downloads\requests-develop\requests-develop\requests\packages\urllib3\connectionpool.py"", line 434
    except Empty as e:
                  ^
SyntaxError: invalid syntax
</code></pre>

<p>Trying to use urllib3.</p>

<pre><code>C:\Users\yao\Desktop\ad stuff\find urls&gt;reqs.py
Traceback (most recent call last):
  File ""C:\Users\yao\Desktop\ad stuff\find urls\reqs.py"", line 1, in &lt;module&gt;
    import re, csv, urllib3
  File ""build\bdist.win32\egg\urllib3\__init__.py"", line 16, in &lt;module&gt;
  File ""C:\Python25\lib\site-packages\urllib3-dev-py2.5.egg\urllib3\connectionpool.py"", line 435
    except Empty as e:
                  ^
SyntaxError: invalid syntax
</code></pre>

<p>Just recently installed setuptools, pip, requests, urllib3. In that order. Did I mess up anything? Using python 2.5 btw.</p>
",1,1353685449,python;installation;python-requests;urllib3,True,3273,1,1353828049,https://stackoverflow.com/questions/13532163/getting-this-weird-invalid-syntax-error-when-using-urllib3-requests
12166266,How to make a http request with cookies using urllib3 Python library?,"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://stackoverflow.com/questions/2422922/python-urllib3-and-how-to-handle-cookie-support"">Python urllib3 and how to handle cookie support?</a>  </p>
</blockquote>



<p>I am trying to retrieve source code from a webpage with an already issued cookie and write the source code to a txt file. If I remove the <code>cookies=cookie</code> portion I can retrieve the source code but I need to somehow send the cookie with the http request.</p>

<pre><code>output = open('Filler.txt', 'w+')
http = urllib3.PoolManager()
cookie =('users' , '1597413515')
r = http.request('http://google.com' , 'GET' , cookies=cookie)
output.write(r.data)
output.close()
</code></pre>

<p>I get a KeyError: None</p>
",0,1346182391,python;http;cookies;httprequest;urllib3,True,4724,1,1346328094,https://stackoverflow.com/questions/12166266/how-to-make-a-http-request-with-cookies-using-urllib3-python-library
11964962,How do I respond directly to a post request using urllib3 and app engine?,"<p>I am trying to have a persistent connection to a third party using app engine. Specifically I am hooking into a real time bidding environment where I need to respond in under 100ms, and thus a persistent connection greatly accelerates the process.</p>

<p>In order to do this I am attempting to use urllib3 (if there is a better way please tell me)
When my request handler's post method gets called, I want to write back out to the calling url keeping the connection open. I understand how to open a request with urllib3, but how do I persist the connection that was created when the post method on the handler was called.</p>

<p>At the moment I am trying: </p>

<pre><code>http = urllib3.PoolManager()
r = http.request('POST', self.request.url, fields={""foo"":""bar""})
</code></pre>

<p>But I fear I am opening an entirely new connection doing this.</p>

<p>Thanks,
Sam</p>
",1,1345012145,python;google-app-engine;urllib3,True,911,1,1345016055,https://stackoverflow.com/questions/11964962/how-do-i-respond-directly-to-a-post-request-using-urllib3-and-app-engine
10781424,&#39;HTTPConnectionPool&#39; object has no attribute &#39;get_url&#39;,"<p>this is my codes:</p>

<pre><code>import urllib3

url='http://www.google.com/'

http_pool = urllib3.connection_from_url(url)

content = http_pool.get_url('/')

print (content.info())
print ('----------------------------')
page = content.read()

print (page)
</code></pre>

<p>error:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\python\test.py"", line 24, in &lt;module&gt;
    content = http_pool.get_url('/')
AttributeError: 'HTTPConnectionPool' object has no attribute 'get_url'
</code></pre>

<p>I goolge it ,but nothing relevant.
It's so strange that i can't figure out...</p>
",0,1338193067,python;urllib3,True,1116,1,1338193526,https://stackoverflow.com/questions/10781424/httpconnectionpool-object-has-no-attribute-get-url
10691788,Datamining Multithreading vs Multiprocessing,"<p>I wrote and rewrote my little python application to a point where my current python skills aren't enough. I started with a single threaded application with Beautiful Soup as the parser, changed to lxml. Made the script multi-threaded, i discovered twisted but couldn't change this little snippet to twisted. I will just post this here so maybe you guys can point me to better directions to make this maybe a bit faster. To fetch 150k pages i need like 1 hour at this point. Iam happy with this cause i was 3x slower when i had my first attempt to write it.</p>

<pre class=""lang-py prettyprint-override""><code>#! /usr/bin/python
# coding: ISO-8859-1
import time, PySQLPool, Queue, threading
from urllib3 import connection_from_url
from lxml import etree
import cStringIO as StringIO

headers = {   
           'User-Agent'         : 'Mozilla/4.77 [en] (X11; I; IRIX;64 6.5 IP30)',
           'Accept'             : 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
           'Accept-Language'    : 'en-us;q=0.5,en;q=0.3',
           'Accept-Encoding'    : 'gzip, deflate',
           'Accept-Charset'     : 'utf-8;q=0.7,*;q=0.7'
}

t = time.time()
PySQLPool.getNewPool().maxActiveConnections = 60
db = PySQLPool.getNewConnection(username='user', password='pass', host='127.0.0.1', db='fddb')
pool = connection_from_url('http://fddb.info/', maxsize=60, timeout=150, headers=headers)
detailCounter = 0
urls = {}
queue = Queue.Queue()
out_queue = Queue.Queue()

clean_rows = {
              ""Brennwert"":""details_brennwert"",
              ""Kalorien"":""details_kalorien"",
              ""Protein"":""details_protein"",
              ""Kohlenhydrate"":""details_kohlenhydrate"",
              ""davon Zucker"":""details_zucker"",
              ""davon Polyole"":""details_polyole"",
              ""Fett"":""details_fett"",
              ""Ballaststoffe"":""details_ballaststoffe"",
              ""Broteinheiten"":""details_broteinheit"",
              ""Alkohol"":""details_alkohol"",
              ""Cholesterin"":""details_cholesterin"",
              ""Koffein"":""details_koffein"",
              ""Wassergehalt"":""details_wasser"",
              ""Vitamin C"":""details_vitc"",
              ""Vitamin A"":""details_vita"",
              ""Vitamin D"":""details_vitd"",
              ""Vitamin E"":""details_vite"",
              ""Vitamin B1"":""details_vitb1"",
              ""Vitamin B2"":""details_vitb2"",
              ""Vitamin B6"":""details_vitb6"",
              ""Vitamin B12"":""details_vitb12"",
              ""Natrium"":""details_natrium"",
              ""Eisen"":""details_eisen"",
              ""Zink"":""details_zink"",
              ""Magnesium"":""details_magnesium"",
              ""Chlor"":""details_chlor"",
              ""Mangan"":""details_mangan"",
              ""Schwefel"":""details_schwefel"",
              ""Kalium"":""details_kalium"",
              ""Kalzium"":""details_kalzium"",
              ""Phosphor"":""details_phosphor"",
              ""Kupfer"":""details_kupfer"",
              ""Fluor"":""details_fluor""
              }

def rows_escape(text):
    for item, key in clean_rows.items():
        text = text.replace(item, key)
    text = text.rstrip()
    return text

clean_values = {
         ""kJ""   :"""",
         ""kcal"" :"""",
         ""g""    :"""",
         ""mg""   :"""",
         ""%""    :"""",
         "",""    :""."",
         u""\u03bc"": """"
         }

def values_escape(text):
    for item, key in clean_values.items():
        text = text.replace(item, key)
    text = text.rstrip()
    return text

def insertDetails(container, foods_id):
    c = PySQLPool.getNewQuery(db)
    query_rows = ''
    query_values = ''
    for item in container:
        query_rows += item['row'] + ','
        query_values += item['value'] + ','

    c.Query(""INSERT INTO details (%sdetails_id,foods_id) VALUES (%sNULL,%s)"" % (query_rows, query_values, foods_id))
    c.Query(""UPDATE foods SET foods_check = '1' WHERE foods_id=%d"" % (foods_id))

def getHP(url):
    r = pool.request('GET', '/' + url)
    return r.data

class ThreadUrl(threading.Thread):
    def __init__(self, queue, out_queue):
        threading.Thread.__init__(self)
        self.queue = queue
        self.out_queue = out_queue
    def run(self):
        while True:
            host = self.queue.get()
            data = getHP(host[0])
            self.out_queue.put([data, host[1]])
            self.queue.task_done()

class DatamineThread(threading.Thread):
    def __init__(self, out_queue):
        threading.Thread.__init__(self)
        self.out_queue = out_queue
    def run(self):
        while True:
            global detailCounter

            qData = self.out_queue.get()
            data = qData[0]
            foods_id = qData[1]

            container = []
            parser = etree.HTMLParser(encoding='cp1252')
            tree = etree.parse(StringIO.StringIO(data), parser)
            divx = tree.xpath('//div[@style=""background-color:#f0f5f9;padding:2px 4px;"" or @style=""padding:2px 4px;""]')

            for xdiv in divx:
                x = etree.ElementTree(element=xdiv, parser=parser)

                value = x.xpath('string(//div/text())')
                label = x.xpath('string(//*[self::a or self::span]/text())')

                label = rows_escape(label)

                if not ""[nodata]"" in value:
                    if u""\u03bc"" in value:
                        value = values_escape(value)
                        item4 = 0
                        item4 = float(value)
                        item4 = item4 / 1000
                        container.append({'row':label,'value':str(item4)})
                    else:
                        container.append({'row':label,'value':values_escape(value)})

            detailCounter += 1
            container = tuple(container)
            insertDetails(container, foods_id)

            self.out_queue.task_done()

def main():

    c = PySQLPool.getNewQuery(db)
    c.Query(""SELECT foods_id, foods_url FROM foods WHERE foods_check = 0"")
    urls = c.record

    for i in range(6):
        t = ThreadUrl(queue, out_queue)
        t.setDaemon(True)
        t.start()

    for item in urls:
        queue.put([item['foods_url'], item['foods_id']])

    for i in range(6):
        dt = DatamineThread(out_queue)
        dt.setDaemon(True)
        dt.start()

    queue.join()
    out_queue.join()

main()
db.close
print ""Zeit: %.2f New Details: %d"" % (time.time()-t, detailCounter)
</code></pre>
",0,1337630435,python;twisted;urllib3,True,649,1,1337890044,https://stackoverflow.com/questions/10691788/datamining-multithreading-vs-multiprocessing
9021140,urllib3 maxretryError,"<p>I have just started using urllib3, and I am running into a problem straightaway. According to their manuals, I started off with the simple example:</p>

<pre><code>Python 2.7.1+ (r271:86832, Apr 11 2011, 18:13:53) 
[GCC 4.5.2] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import urllib3
&gt;&gt;&gt; 
&gt;&gt;&gt; http = urllib3.PoolManager()
&gt;&gt;&gt; r = http.request('GET', 'http://google.com/')
</code></pre>

<p>I get thrown the following error:</p>

<pre><code>Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
File ""/usr/local/lib/python2.7/dist-packages/urllib3/request.py"", line 65, in request
**urlopen_kw)
File ""/usr/local/lib/python2.7/dist-packages/urllib3/request.py"", line 78, in request_encode_url
return self.urlopen(method, url, **urlopen_kw)
File ""/usr/local/lib/python2.7/dist-packages/urllib3/poolmanager.py"", line 113, in urlopen
return self.urlopen(method, e.new_url, **kw)
File ""/usr/local/lib/python2.7/dist-packages/urllib3/poolmanager.py"", line 113, in urlopen
return self.urlopen(method, e.new_url, **kw)
File ""/usr/local/lib/python2.7/dist-packages/urllib3/poolmanager.py"", line 113, in urlopen
return self.urlopen(method, e.new_url, **kw)
File ""/usr/local/lib/python2.7/dist-packages/urllib3/poolmanager.py"", line 113, in urlopen
return self.urlopen(method, e.new_url, **kw)
File ""/usr/local/lib/python2.7/dist-packages/urllib3/poolmanager.py"", line 109, in urlopen
return conn.urlopen(method, url, **kw)
File ""/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py"", line 309, in urlopen
raise MaxRetryError(url)
urllib3.exceptions.MaxRetryError: Max retries exceeded for url: http://google.com/
</code></pre>

<p>Any clues as to why this happens? Many thanks.</p>
",12,1327594139,python;urllib2;urllib;urllib3,True,28931,1,1327881676,https://stackoverflow.com/questions/9021140/urllib3-maxretryerror
