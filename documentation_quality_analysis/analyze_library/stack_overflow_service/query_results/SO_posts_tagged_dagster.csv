post_id,title,body,score,creation_date,tags,is_answered,view_count,answer_count,last_activity_date,link
78327619,Dagster dynamically created GraphDefinition not showing up in UI,"<p>I have dagster instance with a job defined, that creates dynamic <code>GraphDefinition</code>s according to configuration and executes them.</p>
<p>This is my <code>repo.py</code></p>
<pre class=""lang-py prettyprint-override""><code>from dagster import op, job, Config, GraphDefinition, NodeInvocation, DependencyDefinition

@op
def name() -&gt; str:
    print('Steve')
    return 'Steve'


class GreetingConfig(Config):
    greeting: str


@op 
def greet(config: GreetingConfig, name: str) -&gt; str:
    print(f'{config.greetig} {name}')
    return f'{config.greeting} {name}'


@op
def run_pipeline(config: dict)
    graph_job = create_graph_job(config).to_job()
    graph_job.execute_in_process(config['run_config'])


@job
def run():
    run_pipeline()


def create_dependency_graph(config: dict):
    op_defs = []
    for op in config.get('ops', []):
        if op['def'] == name.__name__:
            op_defs.append(name)
        elif op['def'] == greet.__name__:
            op_defs.append(greet)

    return build_graph(config, op_defs)

# Logic taken from https://docs.dagster.io/concepts/ops-jobs-graphs/graphs#using-graphdefinitions
def build_graph(config: dict, op_defs: list) -&gt; GraphDefinition:
    deps = {}
    for op_data in config['ops']
        def_name = op_data['def']
        alias = op_data.get('alias', def_name)
        op_deps_entry = {}
        for input_name, input_data in op_data.get('deps', {}).items():
            op_deps_entry[input_name] = DependencyDefinition(node=input_data['op'], output=input_data.get('output', 'result'))
        deps[NodeInvocation(name=def_name, alias=alias)] = op_deps_entry
    
    return GraphDefinition(config['name'], description=config.get('description'), dependencies=deps) 
</code></pre>
<p>I have an external service which executes the <code>run</code> job with the <code>DagsterGraphQLClient</code> like so</p>
<pre class=""lang-py prettyprint-override""><code>client = DagsterGraphQLClient('dagster', port_number=3000)
client.submit_job_execution('run', 
                            run_config={
                                         'ops': {
                                           'run_pipeline': {
                                             'config': {
                                               'name': 'test_pipeline',
                                               'ops': [
                                                 {'def': 'name'}
                                                 {'def': 'greet', 'deps': {'name': {'op': 'name'}}}
                                               ],
                                               'run_config': {
                                                 'ops': {
                                                   'greet': {'config': {'greeting': 'hi'}}
                                                 }
                                               }
                                             }
                                           }
                                         }
                                       }
                           )
</code></pre>
<p>When I execute the client code, I can see in dagster's UI that the job has ran, and completed successfully. I can also see that</p>
<pre><code>Steve
Hi Steve
</code></pre>
<p>is printed to stdout. The problem is, when I look at the flow of execution, I can only see 1 op that ran, <code>run_pipeline</code>. Is there a way I can make dagster detect that <code>greet</code> and <code>name</code> also ran? I want to be able to differentiate between them and perhaps re-execute them separately.</p>
",0,1713175786,dagster,False,12,0,1713178472,https://stackoverflow.com/questions/78327619/dagster-dynamically-created-graphdefinition-not-showing-up-in-ui
78320052,Providing dynamically created pydantic model as type annotation to function without mypy errors,"<p>I am using a pluggable architecture which registers pydantic models with a central registry on startup.</p>
<p>These models are then added to an existing model as a <code>Union</code> field dynamically.</p>
<p>This resulting class is then used by a framework we use (dagster) to provide yaml configuration in a UI.</p>
<p>The issue I am having is that dagster uses the type annotation of an argument called <code>config</code> on decorated functions to register and use these config classes. I am using a factory function to create these functions in the following way:</p>
<pre class=""lang-py prettyprint-override""><code># assets.py
from pydantic import BaseModel
from dagster import asset, AssetsDefinition

# Factory function to dynamically create an asset 
def create_asset(identifier: str, config_class: Type[BaseModel]) -&gt; AssetsDefinition:

    # Library code wrapped in framework stuff
    @asset(key=identifier)
    def _inner(config: config_class) -&gt; None:
        ...
    return __inner

</code></pre>
<pre class=""lang-py prettyprint-override""><code># Library entrypoint code
# Register all plugins
plugin_configs: List[BaseModel] = register_plugins()

class MyBase(BaseModel):
    configs: Any # will be replaced with a Union[&lt;registed config classes&gt;] field

config_class: MyBase = create_updated_model(MyBase, plugin_configs)

# Load a list of ids to create assets for in config
my_list_of_ids: List[str] = load_asset_ids()

assets = [create_asset(id, config_class) for id in my_list_of_ids]
</code></pre>
<p>When I run mypy on this I will get:</p>
<pre><code>error: Variable &quot;assets.config_class&quot; is not valid as a type  [valid-type]
</code></pre>
<p>If the framework wasn't reliant on the type annotation for actually using the config class I could just create a protocol in the asset module for type hinting / IDE help to avoid this.</p>
<p>My fundamental understanding of the problem here is shaky at best after reading this: <a href=""https://mypy.readthedocs.io/en/stable/common_issues.html#variables-vs-type-aliases"" rel=""nofollow noreferrer"">https://mypy.readthedocs.io/en/stable/common_issues.html#variables-vs-type-aliases</a></p>
<p>For now I am ignoring this error via <code># type: ignore</code> but I want to know if this is solvable in a way that keeps the framework happy (that needs the actual dynamically generated class as the annotation) and mypy happy?</p>
<p>Ran mypy, expected no errors but got  <code>Variable </code>asset.config_class<code> is not valid as a type  [valid-type]</code></p>
",0,1712998344,python;mypy;dagster,False,16,0,1712998344,https://stackoverflow.com/questions/78320052/providing-dynamically-created-pydantic-model-as-type-annotation-to-function-with
78179960,appending variable to image artefacts,"<p>this is how i create and upload docker images for Dagster to ECR using my Github action:</p>
<pre><code># For each code location, the &quot;build-push-action&quot; builds the docker
      # image and a &quot;set-build-output&quot; command records the image tag for each code location.
      # To re-use the same docker image across multiple code locations, build the docker image once
      # and specify the same tag in multiple &quot;set-build-output&quot; commands. To use a different docker
      # image for each code location, use multiple &quot;build-push-actions&quot; with a location specific
      # tag.

      - name: Build and upload Docker image 
        if: steps.prerun.outputs.result != 'skip'
        uses: docker/build-push-action@v4
        with:
          context: .
          push: true
          tags: ${{ env.IMAGE_REGISTRY }}:${{ env.IMAGE_TAG }}-${{ github.head_ref }}-data
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Update build session with image tag for data
        id: ci-set-build-output-example-location
        if: steps.prerun.outputs.result != 'skip'
        uses: dagster-io/dagster-cloud-action/actions/utils/dagster-cloud-cli@v0.1
        with:
          command: &quot;ci set-build-output --location-name=teal_data --image-tag=$IMAGE_TAG-${{ github.head_ref }}-data&quot;

</code></pre>
<p>Every time, there's a new image, there are 3 new entries in ECR.</p>
<p><strong>Entry 1:</strong></p>
<p>Correct full name including the &quot;github.head_ref&quot; as I specifided.</p>
<p>ArtifactType: Image Index</p>
<p>Size: Eg 300 mb.</p>
<p><strong>entry2:</strong></p>
<p>Name: &quot;-&quot;</p>
<p>github.head_ref is not appended in the name.</p>
<p>Artifact type: Image</p>
<p>Size: 0 Mb</p>
<p><strong>Entry 3:</strong></p>
<p>Artifact type: Image</p>
<p>Size: actual size eg 300 mb.</p>
<p>Why is this so? Are these extra images really required?</p>
<p><strong>Is it possible to append github.head_ref to the remaining image entries as well?</strong></p>
<p>Is the dagster branch deployment dependent on the ImageIndex, Image or a combination of all? If I delete all the &quot;Image&quot; components, will it have an impact on my dagster deployment?</p>
<p><a href=""https://i.stack.imgur.com/5zRYp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5zRYp.png"" alt=""enter image description here"" /></a></p>
<p>Note:</p>
<p>I have already checked out this: <a href=""https://stackoverflow.com/questions/75811044/github-action-docker-build-push-actionv4-to-ecr-returns-untagged-images"">Github Action docker/build-push-action@v4 to ECR returns untagged images</a> question but it <strong>DOES NOT answer my concerns</strong> because</p>
<p>a) I am not using QEMU</p>
<p>b) the provenance: false tag also did not help the OP.</p>
",0,1710762330,amazon-web-services;docker;github-actions;amazon-ecr;dagster,False,21,0,1710762330,https://stackoverflow.com/questions/78179960/appending-variable-to-image-artefacts
78163936,How to chain jobs in Dagster?,"<p>I need to chain several jobs. Some have to be started right after other have finished and some need results of other jobs as an input.</p>
<p>Seems that I can start one job after the other by using sensors. AI suggests using <code>@solid</code> and <code>@pipeline</code> decorators, but I was unable to find suitable example of their usage in Dagster documentation or the internet. I can't figure out how to pass output from one job to the other. <code>job_3(job_2())</code> call doesn't seem like a Dagster approach, isn't it?</p>
<p>Here is the code to illustrate an issue:</p>
<pre class=""lang-py prettyprint-override""><code>@job
def job_1():
    save_to_db_op(
        make_api_call_op()
    )

@job
def job_2():
    out_1, out_2 = process_data_op(
        make_another_api_call_op()
    )
    save_to_db_op(out_1)
    return out_2  # I need to pass it to another job


@job
def job_3(out_2): # how to pass input here?
   process_op(out_2)
   do_some_other_staff_op()


# this function is a pseudocode to represent what I want to recreate in Dagster
def figure_it_out_pipeline():
   job_1() # wait until complete
   job_3(
       job_2
   )

</code></pre>
",0,1710458186,python;python-3.x;jobs;job-scheduling;dagster,False,55,0,1710458186,https://stackoverflow.com/questions/78163936/how-to-chain-jobs-in-dagster
78148993,Dagster Limiting Concurrency in a Job,"<p>my code location as below defs which uses k8s_job_executor</p>
<pre><code>defs = Definitions(
    assets=[the_asset], jobs=[asset_job, op_job], executor=k8s_job_executor
)
</code></pre>
<p>All jobs/schedules running fine with above defs.</p>
<p>Now i want to create an op based job, this new op should not run in parallel. so i created job with below code to limit concurrency</p>
<pre><code>
@op()
def run_query():
 ...

@job(
    executor_def=multiprocess_executor,
    config={
        &quot;execution&quot;: {
            &quot;config&quot;: {
                &quot;multiprocess&quot;: {
                    &quot;max_concurrent&quot;: 1,
                &quot;tag_concurrency_limits&quot;: [
                        {
                            &quot;key&quot;: &quot;database&quot;,
                            &quot;value&quot;: &quot;graph&quot;,
                            &quot;limit&quot;: 1,
                        }
                    ],
                },
            }
        }
    },
)
def data_load_job() -&gt; None:
</code></pre>
<p>But code is giving error</p>
<pre><code>dagster._core.errors.DagsterInvalidConfigError: Invalid default_value for Field.
    Error 1: Received unexpected config entry &quot;multiprocess&quot; at path root:config. Expected: &quot;{ max_concurrent?: Int? retries?: { disabled?: { } enabled?: { } } start_method?: { forkserver?: { preload_modules?: [String] } spawn?: { } } tag_concurrency_limits?: [{ key: String limit: Int value?: (String | { applyLimitPerUniqueValue: Bool }) }] }&quot;.
</code></pre>
<p>how to fix the error ?
Is there a way to limit the op  concurrancy in job ?</p>
",0,1710264337,python;dagster,False,39,0,1710264337,https://stackoverflow.com/questions/78148993/dagster-limiting-concurrency-in-a-job
74561519,How do I load a software defined asset in dagster from outside dagster?,"<p>I'm failry new to dagster, but I really hope it has the feature of loading a software deffined asset from outside dagster.</p>
<p>To explain my ask:</p>
<p>consider this dagster graph:</p>
<pre class=""lang-py prettyprint-override""><code>
@asset
def users()-&gt;list[int]:
    return [1,2,3]

@asset
def new_users(users)-&gt;list[int]:
    return [u for u in users if is_new(u)]
</code></pre>
<p>The details don't really matter, just that there is some dag that generates some output.</p>
<p>Outside the dagster project, I have some jupyter notebooks. I'd like to be able to load the <code>new_users</code> list. If I had to invent an API for it I'd look something like:</p>
<pre class=""lang-py prettyprint-override""><code># In some jupyter notebook for example
from dagster.{something} import Project 

project = Project(...)

new_users = project.load_asset(asset='load_asset', force_refresh=False)
</code></pre>
<p>Does dagster have this type of functionality?</p>
",1,1669296352,dagster,True,1460,2,1709752011,https://stackoverflow.com/questions/74561519/how-do-i-load-a-software-defined-asset-in-dagster-from-outside-dagster
78083649,Second query not executing,"<p>I'm trying to implement a datalake via python with Trino, Dagster as well as openpyxl and I am trying to initialize a table inside a catalog and a schema, (both defined in my config). My first query: &quot; Create schema if not exists example&quot; works whereas the second one does not. Could this be a problem with the transaction ending, so then my 2nd query is not taken into account?</p>
<p>I've tried not closing the cursor before executing the query with no avail, also tried inputting incorrect information inside of the query so that it would force an exception, however it did not trigger it.</p>
<p>Here is a code snippet where lista contains a list with tuples of type (name, type):</p>
<pre><code>@op(required_resource_keys={'trino'})
def init(context,lista):
    trino = context.resources.trino
    with trino.get_connection() as conn:
        cursor = conn.cursor()
        cursor.execute(&quot;&quot;&quot;CREATE SCHEMA if not exists my_catalog.example&quot;&quot;&quot;)
        try:
            columns_definition = ', '.join([f'{col[0]} {col[1]}' for col in lista])
            query = f'''CREATE TABLE if not exists ex1 ({columns_definition})'''
            cursor.execute(query)
            context.log.info(f'Table created with columns: {columns_definition}')
            conn.commit()
        except Exception as e:
            conn.rollback()
            context.log.error(f'Error creating table: {e}')
    return []
@repository
def workspace():
    config = {
        &quot;resources&quot;: {
            &quot;trino&quot;: {
                &quot;config&quot;: {
                    &quot;host&quot;: &quot;trino&quot;,
                    &quot;port&quot;: &quot;8060&quot;,
                    &quot;user&quot;: &quot;trino&quot;,
                    &quot;password&quot;: &quot;&quot;,
                    &quot;catalog&quot;: &quot;my_catalog&quot;,
                    &quot;schema&quot;: &quot;example&quot;   
                }
            }
        }
    }
    resource_config = config.get(&quot;resources&quot;, {}).get(&quot;trino&quot;, {}).get(&quot;config&quot;, {})
    with build_op_context(resources={'trino':trino_resource.configured(resource_config)}) as con:
        return [init(con,read_files_op(con))]
</code></pre>
",0,1709232532,python;trino;dagster,False,28,0,1709232532,https://stackoverflow.com/questions/78083649/second-query-not-executing
78077772,Dagster error - attempt to write a readonly database,"<p>I'm trying to set up a gRPC server for Dagster.  I have the gRPC server running in one process, runing the simple hello-dagster app, and Dagster in another.  When I go to materialize the assets, I get the following error:</p>
<pre><code>dagster._core.errors.DagsterLaunchFailedError: Error during RPC setup for executing run: sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) attempt to write a readonly database
[SQL: PRAGMA main.table_info(&quot;alembic_version&quot;)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
</code></pre>
<p>In searching for a solution to this problem, I ran across a similar issue <a href=""https://github.com/dagster-io/dagster/issues/18029"" rel=""nofollow noreferrer"">here</a>, but with no solution provided.  This issue made reference to permissions on the directory indicated by <code>DAGSTER_HOME</code>.  The permissions of this directory on my system (Linux) are: <code>drwxrwxr-x</code>.</p>
<p>Anyone have any thoughts about how to fix this error, or how to determine where the persmissions problem lies (if, indeed, that is the problem)?</p>
",0,1709156615,dagster,False,40,0,1709156615,https://stackoverflow.com/questions/78077772/dagster-error-attempt-to-write-a-readonly-database
78026256,filter out dagster assets based on group_name,"<p>In my assets file, I have 3 groups but they are differentiated based on their <strong>group_name</strong>*</p>
<p><strong>assets/my_assets.py:</strong></p>
<pre><code>@asset(
    group_name=&quot;group1&quot;
)
def group1_data(context: AssetExecutionContext):
    x = 1 + 3
 
   
@asset(
    group_name=&quot;group1&quot;
)
def group1_full_data(context: AssetExecutionContext):
    x = 1 + 6

@asset(
    group_name=&quot;group2&quot;
)
def group2_data(context: AssetExecutionContext):
    x = 1 + 1       
</code></pre>
<p><strong>assets/init.py:</strong></p>
<pre><code>all_assets = load_assets_from_modules([my_assets])
</code></pre>
<p>Now when I load them using load_assets_from_modules, I always end up loading all assets together. Is it not possible to load only those with a specific group name?</p>
<p>Because I want to run 2 different jobs for 2 different groups:</p>
<pre><code>from dagster import define_asset_job, load_assets_from_modules
from ..assets import my_assets

my_group1_job = define_asset_job(name=&quot;group1_job&quot;, 
                                         selection=load_assets_from_modules([my_assets]),
                                         description=&quot;Loads only group1 data&quot;)
</code></pre>
",0,1708422456,python;assets;jobs;directed-acyclic-graphs;dagster,False,74,1,1709133525,https://stackoverflow.com/questions/78026256/filter-out-dagster-assets-based-on-group-name
70243762,ChildProcessCrashException in Dagster multiprocess execution in multi-container Docker deployment,"<p>I have a Dagster job that is training a CNN (using Keras). The Op that runs <code>fit()</code> is causing the following error:</p>
<pre><code>Multiprocess executor: child process for step train unexpectedly exited with code -9
dagster.core.executor.child_process_executor.ChildProcessCrashException

Stack Trace:
  File &quot;/usr/local/lib/python3.7/site-packages/dagster/core/executor/multiprocess.py&quot;, line 163, in execute
    event_or_none = next(step_iter)
,  File &quot;/usr/local/lib/python3.7/site-packages/dagster/core/executor/multiprocess.py&quot;, line 268, in execute_step_out_of_process
    for ret in execute_child_process_command(command):
,  File &quot;/usr/local/lib/python3.7/site-packages/dagster/core/executor/child_process_executor.py&quot;, line 157, in execute_child_process_command
    raise ChildProcessCrashException(exit_code=process.exitcode)
</code></pre>
<p>No additional output is given. I am using a multi-container local Docker deployment.</p>
<p>Things tried:</p>
<ul>
<li>I run the code locally (non-Docker) by using <code>execute_in_process()</code> and this works without error.</li>
<li>Due to the mention of executor and multiprocess in the stack trace I tried setting the <code>execution</code> to <code>in_process</code> but this merely hangs.</li>
</ul>
<p>Any advice would be greatly appreciated.</p>
",0,1638785261,python;dagster,False,1429,2,1708719304,https://stackoverflow.com/questions/70243762/childprocesscrashexception-in-dagster-multiprocess-execution-in-multi-container
78023338,Kubernetes pods container initialization doesn&#39;t finish for Dagster,"<h2>Installation set-up:</h2>
<p>I'm trying to run dagster on Kubernetes. I use the default values.yaml file and the only change I'm making is replacing &quot;docker.io&quot; with &quot;registry.hub.docker.com&quot; as my network doesn't allow access to &quot;docker.io&quot; registry.</p>
<p>The 4 pods are created:</p>
<pre><code>NAME                                                               READY   STATUS     RESTARTS   AGE
dagster-daemon-5b77cb897f-x5hwn                                    0/1     Init:0/2        0          106s
dagster-dagster-user-deployments-k8s-example-user-code-1-54dhhk    1/1     Running         0          106s
dagster-dagster-webserver-5bd447d567-4btqc                         0/1     Init:0/2        0          106s
dagster-postgresql-0                                               1/1     Running         0          106s
</code></pre>
<p>As you see, daemon and webserver are in init state and they remain in this STATUS indefinitely.</p>
<h2>Error messages and logs:</h2>
<p>There are no failure events at the bottom when I run the 'Kubectl describe':</p>
<pre><code>Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  3m36s  default-scheduler  Successfully assigned dagster/dagster-daemon-5b77cb897f-btj4k to server
  Normal  Pulling    3m35s  kubelet            Pulling image &quot;registry.hub.docker.com/library/postgres:14.6&quot;
  Normal  Pulled     3m25s  kubelet            Successfully pulled image &quot;registry.hub.docker.com/library/postgres:14.6&quot; in 9.028s
  Normal  Created    3m25s  kubelet            Created container check-db-ready
  Normal  Started    3m25s  kubelet            Started container check-db-ready
</code></pre>
<p>But the initContainers seems not to be working:</p>
<pre><code>Init Containers:
  check-db-ready:
    Container ID:  cri-o://75cd07830cbcf6e7e6872e105e3eee8a267a5223353c93798bad0a69352e9b99
    Image:         registry.hub.docker.com/library/postgres:14.6
    Image ID:      f4d4e40f6b50987185528556a7adb03e8ef5b4ce6c21c4eef0030a025b46bdf4
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      sh
      -c
      until pg_isready -h dagster-postgresql -p 5432 -U test; do echo waiting for database; sleep 2; done;
    State:          Running
      Started:      Mon, 19 Feb 2024 10:39:04 -0800
    Ready:          False
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-dg7vm (ro)
  init-user-deployment-k8s-example-user-code-1:
    Container ID:
    Image:         registry.hub.docker.com/busybox:1.28
    Image ID:
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      sh
      -c
      until nslookup k8s-example-user-code-1; do echo waiting for user service; sleep 2; done
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-dg7vm (ro)
Containers:
  dagster:
    Container ID:
    Image:         registry.hub.docker.com/dagster/dagster-celery-k8s:1.5.11
    Image ID:
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      /bin/bash
      -c
      dagster-daemon run -w /dagster-workspace/workspace.yaml
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment Variables from:
      dagster-daemon-env  ConfigMap  Optional: false
    Environment:
      DAGSTER_PG_PASSWORD:                 &lt;set to the key 'postgresql-password' in secret 'dagster-postgresql-secret'&gt;  Optional: false
      DAGSTER_DAEMON_HEARTBEAT_TOLERANCE:  1800
    Mounts:
      /dagster-workspace/ from dagster-workspace-yaml (rw)
      /opt/dagster/dagster_home/dagster.yaml from dagster-instance (rw,path=&quot;dagster.yaml&quot;)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-dg7vm (ro)
Conditions:
  Type              Status
  Initialized       False
  Ready             False
  ContainersReady   False
  PodScheduled      True
</code></pre>
<p>When I try for more specific messages per each container by running:</p>
<pre><code> kubectl logs dagster-daemon-5b77cb897f-x5hwn  -c check-db-ready -n dagster
</code></pre>
<p>I get a long list of:</p>
<pre><code>dagster-postgresql:5432 - no response
waiting for database
...
</code></pre>
<p>Isn't this wierd as the dagster-postgresql-0 POD is actually showing that it is running?</p>
<p>Checking for the other iniContainer by running:</p>
<pre><code>kubectl logs dagster-daemon-5b77cb897f-x5hwn  -c init-user-deployment-k8s-example-user-code-1  -n dagster
</code></pre>
<p>I get this messgae:</p>
<pre><code>Error from server (BadRequest): container &quot;init-user-deployment-k8s-example-user-code-1&quot; in pod &quot;dagster-daemon-5b77cb897f-x5hwn&quot; is waiting to start: PodInitializing
</code></pre>
<p>I'm new to Kubernetes and your help to shed some light on this issue is much appreciated!</p>
",0,1708374503,postgresql;kubernetes;dagster,False,49,0,1708374503,https://stackoverflow.com/questions/78023338/kubernetes-pods-container-initialization-doesnt-finish-for-dagster
77983753,Error on accessing a Dagster Usercode server on ECS?,"<p>I have a problem with the usercode server. The webserver cannot reach it, and even though I systematically permutated all options I can think of at the different variables, I cannot get it to work.</p>
<p>The Code evolved out of the original deploy_ecs example which I needed to adapt due to the ceased support of ECS by docker. I created a terraform setup for the ECS structure and the ECR registry, and I adapted the docker-compose file to the new structure. The usercode server is running, but the webserver cannot reach it. The error message is about DNS resolution. The usercode server is running on port 4000, and the webserver is trying to reach it on dagster-usercode.dagster.local:4000. This is the Cloud Map service name, and it should be resolved to the IP address of the usercode server. This is not happening.</p>
<p>Distributed installation in ECS:</p>
<ul>
<li>1 Webserver</li>
<li>1 Daemon Server</li>
<li>1 Usercode Server</li>
</ul>
<p>workspace.yaml on webserver:</p>
<pre><code>load_from:
  - python_file: job2.py
  - python_package:
      location_name: &quot;webserver-jobs&quot;
      package_name: job3
  - grpc_server:
      host: dagster-usercode.dagster.local
      port: 4000
      location_name: &quot;usercode-jobs&quot;
</code></pre>
<p>Cloud Map in AWS:</p>
<ul>
<li>Namespace dagster.local</li>
<li>Cloud Map Service dagster-usercode, pointing to ECS Service instance dagster_usercode</li>
</ul>
<p>ECS in AWS:</p>
<ul>
<li>Service dagster_usercode is running the task that refers to the usercode server container in ECR, which is found and running according to logs.</li>
</ul>
<p>start command on usercode server is implemented like this:</p>
<pre><code>dagster api grpc -h 0.0.0.0 -p 4000 -f sample_jobs.py
</code></pre>
<p>When loading the workspace.yaml on the webserver, I see these effects:</p>
<ul>
<li>job2 is loaded and can be called</li>
<li>job3: Error</li>
</ul>
<pre><code>dagster._core.errors.DagsterInvariantViolationError: No repositories, jobs, pipelines, graphs, or asset definitions found in &quot;job3&quot;.  File &quot;/usr/local/lib/python3.10/site-packages/dagster/_grpc/server.py&quot;, line 408, in __init__    self._loaded_repositories: Optional[LoadedRepositories] = LoadedRepositories(  File &quot;/usr/local/lib/python3.10/site-packages/dagster/_grpc/server.py&quot;, line 242, in __init__    loadable_targets = get_loadable_targets(  File &quot;/usr/local/lib/python3.10/site-packages/dagster/_grpc/utils.py&quot;, line 60, in get_loadable_targets    else loadable_targets_from_python_package(package_name, working_directory)  File &quot;/usr/local/lib/python3.10/site-packages/dagster/_core/workspace/autodiscovery.py&quot;, line 51, in loadable_targets_from_python_package    return loadable_targets_from_loaded_module(module)  File &quot;/usr/local/lib/python3.10/site-packages/dagster/_core/workspace/autodiscovery.py&quot;, line 116, in loadable_targets_from_loaded_module    raise DagsterInvariantViolationError(
</code></pre>
<ul>
<li>grpc: Error</li>
</ul>
<pre><code>dagster._core.errors.DagsterUserCodeUnreachableError: Could not reach user code server. gRPC Error code: UNAVAILABLE  File &quot;/usr/local/lib/python3.10/site-packages/dagster/_core/workspace/context.py&quot;, line 614, in _load_location    else origin.create_location(self.instance)  File &quot;/usr/local/lib/python3.10/site-packages/dagster/_core/host_representation/origin.py&quot;, line 373, in create_location    return GrpcServerCodeLocation(self, instance=instance)  File &quot;/usr/local/lib/python3.10/site-packages/dagster/_core/host_representation/code_location.py&quot;, line 632, in __init__    list_repositories_response = sync_list_repositories_grpc(self.client)  File &quot;/usr/local/lib/python3.10/site-packages/dagster/_api/list_repositories.py&quot;, line 20, in sync_list_repositories_grpc    api_client.list_repositories(),  File &quot;/usr/local/lib/python3.10/site-packages/dagster/_grpc/client.py&quot;, line 250, in list_repositories    res = self._query(&quot;ListRepositories&quot;, api_pb2.ListRepositoriesRequest)  File &quot;/usr/local/lib/python3.10/site-packages/dagster/_grpc/client.py&quot;, line 173, in _query    self._raise_grpc_exception(  File &quot;/usr/local/lib/python3.10/site-packages/dagster/_grpc/client.py&quot;, line 156, in _raise_grpc_exception    raise DagsterUserCodeUnreachableError(The above exception was caused by the following exception:grpc._channel._InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:   status = StatusCode.UNAVAILABLE details = &quot;DNS resolution failed for dagster-usercode.dagster.local:4000: C-ares status is not ARES_SUCCESS qtype=A name=dagster-usercode.dagster.local is_balancer=0: Domain name not found&quot;   debug_error_string = &quot;UNKNOWN:Error received from peer  {created_time:&quot;2024-02-12T16:21:15.104749061+00:00&quot;, grpc_status:14, grpc_message:&quot;DNS resolution failed for dagster-usercode.dagster.local:4000: C-ares status is not ARES_SUCCESS qtype=A name=dagster-usercode.dagster.local is_balancer=0: Domain name not found&quot;}&quot;&gt;  File &quot;/usr/local/lib/python3.10/site-packages/dagster/_grpc/client.py&quot;, line 171, in _query    return self._get_response(method, request=request_type(**kwargs), timeout=timeout)  File &quot;/usr/local/lib/python3.10/site-packages/dagster/_grpc/client.py&quot;, line 141, in _get_response    return getattr(stub, method)(request, metadata=self._metadata, timeout=timeout)  File &quot;/usr/local/lib/python3.10/site-packages/grpc/_channel.py&quot;, line 1160, in __call__    return _end_unary_response_blocking(state, call, False, None)  File &quot;/usr/local/lib/python3.10/site-packages/grpc/_channel.py&quot;, line 1003, in _end_unary_response_blocking    raise _InactiveRpcError(state)  # pytype: disable=not-instantiableThe above exception occurred during handling of the following exception:dagster._core.errors.DagsterUserCodeUnreachableError: Could not reach user code server. gRPC Error code: UNAVAILABLE  File &quot;/usr/local/lib/python3.10/site-packages/dagster/_grpc/server_watcher.py&quot;, line 119, in watch_grpc_server_thread    watch_for_changes()  File &quot;/usr/local/lib/python3.10/site-packages/dagster/_grpc/server_watcher.py&quot;, line 82, in watch_for_changes    new_server_id = client.get_server_id(timeout=REQUEST_TIMEOUT)  File &quot;/usr/local/lib/python3.10/site-packages/dagster/_grpc/client.py&quot;, line 233, in get_server_id    res = self._query(&quot;GetServerId&quot;, api_pb2.Empty, timeout=timeout)  File &quot;/usr/local/lib/python3.10/site-packages/dagster/_grpc/client.py&quot;, line 173, in _query    self._raise_grpc_exception(  File &quot;/usr/local/lib/python3.10/site-packages/dagster/_grpc/client.py&quot;, line 156, in _raise_grpc_exception    raise DagsterUserCodeUnreachableError(The above exception was caused by the following exception:grpc._channel._InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:   status = StatusCode.UNAVAILABLE details = &quot;DNS resolution failed for dagster-usercode.dagster.local:4000: C-ares status is not ARES_SUCCESS qtype=AAAA name=dagster-usercode.dagster.local is_balancer=0: Domain name not found&quot;    debug_error_string = &quot;UNKNOWN:Error received from peer  {created_time:&quot;2024-02-12T16:21:04.309598593+00:00&quot;, grpc_status:14, grpc_message:&quot;DNS resolution failed for dagster-usercode.dagster.local:4000: C-ares status is not ARES_SUCCESS qtype=AAAA name=dagster-usercode.dagster.local is_balancer=0: Domain name not found&quot;}&quot;&gt;  File &quot;/usr/local/lib/python3.10/site-packages/dagster/_grpc/client.py&quot;, line 171, in _query    return self._get_response(method, request=request_type(**kwargs), timeout=timeout)  File &quot;/usr/local/lib/python3.10/site-packages/dagster/_grpc/client.py&quot;, line 141, in _get_response    return getattr(stub, method)(request, metadata=self._metadata, timeout=timeout)  File &quot;/usr/local/lib/python3.10/site-packages/grpc/_channel.py&quot;, line 1160, in __call__    return _end_unary_response_blocking(state, call, False, None)  File &quot;/usr/local/lib/python3.10/site-packages/grpc/_channel.py&quot;, line 1003, in _end_unary_response_blocking    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable
</code></pre>
<p>What's missing? from the ECS side all looks like a go.
The job from the package is not found, but this is not the main issue.
My major concern is the usercode server, and I am at a loss for next steps.</p>
<p>Any help is highly appreciated.</p>
",0,1707764822,docker;dns;amazon-ecs;dagster;aws-cloudmap,True,113,1,1708122182,https://stackoverflow.com/questions/77983753/error-on-accessing-a-dagster-usercode-server-on-ecs
76720358,Dagster+dbt : referring a dagster asset as a source in dbt model,"<p>my dbt_model’s source is a dasgter asset out of dbt, how can I refer this asset inside a dbt model definiton?</p>
<p>the dagster asset =</p>
<pre><code>@asset(
    group_name=&quot;. .... &quot;,
    io_manager_key=&quot;..   &quot;,
)
def dld_sales():
</code></pre>
<p>dbt model =</p>
<pre><code>with source as (
    select * from {{ ref(&quot;dld_sales&quot;) }}
),
</code></pre>
<p>gives me error:</p>
<pre><code>Model ‘model.dbt_project.stg_dld_sales’ (models/staging/stg_dld_sales.sql) depends on a node named ‘dld_sales’ which was not found
</code></pre>
",0,1689762829,dagster,False,226,1,1707677002,https://stackoverflow.com/questions/76720358/dagsterdbt-referring-a-dagster-asset-as-a-source-in-dbt-model
77900196,How Dagster execute Window .Bat file?,"<p>I would like to know how Dagster enable the job to execute Window *.BAT file by login into other Window server and execute the *.bat file which located in that Window Server?</p>
<p>Anyone of you know about the answer, related solution or code sample about my question above?</p>
",0,1706536313,dagster,False,47,0,1706536541,https://stackoverflow.com/questions/77900196/how-dagster-execute-window-bat-file
75109289,why does kubernetes delete secrets after helm upgrade?,"<p>when performing helm upgrade, I find that secrets that are created upon initial install are deleted. Why is this? The example I am using is dagster. When installing with:</p>
<p><code>helm install dagster dagster/dagster \                                                                                                                                                       --namespace dagster \ --create-namespace</code></p>
<p>everything starts up fine and secrets are created. When updating the image and tag and performing an upgrade with:</p>
<p><code>helm upgrade -f charts/dagster-user-deployments/values.yaml dagster ./charts/dagster-user-deployments -n dagster</code></p>
<p>the image is upgraded, but all secrets are deleted. Why would/ could this happen?</p>
<p>After running the upgrade command, I expect secrets to still be in place, and the new image to be pulled and run.</p>
",4,1673613364,kubernetes;kubernetes-helm;kubectl;dagster,True,1142,2,1705887745,https://stackoverflow.com/questions/75109289/why-does-kubernetes-delete-secrets-after-helm-upgrade
77724854,Unset environment variable when pushing container to Dagster cloud,"<p>I'm attempting to push a container to Dagster Cloud with the following agent configuration:</p>
<pre><code>locations:
  - location_name: unit_ingestion
    code_source:
      package_name: unit_ingestion
    build:
      directory: ./orchestration
      registry: &quot;****&quot;
    container_context:
      ecs:
        env_vars:
          - ENV
          - TASK_ROLE_ARN
        server_resources:
          cpu: &quot;256&quot;
          memory: &quot;512&quot;
        run_resources:
          cpu: &quot;256&quot;
          memory: &quot;1024&quot;
        task_role_arn:
          env: TASK_ROLE_ARN
</code></pre>
<p>However, this deployment fails with the following message:</p>
<pre><code>Error loading unit_ingestion: {'__typename': 'PythonError', 'message': 'dagster._core.errors.DagsterInvalidConfigError: Errors while parsing ECS container context
    Error 1: Post processing at path root:task_role_arn of original value {\'env\': \'TASK_ROLE_ARN\'} failed:
dagster._config.errors.PostProcessingError: You have attempted to fetch the environment variable &quot;TASK_ROLE_ARN&quot; which is not set. In order for this execution to succeed it must be set in this environment.

Stack Trace:
  File &quot;/dagster/dagster/_config/post_process.py&quot;, line 79, in _post_process
    new_value = context.config_type.post_process(config_value)
  File &quot;/dagster/dagster/_config/source.py&quot;, line 40, in post_process
    return str(_ensure_env_variable(cfg))
  File &quot;/dagster/dagster/_config/source.py&quot;, line 16, in _ensure_env_variable
    raise PostProcessingError(

', 'stack': ['  File &quot;/dagster-cloud/dagster_cloud/workspace/user_code_launcher/user_code_launcher.py&quot;, line 1278, in _reconcile
    new_dagster_servers[to_update_key] = self._start_new_dagster_server(
', '  File &quot;/dagster-cloud/dagster_cloud/workspace/user_code_launcher/user_code_launcher.py&quot;, line 1573, in _start_new_dagster_server
    return self._start_new_server_spinup(
', '  File &quot;/dagster-cloud/dagster_cloud/workspace/ecs/launcher.py&quot;, line 330, in _start_new_server_spinup
    ).merge(EcsContainerContext.create_from_config(metadata.container_context))
', '  File &quot;/dagster-aws/dagster_aws/ecs/container_context.py&quot;, line 380, in create_from_config
    raise DagsterInvalidConfigError(
']}
</code></pre>
<p>From this I gather that Dagster Cloud is assuming that the <code>TASK_ROLE_ARN</code> hasn't been defined. However, I have done so:</p>
<p><a href=""https://i.stack.imgur.com/sZkNp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sZkNp.png"" alt=""Dagster Cloud Environment Variables"" /></a></p>
<p>As far as I can tell, this configuration should be acceptable to Dagster so I'm not sure what's causing this error. Any help would be appreciated.</p>
",0,1703732730,deployment;environment-variables;dagster,True,89,1,1705398139,https://stackoverflow.com/questions/77724854/unset-environment-variable-when-pushing-container-to-dagster-cloud
77760598,Dagster Error when using Config for user defined parameter and passing asset results to downstream assets,"<p>I am setting up a pipeline that uses Config from dagster so that the user can put in a user defined parameter when running the pipeline. My use case is running a sql query to pull in data and the user can define the start and end date for the data pull. I am running into a config error when passing on the results from that asset to another asset.</p>
<p>Here is the dagster documentation on running configurations: <a href=""https://docs.dagster.io/concepts/configuration/config-schema"" rel=""nofollow noreferrer"">https://docs.dagster.io/concepts/configuration/config-schema</a></p>
<p>I have reproduced the error with a simple script below.</p>
<p>When I run this script in dagster UI, it prompts me that the scaffold is missing and it then generates the scaffold and I can enter the parameter - in this case I add &quot;Bananas&quot; to the fruit_select parameter. The parameter is used in the filter_data asset. I would like to then use that result from that asset (which is df2) in the following asset called filter_again.</p>
<p><strong>This is the error I keep running into:</strong>
<code>dagster._core.errors.DagsterInvalidConfigError: Error in config for op Error 1: Missing required config entry &quot;config&quot; at the root. Sample config for missing entry: {'config': {'fruit_select': '...'}}</code></p>
<p>Any help would be greatly appreciated! My goal is to have a user define a parameter when running the pipeline, the parameter is used in one of the assets to pull data or manipulate data, and then those results get passed on to another asset for another task.</p>
<pre><code>import pandas as pd
import random
from datetime import datetime, timedelta
from dagster import asset, MaterializeResult, MetadataValue, Config, materialize

@asset 
def generate_dataset():
    # Function to generate random dates
    def random_dates(start_date, end_date, n=10):
        date_range = end_date - start_date
        random_dates = [start_date + timedelta(days=random.randint(0, date_range.days)) for _ in range(n)]
        return random_dates
    
    # Set seed for reproducibility
    random.seed(42)
    
    # Define the number of rows
    num_rows = 100
    
    # Generate random data
    fruits = ['Apple', 'Banana', 'Orange', 'Grapes', 'Kiwi']
    fruit_column = [random.choice(fruits) for _ in range(num_rows)]
    units_column = [random.randint(1, 10) for _ in range(num_rows)]
    start_date = datetime(2022, 1, 1)
    end_date = datetime(2022, 12, 31)
    date_column = random_dates(start_date, end_date, num_rows)
    
    # Create a DataFrame
    df = pd.DataFrame({
        'fruit': fruit_column,
        'units': units_column,
        'date': date_column
    })
    
    # Display the DataFrame
    print(df)
    return df

class fruit_config(Config):
    fruit_select: str 

@asset(deps=[generate_dataset]) 
def filter_data(config: fruit_config):
    df = generate_dataset()
    df2 = df[df['fruit'] == config.fruit_select]
    print(df2)
    return df2

@asset(deps=[filter_data]) 
def filter_again():
    df2 = filter_data()
    df3 = df2[df2['units'] &gt; 5]
    print(df3)
    return df3
</code></pre>
",1,1704394753,python;config;assets;dagster,True,164,1,1704411444,https://stackoverflow.com/questions/77760598/dagster-error-when-using-config-for-user-defined-parameter-and-passing-asset-res
66485361,Is there a way to get the traceback stack from Dagster failure_hook and success_hook?,"<pre><code>from dagster import HookContext, failure_hook, success_hook


@success_hook(required_resource_keys={&quot;slack&quot;})  
def slack_message_on_success(context: HookContext):  
    message = f&quot;Solid {context.solid.name} finished successfully&quot;
    context.resources.slack.chat.post_message(channel=&quot;#foo&quot;, text=message)


@failure_hook(required_resource_keys={&quot;slack&quot;})
def slack_message_on_failure(context: HookContext):
    message = f&quot;Solid {context.solid.name} failed&quot;
    context.resources.slack.chat.post_message(channel=&quot;#foo&quot;, text=message)
</code></pre>
<p>Is there a field to pull traceback stacks from 'context' to locate the exception triggered and send out an email or slack message?</p>
",1,1614907068,python-3.x;dagster,True,212,2,1704394957,https://stackoverflow.com/questions/66485361/is-there-a-way-to-get-the-traceback-stack-from-dagster-failure-hook-and-success
77731973,jpype._core.JVMNotRunning: Java Virtual Machine is not running,"<blockquote>
<pre><code>jpype._core.JVMNotRunning: Java Virtual Machine is not running
/././lib/python3.10/site-packages/dagster/_core/workspace/context.py:610:
</code></pre>
<p>UserWarning: Error loading repository location
etl:jpype._core.JVMNotRunning: Java Virtual Machine is not running</p>
</blockquote>
<p>my code is</p>
<pre><code>import jpype
jpype.startJVM(jpype.getDefaultJVMPath(), &quot;-ea&quot;)
try:
jpype.addClassPath(&quot;/../../Hapi.jar&quot;)
YourJavaClass = jpype.JClass(&quot;com.netspective.hl7v2.HL7V2Parser&quot;)
java_instance = YourJavaClass()
result = java_instance.messageToJsonprocess('MSH|^~\&amp;|SENDING_APP|SENDING_FACILITY|RECEIVING_APP|RECEIVING_FACILITY|202110171200||ADT^A01|123456789|P|2.3','processed_2.3_ECMC_0be3f24a-c56c-4c.hl7')
finally:
jpype.shutdownJVM()
</code></pre>
<p>when i was run this code code as python file it works.but when i implement this code in dagster it shows the error that i given above.please fix this error.</p>
",0,1703852512,python-3.x;dagster,False,120,0,1703885847,https://stackoverflow.com/questions/77731973/jpype-core-jvmnotrunning-java-virtual-machine-is-not-running
77610206,Get run config from Dagster RunFailureSensorContext,"<p>I've setup a sensor in Dagster:</p>
<pre><code>@sensor(job=excel_to_csv)
def convert_unit_list(ctx: SensorEvaluationContext):

    env = os.getenv(&quot;ENVIRONMENT&quot;)
    region = os.getenv(&quot;AWS_REGION&quot;)
    appcfg = AppConfig(&quot;unit-ingestion&quot;, &quot;developer-units&quot;, env, region)

    bucket = Bucket.LANDING
    bucket_name = bucket.format(env)
    prefix = &quot;unit-ingestion/developer-units&quot;
    ctx.log.info(f&quot;Searching {bucket_name}/{prefix} for new files...&quot;)

    since_key = ctx.cursor
    keys = get_s3_keys(bucket_name, prefix, since_key)
    if not keys:
        return SkipReason(f&quot;No keys found in {bucket_name}/{prefix}&quot;)

    run_config = {
        &quot;s3&quot;: S3Resource(region_name=region),
        &quot;conv_ctx&quot;: ExcelConversionContext(appcfg, bucket, &quot;unit-ingestion/units&quot;, 1, &quot;B:BV&quot;),
    }

    ctx.log.info(f&quot;Found {len(keys)} new keys in {bucket_name}/{prefix}&quot;)
    for key in keys:

        yield RunRequest(key, run_config=run_config)

        ctx.log.info(f&quot;Updating cursor to {key}&quot;)
        ctx.update_cursor(key)
</code></pre>
<p>Now, I'd like to write an additional sensor that runs whenever <code>excel_to_csv</code> fails. I've got it stubbed out already:</p>
<pre><code>@run_failure_sensor(request_job=excel_to_csv)
def excel_to_csv_failure(ctx: RunFailureSensorContext):

    ctx.log.error(f&quot;{ctx.sensor_name} failed to process {ctx.failure_event.asset_key}&quot;)
</code></pre>
<p>However, I'd like to get <code>run_config</code> so I can use it to moved the failed Excel file to rejected S3 bucket. I've looked through <code>ctx</code> and it looks like <code>ctx.dagster_event.step_input_data</code> could be what I want here, but I'm not sure as the documentation doesn't include all the fields for this type. Does anyone know whether this is possible and, if it is, where I can find this data?</p>
",1,1701828762,python;dagster,True,92,1,1703312096,https://stackoverflow.com/questions/77610206/get-run-config-from-dagster-runfailuresensorcontext
77660998,Dagster executable missing from container,"<p>I've been trying to create a container to run in Dagster cloud on an ECS-hybrid deployment model. I'm able to actually push the container to Dagster but I continually get this error:</p>
<pre><code>dagster_cloud.workspace.ecs.client.EcsServiceError: ECS service failed because task arn:aws:ecs:ap-northeast-1:*****:task/Dagster-Cloud-my-cluster-Cluster/1d151e6d40b44588a4ed4446a949d44a failed: CannotStartContainerError: ResourceInitializationError: failed to create new container runtime task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: &quot;dagster&quot;: executable file not found in $PATH: unknown

Task logs:
runc create failed: unable to start container process: exec: &quot;dagster&quot;: executable file not found in $PATH

For more information about the failure, check the ECS console for logs for task arn:aws:ecs:ap-northeast-1:*****:task/Dagster-Cloud-my-cluster-Cluster/1d151e6d40b44588a4ed4446a949d44a in cluster Dagster-Cloud-my-cluster-Cluster.
  File &quot;/dagster-cloud/dagster_cloud/workspace/user_code_launcher/user_code_launcher.py&quot;, line 1304, in _reconcile
    self._wait_for_new_server_ready(
  File &quot;/dagster-cloud/dagster_cloud/workspace/ecs/launcher.py&quot;, line 458, in _wait_for_new_server_ready
    task_arn = self.client.wait_for_new_service(
  File &quot;/dagster-cloud/dagster_cloud/workspace/ecs/client.py&quot;, line 491, in wait_for_new_service
    return self.check_service_has_running_task(
  File &quot;/dagster-cloud/dagster_cloud/workspace/ecs/client.py&quot;, line 607, in check_service_has_running_task
    self._raise_failed_task(task, container_name, logger)
  File &quot;/dagster-cloud/dagster_cloud/workspace/ecs/client.py&quot;, line 526, in _raise_failed_task
    raise EcsServiceError(
</code></pre>
<p>I'm not sure why this is happening as I'm sure I'm installing dagster and dagster-cloud, as per the <a href=""https://docs.dagster.io/dagster-cloud/managing-deployments/code-locations#dagster-cloud-code-requirements"" rel=""nofollow noreferrer"">documentation</a>. My Docker container looks like this:</p>
<pre><code>###############################################################################
# Base container
###############################################################################
FROM python:3.11 AS base

# Define the environment variables necessary to work with poetry
ENV POETRY_VERSION=1.5.1 \
  POETRY_HOME=&quot;/opt/poetry&quot; \
  POETRY_VIRTUALENVS_IN_PROJECT=true \
  POETRY_NO_INTERACTION=1

# Add the poetry bin to our path
ENV PATH=&quot;$POETRY_HOME/bin:$PATH&quot;

###############################################################################
# Poetry installer container
###############################################################################
FROM base AS installer

# Install poetry
RUN curl -sSL https://install.python-poetry.org | python3 -

###############################################################################
# Container that actually builds the application
###############################################################################
FROM base AS builder

# Copy the poetry files from the poetry installer to this container
COPY --from=installer $POETRY_HOME $POETRY_HOME

# Describe the environment variables necessary to install our dependencies
ENV PYTHONFAULTHANDLER=1 \
  PYTHONUNBUFFERED=1 \
  PYTHONHASHSEED=random \
  PIP_NO_CACHE_DIR=off \
  PIP_DISABLE_PIP_VERSION_CHECK=on \
  PIP_DEFAULT_TIMEOUT=100

# Set the working directory to one we can install to easily
WORKDIR /app

# Copy the poetry.lock and pyproject.toml files first to ensure that dependencies
# are only installed when they're updated
COPY poetry.lock pyproject.toml /app/

# Copy in our SSH key so we can retrieve the shared GitHub repo
RUN mkdir -p -m 0700 ~/.ssh &amp;&amp; ssh-keyscan github.com &gt;&gt; ~/.ssh/known_hosts

# Install our project dependencies
RUN --mount=type=ssh poetry install --only dagster --no-ansi --no-root

# Build the project
COPY . /app/
RUN poetry build

###############################################################################
# Create a runtime environment that's much smaller
###############################################################################
FROM python:3.11-alpine AS runtime

# Set the working directory to where Dagster will look for the application
WORKDIR /opt/dagster/app

# Copy the project wheel file
COPY --from=builder /app/dist/*.whl /

# Install the wheel file using pip and then install dagster and dagster-cloud
RUN pip install --no-cache-dir /*.whl \
  &amp;&amp; rm -rf /*.whl
</code></pre>
<p>For this package, my <code>pyproject.toml</code> file lists my dependencies as follows:</p>
<pre class=""lang-ini prettyprint-override""><code>[tool.poetry.group.dagster.dependencies]
dagster = &quot;^1.5.9&quot;
dagster-aws = &quot;^0.21.9&quot;
pendulum = &quot;^2.1.2&quot;
pandas = &quot;^2.1.3&quot;
openpyxl = &quot;^3.1.2&quot;
dagster-cloud = &quot;^1.5.12&quot;
</code></pre>
<p>So, both of these should be installed, but clearly they haven't been. What am I doing wrong here?</p>
",0,1702565370,python;amazon-web-services;docker;dagster,True,149,2,1703205930,https://stackoverflow.com/questions/77660998/dagster-executable-missing-from-container
76526701,How to pass parameter to asset from op in dagster,"<p>I am new to dagster and having difficult in running the asset for different parameter values when job scheduled.</p>
<p>I have created a pipeline using dagster.</p>
<p>Trying to materialize the outcome of upstream asset <code>multiple_num()</code> and using op to pass parameter value to the asset.</p>
<p>simplified example is</p>
<pre><code>@asset
def get_a():
    return a
    
@asset 
def get_b():
    return b
    
@asset
def multiple_num(get_a, get_b):
    return c
    
@op
def get_values():
    values = ['a','s','e']
    for value in values:
        yield RunRequest(
            run_key=None,
            asset_selection = [AssetKey(multiple_num)]
        )
        cleanup_directory(value)

def cleanup_directory(): -&gt; str
    return status
    
@job
def run_values():
   get_values()

</code></pre>
<p>Have to call the asset for different parameters values when job scheduled?</p>
",1,1687377129,python-3.x;pipeline;orchestration;dagster,True,1136,1,1702374616,https://stackoverflow.com/questions/76526701/how-to-pass-parameter-to-asset-from-op-in-dagster
77175532,Is it possible to run an asset from an image?,"<p>I am trying to understand what I can really do with Dagster.</p>
<p>I have some Python codes already containerised and pushed to a container registry.
I was wondering if in a Dagster'op or asset I can read those images and run the final pipeline on my Kubernetes cluster.</p>
<p>For example, I may have an image that performs some multiplication and it's on GCR. Does Dagster offer any tool for pulling this image and then allowing me to do something like:</p>
<pre><code>@asset(PULL_IMAGE)
def my_asset():
    from MY_IMAGE_CODE import function_1, function_2
    function_1() 
</code></pre>
<p>Then, I would like to use Dagster-kubernetes to run this pipeline on Kubernetes.</p>
<p>I tried to get an idea from Dagster docs but I couldn't find anything.
I had a look at various GitHub Repos but many of them are using an old version of Dagster.
I jumped into <a href=""https://docs.dagster.io/_apidocs/libraries/dagster-k8s"" rel=""nofollow noreferrer"">https://docs.dagster.io/_apidocs/libraries/dagster-k8s</a> and <a href=""https://dagster.io/integrations/dagster-docker"" rel=""nofollow noreferrer"">https://dagster.io/integrations/dagster-docker</a> but I am not really understanding how to link them.</p>
",0,1695673690,python;kubernetes;machine-learning;pipeline;dagster,False,158,1,1701769293,https://stackoverflow.com/questions/77175532/is-it-possible-to-run-an-asset-from-an-image
77590019,Dagster : How to test chain of assets with IO managers,"<p>I am new to dagster and currently trying to tests assets.</p>
<p>The idea is the following, I want to be sure that a chain of asset is working properly. Here is the context, I have an asset <code>training_set</code> that from two <strong>SourceAsset</strong> <code>data_source1</code>  and <code>data_source2</code>  creates a dataset from training. Those two <strong>SourceAsset</strong> and the <code>training_set</code> asset use a custom <strong>ParquetIOManager</strong>.</p>
<p>Then I have another asset <code>predictions</code> which takes the <code>training_set</code> asset and a <code>predictor_model</code> <strong>SourceAsset</strong> (using a custom <strong>XGBoostIOManager</strong>  ) as inputs .</p>
<p>Therefore , the problem if I test the <code>training_set</code> and <code>predictions</code> assets like in the doc specified here : <a href=""https://docs.dagster.io/concepts/testing"" rel=""nofollow noreferrer"">https://docs.dagster.io/concepts/testing</a> :</p>
<pre class=""lang-py prettyprint-override""><code>from dagster import asset, materialize_to_memory


@asset
def data_source():
    return get_data_from_source()


@asset
def structured_data(data_source):
    return extract_structured_data(data_source)


# An example unit test using materialize_to_memory
def test_data_assets():
    result = materialize_to_memory([data_source, structured_data])
</code></pre>
<p>Adapted to my code it would look like this :</p>
<p><strong>asset file :</strong></p>
<pre class=""lang-py prettyprint-override""><code>data_source1 = SourceAsset(
    key=&quot;data_source1&quot;  ,
    io_manager_key=&quot;parquet_io_manager&quot;,
)

data_source2  = SourceAsset(
    key=&quot;data_source2&quot;  ,
    io_manager_key=&quot;parquet_io_manager&quot;,
)

xgboost_model= SourceAsset(
    key=f&quot;xgboost_model&quot;,
    io_manager_key=&quot;xgboost_io_manager&quot;,
)

@asset(
    io_manager_key=&quot;parquet_io_manager&quot;,
    partitions_def=fifteen_minutes_partitions,
)
def training_set(
    context: OpExecutionContext,
    data_source1 : pd.DataFrame,
    data_source2  : pd.DataFrame,
) -&gt; pd.DataFrame:
    
    return transform_into_training_set(data_source1, data_source2) 


@asset(
    io_manager_key=&quot;parquet_io_manager&quot;,
    partitions_def=fifteen_minutes_partitions,
)
def predictions(
    context: OpExecutionContext,
    training_set: pd.DataFrame,
    xgboost_model: xgb.XGBRegressor
)-&gt;pd.DataFrame:
    
    predictions = pd.DataFrame(
        xgboost_model.predict(training_set), 
        columns=TARGETS_LABELS
    )
    return predictions
    
</code></pre>
<p><strong>test file :</strong></p>
<pre><code>from assets import *
def test_data_assets():
    result = materialize_to_memory([training_set, predictions])
    assert result.success
</code></pre>
<p>Problem is that it would returns me that the <strong>io_manager</strong> key <code>parquet_io_manager</code> does not exists as it is defined nowhere in the asset file (but in production, it is in a Definition object)</p>
<p>My question is therefore : how can I test that <strong><code>training_set</code></strong> asset and <strong><code>predictions</code></strong>  assets work properly together?</p>
<p>I know I could create a job but sometimes I will not want to test my entire job but just two dependant assets</p>
<p>Have any suggestions?</p>
",0,1701513834,dagster,False,87,0,1701513834,https://stackoverflow.com/questions/77590019/dagster-how-to-test-chain-of-assets-with-io-managers
77231119,Can&#39;t enable run retries for Dagster on Windows with SQL Server,"<p>I am running dagster-webserver 1.4.16 on Windows Server 2016 with SQL Server. I just tried enabling run retries in dagster.yaml but got hit with the following error message when I tried to run dagster-webserver on console:</p>
<p><em>dagster._check.CheckError: Invariant failed. Description: Run retries are enabled, but the configured event log storage does not support them. Consider switching to Postgres or Mysql.</em></p>
<p>I can't switch to Postgres or MySQL as my company is all-in on SQL Server. Does anybody have any ideas about configuring Dagster to use SQL Server or simply use local storage for event logs? I wonder if there is a way to bypass the Postgres/MySQL requirement entirely.</p>
<p>I have looked in the Dagster source code for possibly overriding the existing methods for event logs but haven't found an easy way of doing this.</p>
",1,1696434839,python;sql-server;logging;dagster,False,205,0,1701098004,https://stackoverflow.com/questions/77231119/cant-enable-run-retries-for-dagster-on-windows-with-sql-server
77557614,Dagster : specify multiple TimeWindowPartitionMapping for a same asset dependency,"<p>I have a <code>SourceAsset</code> that represents a dataframe partitioned every 15 minutes, I have another asset that I call <code>lagged_df</code> in which I want to have the previous day and current partition <strong>at the same time</strong> . (In two parameters therefore)</p>
<pre class=""lang-py prettyprint-override""><code>original_dfs = SourceAsset(
    key=&quot;merged_data&quot;,
    io_manager_key=&quot;partitioned_parquet_io_manager&quot;,
    description=&quot;Descriptions&quot;,
    partitions_def=fifteen_minutes_partitions,
    metadata={&quot;io_manager&quot;: TimePartitionConfig(time_partition_column=&quot;timestamp&quot;)},
)


@asset(
    io_manager_key=&quot;partitioned_parquet_io_manager&quot;,
    partitions_def=fifteen_minutes_partitions,
    description=&quot;description&quot;,
    metadata={&quot;io_manager&quot;: TimePartitionConfig(time_partition_column=&quot;timestamp&quot;)},
    ins={
        &quot;previous_day_original_dfs &quot;: AssetIn(
            &quot;original_dfs&quot;,
            partition_mapping=TimeWindowPartitionMapping(start_offset=-4*24*7, end_offset=-4*24*7),
        ),
        &quot;current_original_dfs&quot;: AssetIn(
            &quot;original_dfs&quot;,
            partition_mapping=TimeWindowPartitionMapping(start_offset=0, end_offset=0),
        ),
    },
)
def lagged_df(
    context: OpExecutionContext,
    previous_original_dfs: pd.DataFrame,
    current_original_dfs: pd.DataFrame,
) -&gt; pd.DataFrame:
    foo = 2
    return pd.DataFrame()
</code></pre>
<p>However after launching the code in the debugger I always receive a <code>KeyError previous_day_original_dfs </code></p>
<p>I have tried to remove the <code>current_original_dfs</code> and it works, I do not have anymore an error for <code>previous_original_dfs</code>. Problem is therefore that it seems I can only put one &quot;<code>ins</code>&quot; with the same asset key. Which forces me to read a whole lot of data that I don't need (all data from now to the previous data)</p>
<p>Is there a way to specify two different <code>TimeWindowPartitionMapping</code> for a same asset?</p>
<p>If not do you have any workaround?</p>
",2,1701094865,python;dagster,False,42,0,1701094942,https://stackoverflow.com/questions/77557614/dagster-specify-multiple-timewindowpartitionmapping-for-a-same-asset-dependenc
77537303,Dagster unable to import module,"<p>I'm new to Dagster and I'm struggling importing my module into the Dagster code.
This is my project structure.</p>
<pre class=""lang-bash prettyprint-override""><code>.
├── pyproject.toml
├── README.md
├── setup.cfg
├── setup.py
├── my-project
│   ├── assets.py
│   ├── __init__.py
│   ├── __pycache__
│   │   ├── assets.cpython-311.pyc
│   │   └── __init__.cpython-311.pyc
│   ├── scaled_dataset.csv
│   └── utilities
│       ├── dataset.py
│       ├── __init__.py
│       ├── nn.py
│       ├── plotting.py
│       ├── testing.py
│       └── training.py
</code></pre>
<p>In my <code>__init__.py</code> of <code>my-poject</code> I have set</p>
<pre class=""lang-py prettyprint-override""><code>from dagster import Definitions, load_assets_from_modules

from . import assets, utilities

all_assets = load_assets_from_modules([assets, utilities])

defs = Definitions(
    assets=all_assets,
)

</code></pre>
<p>And in my code in <code>assets.py</code> I have (for example):</p>
<pre class=""lang-py prettyprint-override""><code>from utilities.plotting import plot_dataset
</code></pre>
<p>However, when I run `dagster dev', it returns:</p>
<pre class=""lang-bash prettyprint-override""><code>ModuleNotFoundError: No module named 'utilities'

Stack Trace:
  File &quot;/home/user/PycharmProjects/Dagster/venv/lib/python3.11/site-packages/dagster/_core/code_pointer.py&quot;, line 135, in load_python_module
    return importlib.import_module(module_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/usr/lib/python3.11/importlib/__init__.py&quot;, line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1204, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1176, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1147, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 690, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 940, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 241, in _call_with_frames_removed
  File &quot;/home/user/PycharmProjects/Dagster/my-project/my-project/__init__.py&quot;, line 3, in &lt;module&gt;
    from . import assets, utilities
  File &quot;/home/user/PycharmProjects/Dagster/my-project/my-project/assets.py&quot;, line 12, in &lt;module&gt;
    from utilities.plotting import plot_dataset

</code></pre>
<p>What am I missing?
Thanks</p>
",1,1700746427,python;import;python-module;dagster,True,643,1,1700759092,https://stackoverflow.com/questions/77537303/dagster-unable-to-import-module
77523630,How to run dbt test in dagster,"<p>I'm using dagster as a data orchestration tool, with its dbt integration. Since dagster is a data asset-centred framework, while dbt asset are materialised smoothly, I'm struggling to find a way to run dbt test dynamically within dagster framework. What I mean is, I have multiple tests with different schedules, and I want to dynamically run them without having to write dbt assets every time I add a new dbt test.</p>
<p>I can do <code>dbt build</code>, which runs all the tests within a layer at the same scheduled time. This is not very dynamic.</p>
<p>For example, if I want to run a test on asset-a, I'd like a schedule that follows the schedule of asset-a. I tried using tags:</p>
<pre><code>{{ config(
    tags=[&quot;grexx_a&quot;]
) }}
</code></pre>
<p>in both asset-a.sql and test_asset_a.sql but the test doesnt seem to run. Any suggestions?</p>
",0,1700576968,dbt;dagster,False,154,0,1700675902,https://stackoverflow.com/questions/77523630/how-to-run-dbt-test-in-dagster
77529956,How to connect to files in Dockerized Dagster instance?,"<p>I have managed to deploy Dagster to Docker and I can load the web UI etc.</p>
<p>However, I am really struggling to understand how to create a development environment locally e.g access remote files.</p>
<p>I have tried creating a mount volume and I can SSH onto the host server but I hit permissions issues galore when trying to access the volume.</p>
<p>I am sure I must be missing something or doing something wrong in my approach.</p>
<p>I just want to create a simple data pipeline in a dockerized instance of Dagster - but cannot connect to my files to actually do the dev work.</p>
<p>Any help would be greatly appreciated.</p>
",0,1700656270,docker;dagster,False,66,0,1700656270,https://stackoverflow.com/questions/77529956/how-to-connect-to-files-in-dockerized-dagster-instance
77488973,"azure.core.exceptions.ResourceExistsError: The specified path, or an element of the path, exists and its resource type is invalid for this operation","<p>I'm using dagster and azure to download files from a SFTP server to our blob storage. this is our Azure resource:</p>
<pre><code>class AzureBlobStorageResource(ConfigurableResource):
    storageacc: str

    def get_blob_service_client(self):
        try:
            if is_bitbucket_docker_run_environment is False:
                # Create the BlobServiceClient object
                account_url = f&quot;https://{self.storageacc}.blob.core.windows.net&quot;
                credential = get_credential()
                blob_service_client = BlobServiceClient(
                    account_url, credential=credential
                )
                return blob_service_client
        except Exception:
            return None

    def download_file_obj(self, container_name: str, filename: str) -&gt; str:
        blob_service_client = self.get_blob_service_client()
        return blob_service_client.get_blob_client(
            container_name, filename
        ).download_blob(encoding=&quot;utf8&quot;)

</code></pre>
<p>our dagster asset:</p>
<pre><code>@multi_asset(
    outs={
        &quot;data&quot;: AssetOut(
            key_prefix=[&quot;stfp&quot;], io_manager_key=&quot;adls2_pickle_io_manager&quot;
        ),
        &quot;metadata&quot;: AssetOut(
            key_prefix=[&quot;stfp&quot;], io_manager_key=&quot;adls2_pickle_io_manager&quot;
        ),
    },
    partitions_def=partitions_def,
    compute_kind=&quot;pandas&quot;,
)
def source_files(
        context: OpExecutionContext, 
        stfp: SftpResource,
        azurebs: AzureBlobStorageResource
    ) -&gt; Tuple[PandasDF, PandasDF]:

    start, end = context.partition_time_window
    context.log.info(f&quot;Processing asset partition '{start}-{end}'&quot;)
    
    path = (start).strftime(&quot;%Y%m/%d&quot;)
    context.log.info(path)
    files = whiffle.sftp_list_files(f&quot;{INCOMING_REMOTE_DIR}/{path}&quot;)
    if len(files) &gt; 1:
        context.log.debug(f&quot;More than 1 file in path {path}&quot;)
    elif len(files) == 0:
        context.log.debug(f&quot;No file found in path {path}&quot;)
    else:
        remote_file = f&quot;{INCOMING_REMOTE_DIR}/{path}/{files[0]}&quot;
        context.log.info(f&quot;Downloading file to Blob Storage: {remote_file}&quot;)
        blob_file_path = f&quot;/{path}/{files[0]}&quot;
        stfp.from_sftp_to_blob(remote_file, blob_file_path)

    context.log.info(f&quot;Ingesting file to deltalake: {blob_file_path}&quot;)

    downloaded_blob = azurebs.download_file_obj(
        container_name=BLOB_CONTAINER, 
        filename=BLOB_DIRECTORY+blob_file_path
    )

    shortid = str(uuid.uuid4())[:7]
    data = pd.read_csv(io.StringIO(downloaded_blob.readall()))
    data[&quot;startdatetime_utc&quot;] = pd.to_datetime(data[&quot;startdatetime_utc&quot;], utc=True)
    data[&quot;id&quot;] = shortid
    context.log.info(f&quot;dataframe shape: {data.shape}&quot;)
    context.log.info(
        f&quot;min-max datetime: {data.startdatetime_utc.min()}-{data.startdatetime_utc.max()}&quot;
    )
    context.add_output_metadata(
        metadata={&quot;no_records&quot;: len(data)}, output_name=&quot;data&quot;
    )
    context.add_output_metadata(
        metadata={&quot;sum_value&quot;: data.forecast_raw_kw.sum()}, output_name=&quot;forecasts&quot;
    )

    filename = files[0]
    context.log.info(filename)
    metadata = pd.DataFrame({
        &quot;id&quot;: [shortid], 
        &quot;loaded_at&quot;: [datetime.now(timezone.utc)],
        &quot;filename&quot;: [filename],
    })
        
    context.log.info(f&quot;{len(forecasts)}&quot;)
    context.log.info(data.sample(1))
    
    context.log.info(metadata.sample(1))
    context.log.info(f&quot;metadata dataframe shape: {metadata.shape}&quot;)
    context.add_output_metadata(
        metadata={&quot;no_records&quot;: len(metadata)},
        output_name=&quot;metadata&quot;,
    )

    return data, metadata
</code></pre>
<p>The pipeline runs fine and normally, and then it ran into this issue:</p>
<pre><code>dagster._core.errors.DagsterExecutionHandleOutputError: Error occurred while handling output &quot;data&quot; of step &quot;source_files&quot;:

  File &quot;/usr/local/lib/python3.9/site-packages/dagster/_core/execution/plan/execute_plan.py&quot;, line 273, in dagster_event_sequence_for_step
    for step_event in check.generator(step_events):
  File &quot;/usr/local/lib/python3.9/site-packages/dagster/_core/execution/plan/execute_step.py&quot;, line 482, in core_dagster_event_sequence_for_step
    for evt in _type_check_and_store_output(step_context, user_event):
  File &quot;/usr/local/lib/python3.9/site-packages/dagster/_core/execution/plan/execute_step.py&quot;, line 533, in _type_check_and_store_output
    for evt in _store_output(step_context, step_output_handle, output):
  File &quot;/usr/local/lib/python3.9/site-packages/dagster/_core/execution/plan/execute_step.py&quot;, line 724, in _store_output
    for elt in iterate_with_context(
  File &quot;/usr/local/lib/python3.9/site-packages/dagster/_utils/__init__.py&quot;, line 445, in iterate_with_context
    return
  File &quot;/usr/local/lib/python3.9/contextlib.py&quot;, line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File &quot;/usr/local/lib/python3.9/site-packages/dagster/_core/execution/plan/utils.py&quot;, line 84, in op_execution_error_boundary
    raise error_cls(

The above exception was caused by the following exception:
azure.core.exceptions.ResourceExistsError: The specified path, or an element of the path, exists and its resource type is invalid for this operation.
RequestId:6b583d4d-501f-0098-135f-15cacf000000
Time:2023-11-12T11:55:00.4663911Z
ErrorCode:PathConflict

  File &quot;/usr/local/lib/python3.9/site-packages/dagster/_core/execution/plan/utils.py&quot;, line 54, in op_execution_error_boundary
    yield
  File &quot;/usr/local/lib/python3.9/site-packages/dagster/_utils/__init__.py&quot;, line 443, in iterate_with_context
    next_output = next(iterator)
  File &quot;/usr/local/lib/python3.9/site-packages/dagster/_core/execution/plan/execute_step.py&quot;, line 714, in _gen_fn
    gen_output = output_manager.handle_output(output_context, output.value)
  File &quot;/usr/local/lib/python3.9/site-packages/dagster/_core/storage/upath_io_manager.py&quot;, line 453, in handle_output
    self.dump_to_path(context=context, obj=obj, path=path)
  File &quot;/usr/local/lib/python3.9/site-packages/dagster_azure/adls2/io_manager.py&quot;, line 104, in dump_to_path
    file = self.file_system_client.create_file(str(path))
  File &quot;/usr/local/lib/python3.9/site-packages/azure/core/tracing/decorator.py&quot;, line 78, in wrapper_use_tracer
    return func(*args, **kwargs)
  File &quot;/usr/local/lib/python3.9/site-packages/azure/storage/filedatalake/_file_system_client.py&quot;, line 835, in create_file
    file_client.create_file(**kwargs)
  File &quot;/usr/local/lib/python3.9/site-packages/azure/core/tracing/decorator.py&quot;, line 78, in wrapper_use_tracer
    return func(*args, **kwargs)
  File &quot;/usr/local/lib/python3.9/site-packages/azure/storage/filedatalake/_data_lake_file_client.py&quot;, line 225, in create_file
    return self._create('file', content_settings=content_settings, metadata=metadata, **kwargs)
  File &quot;/usr/local/lib/python3.9/site-packages/azure/storage/filedatalake/_path_client.py&quot;, line 301, in _create
    process_storage_error(error)
  File &quot;/usr/local/lib/python3.9/site-packages/azure/storage/filedatalake/_deserialize.py&quot;, line 221, in process_storage_error
    exec(&quot;raise error from None&quot;)   # pylint: disable=exec-used # nosec
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/usr/local/lib/python3.9/site-packages/azure/storage/filedatalake/_path_client.py&quot;, line 299, in _create
    return self._client.path.create(**options)
  File &quot;/usr/local/lib/python3.9/site-packages/azure/core/tracing/decorator.py&quot;, line 78, in wrapper_use_tracer
    return func(*args, **kwargs)
  File &quot;/usr/local/lib/python3.9/site-packages/azure/storage/filedatalake/_generated/operations/_path_operations.py&quot;, line 1170, in create
    map_error(status_code=response.status_code, response=response, error_map=error_map)
  File &quot;/usr/local/lib/python3.9/site-packages/azure/core/exceptions.py&quot;, line 165, in map_error
    raise error
</code></pre>
<p>Note that I cannot reproduce this issue; when I reran the pipeline on command, the file on Azure Blob Storage got overwritten normally.
The error was also documented widely, I can only find it here: <a href=""https://learn.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/update?view=rest-storageservices-datalakestoragegen2-2019-12-12"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/update?view=rest-storageservices-datalakestoragegen2-2019-12-12</a></p>
<p>My packages:</p>
<pre><code>adlfs==2023.10.0
azure-storage-blob==12.18.3
dagster-azure==0.20.15
dagster==1.4.15
python==3.8
</code></pre>
",0,1700062103,python;azure-blob-storage;dagster,False,235,0,1700062103,https://stackoverflow.com/questions/77488973/azure-core-exceptions-resourceexistserror-the-specified-path-or-an-element-of
75350111,dagster can you trigger a job to run via an api?,"<p>I have been looking all over for the answer, but can't seem to find what I'm looking for</p>
<p>I want to create an api endpoint that can pass information to the dagster assets and trigger a run. For example, I have the following asset in dagster</p>
<pre><code>@asset
def player_already_registered(player_name: str):
    q = text(
            f'''
                SELECT 
                    COUNT(*)
                FROM
                    `player_account_info`
                WHERE
                    summonerName = :player_name
            '''
        )

    result = database.conn.execute(q, player_name=player_name).fetchone()[0]
    return bool(result)
</code></pre>
<p>Say that I have an endpoint already made where I can pass the <code>player_name</code> via a get-parameter. How can I pass the parameter to the asset and then run the job itself?</p>
",4,1675571077,python;python-3.x;rest;pipeline;dagster,True,1967,1,1699498627,https://stackoverflow.com/questions/75350111/dagster-can-you-trigger-a-job-to-run-via-an-api
77389392,Creating and accessing Dagster resource for AWS Secrets Manager,"<p>I am having some difficulty understanding how to use Dagster's library to access AWS Secrets Manager secrets as a resource and accessing the resource in assets</p>
<p>I have a file structure as follows</p>
<pre><code>-proj
    -assets
        __init__.py
        asset_script.py
    -resources 
        __init__.py
        secrets_manager.py
    __init__.py
</code></pre>
<p>in secrets_manager.py I have the following</p>
<pre><code>from dagster import ConfigurableResource
from dagster_aws.secretsmanager import SecretsManagerSecretsResource


class SecretsConnection(ConfigurableResource):
    sm = SecretsManagerSecretsResource(
        region_name=&quot;us-east-1&quot;,
        add_to_environment=False,
    )

    def get_secrets(self):
        return self.sm.fetch_secrets()
</code></pre>
<p>In my top level init (proj/<strong>init</strong>.py) i have the following:</p>
<pre><code>from .assets import (
    asset_script
)
from .resources import (
    secrets_manager
)

assets = load_assets_from_package_module(
    package_module=asset_script,
    group_name=&quot;group_name&quot;,
)

defs = Definitions(
    assets=assets,
    resources={
        &quot;secrets_manager&quot;: secrets_manager.SecretsConnection(),
    },
)

</code></pre>
<p>In my assets file I attempt to access these in the following way:</p>
<pre><code>@asset(required_resource_keys={&quot;secrets_manager&quot;})
def fn_name(context: AssetExecutionContext):
    secrets = context.resources.secrets_manager.get_secrets()
</code></pre>
<p>I get an error that I can't make much sense of (related to the ConfigurableResource SecretsConnection):</p>
<pre><code>Traceback (most recent call last):
  File &quot;venv/lib/python3.8/site-packages/dagster/_grpc/server.py&quot;, line 295, in __init__
    self._loaded_repositories: Optional[LoadedRepositories] = LoadedRepositories(
  File &quot;venv/lib/python3.8/site-packages/dagster/_grpc/server.py&quot;, line 139, in __init__
    loadable_targets = get_loadable_targets(
  File &quot;venv/lib/python3.8/site-packages/dagster/_grpc/utils.py&quot;, line 50, in get_loadable_targets
    else loadable_targets_from_python_module(module_name, working_directory)
  File &quot;/venv/lib/python3.8/site-packages/dagster/_core/workspace/autodiscovery.py&quot;, line 35, in loadable_targets_from_python_module
    module = load_python_module(
  File &quot;venv/lib/python3.8/site-packages/dagster/_core/code_pointer.py&quot;, line 135, in load_python_module
    return importlib.import_module(module_name)
  File &quot;/usr/local/Cellar/python@3.8/3.8.17_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1014, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 991, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 975, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 671, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 843, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;proj/__init__.py&quot;, line 10, in &lt;module&gt;
    from .resources import (
  File &quot;proj/resources/secrets_manager.py&quot;, line 5, in &lt;module&gt;
    class SecretsConnection(ConfigurableResource):
  File &quot;/venv/lib/python3.8/site-packages/dagster/_config/pythonic_config/typing_utils.py&quot;, line 134, in __new__
    return super().__new__(cls, name, bases, namespaces, **kwargs)
  File &quot;/venv/lib/python3.8/site-packages/dagster/_config/pythonic_config/typing_utils.py&quot;, line 78, in __new__
    return super().__new__(cls, name, bases, namespaces, **kwargs)
  File &quot;pydantic/main.py&quot;, line 221, in pydantic.main.ModelMetaclass.__new__
  File &quot;pydantic/fields.py&quot;, line 506, in pydantic.fields.ModelField.infer
  File &quot;pydantic/fields.py&quot;, line 436, in pydantic.fields.ModelField.__init__
  File &quot;pydantic/fields.py&quot;, line 546, in pydantic.fields.ModelField.prepare
  File &quot;pydantic/fields.py&quot;, line 570, in pydantic.fields.ModelField._set_default_and_type
  File &quot;pydantic/fields.py&quot;, line 439, in pydantic.fields.ModelField.get_default
  File &quot;pydantic/utils.py&quot;, line 693, in pydantic.utils.smart_deepcopy
  File &quot;/usr/local/Cellar/python@3.8/3.8.17_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/copy.py&quot;, line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File &quot;/usr/local/Cellar/python@3.8/3.8.17_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/copy.py&quot;, line 270, in _reconstruct
    state = deepcopy(state, memo)
  File &quot;/usr/local/Cellar/python@3.8/3.8.17_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/copy.py&quot;, line 146, in deepcopy
    y = copier(x, memo)
  File &quot;/usr/local/Cellar/python@3.8/3.8.17_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/copy.py&quot;, line 230, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File &quot;/usr/local/Cellar/python@3.8/3.8.17_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/copy.py&quot;, line 146, in deepcopy
    y = copier(x, memo)
  File &quot;/usr/local/Cellar/python@3.8/3.8.17_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/copy.py&quot;, line 230, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File &quot;/usr/local/Cellar/python@3.8/3.8.17_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/copy.py&quot;, line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File &quot;/usr/local/Cellar/python@3.8/3.8.17_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/copy.py&quot;, line 264, in _reconstruct
    y = func(*args)
  File &quot;/usr/local/Cellar/python@3.8/3.8.17_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/copy.py&quot;, line 263, in &lt;genexpr&gt;
    args = (deepcopy(arg, memo) for arg in args)
  File &quot;/usr/local/Cellar/python@3.8/3.8.17_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/copy.py&quot;, line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File &quot;/usr/local/Cellar/python@3.8/3.8.17_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/copy.py&quot;, line 270, in _reconstruct
    state = deepcopy(state, memo)
  File &quot;/usr/local/Cellar/python@3.8/3.8.17_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/copy.py&quot;, line 146, in deepcopy
    y = copier(x, memo)
  File &quot;/usr/local/Cellar/python@3.8/3.8.17_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/copy.py&quot;, line 230, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File &quot;/usr/local/Cellar/python@3.8/3.8.17_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/copy.py&quot;, line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File &quot;/usr/local/Cellar/python@3.8/3.8.17_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/copy.py&quot;, line 270, in _reconstruct
    state = deepcopy(state, memo)
  File &quot;/usr/local/Cellar/python@3.8/3.8.17_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/copy.py&quot;, line 146, in deepcopy
    y = copier(x, memo)
  File &quot;/usr/local/Cellar/python@3.8/3.8.17_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/copy.py&quot;, line 230, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File &quot;/usr/local/Cellar/python@3.8/3.8.17_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/copy.py&quot;, line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File &quot;/usr/local/Cellar/python@3.8/3.8.17_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/copy.py&quot;, line 264, in _reconstruct
    y = func(*args)
  File &quot;/usr/local/Cellar/python@3.8/3.8.17_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/copyreg.py&quot;, line 91, in __newobj__
    return cls.__new__(cls, *args)
TypeError: __new__() missing 1 required positional argument: 'fields'
/venv/lib/python3.8/site-packages/dagster/_core/workspace/context.py:616: UserWarning: Error loading repository location testing:TypeError: __new__() missing 1 required positional argument: 'fields'


</code></pre>
<p>This is followed by the warning:</p>
<pre><code>warnings.warn(f&quot;Error loading repository location {location_name}:{error.to_string()}&quot;)
</code></pre>
",1,1698675339,amazon-web-services;dagster,False,182,1,1698680389,https://stackoverflow.com/questions/77389392/creating-and-accessing-dagster-resource-for-aws-secrets-manager
77356809,In dagster sensors is there a way to re run a RunRequest for a run key,"<p>I have a sensor, whenever there is a new file in my file system it will create a run request with file path as run_key. And a asset job will be invoked. That job will fetch the file and do some operations and write it to an external database.</p>
<p>But it is possible that the external database write could fail so I need to re try the file at a different point of time. I don't want to add a retry mechanism in the asset materialization. So is there a way to make dagster to invalidate the run request and allow me to invoke the run request again with the same filepath ?</p>
",0,1698211575,python;dagster,False,227,0,1698211575,https://stackoverflow.com/questions/77356809/in-dagster-sensors-is-there-a-way-to-re-run-a-runrequest-for-a-run-key
77330395,Running Dagster on Docker: &quot;ImportError: cannot import name &#39;Literal&#39; from &#39;typing&#39; (/usr/local/lib/python3.7/typing.py)&quot;,"<p>I'm trying to set up dagster to run on Docker, eventually on Docker Swarm + Portainer, however while setting this up I'm getting this error:</p>
<p><code> ImportError: cannot import name 'Literal' from 'typing' (/usr/local/lib/python3.7/typing.py)</code></p>
<p>Full stacktrace here:</p>
<pre><code>Traceback (most recent call last):
2023-10-20 13:35:47   File &quot;/usr/local/bin/dagster-daemon&quot;, line 5, in &lt;module&gt;
2023-10-20 13:35:47     from dagster.daemon.cli import main
2023-10-20 13:35:47   File &quot;/usr/local/lib/python3.7/site-packages/dagster/__init__.py&quot;, line 480, in &lt;module&gt;
2023-10-20 13:35:47     from dagster._core.pipes.client import (
2023-10-20 13:35:47   File &quot;/usr/local/lib/python3.7/site-packages/dagster/_core/pipes/client.py&quot;, line 5, in &lt;module&gt;
2023-10-20 13:35:47     from dagster_pipes import (
2023-10-20 13:35:47   File &quot;/usr/local/lib/python3.7/site-packages/dagster_pipes/__init__.py&quot;, line 15, in &lt;module&gt;
2023-10-20 13:35:47     from typing import (
2023-10-20 13:35:47 ImportError: cannot import name 'Literal' from 'typing' (/usr/local/lib/python3.7/typing.py)
2023-10-20 13:53:50 Traceback (most recent call last):
2023-10-20 13:53:50   File &quot;/usr/local/bin/dagster-daemon&quot;, line 5, in &lt;module&gt;
2023-10-20 13:53:50     from dagster.daemon.cli import main
2023-10-20 13:53:50   File &quot;/usr/local/lib/python3.7/site-packages/dagster/__init__.py&quot;, line 480, in &lt;module&gt;
2023-10-20 13:53:50     from dagster._core.pipes.client import (
2023-10-20 13:53:50   File &quot;/usr/local/lib/python3.7/site-packages/dagster/_core/pipes/client.py&quot;, line 5, in &lt;module&gt;
2023-10-20 13:53:50     from dagster_pipes import (
2023-10-20 13:53:50   File &quot;/usr/local/lib/python3.7/site-packages/dagster_pipes/__init__.py&quot;, line 15, in &lt;module&gt;
2023-10-20 13:53:50     from typing import (
2023-10-20 13:53:50 ImportError: cannot import name 'Literal' from 'typing' (/usr/local/lib/python3.7/typing.py)
</code></pre>
<p>My Dockerfile looks like this:</p>
<pre><code>FROM python:3.11-slim

WORKDIR /usr/src/app
ENV DAGSTER_HOME=/usr/src/app


RUN pip install dagster dagster-webserver dagit dagster-postgres SQLAlchemy==1.4.49 pandas pyarrow

# # Copy our code and workspace to /usr/src/app
COPY dagster.yaml
# COPY etl ./etl
COPY pyproject.toml .
COPY setup.cfg .
COPY setup.py .

EXPOSE 3000

CMD [&quot;dagster-webserver&quot;, &quot;-h&quot;, &quot;0.0.0.0&quot;, &quot;-p&quot;, &quot;3000&quot;]
</code></pre>
<p>This is the Dagster yaml file:</p>
<pre><code>run_storage:
  module: dagster_postgres.run_storage
  class: PostgresRunStorage
  config:
    postgres_db:
      username: postgres
      password: demopass
      hostname: dagster-postgres
      db_name: postgres
      port: 5432

event_log_storage:
  module: dagster_postgres.event_log
  class: PostgresEventLogStorage
  config:
    postgres_db:
      username: postgres
      password: demopass
      hostname: dagster-postgres
      db_name: postgres
      port: 5432

schedule_storage:
  module: dagster_postgres.schedule_storage
  class: PostgresScheduleStorage
  config:
    postgres_db:
      username: postgres
      password: demopass
      hostname: dagster-postgres
      db_name: postgres
      port: 5432

run_coordinator:
  module: dagster.core.run_coordinator
  class: QueuedRunCoordinator
  config:
    max_concurrent_runs: 5
    
telemetry:
  enabled: false
nux:
  enabled: false
</code></pre>
<p>And the Docker compose yaml:</p>
<pre><code>services:

  dagster-dagit:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - 3000:3000

  dagster-daemon:
    build:
      context: .
      dockerfile: Dockerfile
    command: &quot;dagster-daemon run&quot;
    environment:
      - PGPASS=${PGPASS}
      - PGUID=${PGUID}

  dagster-postgres:
    image: postgres:13.3
    ports:
      - 5432:5432
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    volumes:
      - dagster-postgres:/var/lib/postgresql/data
volumes:
  dagster-postgres:
    driver: local
</code></pre>
<p>I thought it might be a version incompatibility issue with pandas, however that seems fine. Also looked at a few similar threads suggesting that ENV variables might be missing, but mine are all set.</p>
<p>Any ideas?</p>
<p>Tried different versions to check compatibility and checked whether env variables are set properly.
Expect to have all 3 containers to be up, however only the postgres one is running currently.</p>
",0,1697799665,python;docker;docker-compose;portainer;dagster,False,117,1,1697878171,https://stackoverflow.com/questions/77330395/running-dagster-on-docker-importerror-cannot-import-name-literal-from-typi
77301644,How do you set compute/memory limits to runs in Dagster (self-hosted)?,"<p>I have a <code>Dagster</code> deployment running locally - it's small enough that it can be cloned to a PC/Laptop and run to update some assets (using the local IO pickle manager).</p>
<p>However, the dag has some very wide sections where 50+ assets &quot;could&quot; run at once. There is also some narrower but compute heavy sections, where maybe only 3 assets could be materialized at once but each takes all available CPU/Memory.</p>
<p>The issue is that when either of these things run (the 50+ assets or the 3+ heavy assets) the PC/Laptop becomes entirely unresponsive due to resource starvation.</p>
<p>I know I can set concurrency limits to Dagster, but this does not fully solved my issue. Is there a way to limit available CPU/Memory for Dagster?</p>
<p>I tried using the <code>QueuedRunCoordinator</code> to limit the # of simultaneous runs, but this is not solving my issue of too many &quot;light&quot; assets materializing at the same time, or the few &quot;heavy&quot; assets gobbling up resources.</p>
<pre class=""lang-yaml prettyprint-override""><code>run_coordinator:
  module: dagster.core.run_coordinator
  class: QueuedRunCoordinator
  config:
    max_concurrent_runs: 8
</code></pre>
",0,1697456275,dagster,False,238,0,1697456275,https://stackoverflow.com/questions/77301644/how-do-you-set-compute-memory-limits-to-runs-in-dagster-self-hosted
77279919,Is there a way to get dagster the deamon / source location metrics?,"<p>On occasion I have code location failing to load or crashing due to a bug in my code.</p>
<p>Obviously I need to resolve the bug, but also, I would be able to alert on the error.</p>
<p>I noticed dagster itself is able to detect issues with code location and I am wondering if there are ways to extract these metrics, possibly through a prometheus endpoint or other mechanism.</p>
<p>I noticed dagster itself is able to detect this (failing code location, see image),
but I have not been able to monitor on this event.</p>
<p>I read the docs on <a href=""https://docs.dagster.io/_apidocs/hooks"" rel=""nofollow noreferrer"">hooks</a> and <a href=""https://docs.dagster.io/_apidocs/libraries/dagster-prometheus"" rel=""nofollow noreferrer"">prometheus</a> but these are all on <code>Job</code> level.</p>
<p>Is there a way to gather these statistics on deamon or orchestrator level?</p>
<p><a href=""https://i.stack.imgur.com/rYlwR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rYlwR.png"" alt=""enter image description here"" /></a></p>
",0,1697107795,dagster,False,79,0,1697107795,https://stackoverflow.com/questions/77279919/is-there-a-way-to-get-dagster-the-deamon-source-location-metrics
77250192,Error when adding new assets in dagster which is dependent on duckdb,"<p>When I try to add a new asset in dagster which should create a table based on the csv file gotten from a previous asset, I get the error on my dagster server which runs on localhost:3000</p>
<pre><code>dagster._core.errors.DagsterInvalidDefinitionError: resource with key 'duckdb' required by op 'create_scores_table' was not provided. Please provide a ResourceDefinition to key 'duckdb', or change the required key to one of the following keys which points to an ResourceDefinition: ['io_manager']

  File &quot;/home/ubuntu/Documents/dbt_dagster_proj/dbt_dags/lib/python3.9/site-packages/dagster/_grpc/server.py&quot;, line 295, in __init__
    self._loaded_repositories: Optional[LoadedRepositories] = LoadedRepositories(
  File &quot;/home/ubuntu/Documents/dbt_dagster_proj/dbt_dags/lib/python3.9/site-packages/dagster/_grpc/server.py&quot;, line 139, in __init__
    loadable_targets = get_loadable_targets(
  File &quot;/home/ubuntu/Documents/dbt_dagster_proj/dbt_dags/lib/python3.9/site-packages/dagster/_grpc/utils.py&quot;, line 47, in get_loadable_targets
    else loadable_targets_from_python_module(module_name, working_directory)
  File &quot;/home/ubuntu/Documents/dbt_dagster_proj/dbt_dags/lib/python3.9/site-packages/dagster/_core/workspace/autodiscovery.py&quot;, line 35, in loadable_targets_from_python_module
    module = load_python_module(
  File &quot;/home/ubuntu/Documents/dbt_dagster_proj/dbt_dags/lib/python3.9/site-packages/dagster/_core/code_pointer.py&quot;, line 135, in load_python_module
    return importlib.import_module(module_name)
  File &quot;/home/ubuntu/anaconda3/lib/python3.9/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1030, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1007, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 986, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 680, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 850, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 228, in _call_with_frames_removed
  File &quot;/home/ubuntu/Documents/dbt_dagster_proj/dagster_sportsdata/dagster_sportsdata/__init__.py&quot;, line 7, in &lt;module&gt;
    defs = Definitions(
  File &quot;/home/ubuntu/Documents/dbt_dagster_proj/dbt_dags/lib/python3.9/site-packages/dagster/_core/definitions/definitions_class.py&quot;, line 433, in __init__
    self._created_pending_or_normal_repo = _create_repository_using_definitions_args(
  File &quot;/home/ubuntu/Documents/dbt_dagster_proj/dbt_dags/lib/python3.9/site-packages/dagster/_core/definitions/definitions_class.py&quot;, line 304, in _create_repository_using_definitions_args
    def created_repo():
  File &quot;/home/ubuntu/Documents/dbt_dagster_proj/dbt_dags/lib/python3.9/site-packages/dagster/_core/definitions/decorators/repository_decorator.py&quot;, line 118, in __call__
    repository_definitions = fn()
  File &quot;/home/ubuntu/Documents/dbt_dagster_proj/dbt_dags/lib/python3.9/site-packages/dagster/_core/definitions/definitions_class.py&quot;, line 306, in created_repo
    *with_resources(assets or [], resource_defs),
  File &quot;/home/ubuntu/Documents/dbt_dagster_proj/dbt_dags/lib/python3.9/site-packages/dagster/_core/execution/with_resources.py&quot;, line 105, in with_resources
    transformed_defs.append(cast(T, definition.with_resources(resource_defs)))
  File &quot;/home/ubuntu/Documents/dbt_dagster_proj/dbt_dags/lib/python3.9/site-packages/dagster/_core/definitions/assets.py&quot;, line 1308, in with_resources
    attributes_dict[&quot;resource_defs&quot;] = merge_resource_defs(
  File &quot;/home/ubuntu/Documents/dbt_dagster_proj/dbt_dags/lib/python3.9/site-packages/dagster/_core/definitions/resource_requirement.py&quot;, line 268, in merge_resource_defs
    ensure_requirements_satisfied(
  File &quot;/home/ubuntu/Documents/dbt_dagster_proj/dbt_dags/lib/python3.9/site-packages/dagster/_core/definitions/resource_requirement.py&quot;, line 228, in ensure_requirements_satisfied
    raise DagsterInvalidDefinitionError(

</code></pre>
<p>This is my directory tree structure:</p>
<pre><code>.
├── dagster_sportsdata
│   ├── assets.py
│   ├── __init__.py
│   └── __pycache__
│       ├── assets.cpython-39.pyc
│       └── __init__.cpython-39.pyc
├── dagster_sportsdata.egg-info
│   ├── dependency_links.txt
│   ├── PKG-INFO
│   ├── requires.txt
│   ├── SOURCES.txt
│   └── top_level.txt
├── dagster_sportsdata_tests
│   ├── __init__.py
│   └── test_assets.py
├── footballstanding.csv
├── footscores.csv
├── pyproject.toml
├── README.md
├── setup.cfg
├── setup.py
└── tmp6jvpinn6
    ├── history
    │   ├── runs
    │   │   ├── 22c4874e-639f-4014-bd0c-3b365a1f1b09.db
    │   │   └── index.db
    │   └── runs.db
    ├── schedules
    │   └── schedules.db
    └── storage
        ├── 22c4874e-639f-4014-bd0c-3b365a1f1b09
        │   └── compute_logs
        │       ├── mosjsvar.complete
        │       ├── mosjsvar.err
        │       └── mosjsvar.out
        └── league_standing
</code></pre>
<p>And this is my asset.py file:</p>
<pre><code>import requests
from bs4 import BeautifulSoup
import pandas as pd
from dagster import AssetExecutionContext, asset
from dagster_duckdb import DuckDBResource
from dagster import Definitions
import os


current_directory = os.getcwd() 
database_file = os.path.join(current_directory, &quot;my_duckdb_database.duckdb&quot;)


@asset
def league_standing():
    urls = [
    {&quot;url&quot;: &quot;https://www.skysports.com/ligue-1-table&quot;, &quot;source&quot;: &quot;Ligue 1&quot;},
    {&quot;url&quot;: &quot;https://www.skysports.com/premier-league-table&quot;, &quot;source&quot;: &quot;Premier League&quot;},
    {&quot;url&quot;: &quot;https://www.skysports.com/la-liga-table&quot;, &quot;source&quot;: &quot;la liga&quot;},
    {&quot;url&quot;: &quot;https://www.skysports.com/bundesliga-table&quot;, &quot;source&quot;: &quot;Bundesliga&quot;},
    {&quot;url&quot;: &quot;https://www.skysports.com/serie-a-table&quot;, &quot;source&quot;: &quot;Seria A&quot;},
    {&quot;url&quot;: &quot;https://www.skysports.com/eredivisie-table&quot;, &quot;source&quot;: &quot;Eredivisie&quot;},
    {&quot;url&quot;: &quot;https://www.skysports.com/scottish-premier-table&quot;, &quot;source&quot;: &quot;Scottish premiership&quot;}
    ]
    dfs = []

    for url_info in urls:
        url = url_info[&quot;url&quot;]
        source = url_info[&quot;source&quot;]

        # Send HTTP Request and Parse HTML
        r = requests.get(url)
        soup = BeautifulSoup(r.text, &quot;lxml&quot;)

        # Find and Extract Table Headers
        table = soup.find(&quot;table&quot;, class_=&quot;standing-table__table&quot;)
        headers = table.find_all(&quot;th&quot;)
        titles = [i.text for i in headers]

        # Create an Empty DataFrame
        df = pd.DataFrame(columns=titles)

        # Iterate Through Table Rows and Extract Data
        rows = table.find_all(&quot;tr&quot;)
        for i in rows[1:]:
            data = i.find_all(&quot;td&quot;)
            row = [tr.text.strip() for tr in data]  # Apply .strip() to remove \n
            l = len(df)
            df.loc[l] = row

        # Add a column for source URL
        df[&quot;Source&quot;] = source

        # Append the DataFrame to the list
        dfs.append(df)

    # Concatenate all DataFrames into a single DataFrame
    football_standing = pd.concat(dfs, ignore_index=True)
    football_standing.to_csv(&quot;footballstanding.csv&quot;)


@asset
def get_scores():
    url = &quot;https://www.skysports.com/football-results&quot;
    r = requests.get(url)
    soup = BeautifulSoup(r.text, &quot;lxml&quot;)

    home_team = soup.find_all(&quot;span&quot;, class_=&quot;matches__item-col matches__participant matches__participant--side1&quot;)
    x = [name.strip() for i in home_team for name in i.stripped_strings]

    scores = soup.find_all(&quot;span&quot;, class_=&quot;matches__teamscores&quot;)
    s = [name.strip().replace('\n\n', '\n') for i in scores for name in i.stripped_strings]
    appended_scores = [f&quot;{s[i]}\n{s[i+1]}&quot;.replace('\n', ' ') for i in range(0, len(s), 2)]

    away_team = soup.find_all(&quot;span&quot;, class_=&quot;matches__item-col matches__participant matches__participant--side2&quot;)
    y = [name.strip() for i in away_team for name in i.stripped_strings]

    # Make sure all arrays have the same length
    min_length = min(len(x), len(appended_scores), len(y))
    data = {&quot;Home Team&quot;: x[:min_length], &quot;Scores&quot;: appended_scores[:min_length], &quot;Away Team&quot;: y[:min_length]}
    footballscores = pd.DataFrame(data)
    footballscores.to_csv(&quot;footscores.csv&quot;)

@asset
def create_scores_table(duckdb: DuckDBResource) -&gt; None:
    sports_df = pd.read_csv(
        &quot;footscores.csv&quot;,
        names=['Unnamed: 0', 
               'Home Team', 
               'Scores', 
               'Away Team'],
            
    )

    with duckdb.get_connection() as conn:
        conn.execute(&quot;CREATE TABLE sports.scores AS SELECT * FROM scores_df&quot;)


defs = Definitions(
    assets=[league_standing, get_scores, create_scores_table],
    resources={
        &quot;duckdb&quot;: DuckDBResource(
            database=database_file)
            }
)

</code></pre>
<p>** I noticed that everything seems to work fine until I added this part into the code:**</p>
<pre><code>@asset
def create_scores_table(duckdb: DuckDBResource) -&gt; None:
    sports_df = pd.read_csv(
        &quot;footscores.csv&quot;,
        names=['Unnamed: 0', 
               'Home Team', 
               'Scores', 
               'Away Team'],
            
    )

    with duckdb.get_connection() as conn:
        conn.execute(&quot;CREATE TABLE sports.scores AS SELECT * FROM scores_df&quot;)


defs = Definitions(
    assets=[league_standing, get_scores, create_scores_table],
    resources={
        &quot;duckdb&quot;: DuckDBResource(
            database=database_file)
            }
)
</code></pre>
<p><strong>Please Help</strong></p>
",0,1696688603,python;ubuntu;dbt;duckdb;dagster,False,372,0,1696688603,https://stackoverflow.com/questions/77250192/error-when-adding-new-assets-in-dagster-which-is-dependent-on-duckdb
77184940,How to install using python poetry and mention the fivetran dbt package salesforce_formula_utils in pyproject.toml?,"<p>Trying to install the fivetran dbt package: &quot;salesforce_formula_utils&quot; in my DBT-Dagster project.</p>
<p>The item shown below is not a component of PyPI; rather, it is a repository given by Fivetran. Since it lacks an installation setup file, I have listed it as a dependant package in PyProject.toml using Python poetry.</p>
<p>I gave the package's git location as shown below, but it's still not installing and giving the following error.</p>
<p>Any suggestion would be appriciated.</p>
<p>pyproject.toml
[tool.poetry.dependencies]
fivetran-salesforce-formula-utils = { git = &quot;https://github.com/fivetran/salesforce_formula_utils.git&quot;, tag = &quot;v0.9.0&quot; }</p>
<p>Error:
poetry install
Installing dependencies from lock file
Warning: poetry.lock is not consistent with pyproject.toml. You may be getting improper dependencies. Run <code>poetry lock [--no-update]</code> to fix it.</p>
<p>Unable to determine package info for path: /workspaces/gdp-transforms-us/.venv/src/salesforce_formula_utils</p>
<p>Command ['/tmp/tmp_jtqwzzp/.venv/bin/python', '-'] errored with the following return code 1</p>
<p>No fallback setup.py file was found to generate egg_info.</p>
",1,1695793930,python-3.x;snowflake-cloud-data-platform;dbt;dagster;fivetran,True,242,1,1695810848,https://stackoverflow.com/questions/77184940/how-to-install-using-python-poetry-and-mention-the-fivetran-dbt-package-salesfor
77097587,How to add op_tags to AssetsDefinitions after it has been defined,"<p>I can define an asset with op_tags, like</p>
<pre class=""lang-py prettyprint-override""><code>@asset(op_tags={&quot;model&quot;: &quot;lgbm&quot;})
def model():
    return MODEL
</code></pre>
<p>but I would like to use decorators to update/modify an assets tag, like</p>
<pre class=""lang-py prettyprint-override""><code>@add_resources_to_op_tags
@add_owner_to_op_tags
@asset(op_tags={&quot;model&quot;: &quot;lgbm&quot;})
def model():
    return MODEL
</code></pre>
<p>I have tried updating the <code>asset.node_def.tags</code> attribute, but it is a ReadOnlyDict. Setting it is also not allowed.</p>
<p>Doing asset.node_def._tags = new_tags seems to work, but I wonder if it could have unpredicted side effects, since the tags are defined in multiple attributes inside the asset.</p>
",0,1694612101,dagster,False,76,0,1694612897,https://stackoverflow.com/questions/77097587/how-to-add-op-tags-to-assetsdefinitions-after-it-has-been-defined
77059320,Dagster getting partition using different timezone,"<p>I would like some help in scheduling a dagster job that should get s3 folders on a different timezone. For example it brings all data s3:///2023/09/01/00 to 23 (i.e all hours). But what I want is it to bring  s3:///2023/08/31/22 and s3:///2023/08/31/23 + all folders from s3:///2023/09/01/01 to 21. This is because I want the data to be pulled in Europe Timezone as opposed to UTC (which is what the data is loaded in)</p>
<p>We tried using the timezone feature in the</p>
<pre><code>partitions_def: DailyPartitionsDefinition = (
    DailyPartitionsDefinition(
        start_date=backfill_start_date,
        end_offset=0,
        timezone=&quot;Europe/Amsterdam&quot;
    )
)
</code></pre>
<p>This did not seem to help.</p>
",0,1694087149,dagster,False,103,0,1694087149,https://stackoverflow.com/questions/77059320/dagster-getting-partition-using-different-timezone
77008393,Does Dagster-cloud support autoscaling directly?,"<p>I found in Dagster's documentation a <a href=""https://docs.dagster.io/deployment/guides/kubernetes/customizing-your-deployment#per-job-kubernetes-configuration"" rel=""nofollow noreferrer"">https://docs.dagster.io/deployment/guides/kubernetes/customizing-your-deployment#per-job-kubernetes-configuration</a></p>
<p>On auto scaling using Kubernetes.</p>
<p>Does Dagster contain a direct method to perform that auto-scaling? (Without Kubernetes)</p>
",0,1693401255,autoscaling;dagster,True,84,1,1693924771,https://stackoverflow.com/questions/77008393/does-dagster-cloud-support-autoscaling-directly
77000423,Deploy to different machine profiles using DAGster,"<p>Can I specify the machine or machine type I'd like to deploy each task/workflow in Dagster separately?</p>
<p>(e.g. Having an A-&gt;B DAG, make step A performed on a machine of type M_a, and have step B occur on a machine of type M_b)</p>
<p>For example, if I have some CPU-heavy task and another which is GPU-heavy, I'll probably prefer they would not be run on the same machine type.</p>
",0,1693313759,deployment;dagster,True,63,1,1693924522,https://stackoverflow.com/questions/77000423/deploy-to-different-machine-profiles-using-dagster
77027309,"Dagster: Why do I need to specify ECS cluster name, and separately its CPU/memory requirements?","<p>In Dagster cloud's documentation, for AWS and under <a href=""https://docs.dagster.io/dagster-cloud/deployment/agents/amazon-ecs/configuration-reference#per-deployment-configuration"" rel=""nofollow noreferrer"">Per-deployment configuration</a>, I see that I need to specify both:</p>
<ol>
<li>Cluster name (Under &quot;cluster&quot;)</li>
<li>Requested server resources (Under server_resources. Namely CPU and memory)</li>
</ol>
<p>Since as far as I understood, an ECS cluster contains homogenous machines, wouldn't the cluster name indicate the machines available?</p>
<p>What would happen if I requested resources of type X, while the cluster contained resources of (a different) type Y?</p>
",0,1693638498,amazon-ecs;dagster,True,256,2,1693924309,https://stackoverflow.com/questions/77027309/dagster-why-do-i-need-to-specify-ecs-cluster-name-and-separately-its-cpu-memor
77040970,Dagster reload instead of shutdown and restart,"<p>when developing dagster ops , should I restart the service on each code changes to see the results? Or is it possible to reload it automatically?
I asked chatgpt information regarding Dagster's reloading mechanism.
but the below answer didn't work:
It's great to know that Dagster now supports a feature to help developers iterate more efficiently during pipeline and operation development.
using the reloader=&quot;watch&quot; argument with the execute_pipeline function—sounds like a valuable addition to the workflow. This feature can significantly speed up the development process by automatically detecting code changes and updating the pipeline definitions without the need for manual shutdown and restart.</p>
",0,1693868601,dagster,False,244,0,1693868601,https://stackoverflow.com/questions/77040970/dagster-reload-instead-of-shutdown-and-restart
77008221,Setting GPU resources when using Dagster,"<p>I found in Dagster's documentation a section on customizing the resource requirements for machines orchestrated by that service, in AWS ECS and over Kubernetes:</p>
<pre><code>https://docs.dagster.io/dagster-cloud/deployment/agents/amazon-ecs/configuration-reference#per-deployment-configuration

https://docs.dagster.io/deployment/guides/kubernetes/customizing-your-deployment#per-job-kubernetes-configuration
</code></pre>
<p>And in both I can set CPU and memory.</p>
<p>Can I also set the available GPU resources for tasks that require those instead? (e.g. <a href=""https://aws.amazon.com/ec2/instance-types/#Accelerated_Computing"" rel=""nofollow noreferrer"">AWS' GPU based EC2 machines</a> / <a href=""https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-gpu.html"" rel=""nofollow noreferrer"">here</a>)</p>
",0,1693399921,cloud;gpu;dagster,True,164,1,1693653059,https://stackoverflow.com/questions/77008221/setting-gpu-resources-when-using-dagster
59265886,Caching Dagster&#39;s pipeline results,"<p>Is there a way to cache the output of the solids in the pipeline in such a way that if I run the same pipeline but with a slightly different configuration (think hyper-parameter tuning), certain initial steps in the pipelines that are unaffected by the configuration changes will not be executed multiple times?</p>

<p><code>Raw data -&gt; CPU expensive preprocessing (A) -&gt; model fitting (B) -&gt; model</code></p>

<p>I want to be able to run A once, but multiple variations of B.</p>

<p>Is there an elegant way to do this in Dagster?</p>
",3,1575976261,python;dagster,True,753,2,1693228701,https://stackoverflow.com/questions/59265886/caching-dagsters-pipeline-results
76991877,A schedule running multiple hobs sequentially,"<p>I am trying to create a schedule that will fit my model to sensor data every week. Now, I have about 100 sensors to analyse and the model architecture and parameters are the same. I have a job which can analyse a single sensor and it works fine. However, I do not want to create a separate schedule for every sensor. It would be nice for the same schedule to dynamically create the jobs for each sensor. Is there such a mechanism in Dagster?</p>
<p>Additionally, I would like the jobs to then execute sequentially (due to the current computation constraints). Is it possible to create such a schedule setup in Dagster? Essentially, I want to run the same job  with different inputs in a sequential fashion.</p>
<p>So, at the moment, I have the following schedule setup:</p>
<pre><code>@schedule(
    name=&quot;train_job&quot;,
    cron_schedule=&quot;0 19 * * *&quot;,
    job=train__job,
)
def train_schedule():
    with open(file_relative_path(__file__, &quot;sensor_train.yaml&quot;)) as f:
        sensor_config = yaml.safe_load(f)

    with open(file_relative_path(__file__, &quot;overrides__train.yaml&quot;)) as f:
        base_config = yaml.safe_load(f)

    base_config.update(sensor_config)

    tags = {&quot;trigger_prediction&quot;: &quot;true&quot;}

    return RunRequest(run_key=None, run_config=base_config, tags=tags)
</code></pre>
<p>So, basically here I am performing some config overrides and running a single job. Essentially, I would like to do some overrides for each sensor and launch a job for each of them in a sequential fashion. I am essentially looking to have a synchronous loop around the whole thing where I can configure the inputs.</p>
",0,1693217409,dagster,False,174,0,1693217466,https://stackoverflow.com/questions/76991877/a-schedule-running-multiple-hobs-sequentially
75660408,Dagster: handle TypedDict as op output,"<h2>Context</h2>
<p>To keep track of the origin of my Dataframes, across my different ops, I defined <code>TaggedDF</code> as</p>
<pre class=""lang-py prettyprint-override""><code>class TaggedDF(TypedDict):
    df: DataFrame
    ressource_name: str
</code></pre>
<p>So I could</p>
<pre class=""lang-py prettyprint-override""><code>@op(retry_policy=OPS_RETRY_POLICY)
def read_fec(context, file_name: str) -&gt; TaggedDF:
    &quot;&quot;&quot;
    read fec files from source container
    &quot;&quot;&quot;
    context.log.info(&quot;reading %s&quot;, file_name)

    reg = match(r&quot;.*\/(.*)FEC(.*)\.txt&quot;, file_name)
    blob_client_instance = azure_client.get_blob_client(
        Vars.LANDING_ZONE.value, file_name
    )
    blob_data = blob_client_instance.download_blob()
    df = pd.read_csv(StringIO(blob_data.content_as_text(encoding=&quot;UTF-8&quot;)), sep=&quot;\t&quot;)
    df[&quot;siren&quot;] = reg.group(1)
    df[&quot;period&quot;] = reg.group(2)
    return {&quot;df&quot;: df, &quot;ressource_name&quot;: file_name}
</code></pre>
<h2>Problem</h2>
<p>Dagster does not seem to appreciate <code>TypedDict</code></p>
<pre><code>dagster._core.errors.DagsterTypeCheckError: Error occurred while type-checking output &quot;result&quot; of op &quot;read_fec&quot;, with Python type &lt;class 'dict'&gt; and Dagster type TaggedDF
  File &quot;/home/zar3bski/.cache/pypoetry/virtualenvs/poc-dagster-30bqhDW5-py3.10/lib/python3.10/site-packages/dagster/_core/execution/plan/execute_plan.py&quot;, line 269, in dagster_event_sequence_for_step
    for step_event in check.generator(step_events):
  File &quot;/home/zar3bski/.cache/pypoetry/virtualenvs/poc-dagster-30bqhDW5-py3.10/lib/python3.10/site-packages/dagster/_core/execution/plan/execute_step.py&quot;, line 386, in core_dagster_event_sequence_for_step
    for evt in _type_check_and_store_output(step_context, user_event):
  File &quot;/home/zar3bski/.cache/pypoetry/virtualenvs/poc-dagster-30bqhDW5-py3.10/lib/python3.10/site-packages/dagster/_core/execution/plan/execute_step.py&quot;, line 436, in _type_check_and_store_output
    for output_event in _type_check_output(step_context, step_output_handle, output, version):
  File &quot;/home/zar3bski/.cache/pypoetry/virtualenvs/poc-dagster-30bqhDW5-py3.10/lib/python3.10/site-packages/dagster/_core/execution/plan/execute_step.py&quot;, line 282, in _type_check_output
    with user_code_error_boundary(
  File &quot;/usr/lib/python3.10/contextlib.py&quot;, line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File &quot;/home/zar3bski/.cache/pypoetry/virtualenvs/poc-dagster-30bqhDW5-py3.10/lib/python3.10/site-packages/dagster/_core/errors.py&quot;, line 213, in user_code_error_boundary
    raise error_cls(
The above exception was caused by the following exception:

TypeError: TypedDict does not support instance and class checks
  File &quot;/home/zar3bski/.cache/pypoetry/virtualenvs/poc-dagster-30bqhDW5-py3.10/lib/python3.10/site-packages/dagster/_core/errors.py&quot;, line 206, in user_code_error_boundary
    yield
  File &quot;/home/zar3bski/.cache/pypoetry/virtualenvs/poc-dagster-30bqhDW5-py3.10/lib/python3.10/site-packages/dagster/_core/execution/plan/execute_step.py&quot;, line 287, in _type_check_output
    type_check = do_type_check(type_check_context, dagster_type, output.value)
  File &quot;/home/zar3bski/.cache/pypoetry/virtualenvs/poc-dagster-30bqhDW5-py3.10/lib/python3.10/site-packages/dagster/_core/execution/plan/execute_step.py&quot;, line 188, in do_type_check
    type_check = dagster_type.type_check(context, value)
  File &quot;/home/zar3bski/.cache/pypoetry/virtualenvs/poc-dagster-30bqhDW5-py3.10/lib/python3.10/site-packages/dagster/_core/types/dagster_type.py&quot;, line 171, in type_check
    retval = self._type_check_fn(context, value)
  File &quot;/home/zar3bski/.cache/pypoetry/virtualenvs/poc-dagster-30bqhDW5-py3.10/lib/python3.10/site-packages/dagster/_core/types/dagster_type.py&quot;, line 507, in type_check
    if not isinstance(value, expected_python_type):
  File &quot;/usr/lib/python3.10/typing.py&quot;, line 2385, in __subclasscheck__
    raise TypeError('TypedDict does not support instance and class checks')
</code></pre>
<p>How can I had this feature?</p>
",1,1678182925,dagster,False,101,1,1692391697,https://stackoverflow.com/questions/75660408/dagster-handle-typeddict-as-op-output
76859998,Dagster get partition of job ran before sensor triggered,"<p>I want to get the partition of a job that is than before a sensor is trigger. I not sure how to do that in dagster.</p>
<p>I have a sensor looks like this:</p>
<pre><code>@sensor(job=job2)
def run_job2_after_job1:
date= datetime.now()

run = context.intance.get_run_records(filters = runsfilter(job_name = job1)

for run in runs:
        yield RunRequest(
            run_key = run.dagster_run.run_id,
            partition_key = date
        )
</code></pre>
",0,1691501041,sensors;execution-time;dagster,False,170,0,1691501041,https://stackoverflow.com/questions/76859998/dagster-get-partition-of-job-ran-before-sensor-triggered
76840068,How to using Dagster recommended structure with Docker/ECS,"<p>Dagster recommends structuring your project list <a href=""https://docs.dagster.io/guides/dagster/recommended-project-structure"" rel=""nofollow noreferrer"">this</a>:</p>
<pre><code>project_fully_featured
├── Makefile
├── README.md
├── dbt_project
├── project_fully_featured
│   ├── __init__.py
│   ├── assets
│   │   ├── __init__.py
│   │   ├── activity_analytics
│   │   │   ├── __init__.py
│   │   │   └── activity_forecast.py
│   │   ├── core
│   │   │   ├── __init__.py
│   │   │   ├── id_range_for_time.py
│   │   │   └── items.py
│   │   └── recommender
│   │       ├── __init__.py
│   │       ├── comment_stories.py
│   │       ├── recommender_model.py
│   │       ├── user_story_matrix.py
│   │       └── user_top_recommended_stories.py
│   ├── jobs.py
│   ├── partitions.py
│   ├── resources
│   │   ├── __init__.py
│   │   ├── common_bucket_s3_pickle_io_manager.py
│   │   ├── duckdb_parquet_io_manager.py
│   │   ├── hn_resource.py
│   │   ├── parquet_io_manager.py
│   │   ├── partition_bounds.py
│   │   └── snowflake_io_manager.py
│   ├── sensors
│   │   ├── __init__.py
│   │   ├── hn_tables_updated_sensor.py
│   │   └── slack_on_failure_sensor.py
│   └── utils
├── project_fully_featured_tests
├── pyproject.toml
├── setup.cfg
├── setup.py
└── tox.ini
</code></pre>
<p>I need to do this and run it from docker/ AWS ECS. I am able to get this project working in Docker/ECS: <a href=""https://github.com/dagster-io/dagster/tree/master/examples/deploy_docker"" rel=""nofollow noreferrer"">https://github.com/dagster-io/dagster/tree/master/examples/deploy_docker</a></p>
<p>But I don't know how to convert the deploy_docker example to use the recommended code structure where there is no repo.py, and all assets are in a directory called assets.</p>
<p>Can someone recommend how this might be done?</p>
",0,1691207322,grpc;dagster,False,175,0,1691207322,https://stackoverflow.com/questions/76840068/how-to-using-dagster-recommended-structure-with-docker-ecs
76816039,NameError: name &#39;topstory_ids&#39; is not defined,"<p>I am new to Dagster and trying to follow the tutorial from official documentation, <a href=""https://docs.dagster.io/tutorial/building-an-asset-graph"" rel=""nofollow noreferrer"">https://docs.dagster.io/tutorial/building-an-asset-graph</a></p>
<p>After I copied and pasted this code, I got this error. <code>NameError: name 'topstory_ids' is not defined</code> How do I define <code>topstory_ids</code>?</p>
<pre><code>import json
import os

import pandas as pd  # Add new imports to the top of `assets.py`
import requests

from dagster import asset


@asset(deps=[topstory_ids])  # this asset is dependent on topstory_ids
def topstories() -&gt; None:
    with open(&quot;data/topstory_ids.json&quot;, &quot;r&quot;) as f:
        topstory_ids = json.load(f)

    results = []
    for item_id in topstory_ids:
        item = requests.get(
            f&quot;https://hacker-news.firebaseio.com/v0/item/{item_id}.json&quot;
        ).json()
        results.append(item)

        if len(results) % 20 == 0:
            print(f&quot;Got {len(results)} items so far.&quot;)

    df = pd.DataFrame(results)
    df.to_csv(&quot;data/topstories.csv&quot;)
</code></pre>
<p><a href=""https://i.stack.imgur.com/0gBiJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0gBiJ.png"" alt=""enter image description here"" /></a></p>
",1,1690938358,dagster,False,69,1,1690942227,https://stackoverflow.com/questions/76816039/nameerror-name-topstory-ids-is-not-defined
76811385,How to make @op run in sequence when job invoked from sensor?,"<p>can you please help me overcome the below issue
i have an sensor [my_sensor] defined on a job [my_job].
[my_job] executes op [my_op] which requires an input [filename] from sensor which i am passing from sensor using</p>
<pre><code>yield RunRequest(
    run_key=None,
    run_config=RunConfig(
        ops={&quot;my_op&quot;: FileConfig(filename=&quot;customer_name&quot;)}
    ),
)
</code></pre>
<p>[my_op] returns an dynamic output. I want to process all the dynamic outputs sequentially not in parallel.
I am trying with the below config on job but still the dynamic output mappings are running in parallel</p>
<pre><code>@job(
    config={
        &quot;ops&quot;: {&quot;my_op&quot;: {&quot;config&quot;: {&quot;filename&quot;: &quot;my_sensor&quot;}}},
        &quot;execution&quot;: {
            &quot;config&quot;: {
                &quot;multiprocess&quot;: {
                    &quot;max_concurrent&quot;: 1
                }
            }
        }
    }
)
def my_job():
    file_list = my_op()
    success_query, failed_query = file_list.map(write_json_file)
    success_query.map(success)
    failed_query.map(failure)
</code></pre>
",0,1690891022,dagster,False,243,0,1690891022,https://stackoverflow.com/questions/76811385/how-to-make-op-run-in-sequence-when-job-invoked-from-sensor
76703057,DagsterImportError: Encountered ImportError: `cannot import name &#39;URL&#39; from &#39;datasets&#39;,"<p>I am running <code>dagster dev</code> in a folder created by running <code>dagster project scaffold --name dagster_pipeline</code>:</p>
<p><a href=""https://i.stack.imgur.com/Tt9T1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Tt9T1.png"" alt=""enter image description here"" /></a></p>
<p>assets.py imports a constant from datasets.py that I have added:</p>
<pre><code>import requests
from dagster import asset

from datasets import URL

@asset
def datasets(url: str = URL) -&gt; str:
    return requests.get(url).text
</code></pre>
<p><strong>init</strong>.py is unchanged:</p>
<pre><code>from dagster import Definitions, load_assets_from_modules

from . import assets

all_assets = load_assets_from_modules([assets])

defs = Definitions(
    assets=all_assets,
)
</code></pre>
<p>I see the following error in the output:</p>
<pre><code>dagster._core.errors.DagsterImportError: Encountered ImportError: `cannot import name 'URL' from 'datasets' (C:\Users\agevorgyan\AppData\Local\Programs\Python\Python311\Lib\site-packages\datasets\__init__.py)` while importing module dagster_pipeline. Local modules were resolved using the working directory `C:\Users\agevorgyan\IdeaProjects\{project_name}\dagster_pipeline`. If another working directory should be used, please explicitly specify the appropriate path using the `-d` or `--working-directory` for CLI based targets or the `working_directory` configuration option for workspace targets.

</code></pre>
<p>What am I doing wrong? I saw a similar error while running code copied from the tutorial, so is this a bug?</p>
",0,1689585761,dagster,False,479,1,1689589566,https://stackoverflow.com/questions/76703057/dagsterimporterror-encountered-importerror-cannot-import-name-url-from-dat
76633650,Get the previous scheduled parition date in a dagster asset,"<p>Is there a way to get the previous scheduled partition key in an asset?</p>
<p>If i use the below in a scheduled asset it will return a partition key even if the should_execute in the schedule is False:</p>
<pre><code>@asset(partitions_def=partition_def)
def my_asset(context):
    current_partition_window = context.partition_time_window
    prev_window = daily_partitions_def.get_prev_partition_window(current_partition_window.start)
    prev_partition_key = prev_window.start
    return prev_partition_key

my_job = define_asset_job(&quot;my_job&quot;, selection=[my_asset], partitions_def=partitions_def)
</code></pre>
<p>e.g. if my partition is:</p>
<pre><code>partitions_def = DailyPartitionsDefinition(start_date='2023-07-03', timezone='America/New_York', fmt='%Y-%m-%d', end_offset=1)
</code></pre>
<p>and my scheduled job is:</p>
<pre><code>def skip_july_4th(context) -&gt; bool:
    dt = context.scheduled_execution_time
    if '2023-07-04' in str(dt):
        return False
    else:
        return True

@schedule(job=my_job, should_execute=skip_july_4th, cron_schedule=&quot;40 22 * * 1-5&quot;, execution_timezone='America/New_York')
def nyse_schedule(context):
    partition_key = daily_partitions_def.get_partition_key_for_timestamp(
        context.scheduled_execution_time.timestamp()
    )
    request = us_job.run_request_for_partition(partition_key=partition_key)
    yield request

</code></pre>
<p>The example above will return a previous partition key of '2023-07-04' on '2023-07-05' even thou it will return False for the given <code>should_execute</code> function in the schedule. How can I get the previous scheduled partition key in the asset.</p>
",0,1688698062,dagster,False,157,0,1689005856,https://stackoverflow.com/questions/76633650/get-the-previous-scheduled-parition-date-in-a-dagster-asset
76491896,How to provide parameters defined at the source asset declaration to the IO Manager?,"<p>So my initial task was to create an IO Manager that should connect to a database and return data as pandas dataframe.</p>
<p>(I am using dagster 1.3.10)</p>
<h2>Design</h2>
<p>IMO, the credentials (ip, port, user, password) must be parameters of the IO manager because I want different resources for different credentials.
But the other interesting parameters that can be used to perform a database query (select fields, optional filters, sorting, limit, ...) should be linked to an asset definition.</p>
<p>I had no trouble creating the credentials parameter, like this:</p>
<pre class=""lang-py prettyprint-override""><code>@io_manager(
    config_schema={
        'ip': Field(str, is_required=True),
        'port': Field(int, default_value=5432),
        'username': Field(str, is_required=True),
        'password': Field(str, is_required=True)
    }
)
def database_io_manager(init_context):
    return DatabaseIOManager(
        ip=init_context.resource_config.get('ip'),
        port=init_context.resource_config.get('port'),
        username=init_context.resource_config.get('username'),
        password=init_context.resource_config.get('password'),
    )
</code></pre>
<p>Then I can just provide this function in the resources dict that I provide to definitions</p>
<pre class=""lang-py prettyprint-override""><code>defs = Definitions(resources={'database_io_manager': database_io_manager})
</code></pre>
<p>So now I can use this IO manager in my assets definitions</p>
<pre class=""lang-py prettyprint-override""><code>@asset(io_manager_key='database_io_manager')
def my_asset():
    pass
</code></pre>
<p>Now like I said, I want the query parameters to be at the asset level, so I've created a configuration.</p>
<pre class=""lang-py prettyprint-override""><code>from dagster import Config
import pydantic
class DatabaseConfig(Config):
    fields: List[str] = pydantic.Field()
</code></pre>
<p>I provide this configuration to the asset in the <code>metadata</code> attribute.</p>
<pre class=""lang-py prettyprint-override""><code>asset(io_manager_key='database_io_manager',metadata={'io_manager': DatabaseConfig(fields='*')})
def my_asset():
    pass
</code></pre>
<p>And I can use this in my IO manager with a custom method</p>
<pre class=""lang-py prettyprint-override""><code> def load_metadata(self, context: Union[InputContext, OutputContext]) -&gt; None:
        config: DatabaseConfig = context.metadata.get(&quot;io_manager&quot;)
        if not isinstance(config, DatabaseConfig):
            raise ValueError('wrong config type')
        self.fields = config.fields
</code></pre>
<h2>Problem</h2>
<p>This work with <code>Asset</code> but not with <code>SourceAsset</code>.</p>
<p>If I define a source asset like this:</p>
<pre class=""lang-py prettyprint-override""><code>my_source_asset = SourceAsset(
    key='my_source_asset',
    io_manager_key='database_io_manager',
    metadata=DatabaseConfig(fields='*')
)
</code></pre>
<p>I can see the metadata associated with this source asset in dagit, but when effectively loading the asset, the metadata dict is empty.</p>
<p>Is it a bug? Am I missing something?</p>
<h2>Other (minor) problems</h2>
<h3>unserializable</h3>
<p>I tried to provide a minimal replication example and in the process of doing so I encountered other issues.</p>
<p>The first that bugs me is that this <code>DatabaseConfig</code> object is not displayed by dagit. It says 'unserializable'. But I am extending the <code>Config</code> class and I've tested to call the <code>json()</code> method on it and it works well.</p>
<p>Bonus 1: What can I do to make the <code>DatabaseConfig</code> class serializable as dagit wants it?</p>
<h3>zero io manager use</h3>
<p>With the code that can be found at the end of this question, when I look in dagit I have zero use of my io managers.
<img src=""https://i.stack.imgur.com/ClcnH.png"" alt=""zero uses"" /></p>
<p>Bonus 2: Why can't I see the IO managers uses ?</p>
<hr />
<pre class=""lang-py prettyprint-override""><code># minimal_example_bug_dagster.py
from __future__ import annotations
import pickle
from typing import Union
import pydantic

from dagster import (
    Config,
    Definitions,
    InputContext,
    OutputContext,
    SourceAsset,
    asset,
    IOManager,
    fs_io_manager,
    io_manager,
)


class CustomIOConfig(Config):
    custom_file_name: str = pydantic.Field()


class CustomIOManager(IOManager):
    my_attribute: str = None

    def get_key(self, context: Union[InputContext, OutputContext]) -&gt; str:
        return context.asset_key.path[:-1]

    def load_metadata(self, context: Union[InputContext, OutputContext]) -&gt; None:
        context.log.info(context.metadata)
        config: CustomIOConfig = context.metadata.get(&quot;io_manager&quot;)
        self.my_attribute = config.custom_file_name

    def load_input(self, context: InputContext) -&gt; str:
        context.log.info(f&quot;Inside load_input for {self.get_key(context)}&quot;)
        self.load_metadata(context)
        pickle.load(open(self.my_attribute, &quot;rb&quot;))

    def handle_output(self, context: &quot;OutputContext&quot;, obj: str) -&gt; None:
        context.log.info(f&quot;Inside handle_output for {self.get_key(context)}&quot;)
        self.load_metadata(context)
        pickle.dump(obj, open(self.my_attribute, &quot;wb&quot;))


@asset(
    metadata={&quot;io_manager&quot;: CustomIOConfig(custom_file_name=&quot;foo&quot;)},
    io_manager_key=&quot;custom_io_manager&quot;,
)
def my_asset():
    return &quot;Hello&quot;


my_source_asset = SourceAsset(
    &quot;my_source_asset&quot;,
    metadata={&quot;io_manager&quot;: CustomIOConfig(custom_file_name=&quot;bar&quot;)},
    io_manager_key=&quot;custom_io_manager&quot;,
)


@asset(io_manager_key=&quot;fs_io_manager&quot;)
def using_both_assets(my_asset, my_source_asset):
    return f&quot;{my_asset}, {my_source_asset}&quot;


@io_manager
def custom_io_manager(init_context):
    return CustomIOManager()


defs = Definitions(
    assets=[my_asset, my_source_asset, using_both_assets],
    resources={&quot;fs_io_manager&quot;: fs_io_manager, &quot;custom_io_manager&quot;: custom_io_manager},
)
</code></pre>
",0,1686931431,python;dagster,True,609,1,1688365984,https://stackoverflow.com/questions/76491896/how-to-provide-parameters-defined-at-the-source-asset-declaration-to-the-io-mana
76540568,Dagster UI not showing on Digital Ocean with nginx,"<p>I'm trying to run open-source Dagster on a digital ocean droplet. This droplet is working OK for MLFlow and accessing our database via phpmyadmin.</p>
<p>When I try to access the Dagster UI via <code>www.dev.ourdomainname.co.uk/dagsterapp</code> I get the title &quot;Dagit&quot; and the console shows these errors:</p>
<p><code>Loading failed for the &lt;script&gt; with source “https://www.dev.ourdomainname.co.uk/static/js/main.751d2cd1.js</code></p>
<p>and some &quot;Content-Security-Policy warnings&quot;.</p>
<p>Interestingly, pointing the browser at: <code>www.dev.ourdomainname.co.uk/dagsterapp/dagit_info</code> does show the expected version numbers.</p>
<p>Why does the react front end not get loaded properly? What should I be checking?</p>
<p>Our nginx.conf looks like this:</p>
<pre><code>events
{
    worker_connections 1024;
}
http
{
    fastcgi_read_timeout 900;
    proxy_read_timeout 900;
    server
    {
        listen 80 default_server;
        listen [::]:80 default_server;
        server_name dev.ourdomainname.co.uk www.dev.ourdomainname.co.uk;
        if ($host = dev.ourdomainname.co.uk)
        {
            return 301 https://$host$request_uri;
        }
        if ($host = www.dev.ourdomainname.co.uk)
        {
            return 301 https://$host$request_uri;
        }
    return 404;
    }
    server
    {
        listen 443 ssl;
        listen [::]:443 ssl;
        ssl_certificate /etc/letsencrypt/live/dev.ourdomainname.co.uk/fullchain.pem;
        ssl_certificate_key /etc/letsencrypt/live/dev.ourdomainname.co.uk/privkey.pem;
        server_name dev.ourdomainname.co.uk www.dev.ourdomainname.co.uk;
        root /;
        location /dagsterapp/ {
            resolver 127.0.0.11 valid=5s;
            set $upstream_endpoint http://dagsterapp:3000;
            rewrite ^/dagsterapp(/.*)$ $1 break;
            proxy_pass $upstream_endpoint;
            proxy_redirect off;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_pass_header Content-Type;
        }
        location /mlflow/ {
            resolver 127.0.0.11 valid=5s;
            set $upstream_endpoint http://mlflow:5000;
            rewrite ^/mlflow(/.*)$ $1 break;
            proxy_pass $upstream_endpoint;
            proxy_redirect off;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
        location /phpmyadmin/ {
            client_max_body_size 50M;
            resolver 127.0.0.11 valid=5s;
            set $upstream_endpoint http://phpmyadmin:80;
            rewrite ^/phpmyadmin(/.*)$ $1 break;
            proxy_pass $upstream_endpoint;
            proxy_redirect off;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
    }
}
</code></pre>
",0,1687526681,reactjs;docker;nginx;digital-ocean;dagster,True,250,1,1687810720,https://stackoverflow.com/questions/76540568/dagster-ui-not-showing-on-digital-ocean-with-nginx
76471523,dagster: run every hour and pass last execution time to asset,"<p>I'm new to dagster to please be patient for not fully grasping it yet.</p>
<p>I want to &quot;run an asset&quot; every hour which should be possible with schedules. What i also want is to pass the last execution date into the asset. This is used to fetch only data modified since the last call.</p>
<p>So the issue is 2-fold:</p>
<ul>
<li>How do I store the last execution date?</li>
<li>How do I load the last exectuion date upon execution and pass it to the asset?</li>
</ul>
",0,1686731183,python;dagster,True,387,1,1686732103,https://stackoverflow.com/questions/76471523/dagster-run-every-hour-and-pass-last-execution-time-to-asset
76398303,Unable to open Dagster UI on local machine when dagster script is running from inside a docker container,"<p>I am running into an issue trying to open the Dagster UI from inside a docker container. Furthermore, this is a step towards my larger of goal, of opening the dagster UI from a docker container, thats running on a Google Compute Engine. I am able to open and view the dagster UI locally when it is not inside the container. Here are some diagrams to explain my issue and goals.</p>
<p><a href=""https://i.stack.imgur.com/sSNa3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sSNa3.png"" alt=""enter image description here"" /></a></p>
<p>My issue:</p>
<p>I am unable to open the Dagster UI on my local machine, when the dagster script is running from inside the docker container.</p>
<p><strong>What I have tried:</strong></p>
<pre><code># My docker image:

FROM python:3.8-slim-buster

# This is to deal with an issue with the psutil package.
RUN apt-get update -y &amp;&amp; apt-get install -y gcc
RUN apt-get -y install xauth

COPY requirements.txt requirements.txt
RUN pip3 install -r requirements.txt

COPY . .

EXPOSE 3000

CMD dagster dev -f run_dagster_pipeline.py

# Building the container. 

docker build -t py-gce .

# Output:

=&gt; [internal] load build definition from Dockerfile                                            0.0s
 =&gt; =&gt; transferring dockerfile: 553B                                                            0.0s
 =&gt; [internal] load .dockerignore                                                               0.0s
 =&gt; =&gt; transferring context: 2B                                                                 0.0s
 =&gt; resolve image config for docker.io/docker/dockerfile:1                                      1.2s
 =&gt; CACHED docker-image://docker.io/docker/dockerfile:1@sha256:39b85bbfa7536a5feceb7372a081764  0.0s
 =&gt; [internal] load build definition from Dockerfile                                            0.0s
 =&gt; [internal] load .dockerignore                                                               0.0s
 =&gt; [internal] load metadata for docker.io/library/python:3.8-slim-buster                       0.8s
 =&gt; [internal] load build context                                                               0.2s
 =&gt; =&gt; transferring context: 375.92kB                                                           0.2s
 =&gt; [1/6] FROM docker.io/library/python:3.8-slim-buster@sha256:eb48d017c5e117d9fbcbe991b4dbc61  0.0s
 =&gt; CACHED [2/6] RUN apt-get update -y &amp;&amp; apt-get install -y gcc                                0.0s
 =&gt; [3/6] RUN apt-get -y install xauth                                                          1.5s
 =&gt; [4/6] COPY requirements.txt requirements.txt                                                0.0s 
 =&gt; [5/6] RUN pip3 install -r requirements.txt                                                 68.0s 
 =&gt; [6/6] COPY . .                                                                              0.3s 
 =&gt; exporting to image                                                                          2.2s 
 =&gt; =&gt; exporting layers                                                                         2.2s 
 =&gt; =&gt; writing image sha256:34ffd1197f7235be0493d3be985d70a053c2a48b9c421ee328e0f324b041dd13    0.0s 
 =&gt; =&gt; naming to docker.io/library/py-gce                                                       0.0s

# Running the container and porting to different port. 

docker run -p 8080:3000 py-gce

# Output:

2023-06-03 17:37:12 +0000 - dagster - INFO - Using temporary directory /tmpkcsq9so9 for storage. This will be removed when dagster dev exits.
2023-06-03 17:37:12 +0000 - dagster - INFO - To persist information across sessions, set the environment variable DAGSTER_HOME to a directory to use.
2023-06-03 17:37:12 +0000 - dagster - INFO - Launching Dagster services...

  Telemetry:

  As an open source project, we collect usage statistics to inform development priorities. For more
  information, read &lt;https://docs.dagster.io/getting-started/telemetry&gt;.

  We will not see or store solid definitions, pipeline definitions, modes, resources, context, or
  any data that is processed within solids and pipelines.

  To opt-out, add the following to $DAGSTER_HOME/dagster.yaml, creating that file if necessary:

    telemetry:
      enabled: false

  Welcome to Dagster!

  If you have any questions or would like to engage with the Dagster team, please join us on Slack

2023-06-03 17:37:14 +0000 - dagster.daemon - INFO - Instance is configured with the following daemons: ['AssetDaemon', 'BackfillDaemon', 'SchedulerDaemon', 'SensorDaemon']
2023-06-03 17:37:14 +0000 - dagster.daemon.SensorDaemon - INFO - Not checking for any runs since no sensors have been started.
2023-06-03 17:37:14 +0000 - dagit - INFO - Serving dagit on &lt;http://127.0.0.1:3000&gt; in process 9
2023-06-03 17:38:14 +0000 - dagster.daemon.SensorDaemon - INFO - Not checking for any runs since no sensors have been started.

# Go to open the web UI trying these URLs:

&lt;http://localhost:8080/&gt;
&lt;http://localhost:3000/&gt;
http://localhost:127.0.0.1:3000
http://localhost:127.0.0.1:8080
http://localhost:0.0.0.0:8080
http://localhost:0.0.0.0:3000

&lt;http://127.0.0.1:3000&gt;
http://127.0.0.1:0.0.0.0:3000
&lt;http://0.0.0.0:3000&gt;

&lt;http://127.0.0.1:8080&gt;
http://127.0.0.1:0.0.0.0:8080
&lt;http://0.0.0.0:8080&gt;
</code></pre>
<p>But all I get is:</p>
<p>&quot;This site cant be reached&quot;</p>
<pre><code># The next thing I try is to search the IP address for my docker container 
# using the following commands:

docker ps

# To get my docker image ID

docker inspect &quot;my address&quot;

# I search for my DOCKER IP address and get:
&quot;IPAddress&quot;: &quot;172.17.0.2&quot;,

# Then I try these URL's

http://localhost:172.17.0.2:8080
http://localhost:172.17.0.2
http://localhost:172.17.0.2:3000
&lt;http://172.17.0.2:8080&gt;
</code></pre>
<p>None of those work; however, this time it fails because it got stuck, and not because it was immediately denied.</p>
<p>I found this post where they state that: “The <code>172.17.0.2</code> IP address is internal to the virtual machine that is running the Docker engine. You won’t be able to access it from your Windows host. The actual IP address of that virtual machine should be <code>192.168.99.100</code> which is the IP that ports would be mapped to if, in fact, you mapped any ports.”</p>
<p><a href=""https://forums.docker.com/t/unable-to-access-my-first-container-on-172-17-0-2/54106"" rel=""nofollow noreferrer"">https://forums.docker.com/t/unable-to-access-my-first-container-on-172-17-0-2/54106</a></p>
<p>It appears they are obtain the <code>192.168.99.100</code> IP from running docker-machine IP; however, I do not have docker machine on my machine. I downloaded it, but still failed, then went onto read that <code>192.168.99.100</code> is the default value so should not be unique for my machine anyways.</p>
<p>&quot;This site cant be reached: <strong>192.168.99.100</strong> took too long to respond&quot;</p>
<p>I am pretty lost at this point, have been trying for hours to resolve this seemingly simple task!</p>
<p>Also, like shown in my first diagram, this is just a step towards my final goal of opening the Dagster UI from inside a docker container inside a GCE. So if you have any tips for that endevor, feel free to make suggestions for that as well.</p>
<p>I have been all up down stack overflow and found similar issues, but the solutions are not working for me. I am not sure if this is a Dagster issue or Docker issue.</p>
",0,1685832286,docker;http;localhost;ip;dagster,True,777,1,1686649738,https://stackoverflow.com/questions/76398303/unable-to-open-dagster-ui-on-local-machine-when-dagster-script-is-running-from-i
76415555,write test for dagster asset job,"<p>I am trying to write a simple test for a dagster job and I can't get it through...</p>
<p>I am using dagster 1.3.6</p>
<p>So I have defined this job using the function <code>dagster.define_asset_job</code></p>
<pre class=""lang-py prettyprint-override""><code>from dagster import define_asset_job
my_job: UnresolvedAssetJobDefinition = define_asset_job(
    name='name_for_my_job',
    selection=AssetSelection.assets(
        source_asset_1,
        source_asset_2,
        asset_1,
        asset_2
    )
)
</code></pre>
<h2>Intuitive try</h2>
<p>By reading the documentation, I figured that I had to call the <code>execute_in_process</code> method, which is defined in the <code>JobDefinition</code> class.</p>
<pre class=""lang-py prettyprint-override""><code>from my_package import my_job
def test_documentation():
    result = my_job.execute_in_process()
    assert result.success
</code></pre>
<p>But like I've highligted in the first code block, <code>my_job</code> is of type <code>UnresolvedAssetJobDefinition</code>. By digging a bit more in the code, I see that there is a <code>resolve</code> method, which returns a <code>JobDefinition</code>.</p>
<p>So I wanted to do that, but I've seen that you can't call <code>resolve</code> without parameter; you are required to provide <code>asset_graph</code>.</p>
<p>But it's exactly what I was trying to avoid. I don't want to provide the list of the assets/source assets, I want them to be deduced from the job definition.</p>
<h2>Journey</h2>
<p>I've seen that in addition to the <code>UnresolvedAssetJobDefinition.resolve().execute_in_process()</code>, I could look at the <code>materialize_to_memory</code> function; but I faced the same issue: I need to provide a list of assets.</p>
<p>I spent some time trying to get the assets out of the <code>UnresolvedAssetJobDefinition</code>.
I've seen that there is a <code>.selection</code> property that allows me to get a <code>KeysAssetSelection</code>, which basically contains a list of <code>AssetKey</code>.</p>
<p>But I need a list of <code>Union[AssetsDefinition, SourceAsset]</code> and I don't know how to convert an <code>AssetKey</code> into an <code>AssetDefinition</code>.</p>
<h2>Last try</h2>
<p>Hereafter there is my last try, you can see that I am just trying to wire things together, as a admission of my weakness I am not even trying to use the job definition to get the assets.</p>
<pre class=""lang-py prettyprint-override""><code>import pytest
from my_package import my_job, source_asset_1, source_asset_2, asset_1, asset_2
from dagster._core.definitions.asset_graph import AssetGraph

@pytest.fixture
def test_resources() -&gt; Mapping[str, object]:
    return {
        &quot;parquet_io_manager&quot;: parquet_io_manager.configured({'data_path': DATA_FOLDER }),
    }



def test_my_job(
    test_resources: Mapping[str, object],
):
    graph = AssetGraph.from_assets([source_asset_1, source_asset_2, asset_1, asset_2])
    job = my_job.resolve(asset_graph=graph)
    result = job.execute_in_process(resources=test_resources)
    assert result.success
</code></pre>
<p>but I can't quite get what I want. In the last example, I got this error</p>
<blockquote>
<p><code>dagster._core.errors.DagsterInvalidSubsetError: AssetKey(s) {AssetKey(['source_asset_1']), AssetKey(['source_asset_2']), AssetKey(['asset_1']), AssetKey(['asset_2'])}</code> were selected, but no AssetsDefinition objects supply these keys. Make sure all keys are spelled correctly, and all AssetsDefinitions are correctly added to the <code>Definitions</code>.</p>
</blockquote>
<h1>Help</h1>
<p>I know that I can test each asset by just importing and calling the function decorated by the <code>@asset</code> dagster keyword.
But I want to be able to launch all the assets from the job, without having to rewrite this test function.</p>
<p>Do you think that it's something possible? Am I doing something wrong?
I must miss something obvious... any help would be appreciated.</p>
<p>Have a nice day!</p>
",1,1686060574,python;testing;dagster,True,812,1,1686068927,https://stackoverflow.com/questions/76415555/write-test-for-dagster-asset-job
76394305,create multiple jobs from similar assets with different partitions and schedules,"<p>I have a <code>assets.py</code> with the following:</p>
<pre><code>from dagster import asset, job, Output

from my_modules import some_func


from ..partitions import daily_partitions_def, t1_daily_partitions_def

@asset(partitions_def=t1_daily_partitions_def)
def get_rfr_final_asset(context):
    dt = context.partition_key
    some_func(dt, provisonal=True)
    return True

@asset(partitions_def=daily_partitions_def)
def get_rfr_provisional_asset(context):
    dt = context.partition_key
    some_func(dt, provisonal=False)
    return True
</code></pre>
<p>In my main dagster file, how can I create two jobs, one with the first asset and a second with the second asset both on different schedules. If I use <code>load_assets_from_modules</code> it groups both assets together, I'd like a cleaner way to do this instead of having multiple files for each asset.</p>
",1,1685758353,dagster,True,916,1,1686068446,https://stackoverflow.com/questions/76394305/create-multiple-jobs-from-similar-assets-with-different-partitions-and-schedules
76341717,Get Dagster Config Schema in Python,"<p>I am trying to programmatically kick off a Dagster job from my Python code. So far I am all set with using the python client to kick off a job with either the default or fully specified config.</p>
<p>The issue I am running into is not having the ability to ask Dagster what the config schema is. Is there a way to import the schema over API so that I don't need to hardcode it in?</p>
",0,1685110855,python;dagster,False,564,1,1685758768,https://stackoverflow.com/questions/76341717/get-dagster-config-schema-in-python
75384908,How do I add the materialization runtime to a software defined asset in Dagster?,"<p>I would like to keep track of how long it takes to materialize software defined assets over time (using Dagster).</p>
<p>Ideally I'd add the &quot;duration&quot; to the materialization metadata.</p>
<p>I could do this very crudely</p>
<pre class=""lang-py prettyprint-override""><code>import time

@asset
def my_asset():
   start_time = time.time()
   x = ...
   return Output(x, metadata:{'duration': time.time() - start_time})


</code></pre>
<p>But ideally I could avoid this boilerplate. Is this a built-in functionality?</p>
",1,1675854553,dagster,False,145,1,1684833623,https://stackoverflow.com/questions/75384908/how-do-i-add-the-materialization-runtime-to-a-software-defined-asset-in-dagster
76234492,Docker AWS ECS integration on Mac NoCredentialProviders error when running &quot;docker compose up&quot;,"<p>When I am trying to run docker compose up to deploy my infrastructure to AWS using Docker's ECS integration. I'm getting the following error
NoCredentialProviders: no valid providers in the chain. Deprecated.
For verbose messaging see aws.Config.CredentialsChainVerboseError</p>
<p>I have created ecs context as below
docker context create ecs dagster-ecs -&gt; (An existing AWS profile)</p>
<p>but while deploying
docker --context dagster-ecs compose --project-name dagster up</p>
<p>NoCredentialProviders: no valid providers in chain. Deprecated.
For verbose messaging see aws.Config.CredentialsChainVerboseError</p>
<p>Docker Compose File</p>
<pre><code>---
version: &quot;3.8&quot;

services:
  # This service runs dagit. It has no user code; instead it loads its
  # jobs from the gRPC server running in the user_code service.
  # Because our instance uses the QueuedRunCoordinator, any runs submitted from
  # dagit will be put on a queue and later dequeued and launched by
  # the dagster-daemon service.
  dagit:
    platform: linux/amd64
    build:
      context: .
      dockerfile: ./Dockerfile
      target: dagit
    image: &quot;$REGISTRY_URL/deploy_ecs/dagit&quot;
    container_name: dagit
    command: &quot;dagit -h 0.0.0.0 -p 3000 -w workspace.yaml&quot;
    ports:
      - &quot;3000:3000&quot;
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      DAGSTER_POSTGRES_DB: &quot;postgres_db&quot;
      DAGSTER_POSTGRES_HOSTNAME: &quot;postgresql&quot;
      DAGSTER_POSTGRES_PASSWORD: &quot;postgres_password&quot;
      DAGSTER_POSTGRES_USER: &quot;postgres_user&quot;
    depends_on:
      - postgresql
      - user_code
    x-aws-role:
      Statement:
        - Effect: &quot;Allow&quot;
          Action:
            - &quot;ecs:DescribeTasks&quot;
            - &quot;ecs:StopTask&quot;
          Resource:
            - &quot;*&quot;
        - Effect: &quot;Allow&quot;
          Action:
            - &quot;iam:PassRole&quot;
          Resource:
            - &quot;*&quot;
          Condition:
            StringLike:
              iam:PassedToService: &quot;ecs-tasks.amazonaws.com&quot;

  # This service runs the dagster-daemon process, which is responsible for
  # taking runs off of the queue and launching them, as well as creating
  # runs from schedules or sensors.
  daemon:
    platform: linux/amd64
    build:
      context: .
      dockerfile: ./Dockerfile
      target: dagster
    image: &quot;$REGISTRY_URL/deploy_ecs/daemon&quot;
    container_name: daemon
    command: &quot;dagster-daemon run&quot;
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      DAGSTER_POSTGRES_HOSTNAME: &quot;postgresql&quot;
      DAGSTER_POSTGRES_USER: &quot;postgres_user&quot;
      DAGSTER_POSTGRES_PASSWORD: &quot;postgres_password&quot;
      DAGSTER_POSTGRES_DB: &quot;postgres_db&quot;
    depends_on:
      - postgresql
      - user_code
    x-aws-role:
      Statement:
        - Effect: &quot;Allow&quot;
          Action:
            - &quot;ec2:DescribeNetworkInterfaces&quot;
            - &quot;ecs:DescribeTaskDefinition&quot;
            - &quot;ecs:DescribeTasks&quot;
            - &quot;ecs:ListAccountSettings&quot;
            - &quot;ecs:RegisterTaskDefinition&quot;
            - &quot;ecs:RunTask&quot;
            - &quot;ecs:TagResource&quot;
            - &quot;secretsmanager:DescribeSecret&quot;
            - &quot;secretsmanager:ListSecrets&quot;
            - &quot;secretsmanager:GetSecretValue&quot;
          Resource:
            - &quot;*&quot;
        - Effect: &quot;Allow&quot;
          Action:
            - &quot;iam:PassRole&quot;
          Resource:
            - &quot;*&quot;
          Condition:
            StringLike:
              iam:PassedToService: &quot;ecs-tasks.amazonaws.com&quot;

  # This service runs a gRPC server that serves information about your
  # repository. By setting DAGSTER_CURRENT_IMAGE to its own image, we tell the
  # run launcher to use this same image when launching runs in a new container.
  # Multiple containers like this can be deployed separately - each needs to
  # run on its own port and have its own entry in the workspace.yaml file.
  user_code:
    platform: linux/amd64
    build:
      context: .
      dockerfile: ./Dockerfile
      target: user_code
    image: &quot;$REGISTRY_URL/deploy_ecs/user_code&quot;
    container_name: user_code
    command: &quot;dagster api grpc -h 0.0.0.0 -p 4000 -f repo.py&quot;
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      DAGSTER_POSTGRES_DB: &quot;postgres_db&quot;
      DAGSTER_POSTGRES_HOSTNAME: &quot;postgresql&quot;
      DAGSTER_POSTGRES_PASSWORD: &quot;postgres_password&quot;
      DAGSTER_POSTGRES_USER: &quot;postgres_user&quot;
      DAGSTER_CURRENT_IMAGE: &quot;$REGISTRY_URL/deploy_ecs/user_code&quot;

  # This service runs the postgres DB used by dagster for run storage, schedule
  # storage, and event log storage. In a real deployment, you might choose to
  # remove this in favor of an RDS instance.
  postgresql:
    image: postgres:11
    container_name: postgresql
    environment:
      POSTGRES_DB: &quot;postgres_db&quot;
      POSTGRES_PASSWORD: &quot;postgres_password&quot;
      POSTGRES_USER: &quot;postgres_user&quot;

</code></pre>
",1,1683880249,amazon-web-services;docker;docker-compose;amazon-ecs;dagster,False,115,0,1683896560,https://stackoverflow.com/questions/76234492/docker-aws-ecs-integration-on-mac-nocredentialproviders-error-when-running-dock
76232300,How to create DAGs with Spark?,"<p>I'm new to Spark and I've realized that, for the pipeline I'm making, it would be much more convenient to have a DAG to represent the pipeline to improve monitoring, scheduling, etc.</p>
<p>I connected Spark to my MySQL database and ran a few scripts with Spark dataframes using PyTorch and it worked great. I was able to apply machine learning models and stuff.</p>
<p>The problems started once I started looking to set up a DAG. I had read Dagster is more lightweight then airflow, so I decided to use Dagster, but this created issues.</p>
<p>My goal was, for each set of transformations to do to my Spark data frame, I was going to define separate @op functions in dagster that would let me put them into a nice flow chart so I could observe them during execution from the dagit GUI.</p>
<p>However, this doesn't work because apparently you can't pass Spark DFs between these functions since dagster serializes the outputs and then deserializes them once inputted into the next function.</p>
<p>Airflow also has a similar problem it seems whereby, in order to pass data between wo tasks, you have to use the XCom (Cross Communication) to facilitate communication and data exchange between tasks within a DAG.</p>
<p>Thus, it seems like neither of these are suitable for passing data between different tasks, so I'm confused, how does one use DAGs to organize data processing in Spark?</p>
",0,1683849335,python;apache-spark;airflow;dagster,False,148,1,1683878977,https://stackoverflow.com/questions/76232300/how-to-create-dags-with-spark
75703813,Reduce a partitioned asset to a single data frame,"<p>I have a software defined asset that compares a (very) large set of partitions to a single small data frame. I've made a toy explainer that shows the situation:</p>
<pre class=""lang-py prettyprint-override""><code>partitions = dagster.StaticPartitionsDefinition([&quot;a&quot;, &quot;b&quot;])

@dagster.asset(partitions_def=partitions)
def large_dataframes(context):
    return pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))

@dagster.asset
def small_dataframe(context):
    return pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('DEFG'))

@dagster.asset(partitions_def=partitions)
def one_df_filtered_by_the_other(context, large_dataframes, small_dataframe):
    return large_dataframes.join(small_dataframe, rsuffix=&quot;.&quot;)
</code></pre>
<p>which is great; works a treat. Clear and easy to read (in general: loving dagster).</p>
<p>However, now the output of the third asset is a partitioned data frame, even though in practice many of the partitions are going to be empty and the non empty ones are going to be teensy.</p>
<p>Ideally, I'd like to &quot;reduce&quot; the partitions to a single data frame, which itself becomes a new asset I can depend on downstream.</p>
<p>The way I'm doing it, though, feels like a hack and although it seems to be working, I can't shake the sense that I'm missing something. What I've noticed is that if you forget the <code>partitions_def</code> the asset shows up as a dictionary of dataframes. So I can &quot;reduce&quot; them like this:</p>
<pre class=""lang-py prettyprint-override""><code>@dagster.asset
def reduce_to_unpartitioned(context,one_df_filtered_by_the_other):
    return pd.concat(one_df_filtered_by_the_other.values())
</code></pre>
<p>I'm quite nervous though when I get the real thing into production, and the number of partitions is in the hundreds or thousands, whether or not this will behave as I hope, mostly because I've not been able to find an &quot;official&quot; approach in the docs.</p>
<p>Does the above look right? Is there a better way?</p>
",1,1678525219,dagster,True,645,1,1683723941,https://stackoverflow.com/questions/75703813/reduce-a-partitioned-asset-to-a-single-data-frame
76173666,How to implement io_manager that have a parameter at asset level?,"<p>I am a bit confused about the usage of resources, configuration and how they are linked to a context and an asset.</p>
<p>So I have a parquet io manager that is able to manipulate and partitionned not-partitionned datasets.
To do so I check the presence of a partition on the context in the <code>self._get_path()</code> method and provide a unique name for each file, using the key of the asset and a date format of the partition.</p>
<pre class=""lang-py prettyprint-override""><code># from dagter examples
if context.has_asset_partitions:
    end = context.asset_partitions_time_window
</code></pre>
<p>Now I have an issue if the same asset is used with different paritions sizes because the names not necesserally are the same during the reading and writing of the files. <em>e.g.</em> I have some 1h partitions asset and some 1d partitions asset using the same base asset.</p>
<p>The solution to this, IMO is to use the <code>filters</code> kwargs from <code>pandas.read_parquet</code>, that would allow me to get only the data inside the time window of the partition.
So I want to provide a string parameter to my io manager for it to know which column has to be used to filter the partition interval.</p>
<p>This parameter is obviously linked to an asset.</p>
<p>I could add this as a parameter of my io_manger constructor and create one instance of io_manager per different column name. But I find it cumbersome and my intuition tells me that I should be using the InputContext to retrieve this information. (the same way I am using the context to get the start,end of the partition)</p>
<p>So maybe I should create a ConfigurableResource with only one string attribute (the time column's name), instantiate one object per different column name and provide it to the asset construction (via required_resource_keys?). If this is the right solution, how can I access to the ressource in the io_manager?</p>
<p>Or is there any other parameter of the asset constructor that I should be using to achieve what I want?</p>
",0,1683206306,python;parquet;dagster,True,613,2,1683582243,https://stackoverflow.com/questions/76173666/how-to-implement-io-manager-that-have-a-parameter-at-asset-level
76201568,Execute ECS task from Dagster,"<p>I am relatively new to Dagster. I am trying to execute an ECS task from dagster using this standalone python file as a job:</p>
<pre><code>from dagster import job, op

@op()
def my_op(context):
  context.log.info('running')

@job(
  tags={
    &quot;ecs/task_definition&quot;: &quot;arn_task_definition&quot;,
    &quot;ecs/container_name&quot;: &quot;container_name&quot;
  }
)
def my_job():
  my_op()
</code></pre>
<p>Command to execute:</p>
<pre><code>dagster dev -f hello_dag.py
</code></pre>
<p>In the logs, I am able to get &quot;running&quot;. But, I couldn't find any logs related to ECS. I also checked ECS console, but there are no task triggered. Can someone please help with executing the ECS task? Thanks</p>
",0,1683556195,python;amazon-ecs;dagster,False,156,0,1683556195,https://stackoverflow.com/questions/76201568/execute-ecs-task-from-dagster
76153838,How not to overwrite materialized assets in Dagster,"<p>I'm new to Dagster.</p>
<p>According to the docs, when you use the built-in filesystem IO manager:</p>
<blockquote>
<p>Subsequent materializations of an asset will overwrite previous materializations of that asset.</p>
</blockquote>
<p><a href=""https://docs.dagster.io/_apidocs/io-managers#dagster.FilesystemIOManager"" rel=""nofollow noreferrer"">https://docs.dagster.io/_apidocs/io-managers#dagster.FilesystemIOManager</a></p>
<p>Indeed, if I change the code of an asset and re-materialize it, the new run gets a different code version and data version, however, the new result is written to the same path, overwriting the result of the previous materialization.</p>
<p>I find that strange, because in practice you usually need to keep track of the old data from previous runs, instead of overwriting it.</p>
<p>I think it's possible to create a custom IO manager that would include a unique run ID into the asset path. Is there a better, more out-of-the box way to preserve data from previous runs?</p>
",4,1683022322,python;dagster,False,450,0,1683022322,https://stackoverflow.com/questions/76153838/how-not-to-overwrite-materialized-assets-in-dagster
76007080,How can I partition dagster assets by year?,"<p>Not seeing a built in definition to partition assets by year (just hourly-monthly). Is there a way to manipulate the built in time definitions to accomplish this?</p>
<p>Any help is appreciated!</p>
",-1,1681398721,dagster,False,280,1,1681744870,https://stackoverflow.com/questions/76007080/how-can-i-partition-dagster-assets-by-year
75751083,How do you explicitly name an Output in Dagster?,"<p>I have the following asset in Dagster:</p>
<pre class=""lang-py prettyprint-override""><code>@asset
def some_asset():
    
    output = &quot;some_great_output&quot;

    return Output(
        value=output,
    )
</code></pre>
<p>As it is above, Dagster will name the materialization of this asset &quot;result&quot;, the default name for outputs - not a very useful default name. Hence, I tried to change the output name by specifying the appropriate parameter of the <code>Output</code> object:</p>
<pre class=""lang-py prettyprint-override""><code>@asset
def some_asset():
    
    output = &quot;some_great_output&quot;

    return Output(
        value=output,
        output_name=some_name,
    )
</code></pre>
<p>You would expect that this does the trick, right? It doesn't. This is the error it raises:</p>
<pre><code>dagster._core.errors.DagsterInvariantViolationError: Bad state: Output was explicitly named 'some_name', which does not match the output definition specified for position 0: 'result'.
</code></pre>
<p>How can I fix this?</p>
",0,1678924921,dagster,False,732,1,1681510944,https://stackoverflow.com/questions/75751083/how-do-you-explicitly-name-an-output-in-dagster
74666086,How to run a C# data transformation in dagster,"<p>I am currently evaluating dagster to build data engineering pipelines. I have to incorporate a huge body of existing c# code for a key piece of data transformation (from object storage to object storage) that I cannot simply replace in python.</p>
<p>Do I understand the dagster documentation correctly, that I should run this step (or op) in a docker container that contains the relevant c# programs and than shell out to them via <a href=""https://docs.dagster.io/_apidocs/libraries/dagster-shell"" rel=""nofollow noreferrer"">https://docs.dagster.io/_apidocs/libraries/dagster-shell</a> ?</p>
",0,1670065161,c#;dagster,False,288,1,1681337740,https://stackoverflow.com/questions/74666086/how-to-run-a-c-data-transformation-in-dagster
75789863,cannot import name &#39;Definitions&#39; from &#39;dagster&#39;,"<p>I'm trying out this example project: <a href=""https://github.com/dagster-io/dagster/tree/master/examples/project_fully_featured"" rel=""nofollow noreferrer"">https://github.com/dagster-io/dagster/tree/master/examples/project_fully_featured</a></p>
<p>I added a workspace.yaml file with the lines:</p>
<pre><code>load_from:
  - python_module: project_fully_featured
</code></pre>
<p>Which was necessary to run the <code>dagit</code> command without further options, as described in the github repo readme.</p>
<p>However, when running the <code>dagit</code> command, I get the following error:</p>
<pre><code>/opt/homebrew/lib/python3.9/site-packages/dagster/_core/workspace/context.py:538: UserWarning: Error loading repository location project_fully_featured:dagster._core.errors.DagsterImportError: Encountered ImportError: `cannot import name 'Definitions' from 'dagster' (/opt/homebrew/lib/python3.9/site-packages/dagster/__init__.py)` while importing module project_fully_featured. Local modules were resolved using the working directory `/Users/&lt;name&gt;/Documents/dev/tutorials/tutorial_dagster_project_fully_featured/my-dagster-project`. If another working directory should be used, please explicitly specify the appropriate path using the `-d` or `--working-directory` for CLI based targets or the `working_directory` configuration option for workspace targets. 

Stack Trace:
  File &quot;/opt/homebrew/lib/python3.9/site-packages/dagster/_grpc/server.py&quot;, line 230, in __init__
    self._loaded_repositories: Optional[LoadedRepositories] = LoadedRepositories(
  File &quot;/opt/homebrew/lib/python3.9/site-packages/dagster/_grpc/server.py&quot;, line 104, in __init__
    loadable_targets = get_loadable_targets(
  File &quot;/opt/homebrew/lib/python3.9/site-packages/dagster/_grpc/utils.py&quot;, line 43, in get_loadable_targets
    else loadable_targets_from_python_module(module_name, working_directory)
  File &quot;/opt/homebrew/lib/python3.9/site-packages/dagster/_core/workspace/autodiscovery.py&quot;, line 36, in loadable_targets_from_python_module
    module = load_python_module(
  File &quot;/opt/homebrew/lib/python3.9/site-packages/dagster/_core/code_pointer.py&quot;, line 141, in load_python_module
    raise DagsterImportError(

The above exception was caused by the following exception:
ImportError: cannot import name 'Definitions' from 'dagster' (/opt/homebrew/lib/python3.9/site-packages/dagster/__init__.py)

Stack Trace:
  File &quot;/opt/homebrew/lib/python3.9/site-packages/dagster/_core/code_pointer.py&quot;, line 136, in load_python_module
    return importlib.import_module(module_name)
  File &quot;/opt/homebrew/Cellar/python@3.9/3.9.16/Frameworks/Python.framework/Versions/3.9/lib/python3.9/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1030, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1007, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 986, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 680, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 850, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 228, in _call_with_frames_removed
  File &quot;/Users/&lt;name&gt;/Documents/dev/tutorials/tutorial_dagster_project_fully_featured/my-dagster-project/project_fully_featured/__init__.py&quot;, line 3, in &lt;module&gt;
    from dagster import Definitions

  warnings.warn(
2023-03-20 12:36:20 +0100 - dagit - INFO - Serving dagit on http://127.0.0.1:3000 in process 63845
</code></pre>
<p>When I run python3 in my terminal window and run 'from dagster import Definitions', all seems to work fine, no error thrown there.</p>
",0,1679312736,python;dagster,False,1342,0,1679312736,https://stackoverflow.com/questions/75789863/cannot-import-name-definitions-from-dagster
75700024,How to load assets from dbt project? #12893,"<p>I'm trying to load assets from my dbt project called &quot;dbtz&quot;, but i'm receiving the error</p>
<pre><code>hyprster:dagster_dbt.errors.DagsterDbtCliFatalRuntimeError: Fatal error in the dbt CLI (return code 2): Encountered an error while reading the project:  ERROR: Runtime Error
  Could not find profile named dbtz Encountered an error:
Runtime Error
  Could not run dbt
</code></pre>
<p>the dbt run works fine on the command line, but i just can't use it on dagster</p>
<p>i'm trying to execute the following code:</p>
<pre><code>from dagster_dbt import load_assets_from_dbt_project
from dagster._utils import file_relative_path

DBT_PROJECT_PATH = file_relative_path(__file__, &quot;../../dbtz&quot;)
DBT_PROFILES = file_relative_path(__file__, &quot;../../dbtz/config/profiles.yml&quot;)

dbt_assets = load_assets_from_dbt_project(project_dir=DBT_PROJECT_PATH, profiles_dir=DBT_PROFILES)
</code></pre>
<p>the relative path is correct, i can cat the profiles.yml, for example</p>
<p>my profiles.yml is:</p>
<pre><code>dbtz:
  target: dev
  outputs:
    dev:
      dataset: dev
      job_execution_timeout_seconds: 600
      job_retries: 0
      keyfile: ../service_account.json
      location: US
      method: service-account
      priority: interactive
      project: site-hypr
      threads: 1
      type: bigquery
    prod:
      dataset: prod
      job_execution_timeout_seconds: 600
      job_retries: 0
      keyfile: ../service_account.json
      location: US
      method: service-account
      priority: interactive
      project: site-hypr
      threads: 1
      type: bigquery
</code></pre>
",0,1678474673,python-3.x;dbt;dagster,False,674,1,1678554215,https://stackoverflow.com/questions/75700024/how-to-load-assets-from-dbt-project-12893
75685796,Dagster is detecting my code but not my sensors?,"<p>I have a simple local dagster project. One of its files is as follows:</p>
<pre class=""lang-py prettyprint-override""><code>from dagster import asset, Definitions, build_asset_reconciliation_sensor, AssetSelection

group_name = 'for_testing'

@asset(group_name=group_name)
def hello():
    return 'Hello'

@asset(group_name=group_name)
def loud_hello(hello):
    return hello.upper()

update_sensor = build_asset_reconciliation_sensor(
    name=&quot;update_sensor&quot;, asset_selection=AssetSelection.assets(hello, loud_hello)
)

defs = Definitions(assets=[hello, loud_hello], sensors=[update_sensor])
</code></pre>
<p>I am running dagit locally, and I can see and materialize the assets (<code>hello</code> &amp; <code>loud_hello</code>)</p>
<p>But:</p>
<ul>
<li>Can't see the sensor in dagit</li>
<li>The sensor is not working (loud_hello does not update when I update hello)</li>
</ul>
<p>Any ideas?</p>
",1,1678371347,dagster,True,544,1,1678381576,https://stackoverflow.com/questions/75685796/dagster-is-detecting-my-code-but-not-my-sensors
75681976,Call downstream assets or ops in parallel with Dagster,"<p>I build a data pipeline with Dagster using the <code>asset</code> API.</p>
<p>I am looking for a way to explicitly call/execute an <code>asset</code> (or <code>op</code>) for a list of items. While <code>assets</code> are executed in parallel if they are independent, I did not find a way to include iteration.</p>
<p>Simple example: Iterate over a list of files and perform a task for each file. The tasks are independent.</p>
<pre class=""lang-py prettyprint-override""><code>@asset
def iterate_files():
    path = Path(&quot;/some/path&quot;)
    
    for file in path.iterdir():
        # here I would like to start parallel ops
        ...

</code></pre>
<p>If I pass a list to a downstream <code>asset</code>, the operation is performed sequentially:</p>
<pre class=""lang-py prettyprint-override""><code>
@asset
def list_of_files():
    path = Path(&quot;/some/path&quot;)
    return list(path.iterdir())

@asset
def process_files(list_of_files):
    for file in list_of_files):
        with open(file, 'rt') as f:
            for line in f:
                # write to a DB or other operation

</code></pre>
<p>Is it possible to achieve a parallel execution of the downstream task with Dagster?</p>
",2,1678350000,python;dagster,False,463,0,1678355783,https://stackoverflow.com/questions/75681976/call-downstream-assets-or-ops-in-parallel-with-dagster
75641630,stream data between tasks in pipeline orchestration tool Prefect/Dagster/Airflow,"<p>How can I stream data between tasks in a workflow with the help of a data pipeline orchestration tool like Prefect, Dagster or Airflow?</p>
<p>I am looking for a good data pipeline orchestration tool.
I think I have a fairly decent overview now of what Apache Airflow is capable of.
One thing I am missing in Airflow is the possibility to stream data between tasks.</p>
<p>I have an existing Python pipeline which extracts, transforms and loads data and uses Unix pipes in between. In bash syntax: <code>extract | transform | load</code> meaning all three processes/tasks run in parallel.</p>
<p>I am aware that I could use Airflow with two intermediary storage targets in between and then start pulling the data as soon as it is available. My understanding is that I would have to create 3 distinct DAGs for this or keep everything in a single task where I would have to parallelize the processes manually. I could be wrong but that seems like a bad architecture for this solution. It should be possible to represent this workflow in a single abstraction and let the orchestration tool take care of parallelization.</p>
<p>I am also aware that using pipes might not work for all executors since they might reside on different nodes. However for this solution it would be fine to restrict the workflow to a single node or use an alternative way of streaming the data as long as it stays simple.</p>
<p>ELT would be another approach, but I don't like it much because it is way more sensible to remove sensitive data before it reaches the destination, not after. Plus the transform step in between allows me to reduce the amount of data I have to transfer and store considerably and also reduces the complexity of maintaining a temporary schema in the destination database :)
Somehow the current shift to ELT does not appeal to me much.</p>
",0,1678012820,airflow;data-pipeline;prefect;dagster,True,649,2,1678206162,https://stackoverflow.com/questions/75641630/stream-data-between-tasks-in-pipeline-orchestration-tool-prefect-dagster-airflow
68993825,Start Dagster Schedule automatically,"<p>Hi i am learning dagster and i want help with starting schedule
I am able to add and start schedule in dagit but i want to start schedule automatically instead of turn on every schedule from dagit.</p>
<p>#Here is my code</p>
<pre><code>@solid()
def test(context):
   context.log.info(&quot;test&quot;)
  
@pipeline()
def testPipeline():
    test()

@schedule(
    cron_schedule=&quot;* * * * *&quot;, pipeline_name=&quot;testPipeline&quot;, execution_timezone=&quot;Asia/Kolkata&quot;
)
def scheduleTest():
    return {}

@repository()
def testRepo():
    return [testPipeline, scheduleTest]
</code></pre>
",6,1630391226,python-3.x;dagster,True,854,3,1678103003,https://stackoverflow.com/questions/68993825/start-dagster-schedule-automatically
71659188,Dagster Daemon sends heartbeats but starts no runs,"<p>I created a simplified project with dagster and started the dagster-daemon, however it starts no runs even though it sends heartbeats to the default sqlite daemon_heartbeats table.</p>
<p>I put all the files I needed in the same folder (at this point, they're just like hello world files), created a workspace.yaml and a dagster.yaml manually. Here is how the files look like:<a href=""https://i.stack.imgur.com/2JINH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2JINH.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/kqVeP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kqVeP.png"" alt=""enter image description here"" /></a></p>
<p>I also configured my repository like the following:
<a href=""https://i.stack.imgur.com/wZPcl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wZPcl.png"" alt=""enter image description here"" /></a></p>
<p>I believe, I did everything right, because I can load the repository from dagit without problems and I even see the option to turn the schedule on and off.
<a href=""https://i.stack.imgur.com/AVIun.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AVIun.png"" alt=""enter image description here"" /></a></p>
<p>After doing all this, I started the dagster-daemon with <code>dagster-daemon run</code>. The logs are alright (in the sense that I don't get any errors):
<a href=""https://i.stack.imgur.com/9jFpI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9jFpI.png"" alt=""enter image description here"" /></a></p>
<p>And the daemon_heartbeats table does get updated regularly:
<a href=""https://i.stack.imgur.com/h1WfV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/h1WfV.png"" alt=""enter image description here"" /></a></p>
<p>Nevertheless, when I check the daemon on dagit, I get the message that the daemon is not running.
<a href=""https://i.stack.imgur.com/VVD60.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VVD60.png"" alt=""enter image description here"" /></a></p>
<p>Does anyone have an idea of what could be the issue here? Am I overlooking somethin basic?</p>
<p>Thanks in advance</p>
",0,1648544343,dagster,False,1004,1,1677595632,https://stackoverflow.com/questions/71659188/dagster-daemon-sends-heartbeats-but-starts-no-runs
75434895,How to use dagster with great expectations?,"<h2>The issue</h2>
<p>I'm trying out great expectations with dagster, as per <a href=""https://docs.dagster.io/0.15.9/integrations/great-expectations"" rel=""nofollow noreferrer"">this guide</a></p>
<p>My pipeline seems to execute correctly until it reaches this block:</p>
<pre class=""lang-py prettyprint-override""><code>expectation = dagster_ge.ge_validation_op_factory(
        name='ge_validation_op',
        datasource_name='dev.data-pipeline-data-storage.data_pipelines.raw_data.sirene_update',
        suite_name='suite.data_pipelines.raw_data.sirene_update',
    )
    
if expectation[&quot;success&quot;]:  
    print(&quot;Success&quot;) 
</code></pre>
<p>trying to call <code>expectation[&quot;success&quot;]</code> results in a</p>
<pre><code># TypeError: 'SolidDefinition' object is not subscriptable
</code></pre>
<p>When I go inside the code of <code>ge_validation_op_factory</code>, there is a <code>_ge_validation_fn</code> that should <code>yield ExpectationResult</code>, but somehow it gets coverted into a <code>SolidDefinition</code>...</p>
<p>Dagster version = 0.15.9;
Great Expectations version = 0.15.44</p>
<h2>Code to reproduce the error</h2>
<p>In my code, I am trying to interact with an <code>s3</code> bucket, so it would be a bit tedious to re-create the code for my example but here it is anyway:</p>
<p>In a <code>gx_postprocessing.py</code></p>
<pre class=""lang-py prettyprint-override""><code>import json
import boto3
import dagster_ge
from dagster import (
    op,
    graph,
    Field,
    String,
    OpExecutionContext,
)

from typing import List, Dict


@op(
    config_schema={
        &quot;bucket&quot;: Field(
            String,
            description=&quot;s3 bucket name&quot;,
        ),
        &quot;path_in_s3&quot;: Field(
            String,
            description=&quot;Prefix representing the path to data&quot;,
        ),
        &quot;technical_date&quot;: Field(
            String,
            description=&quot;date string to fetch data&quot;,
        ),
        &quot;file_name&quot;: Field(
            String,
            description=&quot;file name that contains the data&quot;,
        ),
    }
)
def read_in_json_datafile_from_s3(context: OpExecutionContext):
    bucket = context.op_config[&quot;bucket&quot;]
    path_in_s3 = context.op_config[&quot;path_in_s3&quot;]
    technical_date = context.op_config[&quot;technical_date&quot;]
    file_name = context.op_config[&quot;file_name&quot;]

    object = f&quot;{path_in_s3}/&quot; f&quot;technical_date={technical_date}/&quot; f&quot;{file_name}&quot;

    s3 = boto3.resource(&quot;s3&quot;)

    content_object = s3.Object(bucket, object)
    file_content = content_object.get()[&quot;Body&quot;].read().decode(&quot;utf-8&quot;)
    json_content = json.loads(file_content)

    return json_content


@op
def process_example_dq(data: List[Dict]):
    return len(data)


@op
def postprocess_example_dq(numrows, expectation):
    if expectation[&quot;success&quot;]:
        return numrows
    else:
        raise ValueError


@op
def validate_example_dq(context: OpExecutionContext):

    expectation = dagster_ge.ge_validation_op_factory(
        name='ge_validation_op',
        datasource_name='my_bucket.data_pipelines.raw_data.example_update',
        suite_name='suite.data_pipelines.raw_data.example_update',
    )

    return expectation


@graph(
    config={
        &quot;read_in_json_datafile_from_s3&quot;: {
            &quot;config&quot;: {
                &quot;bucket&quot;: &quot;my_bucket&quot;,
                &quot;path_in_s3&quot;: &quot;my_path&quot;,
                &quot;technical_date&quot;: &quot;2023-01-24&quot;,
                &quot;file_name&quot;: &quot;myfile_20230124.json&quot;,
            }
        },
    },
)
def example_update_evaluation():
    output_dict = read_in_json_datafile_from_s3()
    nb_items = process_example_dq(data=output_dict)
    expectation = validate_example_dq()
    postprocess_example_dq(
        numrows=nb_items,
        expectation=expectation,
    )
</code></pre>
<p>Do not forget to add <code>great_expectations_poc_pipeline</code> to your <code>__init__.py</code> where the <code>pipelines=[..]</code> are listed.</p>
",0,1676285455,great-expectations;dagster,True,397,1,1676328630,https://stackoverflow.com/questions/75434895/how-to-use-dagster-with-great-expectations
66567379,Dagster Installation on MacOS Big Sur,"<p>I am trying to install Dagster and Dagit using:<code>python3 -m pip install dagster dagit --user</code>
But, Always when it starts to build wheel for grpcio, it throws an error and stops the installation.</p>
<pre><code>ERROR: Command errored out with exit status 1: /Library/Developer/CommandLineTools/usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'/private/var/folders/cb/vkzjvbbn2l5gbb31y3m61d_40000gp/T/pip-install-3005iws8/grpcio/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'/private/var/folders/cb/vkzjvbbn2l5gbb31y3m61d_40000gp/T/pip-install-3005iws8/grpcio/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record /private/var/folders/cb/vkzjvbbn2l5gbb31y3m61d_40000gp/T/pip-record-8m4mi_rs/install-record.txt --single-version-externally-managed --compile --user --prefix= Check the logs for full command output.
</code></pre>
<p>Is anyone using Dagster and Dagit on Mac?</p>
<p><strong>EDIT: I was able to install Dagster by providing the required version in my requirements.txt File.</strong></p>
",0,1615388566,python-3.x;macos;dagster,True,917,2,1676153547,https://stackoverflow.com/questions/66567379/dagster-installation-on-macos-big-sur
75261880,How do I write a Dagster asset that depends on an earlier partition of itself?,"<p>I was using <code>depends_on_past</code> with Airflow. I'm now using Dagster, with software-defined assets, and I was told that the way to get similar functionality is with <code>build_asset_reconciliation_sensor</code> and a daily-partitioned asset where each partition depends on past partitions of itself.  How do I write a such an asset?</p>
<p>I tried creating a daily-partitioned asset, but I wasn't sure how to make it depend on earlier partitions of itself</p>
",0,1674843252,data-partitioning;dagster,True,721,1,1674843307,https://stackoverflow.com/questions/75261880/how-do-i-write-a-dagster-asset-that-depends-on-an-earlier-partition-of-itself
75099470,Getting current execution date in a task or asset in dagster,"<p>Is there an easier way than what I'm doing to get the current date in an <code>dagster</code> asset, than what I'm currently doing?</p>
<pre><code>def current_dt():
    return datetime.today().strftime('%Y-%m-%d')

@asset
def my_task(current_dt):
    return current_dt

</code></pre>
<p>In <code>airflow</code> these are passed by default in the python callable function definition ex: <code>def my_task(ds, **kwargs):</code></p>
",1,1673541050,python;dagster,True,656,1,1673559906,https://stackoverflow.com/questions/75099470/getting-current-execution-date-in-a-task-or-asset-in-dagster
75100247,Dagster -Execute an @Op only when all parallel executions are finished(DynamicOutput),"<p>I have a problem that in fact I am not able to solve in dagster.</p>
<p>I have the following configuration:</p>
<p>I have step 1 where I get the data from an endpoint</p>
<p>step 2 gets a list of customers dynamically:</p>
<p>step 3 is the database update with the response from step 1, for each customer from step 2, but in parallel.</p>
<p>before calling step 3, I have a function that serves to create DynamicOutput for each client of step 2, with the name &quot;parallelize_clients &quot;so that when it is invoked, it parallelizes the update processes of step_3 and finally I have a graph to join operations.</p>
<pre><code>@op()
def step_1_get_response():
    return {'exemple': 'data'}

@op()
def step_2_get_client_list():
    return ['client_1', 'client_2', 'client_3'] #the number of customers is dynamic.

@op(out=DynamicOut())
def parallelize_clients(context, client_list):
    for client in client_list:
        yield DynamicOutput(client, mapping_key=str(client))


@op()
def step_3_update_database_cliente(response, client):
    ...OPERATION UPDATE IN DATABASE CLIENT

@graph()
def job_exemple_graph():
    response = step_1_get_response()
    clients_list = step_2_get_client_list()
    clients = parallelize_clients(clients_list)
    #run the functions in parallel
    clients.map(lambda client: step_3_update_database_cliente(response, client))
</code></pre>
<p>According to the documentation, an @Op starts as soon as its dependencies are fulfilled, and in the case of Ops that have no dependency, they are executed instantly, without having an exact order of execution. Example: My step1 and step2 have no dependencies, so both are running in parallel automatically. After the clients return, the &quot;parallelize_clients()&quot; function is executed, and finally, I have a map in the graph that dynamically creates several executions according to the amount of client(DynamicOutput)</p>
<p><a href=""https://i.stack.imgur.com/8cW8D.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8cW8D.png"" alt=""enter image description here"" /></a></p>
<p>So far it works, and everything is fine. Here's the problem. I need to execute a specific function only when step3 is completely finished, and as it is created dynamically, several executions are generated in parallel, however, I am not able to control to execute a function only when all these executions in parallel are finished.</p>
<p>in the graph I tried to put the call to an op &quot;exemplolaststep() step_4&quot; at the end, however, step 4 is executed together with &quot;step1&quot; and &quot;step2&quot;, and I really wanted step4 to only execute after step3, but not I can somehow get this to work. Could someone help me?</p>
<p><a href=""https://i.stack.imgur.com/1ul3n.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1ul3n.png"" alt=""enter image description here"" /></a></p>
<p>I tried to create a fake dependency with</p>
<pre><code>
@op(ins={&quot;start&quot;: In(Nothing)})
def step_4():
    pass

</code></pre>
<p>and in the graph, when calling the operations, I tried to execute the map call inside the step_4() function call; Example</p>
<pre><code>@graph()
def job_exemple_graph():
    response = step_1_get_response()
    clients_list = step_2_get_client_list()
    clients = parallelize_clients(clients_list)
    #run the functions in parallel
    step_4(start=clients.map(lambda client: step_3_update_database_cliente(response, client)))
</code></pre>
<p>I have tried other approaches as well, however, to no avail.</p>
",1,1673544957,dagster,True,526,1,1673550797,https://stackoverflow.com/questions/75100247/dagster-execute-an-op-only-when-all-parallel-executions-are-finisheddynamicou
74981155,how to iterate over a list of values returning from ops to jobs in dagster,"<p>I am new to the dagster world and working on ops and jobs concepts. \</p>
<p>my requirement is to read a list of data from <code>config_schema</code> and pass it to <code>@op</code> function and return the same list to jobs. \</p>
<p>The code is show as below</p>
<pre><code>@op(config_schema={&quot;table_name&quot;:list})
def read_tableNames(context):
    lst=context.op_config['table_name']
    return lst

@job
def write_db():
    tableNames_frozenList=read_tableNames()
    print(f'--------------&gt;',type(tableNames_frozenList))
    print(f'--------------&gt;{tableNames_frozenList}')
</code></pre>
<p>when it accepts the list in @op function, it is showing as a frozenlist type but when i tried to return to jobs it conver it into <code>&lt;class 'dagster._core.definitions.composition.InvokedNodeOutputHandle'&gt;</code> data type</p>
<p>My requirement is to fetch the list of data and iterate over the list and perform some operatiosn on individual data of a list using @ops</p>
<p>Please help to understand this <br />
Thanks in advance !!!</p>
",2,1672654513,etl;pipeline;dagster,True,845,1,1672765321,https://stackoverflow.com/questions/74981155/how-to-iterate-over-a-list-of-values-returning-from-ops-to-jobs-in-dagster
74903516,How do I pass data to an op in a different module in dagster?,"<p>I am new to dagster and am having a difficult time sorting this one out.  I have to jobs defined in my dagster pipeline and I want to pass data from an op in one job to an op in another</p>
<p>My setup is as such (simplified example)</p>
<p>job1.py</p>
<pre class=""lang-py prettyprint-override""><code>@op()
def generate_num():
    return 3
@op()
def increase_num(generate_num):
    return generate_num + 1
@job()
def increment_up():
    increase_num(generate_num))
   
</code></pre>
<p>job2.py</p>
<pre><code>@op()
def decrease_num(generate_num)
    generate_num - 1
@op()
def multiple_num(decrease_num)
    decrease_num * 2
@job()
def get_multiple():
    multiple_num(decrease_num())
</code></pre>
<p>where the value returned from &quot;generate_num&quot; is passed to job2.py.  Is this totally off base to try?</p>
",1,1671825183,python;orchestration;dagster,False,1089,2,1672177724,https://stackoverflow.com/questions/74903516/how-do-i-pass-data-to-an-op-in-a-different-module-in-dagster
74772185,How to create an EMR cluster and submit a spark-submit step using Dagster?,"<p>I want to create a Dagster app that creates an EMR cluster and adds a spark-submit step, but due to a lack of documentation or examples I can't figure out how to do that (copilot also struggles with it :-)).</p>
<p>The idea is to create a scheduler with Dagster that creates an EMR cluster and runs scala-spark app as one of its steps.</p>
<p>Here's the code I have (it's not working correctly, but you may get a sense about what I was trying to do):</p>
<pre><code>from dagster_shell import create_shell_command_op
from dagster_aws.emr.emr import EmrJobRunner
from dagster import graph, op

@op
def create_emr_cluster(context):
emr_job_runner = EmrJobRunner('us-east-1', aws_access_key_id='ACCESS_KEY', aws_secret_access='SECRET_KEY')
    cluster_id = emr_job_runner.create_cluster()
    step_dict = emr_job_runner.construct_step_dict_for_command('Spark Step', 'spark-submit --class org.apache.spark.examples.SparkPi --deploy-mode cluster  s3://my-bucket/spark-examples.jar stage')
    emr_job_runner.add_job_flow_steps(None, cluster_id, [step_dict])

@graph
def my_graph():
    # a = create_shell_command_op('echo &quot;hello, world!&quot;', name=&quot;a&quot;) # this will invoke spark-submit on an existing cluster
    # a()
    create_emr_cluster()

my_job = my_graph.to_job()
</code></pre>
<p>How can I do it?</p>
",1,1670853444,python;apache-spark;amazon-emr;dagster,True,571,1,1671767022,https://stackoverflow.com/questions/74772185/how-to-create-an-emr-cluster-and-submit-a-spark-submit-step-using-dagster
74798282,Create success hook with telegram-bot alert,"<p>I'm new in Dagster and try to create success hook that will send alerts through a telegram bot. Need help, please</p>
<p><strong>Resource:</strong></p>
<pre><code>@resource
def send_message(message):
    class TelegramConnection:
        def telegram_resource(message):
            botid = os.environ['telegram_bot']
            chat = os.environ['telegram_chat']
            bot = telegram.Bot(token=botid)   
            return bot.sendMessage(chat_id=chat, text=message, parse_mode='HTML')
    return TelegramConnection()
</code></pre>
<p><strong>Hook:</strong></p>
<pre><code>def _default_status_message(context: HookContext, status: str) -&gt; str:
    return &quot;Op {op_name} on job {pipeline_name} {status}!\nRun ID: {run_id}&quot;.format(
        op_name=context.op.name,
        pipeline_name=context.pipeline_name,
        run_id=context.run_id,
        status=status,
    )

def _default_success_message(context: HookContext) -&gt; str:
    return _default_status_message(context, status=&quot;succeeded&quot;)

def telegram_on_success(
    message_fn: Callable[[HookContext], str] = _default_success_message,
    dagit_base_url: Optional[str] = &quot;http://localhost:3000&quot;,
):

    @success_hook(required_resource_keys={&quot;telegram&quot;})
    def _hook(context: HookContext):
        text = message_fn(context)
        if dagit_base_url:
            text += &quot;\n&lt;{base_url}/instance/runs/{run_id}|View in Dagit&gt;&quot;.format(
                base_url=dagit_base_url, run_id=context.run_id
            )
        context.resources.telegram.send_message(text=text)  # type: ignore
    return _hook
</code></pre>
<p><strong>Job:</strong></p>
<pre><code>@job (resource_defs={&quot;telegram&quot;: send_message}, hooks={success_hook}, op_retry_policy = default_policy)
def job_text_for_pictures():
    bq = extract_ID_with_photo()
    numbers, numbers_withoun_null = extract_id_for_items()
    df_case, df_prod = extract_data_from_sf()
    subset, query = transform_two_df(df_case, df_prod, numbers, numbers_withoun_null)
    final = final_sql_result(query, subset)
    merged_df = result_merge(final, bq)
    load_df(merged_df)
</code></pre>
<p><strong>Error:</strong></p>
<p>dagster._check.CheckError: Member of set mismatches type. Expected &lt;class 'dagster._core.definitions.hook_definition.HookDefinition'&gt;. Got &lt;function success_hook at 0x00000284AC2BB250&gt; of type &lt;class 'function'&gt;.</p>
<p><strong>UPDATE:</strong></p>
<pre><code>@resource
    def telegram_resource(message):
        class TelegramConnection:
            def send_message(message):
                botid = os.environ['telegram_bot']
                chat = os.environ['telegram_chat']
                bot = telegram.Bot(token=botid)   
                return bot.sendMessage(chat_id=chat, text=message, parse_mode='HTML')
        return TelegramConnection()
</code></pre>
<p><strong>Job:</strong></p>
<pre><code>@job (resource_defs={&quot;telegram&quot;: telegram_resource}, hooks={**telegram_on_success**}, op_retry_policy = default_policy)
    def job_text_for_pictures():
        bq = extract_ID_with_photo()
        numbers, numbers_withoun_null = extract_id_for_items()
        df_case, df_prod = extract_data_from_sf()
        subset, query = transform_two_df(df_case, df_prod, numbers, numbers_withoun_null)
        final = final_sql_result(query, subset)
        merged_df = result_merge(final, bq)
        load_df(merged_df)
</code></pre>
<p><strong>Hook:</strong></p>
<pre><code>def _default_status_message(context: HookContext, status: str) -&gt; str:
        return &quot;Op {op_name} on job {pipeline_name} {status}!\nRun ID: {run_id}&quot;.format(
            op_name=context.op.name,
            pipeline_name=context.pipeline_name,
            run_id=context.run_id,
            status=status,
        )
    
    def _default_success_message(context: HookContext) -&gt; str:
        return _default_status_message(context, status=&quot;succeeded&quot;)
    
    def telegram_on_success(
        message_fn: Callable[[HookContext], str] = _default_success_message,
        dagit_base_url: Optional[str] = &quot;http://localhost:3000&quot;,
    ):
    
@success_hook(required_resource_keys={&quot;telegram&quot;})
        def _hook(context: HookContext):
            text = message_fn(context)
            if dagit_base_url:
                text += &quot;\n&lt;{base_url}/instance/runs/{run_id}|View in Dagit&gt;&quot;.format(
                    base_url=dagit_base_url, run_id=context.run_id
                )
            context.resources.telegram.send_message(**text**)  # type: ignore
        return _hook
</code></pre>
<p><strong>New Error:</strong>
TypeError: telegram_resource..TelegramConnection.send_message() takes 1 positional argument but 2 were given</p>
<p>Stack Trace:
File &quot;C:\Users\AlBelyaev\AppData\Local\Programs\Python\Python310\lib\site-packages\dagster_core\errors.py&quot;, line 188, in user_code_error_boundary
yield
,  File &quot;C:\Users\AlBelyaev\AppData\Local\Programs\Python\Python310\lib\site-packages\dagster_core\execution\plan\execute_plan.py&quot;, line 162, in _trigger_hook
hook_execution_result = hook_def.hook_fn(hook_context, step_event_list)
,  File &quot;C:\Users\AlBelyaev\AppData\Local\Programs\Python\Python310\lib\site-packages\dagster_core\definitions\decorators\hook_decorator.py&quot;, line 198, in _success_hook
fn(context)
,  File &quot;C:\DE\dagster\my-dagster-project\my_dagster_project\hooks\text_for_pictures.py&quot;, line 50, in _hook
context.resources.telegram.send_message(text)  # type: ignore</p>
",1,1671020795,python;telegram-bot;dagster,True,239,1,1671138271,https://stackoverflow.com/questions/74798282/create-success-hook-with-telegram-bot-alert
74770983,dagster: how do I get modified assets only? (external api),"<p>I'm learning dagster so maybe I haven't fully grasped the concepts.</p>
<p>my goal is to query an external web services / api and get the modified records only. I can make the external call a resource or put it into the asset directly, right? The external resource has filter option for last modified.</p>
<p>For me the core question is where and how to I pass in the value (in this case a date)? All records changed after this date (eg. the last run) should be fetched from the external api?</p>
<p>So were do I store this date and how do I pass it to the asset?</p>
",1,1670847462,python;dagster,False,228,1,1670951761,https://stackoverflow.com/questions/74770983/dagster-how-do-i-get-modified-assets-only-external-api
74778938,Dagster sensor to check for new records in a table,"<p>I have 2 tables where 2nd is dependent on 1st. Whenever new records are added in 1st, I want to run a dagster job. I came across sensors but I am not sure if my requirement can be fulfilled using the functionality they provide. Any ideas?</p>
",2,1670893273,python;data-pipeline;dagster,True,1458,1,1670950769,https://stackoverflow.com/questions/74778938/dagster-sensor-to-check-for-new-records-in-a-table
74442272,How to setup a success_hook to with send messages from telegram bot,"<p>I'm new one in Dagster. Could you help me, please? I want to understand how to set up an etl process error notification through a telegram bot</p>
<p>My code:</p>
<pre><code>import pygsheets
import os
import telegram
from dagster import resource
from typing import Callable, Optional
from dagster._core.definitions import success_hook
from dagster._core.execution.context.hook import HookContext
</code></pre>
<p><strong>create a resource:</strong></p>
<pre><code>@resource
def send_message(message):
    botid = os.environ['telegram_bot']
    chat = os.environ['telegram_chat']
    bot = telegram.Bot(token=botid)   
    bot.sendMessage(chat_id=chat, text=message, parse_mode='HTML')
</code></pre>
<p><strong>create ops:</strong></p>
<pre><code>@op (required_resource_keys={&quot;telegram&quot;})
def load_df(merged_df):
    # путь до credentials
    PATH = os.getenv(PATH_TO_FILE)
    gc = pygsheets.authorize(service_file=PATH)
    gc = gc.open_by_key(os.getenv('OPEN_BY_KEY'))
    sheet = gc.worksheet('title', 'Список для ОЦЕНКИ NEW')
    sheet.set_dataframe(merged_df, start=(1,1))

@op(required_resource_keys={'telegram'})
        def telegram_op(context):
            context.resources.telegram.send_message(text=':wave: hey there!')
</code></pre>
<p><strong>create a job:</strong></p>
<pre><code>@job (resource_defs={&quot;telegram&quot;: send_message}, hooks={telegram_on_success}, op_retry_policy = default_policy)
def job_text_for_pictures():
    load_df(merged_df)
</code></pre>
<p><strong>create a hook:</strong></p>
<pre><code>def _default_status_message(context: HookContext, status: str) -&gt; str:
    return &quot;Op {op_name} on job {pipeline_name} {status}!\nRun ID: {run_id}&quot;.format(
        op_name=context.op.name,
        pipeline_name=context.pipeline_name,
        run_id=context.run_id,
        status=status,
    )

def _default_success_message(context: HookContext) -&gt; str:
    return _default_status_message(context, status=&quot;succeeded&quot;)

def telegram_on_success(
    message_fn: Callable[[HookContext], str] = _default_success_message,
    dagit_base_url: Optional[str] = &quot;http://localhost:3000&quot;,
):

    @success_hook(required_resource_keys={&quot;telegram&quot;})
    def _hook(context: HookContext):
        text = message_fn(context)
        if dagit_base_url:
            text += &quot;\n&lt;{base_url}/instance/runs/{run_id}|View in Dagit&gt;&quot;.format(
                base_url=dagit_base_url, run_id=context.run_id
            )
        context.resources.telegram.send_message(text=text)  # type: ignore
    return _hook
</code></pre>
<p><strong>And after that I get an error:</strong> dagster._core.errors.DagsterSubprocessError: During multiprocess execution errors occurred in child processes:
In process 2984: dagster._core.errors.DagsterResourceFunctionError: Error executing resource_fn on ResourceDefinition telegram</p>
",0,1668499314,python;telegram-bot;dagster,False,285,1,1670871600,https://stackoverflow.com/questions/74442272/how-to-setup-a-success-hook-to-with-send-messages-from-telegram-bot
74715076,How to force some HTML to render inside some other website?,"<p>I am working with an open source ETL tool (dagster) that allows me to attach some &quot;metadata&quot; to the results of each operation.</p>
<p>Typically the metadata should be text or numbers, but I want to insert an HTML snippet.</p>
<p>My issue is that the HTML is not &quot;rendering&quot; in the tool's web-ui. Here is a quick screenshot of the UI and the source tree:</p>
<p><a href=""https://i.stack.imgur.com/4HHXs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4HHXs.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/nUGQq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nUGQq.png"" alt=""enter image description here"" /></a></p>
<p>Any ideas on how I can have it &quot;render&quot; in the UI?</p>
",0,1670408545,html;web-deployment;dagster,True,83,1,1670427225,https://stackoverflow.com/questions/74715076/how-to-force-some-html-to-render-inside-some-other-website
74688489,Dagster PySpark not running on EMR,"<p>I am trying to build an pipeline in Dagster which does the following:</p>
<ol>
<li><p>Launch an EMR cluster using the <a href=""https://docs.dagster.io/_modules/dagster_aws/emr/emr#EmrJobRunner"" rel=""nofollow noreferrer"">EmrJobRunner</a> class, by using its
run_job_flow function.</p>
</li>
<li><p>Add one or more steps to that cluster to process data in PySpark by
using the <a href=""https://docs.dagster.io/_modules/dagster_aws/emr/pyspark_step_launcher#emr_pyspark_step_launcher"" rel=""nofollow noreferrer"">emr_pyspark_step_launcher</a> resource.</p>
</li>
<li><p>Shut down the cluster once all steps are finished.</p>
</li>
</ol>
<p>I followed this <a href=""https://docs.dagster.io/integrations/spark#submitting-pyspark-ops-on-emr"" rel=""nofollow noreferrer"">tutorial</a> first, which assumes that you have an EMR cluster running and you hard code the EMR cluster ID as part of the Job specification. This way worked, as I could see my steps being run on EMR. However, when I try to automate the process I noticed that PySpark was running locally and not on EMR. I tried to wrap the emr_pyspark_step_launcher as a Resource which sets the cluster ID as part of the pipeline. The cluster ID can be obtained by using a function in the EmrJobRunner class which returns a cluster ID when providing a cluster name. I am trying to dynamically add the cluster ID during the job after launching the cluster but this isn't working as expected.</p>
<p>This is my code, any help would be appreciated.</p>
<pre><code>from pathlib import Path
from dagster_aws.emr import emr_pyspark_step_launcher
from dagster_aws.emr.emr import EmrJobRunner
from dagster_aws.s3 import s3_resource
from dagster_pyspark import pyspark_resource
from pyspark.sql import DataFrame
from transformations import execute_transformation
from dagster import IOManager, graph, io_manager, op, resource, In, Nothing, Out
from utils.configs import get_emr_cluster_config
import logging


class ParquetIOManager(IOManager):
    def _get_path(self, context):
        return &quot;/&quot;.join(
            [
                context.resource_config[&quot;path_prefix&quot;],
                context.run_id,
                context.step_key,
                context.name,
            ]
        )

    def handle_output(self, context, obj):
        if isinstance(obj, DataFrame):
            obj.write.parquet(self._get_path(context))
        # return obj

    def load_input(self, context):
        spark = context.resources.pyspark.spark_session
        return spark.read.parquet(self._get_path(context.upstream_output))


@io_manager(required_resource_keys={&quot;pyspark&quot;}, config_schema={&quot;path_prefix&quot;: str})
def parquet_io_manager():
    return ParquetIOManager()


@resource
def emr_job_runner(init_context):
    return EmrJobRunner(region=&quot;eu-central-1&quot;)


@resource(
    config_schema={&quot;cluster_name&quot;: str}, required_resource_keys={&quot;emr_job_runner&quot;}
)
def my_pyspark_step_launcher(init_context):
    cluster_id = init_context.resources.emr_job_runner.cluster_id_from_name(
        cluster_name=init_context.resource_config[&quot;cluster_name&quot;]
    )
    init_context.log.info(f&quot;CLUSTER ID during resource initilization: {cluster_id}&quot;)

    return emr_pyspark_step_launcher.configured(
        {
            &quot;cluster_id&quot;: cluster_id,
            &quot;local_pipeline_package_path&quot;: str(Path(__file__).parent.parent),
            &quot;deploy_local_pipeline_package&quot;: True,
            &quot;region_name&quot;: &quot;eu-central-1&quot;,
            &quot;staging_bucket&quot;: &quot;EMR_STAGING_BUCKET&quot;,
            &quot;wait_for_logs&quot;: True,
        }
    )
    

def launch_cluster(emr: EmrJobRunner, log: logging.Logger, emr_config: dict) -&gt; None:
    emr_config = get_emr_cluster_config(
        release_label=emr_config[&quot;emr_release_label&quot;],
        cluster_name=emr_config[&quot;cluster_name&quot;],
        master_node_instance_type=emr_config[&quot;master_node_instance_type&quot;],
        worker_node_instance_type=emr_config[&quot;worker_node_instance_type&quot;],
        worker_node_instance_count=emr_config[&quot;worker_node_instance_count&quot;],
        ec2_subnet_id=emr_config[&quot;ec2_subnet_id&quot;],
        bid_price=emr_config[&quot;worker_node_spot_bid_price&quot;],
    )

    return emr.run_job_flow(log=log, cluster_config=emr_config)


@op(
    config_schema={
        &quot;emr_release_label&quot;: str,
        &quot;cluster_name&quot;: str,
        &quot;master_node_instance_type&quot;: str,
        &quot;worker_node_instance_type&quot;: str,
        &quot;worker_node_instance_count&quot;: int,
        &quot;ec2_subnet_id&quot;: str,
        &quot;worker_node_spot_bid_price&quot;: str,
    },
    required_resource_keys={&quot;emr_job_runner&quot;},
    out=Out(Nothing),
)
def launch_emr_cluster(context) -&gt; None:
    op_config = context.op_config

    cluster_id = launch_cluster(
        emr=context.resources.emr_job_runner, log=context.log, emr_config=op_config
    )

    context.log.info(f&quot;CLUSTER ID: {cluster_id}&quot;)


@op(
    ins={&quot;start&quot;: In(Nothing)},
    required_resource_keys={&quot;pyspark&quot;, &quot;pyspark_step_launcher&quot;},
)
def get_dataframe(context) -&gt; DataFrame:
    return execute_transformation(spark_session=context.resources.pyspark.spark_session)


@graph
def make_and_filter_data():
    get_dataframe(launch_emr_cluster())


run_data_emr = make_and_filter_data.to_job(
    name=&quot;prod&quot;,
    resource_defs={
        &quot;pyspark_step_launcher&quot;: my_pyspark_step_launcher,
        &quot;pyspark&quot;: pyspark_resource,
        &quot;s3&quot;: s3_resource.configured({&quot;region_name&quot;: &quot;eu-central-1&quot;}),
        &quot;io_manager&quot;: parquet_io_manager.configured(
            {&quot;path_prefix&quot;: &quot;s3://EMR_STEP_OUTPUT&quot;}
        ),
        &quot;emr_job_runner&quot;: emr_job_runner,
    },
)
</code></pre>
",1,1670244681,python;pyspark;amazon-emr;dagster,False,319,1,1670256663,https://stackoverflow.com/questions/74688489/dagster-pyspark-not-running-on-emr
74615651,"In dagster, how do I load_asset_value from a job executed in process with mem_io_manager?","<p>For this question, consider I have a repository with one asset:</p>
<pre class=""lang-py prettyprint-override""><code>@asset
def my_int():
    return 1

@repository
def my_repo():
    return [my_int]
</code></pre>
<p>I want to execute it in process (with mem_io_manager), but I would like to retrieve the value returned by my_int from memory later. I can do that with fs_io_manager, for example, using <code>my_repo.load_asset_value('my_int')</code>, after it ran. But the same method with mem_io_manager raises <code>dagster._core.errors.DagsterInvariantViolationError: Attempting to access step_key, but it was not provided when constructing the OutputContext</code>.</p>
<p>Ideally, I would execute it in process and tell the executor to return me one (or more) of the assets, something like:</p>
<pre class=""lang-py prettyprint-override""><code>my_assets = my_repo.get_job('__ASSET_JOB').execute_in_process(return_assets=[my_int, ...])
</code></pre>
",1,1669732262,python;dagster,True,1000,3,1669835324,https://stackoverflow.com/questions/74615651/in-dagster-how-do-i-load-asset-value-from-a-job-executed-in-process-with-mem-io
74613973,Is it possible to transform one asset into another asset using ops in dagster?,"<p>From what I found <a href=""https://docs.dagster.io/concepts/assets/graph-backed-assets"" rel=""nofollow noreferrer"">here</a>, it is possible to use <code>op</code>s and <code>graph</code>s to generate <code>asset</code>s.</p>
<p>However, I would like to use an <code>asset</code> as an input for an <code>op</code>. I am exploring it for a following use case:</p>
<ol>
<li>I fetch a list of country metadata from an external API and store it in my resource:</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>@dagster.asset
def country_metadata_asset() -&gt; List[Dict]:
    ...
</code></pre>
<ol start=""2"">
<li>I use this asset to define some downstream assets, for example:</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>@dagster.asset
def country_names_asset(country_metadata_asset) -&gt; List[str]:
    ...
</code></pre>
<ol start=""3"">
<li>I would like to use this asset to call another data source to retrieve and validate data and then write it to my resource. It returns a huge amount of rows. That is why I need to do it somehow in batch, and I thought that <code>graph</code> with <code>op</code>s would be a better choice for it. I thought to do something like this:</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>@dagster.op(out=dagster.DynamicOut())
def load_country_names(country_names_asset):
    for country_index, country_name in enumerate(country_names_asset):
        yield dagster.DynamicOutput(
            country_name, mapping_key=f&quot;{country_index} {country_name}&quot;
        )

@dagster.graph()
def update_data_graph():
    country_names = load_country_names()
    country_names.map(retrieve_and_process_data)


@dagster.job()
def run_update_job():
    update_data_graph()
</code></pre>
<p>It seems that my approach does not work, and I am not sure if it is conceptually correct. My questions are:</p>
<ol>
<li><p>How to tell dagster that the input for <code>load_country_names</code> is an asset? Should I manually materialise it inside op?</p>
</li>
<li><p>How to efficiently write augmented data that I return from <code>retrieve_and_process_data</code> into my resource? It is not possible to keep data in memory. So I thought to implement it somehow using a custom <code>IOManager</code>, but I am not sure how to do it.</p>
</li>
</ol>
",0,1669724340,python;dagster,True,1323,1,1669743436,https://stackoverflow.com/questions/74613973/is-it-possible-to-transform-one-asset-into-another-asset-using-ops-in-dagster
74615031,How do quickly switch dagster from running with test data and the full data?,"<p>I have a Dagster project where I like to test it with a small subset of data (eg. I run it daily with a list of 10,000 users). This is helpful at catching issues before kicking of a &quot;big&quot; job.</p>
<p>Currently I have a software defined asset called <code>userIds</code> which I modify between runs:</p>
<pre class=""lang-py prettyprint-override""><code>@asset
def userIds():
    userIds = get_user_ids_from_database()
    # return userIds
    return userIds[:10_000]
</code></pre>
<p>But this is probably not how it should be done.</p>
<p>What is the best way to switch the config of an asset in Dagster?</p>
",1,1669729302,dagster,True,183,1,1669739194,https://stackoverflow.com/questions/74615031/how-do-quickly-switch-dagster-from-running-with-test-data-and-the-full-data
74563279,how to specify where to materialize dagster assets?,"<p>I'm new to dagster.</p>
<p>Currently when I materialize assets they end up in <code>/my-dagster-project/tmpav872908/storage/{assetkey}</code></p>
<p>How do I specify where the assest should be stored?</p>
",2,1669304803,dagster,True,917,1,1669317079,https://stackoverflow.com/questions/74563279/how-to-specify-where-to-materialize-dagster-assets
74534627,Dagster - running a cross repository job from a sensor,"<p>I've learned from the docs that a Dagster sensor could be triggered by a job from a different repo. In a similar manner, is there a way to run a cross repo job using the RunRequest inside the sensor, ie. something like this?</p>
<pre><code>@run_status_sensor(
    run_status=DagsterRunStatus.SUCCESS,
    request_job=&lt;job_from_a_different_repo&gt;,
)
def my_sensor(context):
  return RunRequest(...)
</code></pre>
",0,1669128944,dagster,True,584,1,1669133523,https://stackoverflow.com/questions/74534627/dagster-running-a-cross-repository-job-from-a-sensor
74408432,Automatically create jobs in Dagster,"<p>I'm new to Dagster and I'm getting beat up too much to get my scenario to work.</p>
<p>I would like to create jobs dynamically, and I've been to the Dagster documentation but I couldn't reproduce it at all.</p>
<p>My scenario is as follows:</p>
<p>I have 3 functions in python (@op), the first being where I fetch data from an EP. In the second function I process this data, and in the third function I receive a name of &quot;client&quot; as a parameter and with the processed result of the second function, I save it in that client's database. Example:</p>
<pre><code>
@op
def step1():
    return {&quot;a&quot; : 1, &quot;b:&quot; 2, &quot;c: 3&quot;}

@op
def step2(result):
     //process exemple
    return {&quot;a&quot; : 4, &quot;b&quot; : 5, &quot;c&quot; : 6}

@op
def step3(result,client):
  //update in database client


@graph
def process():
    response = step1()
    result_process = step2(response)
    step3(result_process, &quot;name_client&quot;)

@job
def job_examplo():
   process()

</code></pre>
<p>the problem is that I have a list of customers, and I didn't want to create multiple @op by passing a different customer name. I tried to do something like:</p>
<pre><code>clients = [
    &quot;example_microsoft&quot;,
    &quot;example_apple&quot;,
    &quot;example_amazon&quot;,
    ...
    ...
    &quot;examplo_skynet&quot;

]

@graph
def process():
    response = step1()
    result_process = step2(response)
    
    for client_name in clients:
         step3(result_process, &quot;client_name&quot;)

</code></pre>
<p>But when I do this approach, I get an error. Is there any way at this stage I can generate this step 3 dynamically?</p>
<p>Additional Information: I am using DockerRunLauncher</p>
<p>I already tried to use the outputs, but without success. <a href=""https://docs.dagster.io/concepts/ops-jobs-graphs/dynamic-graphs"" rel=""nofollow noreferrer"">https://docs.dagster.io/concepts/ops-jobs-graphs/dynamic-graphs</a></p>
",1,1668203032,jobs;dagster,True,990,1,1668276541,https://stackoverflow.com/questions/74408432/automatically-create-jobs-in-dagster
72948123,Dagster-image cli,"<p>recreate the image deployment describe here:
<a href=""https://github.com/dagster-io/dagster/tree/master/python_modules/automation/automation/docker/images"" rel=""nofollow noreferrer"">https://github.com/dagster-io/dagster/tree/master/python_modules/automation/automation/docker/images</a>
How do I install the dagster-image  cli?
<code>dagster-image build-all --name &lt;YOUR IMAGE&gt;</code>
I tried running pip install -e . from setup.py folder <a href=""https://github.com/dagster-io/dagster/blob/master/python_modules/automation/setup.py"" rel=""nofollow noreferrer"">https://github.com/dagster-io/dagster/blob/master/python_modules/automation/setup.py</a>
to install the command dagster-image , but i get an error below:
<code>[Errno 2] No such file or directory: b'/sbin/dagster-image'</code></p>
",1,1657608648,python;setuptools;setup.py;dagster,True,345,1,1666180359,https://stackoverflow.com/questions/72948123/dagster-image-cli
74058128,How to use repository.load_asset_value from within a running op/asset?,"<p>TL;DR: I am trying to build an asset that collects and returns the outputs of multiple other assets, which may vary in number given some parameters, but are all defined at build time.</p>
<p>I tried using the <a href=""https://github.com/dagster-io/dagster/commit/8752ee7e71b4f1f0fe08309e4fff8e7d2c92253c"" rel=""nofollow noreferrer"">repo.load_asset_value</a>, but I got <code>KeyError: AssetKey(['for_asset_test1'])</code>, even though the asset exists in the repository.</p>
<p>Example of stand alone code reproducing the issue.</p>
<pre><code># temp.py
from dagster import asset, repository, job, define_asset_job
import py_compile


from dagster._core.definitions.reconstruct import repository_def_from_target_def

# Create assets in a for loop
name_suffixes = [f&quot;{x}&quot; for x in [1, 5, 10]]
for_asset_name = &quot;for_asset_test&quot;


def my_asset_factory(
    suffix: str,
    name=for_asset_name,
):
    &quot;&quot;&quot;
    Args:
        name (str): The name of the new asset.

    Returns:
        function: The new op.
    &quot;&quot;&quot;

    @asset(name=f&quot;{name}{suffix}&quot;)
    def for_asset():
        return suffix

    return for_asset


for_assets = [my_asset_factory(suffix) for suffix in name_suffixes]

# Receiving all all assets in for loop
all_for_assets_created = set([f&quot;{for_asset_name}{suffix}&quot; for suffix in name_suffixes])


@asset(non_argument_deps=all_for_assets_created)
def handle_for_assets(context):
    &quot;&quot;&quot;Collect all non_argument_deps outputs into a list and returns the list&quot;&quot;&quot;

    all_input_keys = context.op.input_dict.keys()
    cur_repo = repository_def_from_target_def(context.pipeline_def)
    with cur_repo.get_asset_value_loader() as loader:
        assets_results = [
            loader.load_asset_value(asset_key) for asset_key in all_input_keys
        ]

    return assets_results


all_assets_job = define_asset_job(name=&quot;all_assets_job&quot;)


@repository
def repos():
    return [handle_for_assets, all_assets_job] + for_assets


# Execute with:
# dagster job execute -f src/mock_model/temp.py --job all_assets_job
</code></pre>
<p>I am trying it this way because, since <code>name_suffixes</code> could be changed at will, it would normally require that each of them would be written as arguments for <code>handle_for_assets</code>, which is rather unpractical. Please let me know if there is a better way of achieving the desired result</p>
",1,1665673976,python;dagster,True,176,1,1665685987,https://stackoverflow.com/questions/74058128/how-to-use-repository-load-asset-value-from-within-a-running-op-asset
73993550,How to materialize a downstream asset independently in dagster using python API?,"<p>I have dagster==1.0.11 and I am trying to materialize a downstream_asset independently from its upstream_asset. I want to do it through the python API. Please consider the code below for reference.</p>
<pre><code># example.py
from dagster import asset, materialize, repository


@asset
def upstream_asset():
    return [1, 2, 3]


@asset
def downstream_asset(upstream_asset):
    return upstream_asset + [4]


@repository
def repo():
    return [upstream_asset, downstream_asset]


if __name__ == &quot;__main__&quot;:
    materialize([downstream_asset])
</code></pre>
<p>When I call <code>python example.py</code>, it fails with:</p>
<blockquote>
<p>dagster._core.errors.DagsterInvalidDefinitionError: Input asset '[&quot;upstream_asset&quot;]' for asset '[&quot;downstream_asset&quot;]' is not produced by any of the provided asset ops and is not one of the provided sources</p>
</blockquote>
<p>I would like to achieve the same behavior I get from the dagit UI.</p>
<p>In dagit (UI), if I select <code>downstream_asset</code> and click &quot;Materialize selected&quot;, I'll get</p>
<blockquote>
<p>FileNotFoundError: [Errno 2] No such file or directory: '/Users/pedro.viana/dev/nu/mock-model/dagster_home/storage/upstream_asset'
(full exception in DETAILS section, below)</p>
</blockquote>
<p>But, if I first select <code>upstream_asset</code>, click materialize (it succeeds), delete everything in the DAGSTER_HOME directory but the <em>storage/upstream</em> file (erasing all metadata, but effectively leaving the persisted result of upstream asset there) AND THEN launch dagit again, select downstream_asset and click &quot;Materialize selected&quot;, it will succeed. It has no metadata about the <code>upstream_asset</code>, it simply checks if the file is where <code>upstream_asset</code>'s io_manager tells it should be. It is there and <code>downstream_asset</code>'s materialization succeeds.</p>
<p>How to achieve this behavior with the python API? I would like to call <code>materialize([downstream_asset])</code> and have it succeed IF it finds the upstream persisted result where it should be.</p>
<p>CONTEXT: I can have multiple instances of dagster running in multiple environments, but all with the same code version (and an s3_io_manager). So, if someone materializes an upstream_asset, it is available to all others running the downstream_asset, even if they do not know who and when the upstream was materialized.</p>
<p><strong>Details</strong></p>
<p>Full exception:</p>
<pre><code>dagster._core.errors.DagsterExecutionLoadInputError: Error occurred while loading input &quot;upstream_asset&quot; of step &quot;downstream_asset&quot;:
  File &quot;/Users/pedro.viana/miniforge3/envs/dagtest/lib/python3.6/site-packages/dagster/_core/execution/plan/execute_plan.py&quot;, line 224, in dagster_event_sequence_for_step
    for step_event in check.generator(step_events):
  File &quot;/Users/pedro.viana/miniforge3/envs/dagtest/lib/python3.6/site-packages/dagster/_core/execution/plan/execute_step.py&quot;, line 320, in core_dagster_event_sequence_for_step
    step_input.source.load_input_object(step_context, input_def)
  File &quot;/Users/pedro.viana/miniforge3/envs/dagtest/lib/python3.6/site-packages/dagster/_core/execution/plan/inputs.py&quot;, line 201, in load_input_object
    yield from _load_input_with_input_manager(loader, load_input_context)
  File &quot;/Users/pedro.viana/miniforge3/envs/dagtest/lib/python3.6/site-packages/dagster/_core/execution/plan/inputs.py&quot;, line 867, in _load_input_with_input_manager
    value = input_manager.load_input(context)
  File &quot;/Users/pedro.viana/miniforge3/envs/dagtest/lib/python3.6/contextlib.py&quot;, line 99, in __exit__
    self.gen.throw(type, value, traceback)
  File &quot;/Users/pedro.viana/miniforge3/envs/dagtest/lib/python3.6/site-packages/dagster/_core/execution/plan/utils.py&quot;, line 82, in solid_execution_error_boundary
    ) from e
The above exception was caused by the following exception:
FileNotFoundError: [Errno 2] No such file or directory: '/Users/pedro.viana/dev/nu/mock-model/dagster_home/storage/upstream_asset'
  File &quot;/Users/pedro.viana/miniforge3/envs/dagtest/lib/python3.6/site-packages/dagster/_core/execution/plan/utils.py&quot;, line 47, in solid_execution_error_boundary
    yield
  File &quot;/Users/pedro.viana/miniforge3/envs/dagtest/lib/python3.6/site-packages/dagster/_core/execution/plan/inputs.py&quot;, line 867, in _load_input_with_input_manager
    value = input_manager.load_input(context)
  File &quot;/Users/pedro.viana/miniforge3/envs/dagtest/lib/python3.6/site-packages/dagster/_core/storage/fs_io_manager.py&quot;, line 181, in load_input
    with open(filepath, self.read_mode) as read_obj:
    ```
</code></pre>
",1,1665188283,python;dagster,True,1373,2,1665434302,https://stackoverflow.com/questions/73993550/how-to-materialize-a-downstream-asset-independently-in-dagster-using-python-api
73983827,How can I do an incremental load based on record ID in Dagster,"<p>I am trying to consume an HTTP API in my Dagster code. The API provides a log of &quot;changes&quot; which contain an incrementing ID. It supports an optional parameter <code>fromUpdateId</code>, which lets you only fetch updates that have a higher ID than some value.</p>
<p>This should let me do incremental loads, by looking at the highest update ID I have seen so far, and providing this as the parameter.</p>
<p>How can I accomplish this in Dagster? I am thinking it should be possible to write the highest ID as metadata when I materialize the asset. The metadata would then be available the next time the asset is materialized.</p>
",1,1665128060,data-ingestion;dagster,True,575,1,1665431468,https://stackoverflow.com/questions/73983827/how-can-i-do-an-incremental-load-based-on-record-id-in-dagster
73703219,Log hyperlinks in Dagit?,"<br>
Hi,
<br><br>
<p>I'd like to be able to show hyperlinks in Dagit logs, so that if a process fails then the link can be clicked in the log to open the offending file directly.</p>
<p>I've tried just entering path strings and html, but neither of these approaches work.</p>
<p>According to the Dagster people, it apparently is possible and they do give an example, but I don't understand how to implement this.</p>
<p>If anybody could please help me then that would be amazing!</p>
<p>Thanks,</p>
<p>Phil.
<br><br></p>
<p>Question about adding hyperlinks to logs:<br>
<a href=""https://github.com/dagster-io/dagster/issues/7606"" rel=""nofollow noreferrer"">https://github.com/dagster-io/dagster/issues/7606</a><br>
Code linked as answer by Dagster guy:<br>
<a href=""https://docs.dagster.io/_apidocs/ops#dagster.MetadataValue.url"" rel=""nofollow noreferrer"">https://docs.dagster.io/_apidocs/ops#dagster.MetadataValue.url</a></p>
<p>My code is from the example here:<br>
<a href=""https://medium.com/@stefan-samba/dagstermill-passing-notebook-results-downstream-in-3-steps-58c22d360f89"" rel=""nofollow noreferrer"">https://medium.com/@stefan-samba/dagstermill-passing-notebook-results-downstream-in-3-steps-58c22d360f89</a></p>
<p>The code below runs the job:</p>
<pre><code>import dagstermill as dm
from dagster import In, Out, job
from dagster.utils import script_relative_path

load_data = dm.define_dagstermill_op(
    'load_data',                                                                # Name of the op:
    notebook_path=script_relative_path('load_data.ipynb'),                      # Path to the notebook
    outs={'numbers': Out(list, description='list of numbers')}                  # The ops outputs
)

read_data = dm.define_dagstermill_op(
    'read_data',
    notebook_path=script_relative_path('read_data.ipynb'),
    ins={'numbers': In(list, description='list of numbers obtained from\
                                                        load_data.ipynb')}
)

@job(
    resource_defs={
        'output_notebook_io_manager': dm.local_output_notebook_io_manager,
    }
)

def main_job():
    numbers = load_data()
    read_data(numbers)
</code></pre>
<p>The following two notebooks are the load_data and read_data steps:</p>
<p>Load data:</p>
<pre><code>import dagstermill as dm
result = [1, 2, 3, 4]
# Log result to UI logs
dm.get_context().log.info(f'Step 1 - load_data - results: {result}')
dm.yield_result(result, 'numbers')
</code></pre>
<p>Read data:</p>
<pre><code>import dagstermill as dm
dm.get_context().log.info(f'Step 2 - read_data - loaded numbers: {numbers}')
</code></pre>
",2,1663072734,python;logging;hyperlink;airflow;dagster,False,151,0,1663072734,https://stackoverflow.com/questions/73703219/log-hyperlinks-in-dagit
73601292,How to specify/use idempotent &quot;date of execution&quot; within dagster assets/jobs?,"<p>Coming from airflow, I used jinja templates such as <code>{{ds_nodash}}</code> to translate the date of execution of a dag within my scripts.</p>
<p>For example, I am able to detect and ingest a file at the first of August 2022 if it is in the format : <code>FILE_20220801.csv</code>. I would have a dag with a sensor and an operator that uses <code>FILE_{{ds_nodash}}.csv</code> within its code. In other terms I was sure my dag was idempotent in regards to its execution date.</p>
<p>I am now looking into dagster because of the <strong>assets</strong> abstraction that is quite attractive. Also, dagster is easy to set-up and test locally. But I cannot find similar <strong>jinja</strong> templates that can ensure the idempotency of my executions.</p>
<p>In other words, how do I make sure data that was sent to me during a specific date is going to be processed the same way even if I run it 1, 2 or N days later?</p>
",2,1662311023,dagster,True,584,1,1662477779,https://stackoverflow.com/questions/73601292/how-to-specify-use-idempotent-date-of-execution-within-dagster-assets-jobs
73599189,Calling localhost:3000 on the browser after starting dagit returns ERR_CONNECTION_REFUSED,"<p>I have just started trying out dagster, I am already stumped at the first steps.</p>
<p>In <a href=""https://docs.dagster.io/getting-started/create-new-project"" rel=""nofollow noreferrer"">the guide</a> they specify:</p>
<blockquote>
<p>Then, start the Dagit web server:</p>
<p>dagit</p>
<p>Open http://localhost:3000 with your browser to see the project.</p>
<p>Now, you can start writing assets in my_dagster_project/assets/, define your own ops or jobs and include them in my_dagster_project/repository.py.</p>
</blockquote>
<p>Alright, I started dagit but I get <code>ERR_CONNECTION_REFUSED</code> when I try to open it via the browser. Any ideas?</p>
<p>FYI, I am using WSL2.</p>
",0,1662292744,wsl-2;dagster,True,654,1,1662294379,https://stackoverflow.com/questions/73599189/calling-localhost3000-on-the-browser-after-starting-dagit-returns-err-connectio
73560585,Run a new task in Airflow Kubernetes with it&#39;s own isolated Dockerfile?,"<p>How do I run a new pipeline with it's own isolated Dockerfile in Airflow Kubernetes ?</p>
<p>I've been using Dagster and I can run new pipelines on their own Dockerfile, but can't figure out how to do this in Airflow</p>
",2,1661971614,docker;kubernetes;airflow;dagster,True,418,1,1661977496,https://stackoverflow.com/questions/73560585/run-a-new-task-in-airflow-kubernetes-with-its-own-isolated-dockerfile
73034166,DPI-1047: Cannot locate a 64-bit Oracle Client library: &quot;failed to get message for Windows Error 126&quot;,"<p>I am working with dagster and dbt. I have a test repo that I use both on a Debian VM and on my Windows PC. On Debian everything works fine, here on Windows I receive the following Oracle error and can't fix it.</p>
<pre><code>ERROR  oracle adapter: Got an error when attempting to open an Oracle connection:
'DPI-1047: Cannot locate a 64-bit Oracle Client library:
&quot;failed to get message for Windows Error 126&quot;.
See https://cx-oracle.readthedocs.io/en/latest/user_guide/installation.html for help'

INFO  oracle adapter: Rolling back transaction.
</code></pre>
<p>I am receiving this error even after:</p>
<ul>
<li>Downloading Oracle Instant client and unzipping it in my Oracle folder <em>(C:\Program Files (x86)\Oracle\instantclient_21_6)</em></li>
<li>Setting the ORACLE_HOME and PATH.</li>
<li>Installing and importing cx_Oracle.</li>
<li>Initializing cx_Oracle.</li>
</ul>
",0,1658221606,oracle;cx-oracle;dbt;dagster,False,892,0,1658221606,https://stackoverflow.com/questions/73034166/dpi-1047-cannot-locate-a-64-bit-oracle-client-library-failed-to-get-message-f
67273725,Docstrings for Dagster wrapped functions not appearing in Sphinx,"<p>I've got a project written in python with use of pypspark and now dagster.  We are using Sphinx to build the documentation and napoleon to parse the Google style docstrings.  We've started including prewrapped dagster solids like the following:</p>
<pre><code>@solid(
    config_schema={
        &quot;join_key&quot;: String,
        &quot;join_style&quot;: String,
        &quot;df1_name&quot;: String,
        &quot;df2_name&quot;: String,
    }
)
def join_two_dfs_solid(
    context, df1: SparkDataFrame, df2: SparkDataFrame
) -&gt; SparkDataFrame:
    &quot;&quot;&quot;
    Solid to join two DataFrames on the sepcified key.

    Args:
        context (dict): Dagster Context Dict
        df1 (SparkDataFrame): Spark DataFrame with the same schema
        df2 (SparkDataFrame): Spark DataFrame with the same schema

    Config Parameters:
        join_key (str): name of column to join on.  Specified column must exist in both columns.
        join_style (str): spark join style, e.g., &quot;left&quot;, &quot;inner&quot;, &quot;outer&quot;, etc.; default is &quot;inner&quot;
        df1_name (str): alias name for the first dataframe.
        df2_name (str): alias name for the second dataframe.

    Returns:
        DataFrame
    &quot;&quot;&quot;
    key = context.solid_config[&quot;join_key&quot;]
    join_style = context.solid_config.get(&quot;join_style&quot;, &quot;inner&quot;)
    df1_name = context.solid_config[&quot;df1_name&quot;]
    df2_name = context.solid_config[&quot;df2_name&quot;]

    context.log.info(f&quot;Running join of two dataframes on {key}&quot;)
    check_required_columns(df1, [key])
    check_required_columns(df2, [key])

    output = df1.alias(df1_name).join(
        df2.alias(df2_name),
        sf.col(f&quot;{df1_name}.{key}&quot;) == sf.col(f&quot;{df2_name}.{key}&quot;),
        how=join_style,
    )
    return output
</code></pre>
<p>When we go to build with sphinx-apidoc, I can see that docstring for the function exists by checking <code>join_two_dfs_solid.__doc__</code> and the dagster attached <code>join_two_dfs_solid._description</code> field is empty, which should mean that it uses the docstring.  However, when the sphinx docs build, I get a blank .rst file for the module containing this solid.  Does anyone know if there are any other configuration settings in sphinx or the solid that I need to change to get this to build correctly?</p>
",1,1619470684,python;python-sphinx;dagster,True,223,2,1655583844,https://stackoverflow.com/questions/67273725/docstrings-for-dagster-wrapped-functions-not-appearing-in-sphinx
72638268,Add param to dagster input,"<p>Hello I have the following @job in dagster</p>
<pre><code>@job
def job_extract_faces():
    faces = op_extract_data(op_get_data_path())
    r = op_process((faces, 'a'))
    r = op_process((faces, 'b'))
    r = op_process((faces, 'c'))
    r = op_process((faces, 'd'))
</code></pre>
<p>the proble is that dagster says that the input of the op_process should be the output of op_extrac_data</p>
<p>is there anyway of add a parameter instead of build 4 functions?</p>
<p>thhan</p>
",1,1655330117,python;dagster,True,733,1,1655396630,https://stackoverflow.com/questions/72638268/add-param-to-dagster-input
71576453,"Monitor/alert for orchesration in Dagster/Python, how to record spans?","<p>I'm writing a data processing pipeline orchestrated in Dagster, and I'd like to add monitoring/alerting.</p>
<p>To simplify a use case, we process a few thousand small pieces of data, and each one could be processed by one of 4-5 different major workflows. I'd like to track the time it takes each piece of data to get fully processed and alert if any one takes &gt; 1h. And I'd like to track how many pieces of data each workflow process and alert if any is too far from its normal value.</p>
<p>The challenge I'm coming up against is that <a href=""https://opentelemetry.io/docs/instrumentation/python/manual/"" rel=""nofollow noreferrer"">OpenTelemetry expects spans be identified with context managers</a>:</p>
<pre class=""lang-py prettyprint-override""><code>with tracer.start_as_current_span(&quot;span-name&quot;) as span:
    # do some work
</code></pre>
<p>However, my pipeline work is broken up into multiple Python functions, and the Dagster orchestration framework ties them together. In production the <code>@op</code>s will be run on separate Kubernetes nodes. <a href=""https://docs.dagster.io/concepts/ops-jobs-graphs/jobs-graphs#dynamic-mapping--collect"" rel=""nofollow noreferrer"">Here's an example</a>:</p>
<pre class=""lang-py prettyprint-override""><code>@op(
   out=DynamicOut(str),
)
def find_small_data_sets(context):
    &quot;&quot;&quot;Starts 1 dataset going through the pipeline.&quot;&quot;&quot;
    datasets = db.list_some_things()
    for dataset in datasets:
        yield DynamicOutput(value=data)


@op
def process_data_part_one(data: str) -&gt; str:
    pass # Do some work on one of the data sets.


@op
def process_data_part_two(data: str) -&gt; int:
    # Do more work on a data set.
    # conceptually would be part of the same span
    # as process_data_part_one


@op
def workflow_done(outputs: List[int]) -&gt; int:
    # Finish up the workflow. Here is where a workflow-level
    # span might end.
    return sum(sizes)


@job
def do_full_orchestrated_job():
    &quot;&quot;&quot;This function defines the DAG structure.

    It does not perform the actual runtime execution of my job
    when it gets called.
    &quot;&quot;&quot;
    datasets = find_small_data_sets()
    processed_datasets = (
        datasets
        .map(process_data_part_one)
        .map(process_data_part_two)
    )
    workflow_done(processed_datasets.collect())
</code></pre>
<p>Given I don't have access to instrument the Dagster orchestration framework itself, is there a way I can use OpenTelemetry to monitor my pipeline? Would starting and ending spans in different functions (without a context manager) be possible, especially if the start and end are actually running on different CPUs? Or is there a better tool for this kind of monitoring/alerting?</p>
<p>Thanks for reading!</p>
",1,1647970033,python;orchestration;open-telemetry;dagster;observability,True,655,2,1654048015,https://stackoverflow.com/questions/71576453/monitor-alert-for-orchesration-in-dagster-python-how-to-record-spans
72406557,Dagster Graphql get all jobs in a Repo,"<p>I am looking to use the dagster Graphql as a documented <a href=""https://docs.dagster.io/concepts/dagit/graphql"" rel=""nofollow noreferrer"">here</a></p>
<p>I want to get all jobs in a repo, I am using the &quot;Get a list of jobs within a repository&quot; query outlined in the above documentation</p>
<p>And Get the following error</p>
<pre><code>{
  &quot;error&quot;: {
    &quot;data&quot;: null,
    &quot;errors&quot;: [
      {
        &quot;message&quot;: &quot;Field \&quot;repositoryOrError\&quot; of type \&quot;RepositoryOrError!\&quot; must have a sub selection.&quot;,
        &quot;locations&quot;: [
          {
            &quot;line&quot;: 2,
            &quot;column&quot;: 3
          }
        ]
      },
      {
        &quot;message&quot;: &quot;Argument \&quot;repositorySelector\&quot; has invalid value {repositoryLocationName: repositoryLocationName, repositoryName: repositoryName}.\nIn field \&quot;repositoryName\&quot;: Expected type \&quot;String\&quot;, found repositoryName.\nIn field \&quot;repositoryLocationName\&quot;: Expected type \&quot;String\&quot;, found repositoryLocationName.&quot;,
        &quot;locations&quot;: [
          {
            &quot;line&quot;: 2,
            &quot;column&quot;: 41
          }
        ]
      },
      {
        &quot;message&quot;: &quot;Variable \&quot;$repositoryLocationName\&quot; is never used in operation \&quot;JobsQuery\&quot;.&quot;,
        &quot;locations&quot;: [
          {
            &quot;line&quot;: 1,
            &quot;column&quot;: 17
          }
        ]
      },
      {
        &quot;message&quot;: &quot;Variable \&quot;$repositoryName\&quot; is never used in operation \&quot;JobsQuery\&quot;.&quot;,
        &quot;locations&quot;: [
          {
            &quot;line&quot;: 1,
            &quot;column&quot;: 51
          }
        ]
      }
    ]
  }
}
</code></pre>
<p>I have tried this in both python and the GraphQL Playground</p>
<p>Does anyone have an idea where I might be going wrong?</p>
<p>edit:</p>
<p>Adding the python code that gives this error:</p>
<pre><code>query1 = &quot;&quot;&quot;query JobsQuery(
  $repositoryLocationName:  String!,
  $repositoryName: String!
) {
  repositoryOrError(
    repositorySelector: {
      repositoryLocationName: repositoryLocationName,
      repositoryName: repositoryName
    }
  ) {
    ... on Repository {
      jobs {
        name
      }
    }
  }
}&quot;&quot;&quot;

variables  = {&quot;repositoryLocationName&quot;: &quot;eliaapifetcher&quot;, &quot;repositoryName&quot;: &quot;elia_api_repo&quot;}

url = 'http://localhost:4200/dagster/graphql'

r1 = requests.post(url, json={'query': query1, 'variables': variables})
</code></pre>
",0,1653659850,graphql;dagster,False,520,1,1654025032,https://stackoverflow.com/questions/72406557/dagster-graphql-get-all-jobs-in-a-repo
66601150,Dagster loop over solid&#39;s output and concurrent processing,"<p>I have a Dagster pipeline consisting of two solids (reproducible example below). The first (<code>return_some_list</code>) outputs a list of some objects. The second solid (<code>print_num</code>) accepts an element from the first list (not the full list) and does some processing on that element.</p>
<p>How am I supposed to call the second solid for each element of the list returned by the first solid? Please explain any bests practices as well.</p>
<p>Not sure if this is the best approach (let me know), but I'd like to produce a different solid instance of <code>print_num</code> for each element of the first solid's output. This will help me parallelize the solid in the future and handle long / compute-intensive solids better.</p>
<pre><code>from dagster import execute_pipeline, pipeline, solid

@solid
def return_some_list(context):
    return [1,2,3,4,5]

@solid
def print_num(context, some_num: int):
    print(some_num)
    return some_num


@pipeline
def some_pipeline():
    output_list = return_some_list()
    for some_num in output_list:
        print_num(some_num)

if __name__ == &quot;__main__&quot;:
    result = execute_pipeline(some_pipeline)
</code></pre>
",5,1615557013,python;concurrency;pipeline;dagster,True,3014,2,1653577943,https://stackoverflow.com/questions/66601150/dagster-loop-over-solids-output-and-concurrent-processing
72173805,Submit dagster run programmatically (without dagit graphQL),"<p>Is there a way I can submit dagster run programmatically without using dagit graphQL?</p>
<pre><code>    dagster_instance = DagsterInstance.get()
    dagster_instance.submit_run(
        pipeline_run.run_id,
        workspace=graphene_info.context
    )
</code></pre>
<p>Basically I want to use above code to submit a new run, but I am unable to figure out how to get the pipeline_run and workspace. Also, I am not looking to use Dagit GraphQL APIs.</p>
<p>If there is any other way as well, please suggest.</p>
<p>Appreciate your time and help.</p>
",0,1652107171,dagster,False,892,1,1652947191,https://stackoverflow.com/questions/72173805/submit-dagster-run-programmatically-without-dagit-graphql
72118156,Dagster cannot connect to mongodb locally,"<p>I was going through Dagster tutorials and thought it be a good exercise to connect to my local mongodb.</p>
<pre class=""lang-py prettyprint-override""><code>from dagster import get_dagster_logger, job, op
from pymongo import MongoClient

@op
def connection():
    client = MongoClient(&quot;mongodb://localhost:27017/&quot;)
    return client[&quot;development&quot;]

@job
def execute():
    client = connection()
    get_dagster_logger().info(f&quot;Connection: {client} &quot;)
</code></pre>
<p>Dagster error:</p>
<pre><code>dagster.core.errors.DagsterExecutionHandleOutputError: Error occurred while handling output &quot;result&quot; of step &quot;connection&quot;:
  File &quot;/usr/local/lib/python3.9/site-packages/dagster/core/execution/plan/execute_plan.py&quot;, line 232, in dagster_event_sequence_for_step
    for step_event in check.generator(step_events):
  File &quot;/usr/local/lib/python3.9/site-packages/dagster/core/execution/plan/execute_step.py&quot;, line 348, in core_dagster_event_sequence_for_step
    for evt in _type_check_and_store_output(step_context, user_event, input_lineage):
  File &quot;/usr/local/lib/python3.9/site-packages/dagster/core/execution/plan/execute_step.py&quot;, line 405, in _type_check_and_store_output
    for evt in _store_output(step_context, step_output_handle, output, input_lineage):
  File &quot;/usr/local/lib/python3.9/site-packages/dagster/core/execution/plan/execute_step.py&quot;, line 534, in _store_output
    for elt in iterate_with_context(
  File &quot;/usr/local/lib/python3.9/site-packages/dagster/utils/__init__.py&quot;, line 400, in iterate_with_context
    return
  File &quot;/usr/local/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py&quot;, line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File &quot;/usr/local/lib/python3.9/site-packages/dagster/core/execution/plan/utils.py&quot;, line 73, in solid_execution_error_boundary
    raise error_cls(
The above exception was caused by the following exception:
TypeError: cannot pickle '_thread.lock' object
  File &quot;/usr/local/lib/python3.9/site-packages/dagster/core/execution/plan/utils.py&quot;, line 47, in solid_execution_error_boundary
    yield
  File &quot;/usr/local/lib/python3.9/site-packages/dagster/utils/__init__.py&quot;, line 398, in iterate_with_context
    next_output = next(iterator)
  File &quot;/usr/local/lib/python3.9/site-packages/dagster/core/execution/plan/execute_step.py&quot;, line 524, in _gen_fn
    gen_output = output_manager.handle_output(output_context, output.value)
  File &quot;/usr/local/lib/python3.9/site-packages/dagster/core/storage/fs_io_manager.py&quot;, line 124, in handle_output
    pickle.dump(obj, write_obj, PICKLE_PROTOCOL)

</code></pre>
<p>I have tested this locally in a ipython and it works so the issue is related to dagster.</p>
",2,1651690730,python;mongodb;etl;dagster,True,628,1,1651692743,https://stackoverflow.com/questions/72118156/dagster-cannot-connect-to-mongodb-locally
72031909,Handling user input in dagster,"<p>I am new to dagster and I am trying to understand how user inputs are handled by it. I am testing this out with the following piece of code:</p>
<pre><code>from dagster import job, op


@op
def input_string():
    ret = input('Enter string')
    print(ret)


@job
def my_job():
    input_string()


if __name__ == '__main__':
    my_job.execute_in_process()
</code></pre>
<p>I then run the following in console:</p>
<pre><code>dagit -f test.py
</code></pre>
<p>When I finally &quot;Launch Run&quot; however, I don't get an opportunity to enter input, and instead get an EOFError with the following info:</p>
<blockquote>
<p>dagster.core.errors.DagsterExecutionStepExecutionError: Error occurred
while executing op &quot;input_string&quot;:   File
&quot;C:\Users\username\Anaconda3\lib\site-packages\dagster\core\execution\plan\execute_plan.py&quot;,
line 232, in dagster_event_sequence_for_step
for step_event in check.generator(step_events):   File &quot;C:\Users\username\Anaconda3\lib\site-packages\dagster\core\execution\plan\execute_step.py&quot;,
line 354, in core_dagster_event_sequence_for_step
for user_event in check.generator(   File &quot;C:\Users\username\Anaconda3\lib\site-packages\dagster\core\execution\plan\execute_step.py&quot;,
line 70, in _step_output_error_checked_user_event_sequence
for user_event in user_event_sequence:   File &quot;C:\Users\username\Anaconda3\lib\site-packages\dagster\core\execution\plan\compute.py&quot;,
line 170, in execute_core_compute
for step_output in <em>yield_compute_results(step_context, inputs, compute_fn):   File
&quot;C:\Users\username\Anaconda3\lib\site-packages\dagster\core\execution\plan\compute.py&quot;,
line 138, in <em>yield_compute_results
for event in iterate_with_context(   File &quot;C:\Users\username\Anaconda3\lib\site-packages\dagster\utils_<em>init</em></em>.py&quot;,
line 403, in iterate_with_context
return   File &quot;C:\Users\username\Anaconda3\lib\contextlib.py&quot;, line 137, in <strong>exit</strong>
self.gen.throw(typ, value, traceback)   File &quot;C:\Users\username\Anaconda3\lib\site-packages\dagster\core\execution\plan\utils.py&quot;,
line 73, in solid_execution_error_boundary
raise error_cls( The above exception was caused by the following exception: EOFError: EOF when reading a line   File
&quot;C:\Users\username\Anaconda3\lib\site-packages\dagster\core\execution\plan\utils.py&quot;,
line 47, in solid_execution_error_boundary
yield   File &quot;C:\Users\username\Anaconda3\lib\site-packages\dagster\utils_<em>init</em></em>.py&quot;,
line 401, in iterate_with_context
next_output = next(iterator)   File &quot;C:\Users\username\Anaconda3\lib\site-packages\dagster\core\execution\plan\compute_generator.py&quot;,
line 65, in _coerce_solid_compute_fn_to_iterator
result = fn(context, **kwargs) if context_arg_provided else fn(**kwargs)   File &quot;test.py&quot;, line 14, in input_string
ret = input('Enter string')</p>
</blockquote>
<p>How can I get this to run?</p>
",3,1651075180,python;input;dagster,True,1498,1,1651076281,https://stackoverflow.com/questions/72031909/handling-user-input-in-dagster
71847351,Load multiple python files in to dagster via dagit locally,"<p>I am trying to figure out how to or if there is a way to use <code>dagit</code> command on multiple python job files.</p>
<p>Example: <code>dagit -f hello_cereal_job.py</code></p>
<p>With in the directory I have multiple <code>*_job.py</code> files and when I launch the dagster ui locally with <code>dagit</code> I'd like to have all my jobs visible in the UI.</p>
<p>With this command it only shows me the one job that I have in the file, but I'd like to have all jobs from all files in my directory show up.</p>
",1,1649787060,python;data-science;etl;dagster,True,1084,1,1649788399,https://stackoverflow.com/questions/71847351/load-multiple-python-files-in-to-dagster-via-dagit-locally
71843594,Dynamically scheduling Dagster jobs,"<p>I'm wondering if it is possible to overwrite the cron schedule for a job. In my case, I want to run a Dagster job on every 6th business day for every month. So, I wrote a Python function that returns the next 6th business day of the upcoming month and wrote this in cron notation. Then, after the job ran according to the schedule, I want to overwrite the cron schedule to the next 6th business day of the next month.</p>
<p>This is my solution so far:</p>
<pre><code>next_schedule = find_6th_business_day()
@schedule(cron_schedule=next_schedule, job=my_job, execution_timezone=&quot;Europe/Berlin&quot;)
def my_scheduler(context):
    run_date = context.scheduled_execution_time.strftime(&quot;%Y-%m-%d&quot;)
    # update cron schedule
    global next_schedule
    next_schedule = find_6th_business_day()
    return {&quot;ops&quot;: {&quot;op1&quot;: {&quot;config&quot;: {&quot;date&quot;: run_date}},
                    &quot;op2&quot;: {&quot;config&quot;: {&quot;date&quot;: run_date}}}}
</code></pre>
<p>I thought, it would help, if I define the next_schedule variable as a global one, so that it can be overwritten inside the decorator. But I'm not sure if this solves my problem. May anyone can help here, please? Maybe Dagster has some built-in solution for my problem I am not aware of.</p>
",2,1649770144,python;cron;job-scheduling;dagster,True,1422,1,1649774663,https://stackoverflow.com/questions/71843594/dynamically-scheduling-dagster-jobs
71615796,Is it possible to generate jobs in Dagster dynamically using configuration from database,"<p>Currently, my database has multi departments. I need to apply a data pipeline to all of these departments with different configurations.</p>
<p>I want to load configurations for each department from a database. Then use these configuration to generate a list of Jobs in Dagster.</p>
<p>For example, I have 3 tenants:</p>
<p>Department1: Configuration1</p>
<p>Department2: Configuration2</p>
<p>Department3: Configuration3</p>
<p>These information is stored in my database.</p>
<p>How can I load these information and dynamically create 3 jobs (pipelines):</p>
<p>Pipeline1 for Department1 with Configuration1</p>
<p>Pipeline2 for Department2 with Configuration2</p>
<p>Pipeline3 for Department3 with Configuration3</p>
<p>Is it possible to do it on Dagster? I can do it with Airflow (dynamically generating DAGs) but not sure how to do this in Dagster. I cannot load database configuration outside of op/job in Dagster.</p>
",3,1648205816,dagster,True,1398,1,1648499517,https://stackoverflow.com/questions/71615796/is-it-possible-to-generate-jobs-in-dagster-dynamically-using-configuration-from
71649273,(Dagster) Schedule my_hourly_schedule was started from a location that can no longer be found,"<p>I'm getting the following Warning message when trying to start the dagster-daemon:</p>
<pre><code>Schedule my_hourly_schedule was started from a location Scheduler that can no longer be found in the workspace, or has metadata that has changed since the schedule was started. You can turn off this schedule in the Dagit UI from the Status tab.
</code></pre>
<p>I'm trying to automate some pipelines with dagster and created a new project using <code>dagster new-project Scheduler</code>  where &quot;Scheduler&quot; is my project.</p>
<p>This command, as expected, created a diretory with some hello_world files. Inside of it I put the dagster.yaml file with configuration for a PostgreDB to which I want to right the logs. The whole thing looks like this:</p>
<p><a href=""https://i.stack.imgur.com/CKJ1U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CKJ1U.png"" alt=""enter image description here"" /></a></p>
<p>However, whenever I run dagster-daemon run from the directory where the workspace.yaml file is located, I get the message above. I tried runnning running the daemon from other folders, but it then complains that it can't find any workspace.yaml files.</p>
<p>I guess, I'm running into a &quot;beginner mistake&quot;, but could anyone help me with this?</p>
<p>I appreciate any counsel.</p>
",0,1648478558,dagster,False,748,1,1648495819,https://stackoverflow.com/questions/71649273/dagster-schedule-my-hourly-schedule-was-started-from-a-location-that-can-no-lo
71583339,Define resource_defs in dagster job sensor,"<p>I am trying to build sensor for the execution of pipeline/graph. The sensor would check on different intervals and executes the job containing different ops. Now the Job requires some resource_defs and config. In the offical documentation I don't see how I can define resource_defs for Job. A small hint would be great</p>
<p><strong>Question</strong> :  where or how do i define resource_defs  in sensor ? Do I even have to define it ? its not mentioned in official documentation</p>
<p><a href=""https://docs.dagster.io/concepts/partitions-schedules-sensors/sensors"" rel=""nofollow noreferrer"">https://docs.dagster.io/concepts/partitions-schedules-sensors/sensors</a></p>
<pre><code>### defining Job 
@job(
resource_defs = {&quot;some_API_Module&quot;: API_module , &quot;db_Module&quot; : db} ,
 config = {key : value } 
) 
def job_pipeline ():
    op_1 () ## API is used as required resource  
    op_2 () ##  db is used as required resource 

### defining sensor that triggers the Job
@sensor ( Job = job_pipeline) :
    ### some calculation 
    yield RunRequest(run_key = &quot;&quot; config = {key : value} ) 
</code></pre>
",2,1648020843,python;api;pipeline;orchestration;dagster,False,965,1,1648050534,https://stackoverflow.com/questions/71583339/define-resource-defs-in-dagster-job-sensor
71203318,Dagster with multiple virtual environments,"<p>I have two python scripts prepared for dagster:</p>
<pre><code>dagster1.py
dagster2.py
</code></pre>
<p>However, they need to run on a different virtual environment each.</p>
<pre><code>venv1
venv2
</code></pre>
<p>Is it possible to run both on the same dagster server? If so, how? If not, what would be the alternative or workaround?</p>
",1,1645432702,python;dagster,True,1490,1,1645454523,https://stackoverflow.com/questions/71203318/dagster-with-multiple-virtual-environments
71165201,How to rewrite python script to dagster friendly code,"<p>I have this simple python script. How could I rewrite it in a way that works in dagster?</p>
<pre><code>import logging

from mypackage import function1, function2, function3, function4, function5


def main():
    try:
        function1()
        function2()
    except Exception as e:
        logging.exception(e)
        function4()
    else:
        function5()

if __name__ == '__main__:
    main()
</code></pre>
<p>This is what I've been trying so far, but still long way to go:</p>
<pre><code>import logging

from dagster import success_hook, failure_hook
from mypackage import function1, function2, function3, function4, function5


@solid
def dag_function1() -&gt; bool:
    myvar1 = True
    function1()
    return myvar1


@solid
def dag_function2() -&gt; bool:
    myvar2 = True
    function2()
    return myvar2


@solid
def dag_function3() -&gt; bool:
    myvar3 = True
    function3()
    return myvar3


@failure_hook
def dag_function5():
    logging.exception('NOT SURE HOW TO ACCESS MY EXCEPTION')
    function5()


@success_hook
def dag_function4():
    function4()


def main():
    dag_function3(dag_function1(), dag_function2())
</code></pre>
<p>I have tried something like this, but dagster throws an error dagster.core.errors.DagsterInvariantViolationError: No jobs, pipelines, graphs, or repositories found</p>
",1,1645131037,python;dagster,True,683,1,1645221181,https://stackoverflow.com/questions/71165201/how-to-rewrite-python-script-to-dagster-friendly-code
71164725,Access traceback in dagster&#39;s failure hook,"<p>I have a failure hook in which I would want to send a mail with the exception that has been raised. Is there any way to access it?</p>
<pre><code># PIPELINE
@failure_hook
def email_message_on_failure(context: HookContext):
    logging.exception(e)
    mail_errors = Mail(
        body=str(e)
    )
    mail_errors.send()
</code></pre>
",0,1645128288,python;dagster,False,176,1,1645219942,https://stackoverflow.com/questions/71164725/access-traceback-in-dagsters-failure-hook
70772301,DAGSTER: async ops and jobs and dynamic docker-ops,"<p>Here I have 2 questions.</p>
<ol>
<li>I need to run an aiohttp session which shall simultaneously make several requests to different urls and download several files and return a list of absolute paths to these files on disk. This list shall be passed to another async function.</li>
</ol>
<p>Is there a way to run an &quot;async def&quot; function within a dagster job and build an async pipeline?</p>
<ol start=""2"">
<li>In fact the length of the above mentioned list may differ from case to case. Each file requires a long and heavy processing and there is no way to make it async as the processing is blocking (unfortunately). So the only way is to start such processing in separate threads or processes or (as we do) - in separate docker-containers on different machines.</li>
</ol>
<p>Can dagster dynamically create docker-containers with ops, return any output from them and kill each of them on container-exit?</p>
",2,1642602149,python;dagster,True,1391,1,1642606632,https://stackoverflow.com/questions/70772301/dagster-async-ops-and-jobs-and-dynamic-docker-ops
70465752,What is proper Partition configs for Dagster job?,"<p>Currently, I am facing with <code>dagster.core.errors.PartitionExecutionError</code> but error logs from Dagster seem not obvious to me.</p>
<pre><code>dagster.core.errors.PartitionExecutionError: Error occurred during the evaluation of the `run_config_for_partition` function for partition set download_firebase_data_local_partition_set 
  File &quot;/Users/bryan/miniconda3/envs/dagster-injector/lib/python3.9/site-packages/dagster/grpc/impl.py&quot;, line 292, in get_partition_config
    return ExternalPartitionConfigData(name=partition.name, run_config=run_config)
  File &quot;/Users/bryan/miniconda3/envs/dagster-injector/lib/python3.9/contextlib.py&quot;, line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File &quot;/Users/bryan/miniconda3/envs/dagster-injector/lib/python3.9/site-packages/dagster/core/errors.py&quot;, line 192, in user_code_error_boundary
    raise error_cls(
The above exception was caused by the following exception:

TypeError: daily_download_config() takes 1 positional argument but 2 were given 
  File &quot;/Users/bryan/miniconda3/envs/dagster-injector/lib/python3.9/site-packages/dagster/core/errors.py&quot;, line 185, in user_code_error_boundary
    yield
  File &quot;/Users/bryan/miniconda3/envs/dagster-injector/lib/python3.9/site-packages/dagster/grpc/impl.py&quot;, line 291, in get_partition_config
    run_config = partition_set_def.run_config_for_partition(partition)
  File &quot;/Users/bryan/miniconda3/envs/dagster-injector/lib/python3.9/site-packages/dagster/core/definitions/partition.py&quot;, line 441, in run_config_for_partition
    return copy.deepcopy(self._user_defined_run_config_fn_for_partition(partition))
  File &quot;/Users/bryan/miniconda3/envs/dagster-injector/lib/python3.9/site-packages/dagster/core/definitions/time_window_partitions.py&quot;, line 192, in &lt;lambda&gt;
    run_config_for_partition_fn=lambda partition: fn(
</code></pre>
<p>My current setup is</p>
<pre><code>@graph
def download():
    &quot;&quot;&quot;
    Download data from BigQuery then upload to S3
    &quot;&quot;&quot;
    extract_data_in_date()


@daily_partitioned_config(start_date=datetime(2021, 12, 1))
def daily_download_config(date: datetime):
    return {
        &quot;resources&quot;: {
            &quot;date&quot;: date.strftime(&quot;%Y-%m-%d&quot;)
        }
    }

download_local_job = download.to_job(
    name=f'{NAME}_local',
    resource_defs={
        **{
            &quot;date&quot;: make_values_resource(date=str),
            &quot;project_name&quot;: ResourceDefinition.hardcoded_resource(&quot;test-123&quot;)
        },
        **RESOURCES_LOCAL,
    },
    config=daily_download_config,
    executor_def=in_process_executor
)
</code></pre>
<p>I am not sure where I am wrong, can you please help</p>
",0,1640281981,dagster,True,724,1,1641406320,https://stackoverflow.com/questions/70465752/what-is-proper-partition-configs-for-dagster-job
70024709,How do i pass resources to Jobs so it can be accessible to it&#39;s ops,"<p>I am new to dagster and I'm trying to pass a resource to a dagster op through the job that it's called from I am having issue even after following the docs, I am not sure if I need to pass config again to jobs nothing seems to work. Here is the code.</p>
<p>Error <code>dagster.core.errors.DagsterInvalidConfigError: Error in config for job Error 1: Missing required config entry &quot;resources&quot; at the root. </code></p>
<pre><code>import os
from dotenv import load_dotenv
load_dotenv()


@op
def return_one(context):
    context.log.info(f'return_one {os.environ.get(&quot;BUCKET&quot;)}')
    return 1


@op(required_resource_keys={&quot;boto3_connection&quot;})
def add_two(context, i: int):
    context.log.info(f'##### {context.resources.boto3_connection.get_client()}')
    return i + 2


@op
def multi_three(i: int):
    return i * 3


class Boto3Connector(object):
    def __init__(self, aws_access_key_id, aws_secret_access_key):
        self.aws_access_key_id = aws_access_key_id
        self.aws_secret_access_key = aws_secret_access_key


    def get_client(self, resource=&quot;s3&quot;):
        session = boto3.session.Session()

        session_client = session.client(
            service_name=resource,
            aws_access_key_id=self.aws_access_key_id,
            aws_secret_access_key=self.aws_secret_access_key,
        )
        return session_client


@resource(
    config_schema={
        'aws_access_key_id': StringSource,
        'aws_secret_access_key': StringSource
    })
def boto3_connection(context):
    return Boto3Connector(
        context.resource_config['aws_access_key_id'],
        context.resource_config['aws_secret_access_key']
    )


@job(resource_defs={'boto3_connection': boto3_connection})
def my_job():
    multi_three(add_two(return_one()))```
</code></pre>
",1,1637258930,python;dagster,True,2111,1,1637300824,https://stackoverflow.com/questions/70024709/how-do-i-pass-resources-to-jobs-so-it-can-be-accessible-to-its-ops
69949073,Is it possible to create dynamic jobs with Dagster?,"<p>Consider this example - you need to load table1 from source database, do some generic transformations (like convert time zones for timestamped columns) and write resulting data into Snowflake. This is an easy one and can be implemented using 3 dagster ops.</p>
<p>Now, imagine you need to do the same thing but with 100s of tables. How would you do it with dagster? Do you literally need to create 100 jobs/graphs? Or can you create one job, that will be executed 100 times? Can you throttle how many of these jobs will run at the same time?</p>
",4,1636751346,dagster,True,3197,1,1636760550,https://stackoverflow.com/questions/69949073/is-it-possible-to-create-dynamic-jobs-with-dagster
69677992,DagsterUnmetExecutorRequirementsError with dagster CLI during tutorial,"<p>I just started following the <a href=""https://docs.dagster.io/tutorial/intro-tutorial/single-solid-pipeline"" rel=""nofollow noreferrer"">dagster tutorial</a>. I managed to get the <code>hello_cereal</code> job running with dagit and the Python API, but for some reason when trying with dagster CLI</p>
<pre><code>dagster job execute -f hello_cereal.py
</code></pre>
<p>I am getting a <code>DagsterUnmetExecutorRequirementsError</code>:</p>
<pre><code>2021-10-22 15:33:22 - dagster - ERROR - hello_cereal_job - 272b37fb-9f39-44dc-b63a-dcd3dfbb7956 - 880 - RUN_FAILURE - Execution of run for &quot;hello_cereal_job&quot; failed. Pipeline failure during initialization for pipeline &quot;hello_cereal_job&quot;. This may be due to a failure in initializing the executor or one of the loggers.

dagster.core.errors.DagsterUnmetExecutorRequirementsError: You have attempted to use an executor that uses multiple processes with an ephemeral DagsterInstance. A non-ephemeral instance is needed to coordinate execution between multiple processes. You can configure your default instance via $DAGSTER_HOME or ensure a valid one is passed when invoking the python APIs. You can learn more about setting up a persistent DagsterInstance from the DagsterInstance docs here: https://docs.dagster.io/deployment/dagster-instance#default-local-behavior
</code></pre>
<p>Indeed, I don't have <code>$DAGSTER_HOME</code> set, but since it is working with the web UI and the Python API versions, I was wondering if I made a mistake elsewhere ?</p>
<p>I am on macOS BigSur (11.6), on a fresh miniconda installation (<code>v4.10.3</code>), Python 3.9.5 and dagster <code>0.13.0</code>.</p>
",2,1634910424,python;dagster,True,372,2,1635200991,https://stackoverflow.com/questions/69677992/dagsterunmetexecutorrequirementserror-with-dagster-cli-during-tutorial
69573045,How to run tasks in parallel in dagster?,"<p>I am using dagster to running into local node.js microservices pipelines, in order to execute test.</p>
<p>The ide is execute n docker_files, and n node.js microservices, easily like you can do with dagster.</p>
<p>The problem is that when I execute the first second one task a shell command to execute a docker container, dagsteer keep in that point, and not execute all task in the same level.</p>
<p>Current dag logs like this</p>
<pre><code>login aws
    |
    |
    |
    v
[docker_elastic, docker_kafka, sleep_10]
                                  |
                                  |
                                  |
                                  v
[node_service_one, node_service_two, node_service__three]
</code></pre>
<p>Can I execute at same time all docker_elastic and all node_services?</p>
<p>Is there another easy to configure option to build a local dags easily?</p>
<p>Thanks</p>
",0,1634223880,python;dagster,True,3245,1,1634231250,https://stackoverflow.com/questions/69573045/how-to-run-tasks-in-parallel-in-dagster
69553675,Dagster chaining resources,"<p>I've recently picked up Dagster to evaluate as an alternate to Airflow.</p>
<p>I haven't been able to wrap my head around the concept of <strong>resources</strong> and looking to understand if what I'm trying to do is possible or can be achieved better in a different way.</p>
<p>I have a helpder class like below that helps keep code DRY</p>
<pre><code>from dagster import resource, solid, ModeDefinition, pipeline
from dagster_aws.s3 import s3_resource

class HelperAwsS3:
    def __init__(self, s3_resource):
        self.s3_resource = s3_resource

    def s3_list_bucket(self, bucket, prefix):
        return self.s3_resource.list_objects_v2(
            Bucket=bucket,
            Prefix=prefix
        )

    def s3_download_file(self, bucket, file, local_path):
        self.s3_resource.meta.client.download_file(
            Bucket=bucket,
            Key=file,
            Filename=local_path
        )

    def s3_upload_file(self, bucket, file, local_path):
        self.s3_resource.meta.client.upload_file(
            Bucket=bucket,
            Key=file,
            Filename=local_path
        )
</code></pre>
<p>The <em>s3_resource</em> is actually <em>dagster_aws.s3.s3_resource</em> which will help me connect to AWS using my local aws credenitals.</p>
<p>I am not sure how to pass the <em>s3_resource</em> to the HelperAwsS3 when I make the call in the <strong>@resource</strong> section below.</p>
<pre><code>@resource
def connection_helper_aws_s3_resource(context):
    return HelperAwsS3()
</code></pre>
<p>Any pointers please? Or am I doing it all wrong and it needs doing in a different way?</p>
<p>Thanks for your help.</p>
",2,1634120170,dagster,True,846,1,1634143590,https://stackoverflow.com/questions/69553675/dagster-chaining-resources
67536070,Dagster use dagster&#39;s custom loggers in standard python functions called from solids,"<p>I can configure a custom logger (say, a file logger) which I can successfully use from within a solid from <code>context.log.info</code> (for example). How can I use that same logger from within a standard Python function / class ?</p>
<p>I am using the standard <code>colored_console_logger</code> so that I can directly see in the console what is happening. The idea is to swap it (or use alongside it) with another (custom) logger.</p>
<p>Reproducible example:
test_logging.py</p>
<pre><code>from dagster import solid, pipeline, execute_pipeline, Field, ModeDefinition
from dagster.loggers import colored_console_logger

from random_func import random_func


@solid
def test_logs(context):
    context.log.info(&quot;Hello, world!&quot;)
    random_func()


@pipeline(mode_defs=[
    ModeDefinition(logger_defs={&quot;console_logger&quot;: colored_console_logger})
])
def test_pipeline():
    test_logs()


if __name__ == &quot;__main__&quot;:
    execute_pipeline(test_pipeline,
                     run_config={
                         'loggers': {
                             'console_logger': {
                                 'config': {
                                     'log_level': 'DEBUG',
                                     'name': 'console_logger',
                                 }
                             }
                         }
                     })
</code></pre>
<p>random_func.py</p>
<pre><code>import logging

lgr = logging.getLogger('console_logger')


def random_func():
    lgr.info('in random func')
    print('\nhi\n')
</code></pre>
",1,1621002712,python;logging;pipeline;dagster,True,1265,1,1632862236,https://stackoverflow.com/questions/67536070/dagster-use-dagsters-custom-loggers-in-standard-python-functions-called-from-so
69038703,Adding additional parameters to a solid function,"<p>I want to add additional parameters when calling a solid, that inherits from another solid as like:</p>
<pre class=""lang-py prettyprint-override""><code>from dagster import pipeline, repository, schedule, solid, InputDefinition, solid

@solid
def hello():
    return 1


@solid(
    input_defs=[
        InputDefinition(&quot;name&quot;, str),
        InputDefinition(&quot;age&quot;, int),
    ]
)
def hello2(hello_: int, name: str, age: int):
    print(f&quot;Hello {name}, {age+hello_}&quot;)


@pipeline
def pipe_2():
    x = hello()
    y = hello2(x, &quot;Marcus&quot;, 20)


@repository
def deploy_docker_repository():
    return [pipe_2, my_schedule]
</code></pre>
<p>But when running the pipeline from the dagster API grpc I get the following error:</p>
<pre><code>dagster.core.errors.DagsterInvalidDefinitionError: In @pipeline pipe_2, received invalid
type &lt;class 'str'&gt; for input &quot;name&quot; (at position 1) in solid invocation &quot;hello2&quot;.
Must pass the output from 
previous solid invocations or inputs to the composition 
function as inputs when invoking solids during composition.
</code></pre>
<p>How to fix?</p>
",2,1630634408,python;dagster,True,1385,1,1631034115,https://stackoverflow.com/questions/69038703/adding-additional-parameters-to-a-solid-function
68772481,How can I fix the error &#39;Received unexpected config entry &quot;params&quot; at path root:postgres_db&#39; when running the Dagster default mode?,"<p>I have deployed Dagster on AWS EKS following Dagster guide using Helm <a href=""https://docs.dagster.io/deployment/guides/kubernetes/deploying-with-helm"" rel=""nofollow noreferrer"">https://docs.dagster.io/deployment/guides/kubernetes/deploying-with-helm</a>.</p>
<p>When I try to run the Default mode as per above guide I get the following error message in the job log:</p>
<pre><code>dagster.core.errors.DagsterInvalidConfigError: Errors whilst loading configuration for &lt;dagster.config.field_utils.Selector object at 0x7f48e3ede750&gt;.
Error 1: Received unexpected config entry &quot;params&quot; at path root:postgres_db. Expected: &quot;{ db_name: (String | { env: String }) hostname: (String | { env: String }) password: (String | { env: String }) port?: (Int | { env: String }) username: (String | { env: String }) }&quot;.
</code></pre>
<p>Any tips to overcome this error message would be much appreciated.</p>
",1,1628858324,amazon-web-services;kubernetes;amazon-eks;dagster,False,970,1,1630521836,https://stackoverflow.com/questions/68772481/how-can-i-fix-the-error-received-unexpected-config-entry-params-at-path-root
68804933,Dagster running pipelines with multiple projects,"<p>I need a bit of help for the deployment in AWS using dagster projects, unfortunately couldn't found in the offical documentation.</p>
<p>So a bit of context with the simple solids and pipelines using repo.py is working perfectly file. But the problem starts to occur in aws when I change the structure of solid and pipelines in a <a href=""https://docs.dagster.io/getting-started/create-new-project"" rel=""nofollow noreferrer"">new directory project</a>. So the objective is not to use repo.py for the trigger of pipelines (here is the <a href=""https://docs.dagster.io/deployment/guides/docker"" rel=""nofollow noreferrer"">example</a> which shows I am refereeing too ). This line 65 of docker-compose.yaml file uses this command</p>
<p><code>dagster api grpc -h 0.0.0.0 -p 4000 -f repo.py</code></p>
<p>and the same command goes to our AWS infrastructure for the trigger of pipelines. Instead what I am looking for is to utilise the workspace.yaml file (where i can add multiple python packages ).</p>
<p>So does anyone think can be command be used like this ?  (presently there is no existing '-w' parameter with dagster api )</p>
<p><code>dagster api grpc -h 0.0.0.0 -p 4000 -w workspace.yaml</code></p>
<p>If not then another Idea is to use Module instead of 'repo.py' in the main directory. Dagit works really well with the module</p>
<p><code>dagit -m project-01</code></p>
<p>but can this be possible with the dagster ? so the command would become like this  <code>dagster api grpc -h 0.0.0.0 -p 4000 -m project-01</code> (presently it throws an error that project-01 don't exists )</p>
",1,1629125595,python;docker;devops;etl;dagster,False,757,0,1629125595,https://stackoverflow.com/questions/68804933/dagster-running-pipelines-with-multiple-projects
68462589,Can I use the output of one DynamicResource after a map() for multiple solids?,"<p>I am doing something similar to the <a href=""https://docs.dagster.io/concepts/solids-pipelines/pipelines#dynamic-mapping--collect"" rel=""nofollow noreferrer"">dynamic mapping and collect</a> example in the documentation. The example lists files in a directory, maps each to a solid which computes the file size, and then collects the output for a summary of the overall size.</p>
<p>However, I would like to run multiple solids in parallel on each solid. So to continue with the example: I would list files in a directory; then map so that for each file I would compute the size, check the file permissions, and compute the md5sum all in parallel; and finally collect the output.</p>
<p>I can run these in sequence on each file with something like:</p>
<pre class=""lang-py prettyprint-override""><code>file_results = list_files()
.map(compute_size)
.map(check_permissions)
.map(compute_md5sum)
summarize(file_results.collect())
</code></pre>
<p>But if these aren't actually serial dependencies, it would be nice to parallelize the work on each file. Is there some syntax like this:</p>
<pre class=""lang-py prettyprint-override""><code>file_results = list_files().map(
compute_md5sum(check_permissions(compute_size)))
summarize(file_results.collect())
</code></pre>
",1,1626825723,dagster,True,631,1,1626882510,https://stackoverflow.com/questions/68462589/can-i-use-the-output-of-one-dynamicresource-after-a-map-for-multiple-solids
68020233,Collect Metadata with Dagster,"<p>everyone</p>
<p>I've started using dagster for about a week or so now and I'm fascinated by the tool. However, I was wondering if it's possible to collect the metadata that is produced by dagster in the output.</p>
<p>The regular dagster output goes like this:</p>
<p>2021-06-17 15:12:30 - dagster - DEBUG - my_pipeline- 47989433-702c-4246-9c8d-ab4c8bab4be6 - 13936 - merge_transformations - LOADED_INPUT - Loaded input &quot;clean_daag_df&quot; using input manager &quot;io_manager&quot;, from output &quot;result&quot; of step &quot;clean_dzag&quot;</p>
<p>[...]</p>
<p>2021-06-17 15:12:30 - dagster - DEBUG - my_pipeline - 47989433-702c-4246-9c8d-ab4c8bab4be6 - 13936 - merge_transformations - STEP_SUCCESS - Finished execution of step &quot;merge_transformations&quot; in 98ms.</p>
<p>I'd like to know how to access this information, specially the start and finish time of each solid as well as the pipeline run id and, if possible, the id of each solid execution. (instead of just seeing the output in the screen, I'd like to export it to a file or to a database).</p>
<p>Thanks in advance for any help.</p>
",1,1623936009,python;pipeline;dagster,True,758,1,1624034448,https://stackoverflow.com/questions/68020233/collect-metadata-with-dagster
67928569,How to create partitions with a schedule in Dagster?,"<p>I am trying to create partitions within Dagster that will allow me to do backfills. The documentation has an example but it's to use the days of the week(which I was able to replicate). However, I am trying to create partitions with dates.</p>
<pre><code>DATE_FORMAT = &quot;%Y-%m-%d&quot;
BACKFILL_DATE = &quot;2021-04-01&quot;
TODAY = datetime.today()


def get_number_of_days():
    backfill_date_obj = datetime.strptime(BACKFILL_DATE, DATE_FORMAT)
    delta = TODAY - backfill_date_obj

    return delta


def get_date_partitions():
    return [
        Partition(
            [
                datetime.strftime(TODAY - timedelta(days=x), DATE_FORMAT)
                for x in range(get_number_of_days().days)
            ]
        )
    ]


def run_config_for_date_partition(partition):
    date = partition.value
    return {&quot;solids&quot;: {&quot;data_to_process&quot;: {&quot;config&quot;: {&quot;date&quot;: date}}}}


# ----------------------------------------------------------------------
date_partition_set = PartitionSetDefinition(
    name=&quot;date_partition_set&quot;,
    pipeline_name=&quot;my_pipeline&quot;,
    partition_fn=get_date_partitions,
    run_config_fn_for_partition=run_config_for_date_partition,
)
# EXAMPLE CODE FROM DAGSTER DOCS.
# def weekday_partition_selector(
#     ctx: ScheduleExecutionContext, partition_set: PartitionSetDefinition
# ) -&gt; Union[Partition, List[Partition]]:
#     &quot;&quot;&quot;Maps a schedule execution time to the corresponding partition or list
#     of partitions that
#     should be executed at that time&quot;&quot;&quot;
#     partitions = partition_set.get_partitions(ctx.scheduled_execution_time)
#     weekday = ctx.scheduled_execution_time.weekday() if ctx.scheduled_execution_time else 0
#     return partitions[weekday]

# My attempt. I do not want to partition by the weekday name, but just by the date. 
# Instead of returnng the partition_set, I think I need to do something else with it
# but I'm not sure what it is.
def daily_partition_selector(
    ctx: ScheduleExecutionContext, partition_set: PartitionSetDefinition
) -&gt; Union[Partition, List[Partition]]:
    return partition_set.get_partitions(ctx.scheduled_execution_time)

my_schedule = date_partition_set.create_schedule_definition(
    &quot;my_schedule&quot;,
    &quot;15 8 * * *&quot;,
    partition_selector=daily_partition_selector,
    execution_timezone=&quot;UTC&quot;,
)
</code></pre>
<p>Current dagster UI has all the dates lumped together in the partition section.
<a href=""https://i.stack.imgur.com/TqVzt.png"" rel=""nofollow noreferrer"">Actual Results</a></p>
<p><a href=""https://i.stack.imgur.com/Q2rbM.png"" rel=""nofollow noreferrer"">Expected Results</a></p>
<p>What am I missing that will give me the expected results?</p>
",3,1623359811,python;dagster,True,1558,1,1623363175,https://stackoverflow.com/questions/67928569/how-to-create-partitions-with-a-schedule-in-dagster
67864955,dagster solid Parallel Run Test exmaple,"<p>I tried to run parallel, but it didn't work as I expected</p>
<p>The progress bar doesn't work the way I thought it would.</p>
<p>I think that both operations should be executed at the same time.</p>
<p>but first run find_highest_calorie_cereal after find_highest_protein_cereal</p>
<p><a href=""https://i.stack.imgur.com/pb4sV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pb4sV.png"" alt=""1"" /></a></p>
<pre><code>import csv
import time
import requests
from dagster import pipeline, solid


# start_complex_pipeline_marker_0
@solid
def download_cereals():
    response = requests.get(&quot;https://docs.dagster.io/assets/cereal.csv&quot;)
    lines = response.text.split(&quot;\n&quot;)
    return [row for row in csv.DictReader(lines)]


@solid
def find_highest_calorie_cereal(cereals):
    time.sleep(5)
    sorted_cereals = list(
        sorted(cereals, key=lambda cereal: cereal[&quot;calories&quot;])
    )
    return sorted_cereals[-1][&quot;name&quot;]


@solid
def find_highest_protein_cereal(context, cereals):
    time.sleep(10)
    sorted_cereals = list(
        sorted(cereals, key=lambda cereal: cereal[&quot;protein&quot;])
    )
    # for i in range(1, 11):
    #     context.log.info(str(i) + '~~~~~~~~')
    #     time.sleep(1)

    return sorted_cereals[-1][&quot;name&quot;]


@solid
def display_results(context, most_calories, most_protein):
    context.log.info(f&quot;Most caloric cereal 테스트: {most_calories}&quot;)
    context.log.info(f&quot;Most protein-rich cereal: {most_protein}&quot;)


@pipeline
def complex_pipeline():
    cereals = download_cereals()
    display_results(
        most_protein=find_highest_protein_cereal(cereals),
        most_calories=find_highest_calorie_cereal(cereals),
    )
</code></pre>
",1,1623028648,dagster,False,1167,1,1623214390,https://stackoverflow.com/questions/67864955/dagster-solid-parallel-run-test-exmaple
64752262,What is the value of $repositoryLocationName when running ExecutePipeline in Dagster&#39;s GraphQL API?,"<p>I am attempting to launch a Dagster pipeline run with the GraphQL API. I have Dagit running locally and a working pipeline that I can trigger via the playground.</p>
<p>However, I am now trying to trigger the pipeline via GraphQL Playground, available at <code>/graphql</code>.</p>
<p>I am using the following mutation:</p>
<pre><code>mutation ExecutePipeline(
  $repositoryLocationName: String!
  $repositoryName: String!
  $pipelineName: String!
  $runConfigData: RunConfigData!
  $mode: String!
)
</code></pre>
<p>...and hence am providing the following query params:</p>
<pre><code>{
  &quot;repositoryName&quot;: &quot;my_repo&quot;,
  &quot;repositoryLocationName&quot;: &lt;???&gt;,
  &quot;pipelineName&quot;: &quot;my_pipeline&quot;,
  &quot;mode&quot;: &quot;dev&quot;,
  &quot;runConfigData&quot;: {&lt;MY_RUN_CONFIG&gt;}
}
</code></pre>
<p>I am not sure what value <code>repositoryLocationName</code> should take? I have tried a few but receive the following error:</p>
<pre><code>{
  &quot;data&quot;: {
    &quot;launchPipelineExecution&quot;: {
      &quot;__typename&quot;: &quot;PipelineNotFoundError&quot;
    }
  }
}
</code></pre>
<p><a href=""https://docs.dagster.io/overview/graphql-api#main"" rel=""nofollow noreferrer"">This</a> is the tutorial I am following.</p>
",2,1604927204,graphql;dagster,True,544,2,1623150512,https://stackoverflow.com/questions/64752262/what-is-the-value-of-repositorylocationname-when-running-executepipeline-in-dag
67509243,"How to define a composite solid with multiple arguments, including `Nothing`?","<p>I have a solid that needs to run after 2 solids. One will return a value, another doesn't return anything but has dependency solids and will take time to run.</p>
<p>I execute the pipeline <strong>in <code>multiprocessing</code> mode</strong>, where solids run at the same time if they don't have dependencies defined.</p>
<p>Below is the sample situation I am looking for. Say I have below solids.</p>
<pre class=""lang-py prettyprint-override""><code>@solid(input_defs=[InputDefinition(&quot;start&quot;, Nothing)])
def solid_a(context):
    import time
    time.sleep(2)
    context.log.info('yey')

@solid
def solid_b(context):
    return 1

@composite_solid
def my_composite_solid(wait_solid_a: Nothing, solid_b_output: int):
    some_other_solid(solid_b_output)
</code></pre>
<p>And when executed, these solids will be running in the below timeline.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Time Passed</th>
<th>solid</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>pipeline starts...</td>
</tr>
<tr>
<td>1 sec</td>
<td><code>solid_b</code> started</td>
</tr>
<tr>
<td>3 sec</td>
<td><code>solid_a</code> dependency solids are running. <code>solid_a</code> did not started yet.</td>
</tr>
<tr>
<td>5 sec</td>
<td><code>solid_b</code> finished</td>
</tr>
<tr>
<td>10 sec</td>
<td><code>solid_a</code> started now</td>
</tr>
<tr>
<td>15 sec</td>
<td><code>solid_a</code> finished</td>
</tr>
<tr>
<td>20 sec</td>
<td><code>my_composite_solid</code> <strong>should start now</strong>.</td>
</tr>
</tbody>
</table>
</div>
<p>So, according to this timeline, in order for <code>my_composite_solid</code> to start, I need both <code>solid_a</code> and <code>solid_b</code> to finish executing. However, when I make this, dagster throws an error saying:</p>
<pre><code>dagster.core.errors.DagsterInvalidDefinitionError: @composite_solid 'my_composite_solid' has unmapped input 'wait_solid_a'. Remove it or pass it to the appropriate solid invocation.
</code></pre>
<p>If I don't put the <code>solid_a</code> output as a dependency to <code>my_composite_solid</code>, it will start immediately after the result of <code>solid_b</code>. What should I do?</p>
",2,1620844382,dagster,False,346,0,1620846426,https://stackoverflow.com/questions/67509243/how-to-define-a-composite-solid-with-multiple-arguments-including-nothing
67404366,AWS credentials not found for celery-k8s deployment,"<p>I'm trying to run dagster using celery-k8s and using the examples/celery-k8s as a start. upon running the pipeline from playground I get</p>
<pre><code>Initialization of resources [s3, io_manager] failed.
botocore.exceptions.NoCredentialsError: Unable to locate credentials
</code></pre>
<p>I have configured aws credentials in env variables as mentioned in the document</p>
<pre><code>deployments:
    - name: &quot;user-code-deployment-test&quot;
      image:
        repository: &quot;somasays/dagster-usercode-example&quot;
        tag: &quot;0.5&quot;
        pullPolicy: Always
      dagsterApiGrpcArgs:
        - &quot;-f&quot;
        - &quot;/workspace/repo.py&quot;
      port: 3030
      env:
        AWS_ACCESS_KEY_ID: AAAAAAAAAAAAAAAAAAAAAAAAA
        AWS_SECRET_ACCESS_KEY: qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq
        AWS_DEFAULT_REGION: eu-central-1
</code></pre>
<p>and I can also see these values are set in the env variables of the pod and can also access the s3 location after pip install awscli  and aws s3 ls  see the screenshot below  the job pod however throws <code>Unable to locate credentials</code></p>
<p><a href=""https://i.stack.imgur.com/7qccq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7qccq.png"" alt=""Screenshot"" /></a>
Please help</p>
",0,1620228810,dagster,True,463,1,1620233124,https://stackoverflow.com/questions/67404366/aws-credentials-not-found-for-celery-k8s-deployment
67141562,"How to add tags / debug info to solid context during runtime, that can be retrieved from a failure_hook?","<p>First, let me give my use case.</p>
<p>I have a solid that runs and attempts with particular inputs and configuration. I also have a failure hook that logs to lets say, slack.</p>
<p>During the execution of the solid step, I want to add some information to the context (perhaps selected input variable values, etc), perhaps in the form of a dictionary. And then I want the failure_hook to be able to retrieve these for whatever error logging it performs.</p>
<p>What is the best way to implement this pattern?</p>
",1,1618683681,dagster,True,206,1,1618930079,https://stackoverflow.com/questions/67141562/how-to-add-tags-debug-info-to-solid-context-during-runtime-that-can-be-retrie
67170902,Environment variables in Dagster config YAML,"<p>I'm attempting to provide an environment variable in a config YAML file as such:</p>
<pre class=""lang-yaml prettyprint-override""><code>resources:
  be_warehouse:
    config:
      conn_str:
        env: DB_CONN_STR
  analytics_warehouse:
    config:
      conn_str:
        env: WH_DB_CONN_STR
</code></pre>
<p>but I am receiving the following error:</p>
<pre><code>Invalid scalar at path root:resources:analytics_warehouse:config:conn_str. Value &quot;{'env': 'WH_DB_CONN_STR'}&quot; of type &quot;&lt;class 'dict'&gt;&quot; is not valid for expected type &quot;String&quot;.
</code></pre>
<p>I have seen this syntax used in <a href=""https://github.com/dagster-io/dagster/blob/0.11.4/examples/airline_demo/airline_demo/environments/test_base.yaml"" rel=""nofollow noreferrer"">this official example</a>. Am I missing something obvious?</p>
",1,1618878400,dagster,True,3670,1,1618929861,https://stackoverflow.com/questions/67170902/environment-variables-in-dagster-config-yaml
66890685,Dagster pass ordered dict in the config schema,"<p>How can I pass an Ordered Dict in Dagster solid's config schema?</p>
<p>The simple thing:</p>
<pre><code>from dagster import solid, execute_solid, Field


@solid(config_schema={'my_dict': Field(dict, is_required=True)})
def test_ordered_dict(context):
    print('\n\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;')
    print(context.solid_config['my_dict'])
    print('\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;')


for i in range(20):
    execute_solid(test_ordered_dict,
                  run_config={
                      'solids': {
                          'test_ordered_dict': {
                              'config': {
                                  'my_dict': {
                                      'a': 1,
                                      'b': 2
                                  }
                              }
                          }
                      }
                  })
</code></pre>
<p>doesn't work. Executing this on Windows 10 consistently gives</p>
<pre><code>2021-03-31 18:16:57 - dagster - DEBUG - ephemeral_test_ordered_dict_solid_pipeline - 8e87a380-7ee5-40b6-954e-3ae33b5784e7 - 26348 - test_ordered_dict - STEP_START - Started execution of step &quot;test_ordered_dict&quot;.


&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
{'b': 2, 'a': 1}


&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;
2021-03-31 18:16:57 - dagster - DEBUG - ephemeral_test_ordered_dict_solid_pipeline - 8e87a380-7ee5-40b6-954e-3ae33b5784e7 - 26348 - test_ordered_dict - STEP_OUTPUT - Yielded output &quot;result&quot; of type &quot;Any&quot;. (Type check passed).
</code></pre>
<p>On Linux, it sometimes switches them and this doesn't work for me.</p>
<p>How can I pass <code>collections.OrderedDict</code> in the config schema, so that the entries in the dictionary are ordered <em>exactly</em> as I passed them in the schema?</p>
",0,1617204212,python;etl;pipeline;ordereddict;dagster,True,651,1,1617814972,https://stackoverflow.com/questions/66890685/dagster-pass-ordered-dict-in-the-config-schema
66990345,How do I tell Dagit (the Dagster GUI) to run on an existing Dask cluster?,"<p>I'm using dagster 0.11.3 (the latest as of this writing)</p>
<p>I've created a Dagster pipeline (saved as pipeline.py) that looks like this:</p>
<pre><code>@solid
def return_a(context):
    return 12.34


@pipeline(
    mode_defs=[
        ModeDefinition(
            executor_defs=[dask_executor]  # Note: dask only!
        )
    ]
)
def the_pipeline():
    return_a()
</code></pre>
<p>I have the DAGSTER_HOME environment variable set to a directory that contains a file named dagster.yaml, which is an empty file.  This should be ok because the defaults are reasonable based on these docs: <a href=""https://docs.dagster.io/deployment/dagster-instance"" rel=""nofollow noreferrer"">https://docs.dagster.io/deployment/dagster-instance</a>.</p>
<p>I have an existing Dask cluster running at &quot;scheduler:8786&quot;.  Based on these docs: <a href=""https://docs.dagster.io/deployment/custom-infra/dask"" rel=""nofollow noreferrer"">https://docs.dagster.io/deployment/custom-infra/dask</a>, I created a run config named config.yaml that looks like this:</p>
<pre><code>execution:
  dask:
    config:
      cluster:
        existing:
          address: &quot;scheduler:8786&quot;
</code></pre>
<p>I have SUCCESSFULLY used this run config with Dagster like so:</p>
<pre><code>$ dagster pipeline execute -f pipeline.py -c config.yaml
</code></pre>
<p>(I checked the Dask logs and made sure that it did indeed run on my Dask cluster)</p>
<p><strong>My question is:</strong>  How can I get Dagit to use this Dask cluster?
The only thing I have found that seems related is this:
<a href=""https://docs.dagster.io/_apidocs/execution#executors"" rel=""nofollow noreferrer"">https://docs.dagster.io/_apidocs/execution#executors</a></p>
<p>...but it doesn't even mention Dask as an option (it has dagster.in_process_executor  and dagster.multiprocess_executor, which don't seem at all related to dask).</p>
<p>Probably I need to configure dagster-dask, which is documented here: <a href=""https://docs.dagster.io/_apidocs/libraries/dagster-dask#dask-dagster-dask"" rel=""nofollow noreferrer"">https://docs.dagster.io/_apidocs/libraries/dagster-dask#dask-dagster-dask</a></p>
<p><em><strong>...but where do I put that run config when using Dagit?  There's no way to feed config.yaml to Dagit, for example.</strong></em></p>
",3,1617813371,pipeline;dagster,True,2367,1,1617814782,https://stackoverflow.com/questions/66990345/how-do-i-tell-dagit-the-dagster-gui-to-run-on-an-existing-dask-cluster
66672656,Dagster start pipeline from another pipeline using its outputs,"<p>How am I supposed to start a pipeline B after pipeline A completes, and use pipeline A's outputs into pipeline B?</p>
<p>A piece of code as a starting point:</p>
<pre><code>from dagster import InputDefinition, Nothing, OutputDefinition, pipeline, solid

@solid
def pipeline1_task1(context) -&gt; Nothing:
    context.log.info('in pipeline 1 task 1')


@solid(input_defs=[InputDefinition(&quot;start&quot;, Nothing)],
       output_defs=[OutputDefinition(str, 'some_str')])
def pipeline1_task2(context) -&gt; str:
    context.log.info('in pipeline 1 task 2')
    return 'my cool output'


@pipeline
def pipeline1():
    pipeline1_task2(pipeline1_task1())


@solid(input_defs=[InputDefinition(&quot;print_str&quot;, str)])
def pipeline2_task1(context, print_str) -&gt; Nothing:
    context.log.info('in pipeline 2 task 1' + print_str)


@solid(input_defs=[InputDefinition(&quot;start&quot;, Nothing)])
def pipeline2_task2(context) -&gt; Nothing:
    context.log.info('in pipeline 2 task 2')


@pipeline
def pipeline2():
    pipeline2_task2(pipeline2_task1())


if __name__ == '__main__':
    # run pipeline 1
    # store outputs
    # call pipeline 2 using the above outputs
</code></pre>
<p>Here we have three pipelines: <code>pipeline1</code> has two solids, possibly does whatever stuff we wish and returns output from the second solid. <code>pipeline2</code> is supposed to use the output of <code>pipeline1_task2</code>, eventually do another piece of work and print the output of the first pipeline.</p>
<p>How am I supposed to &quot;connect&quot; the two pipelines?</p>
",2,1615982374,python;pipeline;dagster,True,1800,2,1616113535,https://stackoverflow.com/questions/66672656/dagster-start-pipeline-from-another-pipeline-using-its-outputs
66654379,Dagster use a solid&#39;s AssetMaterialization in another solid (reproducible code included),"<p>So let's say I have two solids. The first does some computations and writes a file to disk. The second solid takes that file and does other things with it, but it needs its filesystem path in order to open it. I can do this with two <code>yield</code>s (one for the <code>AssetMaterialization</code> and the other for the <code>str</code> <code>Output</code>) and explicitly putting the <code>Output</code> in the second solid call:</p>
<pre><code>from dagster import (AssetKey, AssetMaterialization, EventMetadataEntry,
                     Output, execute_pipeline, pipeline, solid)

@solid
def yield_asset(context):
    yield AssetMaterialization(
        asset_key=AssetKey('my_dataset'),
        description='Persisted result to storage',
        metadata_entries=[
            EventMetadataEntry.text('Text-based metadata for this event',
                                    label='text_metadata'),
            EventMetadataEntry.fspath('/path/to/data/on/filesystem'),
            EventMetadataEntry.url('http://mycoolsite.com/url_for_my_data',
                                   label='dashboard_url'),
        ],
    )
    yield Output('/path/to/data/on/filesystem')


@solid
def print_asset_path(context, asset_path: str):
    # do stuff with `asset_path`
    context.log.info(asset_path)


@pipeline
def some_pipeline():
    asset_path = yield_asset()
    print_asset_path(asset_path)


if __name__ == &quot;__main__&quot;:
    result = execute_pipeline(some_pipeline)
</code></pre>
<p>This works fine, and you should get the info message in the logs (<code>2021-03-16 13:23:29 - dagster - INFO - system - 366248ec-6a83-462f-b62f-9fb2514f6f80 - print_asset_path - /path/to/data/on/filesystem</code>) and the <code>AssetMaterialization</code> in <code>dagit</code>.</p>
<p>However, this is kind of inconvenient, since I need to explicitly yield an <code>Output</code> with the filesystem path that I need. Is it possible, and how, to reference the <code>AssetMaterialization</code> in the second solid, and use its properties directly?</p>
<p>Something like (won't work):</p>
<pre><code>@solid
def print_asset_path(context):
    asset_path = context.assets.get_asset_by_key(`my_key`).fspath
    # do stuff with `asset_path`
    context.log.info(asset_path)
</code></pre>
",0,1615894203,python;pipeline;dagster,True,626,1,1616112922,https://stackoverflow.com/questions/66654379/dagster-use-a-solids-assetmaterialization-in-another-solid-reproducible-code-i
66535182,Dagster how to provide presets when creating partitions via PartitionSetDefinition,"<p>When creating a PartitionSetDefinition in Dagster you can pass in a 'mode' that will swap the resources used (for testing purposes you may want to use cloud storage in PROD but use local storage for local development</p>
<p>A mode requires you to specify a set of config values that are usually provided in an environment yaml file but when you create a PartitionSetDefinition like below you can only pass the mode. This is usually done by setting a preset on the pipeline and using that for the run but PartitionSetDefinition only allows the setting of a mode not a preset.</p>
<pre><code>date_partition_set = PartitionSetDefinition(
    name=&quot;date_partition_set&quot;,
    pipeline_name=&quot;my_pipeline&quot;,
    partition_fn=get_date_partitions,
    run_config_fn_for_partition=run_config_for_date_partition,
    mode=&quot;test&quot;
)
</code></pre>
<p>How can you provide the necessary preset/environment values for this?</p>
",0,1615227919,dagster,False,493,1,1615299382,https://stackoverflow.com/questions/66535182/dagster-how-to-provide-presets-when-creating-partitions-via-partitionsetdefiniti
66227615,Daily_schedule triggered runs and backfill runs have different date partition,"<p>I have @daily_schedule triggered daily at 3 minutes past 12am</p>
<p>When triggered by the scheduled tick at <strong>'2021-02-16 00:03:00'</strong></p>
<p>The <em>date</em> input shows <strong>'2021-02-15 00:00:00'</strong>, partition tagged as <strong>'2021-02-15'</strong></p>
<hr />
<p>While if triggered via backfill for partition <strong>'2021-02-16'</strong></p>
<p>The <em>date</em> input shows <strong>'2021-02-16 00:00:00'</strong>, partition tagged as <strong>'2021-02-16'</strong></p>
<hr />
<p>Why does the scheduled tick fill the partition a day before? Is there an option to use the datetime of execution instead (without using cron @schedule)? This descrepency is confusing when I perform queries using the timestamp for exact dates</p>
<p>P.S I have tested both scheduled run and backfil run to have the same Timezone.</p>
<pre><code>
@solid()
def test_solid(_, date):
    _.log.info(f&quot;Input date: {date}&quot;)

@pipeline()
def test_pipeline():
    test_solid()

@daily_schedule(
    pipeline_name=&quot;test_pipeline&quot;,
    execution_timezone=&quot;Asia/Singapore&quot;,
    start_date=START_DATE,
    end_date=END_DATE,
    execution_time=time(00, 03),
    # should_execute=four_hourly_fitler
)
def test_schedule_daily(date):
    timestamp = date.strftime(&quot;%Y-%m-%d %X&quot;)
    return {
        &quot;solids&quot;: {
            &quot;test_solid&quot;:{
                &quot;inputs&quot;: {
                    &quot;date&quot;:{
                        &quot;value&quot;: timestamp
                    }
                }
            }
        }
    }

</code></pre>
",0,1613490373,dagster,False,593,2,1613590707,https://stackoverflow.com/questions/66227615/daily-schedule-triggered-runs-and-backfill-runs-have-different-date-partition
65860584,How to start the grpc server when definitions are in a module,"<p>My project structure looks like this:</p>
<ul>
<li>/definitions (for all dagster python definitions)
<ul>
<li><code>__init__.py</code></li>
<li>repositories.py</li>
<li>/exchangerates
<ul>
<li>pipelines.py</li>
<li>...</li>
</ul>
</li>
<li>...</li>
</ul>
</li>
<li>workspace.yaml</li>
</ul>
<p>I've tried running the grpc server using various methods, especially the following (started in project root):</p>
<ul>
<li><code>dagster api grpc -h 0.0.0.0 -p 4000 -f definitions/repositories.py</code></li>
<li><code>dagster api grpc -h 0.0.0.0 -p 4000 -m definitions</code></li>
<li><code>dagster api grpc -h 0.0.0.0 -p 4000 -m definitions.repositories</code></li>
</ul>
<p>The first command yields the following error:</p>
<blockquote>
<p>dagster.core.errors.DagsterImportError: Encountered ImportError: <code>attempted relative import with no known parent package</code> while importing module repositories from file C:\Users\Klaus\PycharmProjects\dagsterexchangerates\definitions\repositories.py. Consider using the module-based options <code>-m</code> for CLI-based targets or the <code>python_package</code> workspace.yaml target.</p>
</blockquote>
<p>The second and third command yield the following error:</p>
<blockquote>
<p>(stacktrace comes before this)<br />
ModuleNotFoundError: No module named 'definitions'</p>
</blockquote>
<p>How can this be solved?</p>
<p>EDIT:
I've uploaded the current version of the example I'm working on to GitHub: <a href=""https://github.com/kstadler/dagster-exchangerates"" rel=""nofollow noreferrer"">https://github.com/kstadler/dagster-exchangerates</a></p>
<p>EDIT2:
Reflected changes in directory structure</p>
",1,1611413493,dagster,True,3052,1,1611427820,https://stackoverflow.com/questions/65860584/how-to-start-the-grpc-server-when-definitions-are-in-a-module
65619228,Does dagster support dependencies between solids with no outputs?,"<p>I am building a prototype pipeline that does two things:</p>
<ol>
<li>(solid) Clears files out of an existing directory</li>
<li>(solid) Runs a batch process to dump data into that directory.</li>
</ol>
<p>Step #1 is all side-effect, and has no output to pass to #2. Is it possible to express a dependency between these two solids in a pipeline?</p>
",0,1610050112,dagster,True,1287,1,1610057701,https://stackoverflow.com/questions/65619228/does-dagster-support-dependencies-between-solids-with-no-outputs
65327249,Compose a pipeline with other pipelines,"<p>I am currently trying to choose between numerous workflow frameworks. I need an important feature which is workflow composition.</p>
<p>I found nothing on the documentation even in the <a href=""https://docs.dagster.io/_apidocs/pipeline"" rel=""nofollow noreferrer"">API reference</a> or <a href=""https://docs.dagster.io/tutorial/advanced_pipelines"" rel=""nofollow noreferrer"">advanced tutorial</a>.</p>
<p>So my question is : Is it possible to compose pipelines ? i.e. To build some DAGs with already written ones. There is maybe some workarounds but I am interested by its native integration.</p>
<p>Thanks</p>
",1,1608136014,dagster,True,276,1,1608159809,https://stackoverflow.com/questions/65327249/compose-a-pipeline-with-other-pipelines
65249440,Executing a solid when at least one of the required inputs is given,"<p>As an input, I would like to retrieve data based on user input, or &quot;randomly&quot; from a DB if no user input is given. All other downstream tasks of the pipeline would be the same. <br/><br/>
Therefore, I would like to create a pipeline starting with solids A and B, and a downstream solid C executed based on input from solid A <strong>OR</strong> solid B.<br/>
However, when using conditional outputs on solids A and B, solid C is not executed, as one input is not generated by upstream solids. <br/><br/>
Is there a simple way of doing this that I am missing out? <br/><br/>
Thanks for your help.</p>
",1,1607681835,dagster,True,631,2,1607715379,https://stackoverflow.com/questions/65249440/executing-a-solid-when-at-least-one-of-the-required-inputs-is-given
65022207,How to use dictionary yielded from other solid in a composite Solid?,"<p>For example I have a solid named <strong>initiate_load</strong> , it is yielding a <strong>dictionary</strong> and an <strong>integer</strong> , something like :</p>
<pre class=""lang-py prettyprint-override""><code>@solid(
    output_defs=[
        OutputDefinition(name='l_dict', is_required=False),
        OutputDefinition(name='l_int', is_required=False)
    ],
)
def initiate_load(context):
    .... 
    ....

    yield Output(l_dict, output_name='l_dict')
    yield Output(l_int, output_name='l_int')

</code></pre>
<p>I have a composite_solid also ,let's say  <strong>call_other_solid_composite</strong></p>
<p>and I am passing <code>l_dict</code> and <code>l_int</code>  to this <code>composite_solid</code>
and I am using the <code>l_dict</code> to get the values mapped to its keys. Something like.</p>
<pre class=""lang-py prettyprint-override""><code>@composite_solid
def call_other_solid_composite(p_dict,p_int):
    l_val1 = p_dict['val1']
    ...
    ...

</code></pre>
<p>Then I am getting a Error as :<code>TypeError: 'InputMappingNode' object is not subscriptable</code>.
I searched everywhere but can't find a solution . The documentation is also of no help . I have use case that I need to parse those values .
Any help will be appreciated.</p>
",4,1606394291,python;dagster,True,464,1,1607274117,https://stackoverflow.com/questions/65022207/how-to-use-dictionary-yielded-from-other-solid-in-a-composite-solid
64970261,Dagster: how to reexecute failed steps of a pipeline?,"<p>I created a test pipeline and it fails mid-way. I want to programmatically re-execute it but starting at the failed step of the pipeline and move forward. I do not want to repeat execution of the earlier, successful steps.</p>
<pre><code>from dagster import DagsterInstance, execute_pipeline, pipeline, solid, reexecute_pipeline
from random import random

instance = DagsterInstance.ephemeral()

@solid
def step1(context, data):
   return range(10), ('a' + i for i in range(10))

@solid
def step2(context, step1op):
  x,y = step1op

  # simulation of noise
  xx = [el * (1 + 0.1 * random()) for el in x]
  xx2 = [(el - 1)/el for el in xx]
  return zip(xx, xx2), y

@solid
def step3(context, step2op):
   x, y = step2op

   ...
   return x, y


run_config = {...}


@pipeline
def inputs_pipeline():
     step3(step2(step1()))
</code></pre>
",1,1606141276,python;python-3.x;pipeline;dagster,True,720,1,1606141276,https://stackoverflow.com/questions/64970261/dagster-how-to-reexecute-failed-steps-of-a-pipeline
64883988,Testing a dagster pipeline,"<p><strong>Summary</strong>: Dagster run configurations for Dagit vs. PyTest appear to be incompatible for my project</p>
<p>I've been getting errors trying to run pytest on a pipeline and I'd really appreciate any pointers. I've consistently gotten errors of the form:</p>
<pre><code>dagster.core.errors.DagsterInvalidConfigError: 
Error in config for pipeline ephemeral_write_myfunc_to_redis_solid_pipeline
Error 1: Undefined field &quot;myfunc_df_to_list&quot; at path root:solids. 
Expected: &quot;{ myfunc_list?: { outputs?: [{ result?: { json: { path: String } pickle: { path: String } } }] } 
write_myfunc_to_redis?:...&quot;
</code></pre>
<p>A few notes about the project:</p>
<ul>
<li>dagster, version 0.9.15</li>
<li><strong>my pipeline runs in Dagit without errors for the same configuration</strong></li>
<li>the unit tests run for the individual solids that comprise the pipeline</li>
</ul>
<p><strong>Failed solutions:</strong> I've tried populating the configuration files with solids that define the outputs as each pytest error has recommended, but they all have led to errors more opaque than the one before it.</p>
<p>My solids are:</p>
<pre><code>@solid(required_resource_keys={&quot;db&quot;})
def get_myfunc_df(context, query: String) -&gt; myfuncDF:
    do something
    return myfuncDF

@solid
def myfunc_df_to_list(context, df: myfuncDF) -&gt; List:
    do something
    return List

@solid(required_resource_keys={&quot;redis&quot;})
def write_myfunc_to_redis(context, myfunc_list:List) -&gt; None:
    write to redis return None
</code></pre>
<p>And my pipeline is a chain of these solids</p>
<pre><code>@pipeline(
    mode_defs=filter_modes(MODES),
    preset_defs=filter_presets(PRESETS),
    tags={&quot;type&quot;: &quot;myproject&quot;},
)
def myfunc_to_redis_pipeline():
    df = get_myfunc_df()
    myfunc_list = myfunc_df_to_list(df)
    write_myfunc_to_redis(myfunc_list)
</code></pre>
<p>My test code in test_main.py is</p>
<pre><code>    @pytest.mark.myfunc
    def test_myfunc_to_redis_pipeline(self):
        res = execute_pipeline(myfunc_to_redis_pipeline,
                               preset=&quot;test&quot;,)
        assert res.success
        assert len(res.solid_result_list) == 4
        for solid_res in res.solid_result_list:
            assert solid_res.success
</code></pre>
<p>Where the preset &quot;test&quot; is defined with the run configuration in a yaml file:</p>
<pre><code>resources:
  db:
    config:
      file_path: test.csv
</code></pre>
<p>^ This is where it's throwing the most errors and I've been iterating through different permutations of solids to add ala:</p>
<pre><code>solids:
  get_myfunc_df:
    inputs:
      query:
        value: select 1
</code></pre>
<p>but it hasn't solved the problem yet. Is there any reason the Solids for a test would need their output defined despite the fact that while running in Dagit, only the input solid needs to have a definition?</p>
<p>Is this error indicative of something else being amiss?</p>
<p>edit: Here is the stack trace from tox --verbose</p>
<pre><code>self = &lt;repos.myfunc.myfunc.dagster.tests.test_main.Test_myfunc testMethod=test_myfunc_df&gt;

    @pytest.mark.myfunc
    def test_myfunc_df(self):
        &quot;&quot;&quot;myfunc&quot;&quot;&quot;
        result = execute_solid(
            get_myfunc_df,
            mode_def=test_mode,
            run_config=run_config,
&gt;           input_values={&quot;query&quot;: &quot;SELECT 1&quot;},
        )

repos/myfunc/myfunc/dagster/tests/test_main.py:29:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
.tox/repo-myfunc/lib/python3.7/site-packages/dagster/utils/test/__init__.py:324: in execute_solid
    raise_on_error=raise_on_error,
.tox/repo-myfunc/lib/python3.7/site-packages/dagster/core/execution/api.py:335: in execute_pipeline
    raise_on_error=raise_on_error,
.tox/repo-myfunc/lib/python3.7/site-packages/dagster/core/telemetry.py:90: in wrap
    result = f(*args, **kwargs)
.tox/repo-myfunc/lib/python3.7/site-packages/dagster/core/execution/api.py:375: in _logged_execute_pipeline
    tags=tags,
.tox/repo-myfunc/lib/python3.7/site-packages/dagster/core/instance/__init__.py:586: in create_run_for_pipeline
    pipeline_def, run_config=run_config, mode=mode,
.tox/repo-myfunc/lib/python3.7/site-packages/dagster/core/execution/api.py:644: in create_execution_plan
    environment_config = EnvironmentConfig.build(pipeline_def, run_config, mode=mode)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

pipeline_def = &lt;dagster.core.definitions.pipeline.PipelineDefinition object at 0x1359f6210&gt;
run_config = {'resources': {'ge_data_context': {'config': {'ge_root_dir': '/Users/this_user/Workspace/drizly-dagster/repos/datas...cause_you_bought/dagster/tests/test.csv'}}}, 'solids': {'get_myfunc_df': {'inputs': {'query': {'value': 'select 1'}}}}}
mode = 'test'

    @staticmethod
    def build(pipeline_def, run_config=None, mode=None):
        &quot;&quot;&quot;This method validates a given run config against the pipeline config schema. If
        successful, we instantiate an EnvironmentConfig object.

        In case the run_config is invalid, this method raises a DagsterInvalidConfigError
        &quot;&quot;&quot;
        from dagster.config.validate import process_config
        from dagster.core.definitions.executor import ExecutorDefinition
        from dagster.core.definitions.intermediate_storage import IntermediateStorageDefinition
        from dagster.core.definitions.system_storage import SystemStorageDefinition
        from .composite_descent import composite_descent

        check.inst_param(pipeline_def, &quot;pipeline_def&quot;, PipelineDefinition)
        run_config = check.opt_dict_param(run_config, &quot;run_config&quot;)
        check.opt_str_param(mode, &quot;mode&quot;)

        mode = mode or pipeline_def.get_default_mode_name()
        environment_type = create_environment_type(pipeline_def, mode)

        config_evr = process_config(environment_type, run_config)
        if not config_evr.success:
            raise DagsterInvalidConfigError(
                &quot;Error in config for pipeline {}&quot;.format(pipeline_def.name),
                config_evr.errors,
&gt;               run_config,
            )
E           dagster.core.errors.DagsterInvalidConfigError: Error in config for pipeline ephemeral_get_myfunc_df_solid_pipeline
E               Error 1: Undefined field &quot;inputs&quot; at path root:solids:get_myfunc_df. Expected: &quot;{ outputs?: [{ result?: { csv: { path: (String | { env: String }) sep?: (String | { env: String }) } parquet: { path: (String | { env: String }) } pickle: { path: (String | { env: String }) } table: { path: (String | { env: String }) } } }] }&quot;.

.tox/repo-myfunc/lib/python3.7/site-packages/dagster/core/system_config/objects.py:101: DagsterInvalidConfigError
_______________________________________________________________________ Test_myfunc.test_write_myfunc_to_redis ________________________________________________________________________

self = &lt;repos.myfunc.myfunc.dagster.tests.test_main.Test_myfunc testMethod=test_write_myfunc_to_redis&gt;

    @pytest.mark.myfunc
    def test_write_myfunc_to_redis(self):
        &quot;&quot;&quot;Test redis write&quot;&quot;&quot;
        records = [
            (&quot;k&quot;, &quot;v&quot;),
            (&quot;k2&quot;, &quot;v2&quot;),
        ]
        result = execute_solid(
            write_myfunc_to_redis,
            mode_def=test_mode,
            input_values={&quot;myfunc_list&quot;: records},
&gt;           run_config=run_config,
        )

repos/myfunc/myfunc/dagster/tests/test_main.py:56:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
.tox/repo-myfunc/lib/python3.7/site-packages/dagster/utils/test/__init__.py:324: in execute_solid
    raise_on_error=raise_on_error,
.tox/repo-myfunc/lib/python3.7/site-packages/dagster/core/execution/api.py:335: in execute_pipeline
    raise_on_error=raise_on_error,
.tox/repo-myfunc/lib/python3.7/site-packages/dagster/core/telemetry.py:90: in wrap
    result = f(*args, **kwargs)
.tox/repo-myfunc/lib/python3.7/site-packages/dagster/core/execution/api.py:375: in _logged_execute_pipeline
    tags=tags,
.tox/repo-myfunc/lib/python3.7/site-packages/dagster/core/instance/__init__.py:586: in create_run_for_pipeline
    pipeline_def, run_config=run_config, mode=mode,
.tox/repo-myfunc/lib/python3.7/site-packages/dagster/core/execution/api.py:644: in create_execution_plan
    environment_config = EnvironmentConfig.build(pipeline_def, run_config, mode=mode)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

pipeline_def = &lt;dagster.core.definitions.pipeline.PipelineDefinition object at 0x135d39490&gt;
run_config = {'resources': {'ge_data_context': {'config': {'ge_root_dir': '/Users/this_user/Workspace/drizly-dagster/repos/datas...cause_you_bought/dagster/tests/test.csv'}}}, 'solids': {'get_myfunc_df': {'inputs': {'query': {'value': 'select 1'}}}}}
mode = 'test'

    @staticmethod
    def build(pipeline_def, run_config=None, mode=None):
        &quot;&quot;&quot;This method validates a given run config against the pipeline config schema. If
        successful, we instantiate an EnvironmentConfig object.

        In case the run_config is invalid, this method raises a DagsterInvalidConfigError
        &quot;&quot;&quot;
        from dagster.config.validate import process_config
        from dagster.core.definitions.executor import ExecutorDefinition
        from dagster.core.definitions.intermediate_storage import IntermediateStorageDefinition
        from dagster.core.definitions.system_storage import SystemStorageDefinition
        from .composite_descent import composite_descent

        check.inst_param(pipeline_def, &quot;pipeline_def&quot;, PipelineDefinition)
        run_config = check.opt_dict_param(run_config, &quot;run_config&quot;)
        check.opt_str_param(mode, &quot;mode&quot;)

        mode = mode or pipeline_def.get_default_mode_name()
        environment_type = create_environment_type(pipeline_def, mode)

        config_evr = process_config(environment_type, run_config)
        if not config_evr.success:
            raise DagsterInvalidConfigError(
                &quot;Error in config for pipeline {}&quot;.format(pipeline_def.name),
                config_evr.errors,
&gt;               run_config,
            )
E           dagster.core.errors.DagsterInvalidConfigError: Error in config for pipeline ephemeral_write_myfunc_to_redis_solid_pipeline
E               Error 1: Undefined field &quot;get_myfunc_df&quot; at path root:solids. Expected: &quot;{ myfunc_list?: { outputs?: [{ result?: { json: { path: String } pickle: { path: String } } }] } write_myfunc_to_redis?: { outputs?: [{ result?: { json: { path: String } pickle: { path: String } } }] } }&quot;.

.tox/repo-myfunc/lib/python3.7/site-packages/dagster/core/system_config/objects.py:101: DagsterInvalidConfigError
=============================================================================== short test summary info ===============================================================================
FAILED repos/myfunc/myfunc/dagster/tests/test_main.py::Test_myfunc::test_myfunc_df - dagster.core.errors.DagsterInvalidConfigError: Error in config for pipeli...
FAILED repos/myfunc/myfunc/dagster/tests/test_main.py::Test_myfunc::test_write_myfunc_to_redis - dagster.core.errors.DagsterInvalidConfigError: Error in conf
</code></pre>
<p><strong>Solution below works</strong>
The key issue was that the pipeline required solids to be defined in the config as written and the solids were being passed both that same config and input_values in their test function. My change was to remove &quot;input_values&quot; as an argument and pass them via the run configuration. Since my interstitial solids require more complex objects and my configuration file is yaml, I made the following addition to all of my solid tests:</p>
<pre><code>        this_solid_run_config = copy.deepcopy(run_config)
        input_dict = {&quot;df&quot;: pd.DataFrame(['1', '2'], columns = ['key', 'value'])}
        this_solid_run_config.update({&quot;solids&quot;:
                                  {&quot;myfunc_df_to_list&quot;:
                                       {&quot;inputs&quot;:input_dict
                                                  }
                                   }
                              }
                             )
</code></pre>
",2,1605650465,dagster,True,1841,1,1606067042,https://stackoverflow.com/questions/64883988/testing-a-dagster-pipeline
59379523,dagster pipeline executes successfully when run with `execute_pipeline` but not when run with dagit,"<p>I'm running into a <code>LoweringError</code> that has to do with <code>numba</code> compilation when running a <code>dagster</code> pipeline through <code>dagit</code>, but not when run directly with <code>execute_pipeline</code>. Not really sure how to go about debugging it.</p>

<p>Minimal working example (file <code>dagster_umap_pipeline.py</code>)</p>

<pre><code>from dagster import solid, pipeline
from umap import UMAP
import numpy as np


@solid
def random_array(context):
    return np.random.rand(1000, 100)


@solid
def fit_umap(context, X):
    model = UMAP(15, 15)
    model.fit(X)
    return model


@pipeline
def fit_umap_pipeline():
    X = random_array()
    model = fit_umap(X)
</code></pre>

<p>Running from the python interpreter works just fine:</p>

<pre><code>&gt;&gt; from dagster import execute_pipeline
&gt;&gt; from dagster_umap_pipeline import fit_umap_pipeline
&gt;&gt; result = execute_pipeline(fit_umap_pipeline)
&gt;&gt; assert result.success  # This passes
</code></pre>

<p>From dagit, the pipeline fails:</p>

<p><code>dagit -f dagster_umap_pipeline.py -n fit_umap_pipeline</code></p>

<p><code>fit_umap.compute</code> fails with the following error log:</p>

<pre><code>numba.errors.LoweringError: Failed in nopython mode pipeline (step: nopython mode backend) [1m[1mgenerator didn't yield [1m File ""../miniconda3/envs/platewatch/lib/python3.6/site-packages/umap/umap_.py"", line 331:[0m [1mdef compute_membership_strengths(knn_indices, knn_dists, sigmas, rhos): &lt;source elided&gt; rows = np.zeros((n_samples * n_neighbors), dtype=np.int64) [1m cols = np.zeros((n_samples * n_neighbors), dtype=np.int64) [0m [1m^[0m[0m [0m [0m[1m[1] During: lowering ""id=1[LoopNest(index_variable = parfor_index.31, range = (0, $0.22, 1))]{281: &lt;ir.Block at /Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/umap/umap_.py (331)&gt;}Var(parfor_index.31, /Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/umap/umap_.py (331))"" at /Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/umap/umap_.py (331)[0m ------------------------------------------------------------------------------- This should not have happened, a problem has occurred in Numba's internals. You are currently using Numba version 0.46.0. Please report the error message and traceback, along with a minimal reproducer at: https://github.com/numba/numba/issues/new If more help is needed please feel free to speak to the Numba core developers directly at: https://gitter.im/numba/numba Thanks in advance for your help in improving Numba!
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/dagster/core/errors.py"", line 114, in user_code_error_boundary
    yield
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/dagster/core/engine/engine_inprocess.py"", line 635, in _user_event_sequence_for_step_compute_fn
    for event in gen:
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/dagster/core/execution/plan/compute.py"", line 75, in _execute_core_compute
    for step_output in _yield_compute_results(compute_context, inputs, compute_fn):
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/dagster/core/execution/plan/compute.py"", line 52, in _yield_compute_results
    for event in user_event_sequence:
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/dagster/core/definitions/decorators.py"", line 418, in compute
    result = fn(context, **kwargs)
  File ""dagster_umap_pipeline.py"", line 14, in fit_umap
    model.fit(X)
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/umap/umap_.py"", line 1417, in fit
    self.verbose,
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/umap/umap_.py"", line 478, in fuzzy_simplicial_set
    knn_indices, knn_dists, sigmas, rhos
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/numba/dispatcher.py"", line 420, in _compile_for_args
    raise e
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/numba/dispatcher.py"", line 353, in _compile_for_args
    return self.compile(tuple(argtypes))
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/numba/compiler_lock.py"", line 32, in _acquire_compile_lock
    return func(*args, **kwargs)
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/numba/dispatcher.py"", line 768, in compile
    cres = self._compiler.compile(args, return_type)
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/numba/dispatcher.py"", line 77, in compile
    status, retval = self._compile_cached(args, return_type)
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/numba/dispatcher.py"", line 91, in _compile_cached
    retval = self._compile_core(args, return_type)
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/numba/dispatcher.py"", line 109, in _compile_core
    pipeline_class=self.pipeline_class)
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/numba/compiler.py"", line 528, in compile_extra
    return pipeline.compile_extra(func)
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/numba/compiler.py"", line 326, in compile_extra
    return self._compile_bytecode()
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/numba/compiler.py"", line 385, in _compile_bytecode
    return self._compile_core()
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/numba/compiler.py"", line 365, in _compile_core
    raise e
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/numba/compiler.py"", line 356, in _compile_core
    pm.run(self.state)
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/numba/compiler_machinery.py"", line 328, in run
    raise patched_exception
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/numba/compiler_machinery.py"", line 319, in run
    self._runPass(idx, pass_inst, state)
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/numba/compiler_lock.py"", line 32, in _acquire_compile_lock
    return func(*args, **kwargs)
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/numba/compiler_machinery.py"", line 281, in _runPass
    mutated |= check(pss.run_pass, internal_state)
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/numba/compiler_machinery.py"", line 268, in check
    mangled = func(compiler_state)
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/numba/typed_passes.py"", line 380, in run_pass
    NativeLowering().run_pass(state) # TODO: Pull this out into the pipeline
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/numba/typed_passes.py"", line 325, in run_pass
    lower.lower()
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/numba/lowering.py"", line 179, in lower
    self.lower_normal_function(self.fndesc)
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/numba/lowering.py"", line 220, in lower_normal_function
    entry_block_tail = self.lower_function_body()
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/numba/lowering.py"", line 245, in lower_function_body
    self.lower_block(block)
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/numba/lowering.py"", line 260, in lower_block
    self.lower_inst(inst)
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/contextlib.py"", line 99, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/numba/errors.py"", line 725, in new_error_context
    six.reraise(type(newerr), newerr, tb)
  File ""/Users/ben.fogelson/miniconda3/envs/platewatch/lib/python3.6/site-packages/numba/six.py"", line 669, in reraise
    raise value
</code></pre>

<p>Relevant package versions:</p>

<pre><code>umap-learn                0.3.10                   py36_0    conda-forge
numba                     0.46.0           py36h6440ff4_0
dagster                   0.6.6                    py36_0    conda-forge
dagit                     0.6.6                      py_0    conda-forge
</code></pre>
",0,1576604742,python;numba;dagster,True,1480,2,1605882553,https://stackoverflow.com/questions/59379523/dagster-pipeline-executes-successfully-when-run-with-execute-pipeline-but-not
64877353,"How can you ensure, that the same pipeline is not executed twice at the same time","<p>Hey :) i have a questions in regards to locking or mutex behavior.</p>
<p><strong>Scenarios</strong>:</p>
<p>Lets assume the following scenarios:</p>
<ol>
<li>The pipeline is working with some local files. These files were placed by CI-CD jobs. After processing i'd like to remove the files. This would result in a race condition if the job takes longer than the schedule interval</li>
<li>two pipelines are very resource heavy and therefore cannot be run in parallel.</li>
</ol>
<p><strong>possible solutions</strong></p>
<ul>
<li>Currently i would use some kind of Mutex or Lock either in a running service, where pipelines can register and are allowed to be executed or not.</li>
<li>duplicate the data to ensure that each workflow can cleanup and use their own data.</li>
<li>create a local lock file and ensure that the file will be removed if successful.</li>
<li>create a smaller schedule interval and check if lock exist. Exit cleanly if condition is not fulfilled.</li>
</ul>
<p>I know that this might not be a normal use case for dagster, but i'd also like to use dagster for other workflows such as cleanup tasks and trigger of other pipelines.</p>
<p>Thanks</p>
",0,1605623956,python;dagster,True,1157,3,1605662789,https://stackoverflow.com/questions/64877353/how-can-you-ensure-that-the-same-pipeline-is-not-executed-twice-at-the-same-tim
60970993,Cross Validation using Dagster,"<p>I've started using Dagster in our ML pipeline, and am running into some basic issues that I'm wondering if I'm missing something trivial here or if this is just how it is...</p>

<p>Say I have a simple ML pipepline:</p>

<pre><code>Load raw data --&gt; Process data into table --&gt; Split train / test --&gt; train model --&gt; evaluate model.
</code></pre>

<p>A linear model is straight forward in Dagster. But what if I want to add a little loop, say for cross-validation purposes:</p>

<pre><code>Load raw data --&gt; Process data into table --&gt; Split into k folds, and for each fold:
  - fold 1: train model --&gt; evaluate
  - fold 2: train model --&gt; evaluate
  - fold 3: train model --&gt; evaluate
  --&gt; summarize cross validation results.
</code></pre>

<p>Is there a nice &amp; clean way to do this in Dagster? The way I've been doing things is:</p>

<pre><code>Load raw data --&gt; Process data into table --&gt; Split into K folds --&gt; choose fold k --&gt; train model --&gt; evaluate model
</code></pre>

<p>With the fold ""k"" as an input parameter for the pipeline. And then running the pipeline K times.</p>

<p>What am I missing here?</p>
",2,1585744464,python;machine-learning;architecture;pipeline;dagster,True,611,1,1604611370,https://stackoverflow.com/questions/60970993/cross-validation-using-dagster
62599568,Dagster failure notification systems,"<p>Is there a way in dagster to receive notifications when certain events occur, such as failures? For example, is there an integration with a tool like sentry available?</p>
",1,1593191047,sentry;dagster,True,1685,2,1603476043,https://stackoverflow.com/questions/62599568/dagster-failure-notification-systems
63691520,Dagster: Multiple and Conditional Outputs (Type check failed for step output xxx PySparkDataFrame),"<p>I'm executing the Dagster tutorial, and I got stuck at the <a href=""https://docs.dagster.io/tutorial/basics_solids#multiple-and-conditional-outputs"" rel=""nofollow noreferrer"">Multiple and Conditional Outputs</a> step.</p>
<p>In the <code>solid</code> definitions, it asks to declare (among other things):</p>
<pre><code>output_defs=[
    OutputDefinition(
        name=&quot;hot_cereals&quot;, dagster_type=DataFrame, is_required=False
    ),
    OutputDefinition(
        name=&quot;cold_cereals&quot;, dagster_type=DataFrame, is_required=False
    ),
],
</code></pre>
<p>But there's no information where the <code>DataFrame</code> cames from.
Firstly I have tried with <code>pandas.DataFrame</code> but I faced the error: <code>{dagster_type} is not a valid dagster type</code>. It happens when I try to submit it via <code>$ dagit -f multiple_outputs.py</code>.
Then I installed the <code>dagster_pyspark</code> and gave a try with the <code>dagster_pyspark.DataFrame</code>. This time I managed to summit the DAG to the UI. However, when I run it from the UI, I got the following error:</p>
<pre><code>dagster.core.errors.DagsterTypeCheckDidNotPass: Type check failed for step output hot_cereals of type PySparkDataFrame.
  File &quot;/Users/bambrozio/.local/share/virtualenvs/dagster-tutorial/lib/python3.7/site-packages/dagster/core/execution/plan/execute_plan.py&quot;, line 210, in _dagster_event_sequence_for_step
    for step_event in check.generator(step_events):
  File &quot;/Users/bambrozio/.local/share/virtualenvs/dagster-tutorial/lib/python3.7/site-packages/dagster/core/execution/plan/execute_step.py&quot;, line 273, in core_dagster_event_sequence_for_step
    for evt in _create_step_events_for_output(step_context, user_event):
  File &quot;/Users/bambrozio/.local/share/virtualenvs/dagster-tutorial/lib/python3.7/site-packages/dagster/core/execution/plan/execute_step.py&quot;, line 298, in _create_step_events_for_output
    for output_event in _type_checked_step_output_event_sequence(step_context, output):
  File &quot;/Users/bambrozio/.local/share/virtualenvs/dagster-tutorial/lib/python3.7/site-packages/dagster/core/execution/plan/execute_step.py&quot;, line 221, in _type_checked_step_output_event_sequence
    dagster_type=step_output.dagster_type,
</code></pre>
<p>Does anyone know how to fix it? Thanks for the help!</p>
",0,1598976805,dagster,False,1938,3,1601439183,https://stackoverflow.com/questions/63691520/dagster-multiple-and-conditional-outputs-type-check-failed-for-step-output-xxx
62448972,Noneable type for Field in solid config not allowing null values,"<p>I would like Dagster to accept empty parameters in the config.yaml and treat them as having a value of None.</p>

<p>When I start dagit I can see that the parameter is null. This makes sense because I've left the value of the parameter empty in the config.yaml.
<a href=""https://i.stack.imgur.com/UpSHE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UpSHE.png"" alt=""Dagit shows the parameter as null""></a></p>

<p>However, when I execute the pipeline I get the following error:
<a href=""https://i.stack.imgur.com/UR0xd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UR0xd.png"" alt=""The error shows it is expecting a string type""></a></p>

<p>I'm not sure why it's expecting a value of class str when I've specified Noneable(str) type in the solid configuration. And funnily enough, when I pass in a non-null value for <em>example_parameter</em> in dagit, the pipeline executes perfectly fine.</p>

<p>Below is the config I used for the solid</p>

<pre><code>@solid(
    description=""Example solid"",
    config={
        ""example_parameter"": Field(
                 Noneable(str),
                 is_required=False
        )
    }
)
def example_solid(context):
""""""an example solid""""""
   etc...
</code></pre>

<p>How can I make dagster accept null values and parse them as None?</p>
",0,1592479686,config;nonetype;valueerror;dagster,False,381,1,1592588440,https://stackoverflow.com/questions/62448972/noneable-type-for-field-in-solid-config-not-allowing-null-values
61330816,How would you parameterize Dagster pipelines to run same solids with multiple different configurations/assets?,"<p>Let's say I create a Dagster pipeline with the following solids:</p>

<ol>
<li>Execute SQL query from file and get results</li>
<li>Write results to a table</li>
</ol>

<p>I want to do this for say 10 different tables in parallel. Each table requiring a different SQL query.
What would be the best approach?</p>
",7,1587412032,python;dagster,True,3491,1,1591407566,https://stackoverflow.com/questions/61330816/how-would-you-parameterize-dagster-pipelines-to-run-same-solids-with-multiple-di
62076897,Is the working directory of the dagster main process different of the scheduler processes,"<p>I'm having an issue with the loading of a file from dagster code (setup, not pipelines). Say I have the following project structure:</p>

<pre><code>pipelines
-app/
--environments
----schedules.yaml
--repository.py
--repository.yaml
</code></pre>

<p>When I run dagit while inside the project folder(<code>$cd project &amp;&amp; dagit -y app/repository.yaml</code>), this folder becomes the working dir and inside the <code>repository.py</code> I could load a file knowing the root is <code>project</code></p>

<pre class=""lang-py prettyprint-override""><code># repository.py

with open('app/evironments/schedules.yaml', 'r'):
   # do something with the file
</code></pre>

<p>However, if I set up a schedule the pipelines in the project do not run. Checking the cron logs it seems the <code>open</code> line throws a file not found exception. I was wondering if this happens because the working directory is different when executing the cron.</p>

<p>For context, I'm loading a config file with parameters of cron_schedules for each pipeline. Also, here's the tail of the stacktrace in my case:</p>

<pre><code>  File ""/home/user/.local/share/virtualenvs/pipelines-mfP13m0c/lib/python3.8/site-packages/dagster/core/definitions/handle.py"", line 190, in from_yaml
    return LoaderEntrypoint.from_file_target(
  File ""/home/user/.local/share/virtualenvs/pipelines-mfP13m0c/lib/python3.8/site-packages/dagster/core/definitions/handle.py"", line 161, in from_file_target
    module = import_module_from_path(module_name, os.path.abspath(python_file))
  File ""/home/user/.local/share/virtualenvs/pipelines-mfP13m0c/lib/python3.8/site-packages/dagster/seven/__init__.py"", line 75, in import_module_from_path
    spec.loader.exec_module(module)
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 783, in exec_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 219, in _call_with_frames_removed
  File ""/home/user/pipelines/app/repository.py"", line 28, in &lt;module&gt;
    schedule_builder = ScheduleBuilder(settings.CRON_PRESET, settings.ENV_DICT)
  File ""/home/user/pipelines/app/schedules.py"", line 12, in __init__
    self.cron_schedules = self._load_schedules_yaml()
  File ""/home/user/pipelines/app/schedules.py"", line 16, in _load_schedules_yaml
    with open(path) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'app/environments/schedules.yaml'
</code></pre>
",1,1590713402,dagster,True,2030,1,1590717364,https://stackoverflow.com/questions/62076897/is-the-working-directory-of-the-dagster-main-process-different-of-the-scheduler
62025039,How to avoid running the rest of a dagster pipeline under certain conditions,"<p>say I have two solids in Dagster connected on a pipeline. The first solid may do some process and generate a valid input so that the rest of the pipeline executes, or generate an invalid input that should not be further processed. To achieve this result, I raise an error when the data meets the invalid condition so the pipeline stops and the rest of the solids are skipped. </p>

<p>Raising an error to solve my use case seems hacky, is there a way that I can skip the execution of the rest of the pipeline without resorting to exceptions?</p>

<pre class=""lang-py prettyprint-override""><code>from dagster import solid, pipeline

@solid
def solid_1(context, x: int):
    y = x + 1

    if y%2 == 0:
        raise ""No even number is further processed""

    return y

@solid
def solid_2(context, y:int):
    return y**2

@pipeline
def toy_pipeline():
    solid_2(solid_1())
</code></pre>

<p>In this quite contrived example, the solid 2 should only be executed when the output from the first solid is odd.</p>

<p>In my actual use case, the first solid polls a DB and sometimes finds no data to process. In this case it makes sense not to mark the execution as failed but rather as a success. It could be possible to check in each downstream solid whether the data meets the conditions, yet this quickly adds boilerplate. It would be ideal to have a way to skip the execution of all downstream solids when the solid that receives the data finds no data to process.</p>
",4,1590504933,dagster,True,4296,1,1590517348,https://stackoverflow.com/questions/62025039/how-to-avoid-running-the-rest-of-a-dagster-pipeline-under-certain-conditions
59507906,Error: &#39;dagster.core.types.runtime&#39; is not a package,"<p>Installed dagster for the first time in a conda environment and tried to run the airline demo as described <a href=""https://dagster.readthedocs.io/en/0.6.6/sections/learn/airline_demo.html"" rel=""nofollow noreferrer"">here</a>.  The following are the steps I followed.</p>

<pre><code>conda env create -n dagster python=3.7
conda activate dagster
pip install dagster dagit
git clone git@github.com/dagster-io/dagster.git
cd dagster/examples
pip install -e '[.full]'
docker-compose up -d
cd dagster_examples/airline_demo
dagit
</code></pre>

<p>I then get the following stack trace ending in the <code>'dagster.core.types.runtime' is not a package</code> message:</p>

<pre><code>Loading repository...
Traceback (most recent call last):
  File ""/Users/timrozmajzl/miniconda3/envs/dagster/bin/dagit-cli"", line 8, in &lt;module&gt;
    sys.exit(main())
  File ""/Users/timrozmajzl/miniconda3/envs/dagster/lib/python3.7/site-packages/dagit/cli.py"", line 110, in main
    cli = create_dagit_cli()
  File ""/Users/timrozmajzl/miniconda3/envs/dagster/lib/python3.7/site-packages/dagit/cli.py"", line 21, in create_dagit_cli
    return ui(auto_envvar_prefix='DAGIT')  # pylint: disable=no-value-for-parameter
  File ""/Users/timrozmajzl/miniconda3/envs/dagster/lib/python3.7/site-packages/click/core.py"", line 764, in __call__
    return self.main(*args, **kwargs)
  File ""/Users/timrozmajzl/miniconda3/envs/dagster/lib/python3.7/site-packages/click/core.py"", line 717, in main
    rv = self.invoke(ctx)
  File ""/Users/timrozmajzl/miniconda3/envs/dagster/lib/python3.7/site-packages/click/core.py"", line 956, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/Users/timrozmajzl/miniconda3/envs/dagster/lib/python3.7/site-packages/click/core.py"", line 555, in invoke
    return callback(*args, **kwargs)
  File ""/Users/timrozmajzl/miniconda3/envs/dagster/lib/python3.7/site-packages/dagit/cli.py"", line 72, in ui
    host_dagit_ui(handle, host, port, storage_fallback, reload_trigger)
  File ""/Users/timrozmajzl/miniconda3/envs/dagster/lib/python3.7/site-packages/dagit/cli.py"", line 81, in host_dagit_ui
    app = create_app(handle, instance, reloader)
  File ""/Users/timrozmajzl/miniconda3/envs/dagster/lib/python3.7/site-packages/dagit/app.py"", line 160, in create_app
    version=__version__,
  File ""/Users/timrozmajzl/miniconda3/envs/dagster/lib/python3.7/site-packages/dagster_graphql/implementation/context.py"", line 17, in __init__
    self.repository_definition = self.get_handle().build_repository_definition()
  File ""/Users/timrozmajzl/miniconda3/envs/dagster/lib/python3.7/site-packages/dagster/core/definitions/handle.py"", line 392, in build_repository_definition
    obj = self.entrypoint.perform_load()
  File ""/Users/timrozmajzl/miniconda3/envs/dagster/lib/python3.7/site-packages/dagster/core/definitions/handle.py"", line 445, in entrypoint
    return self.data.get_repository_entrypoint(from_handle=self)
  File ""/Users/timrozmajzl/miniconda3/envs/dagster/lib/python3.7/site-packages/dagster/core/definitions/handle.py"", line 522, in get_repository_entrypoint
    return LoaderEntrypoint.from_yaml(self.repository_yaml, from_handle=from_handle)
  File ""/Users/timrozmajzl/miniconda3/envs/dagster/lib/python3.7/site-packages/dagster/core/definitions/handle.py"", line 175, in from_yaml
    return LoaderEntrypoint.from_module_target(module_name, fn_name, from_handle)
  File ""/Users/timrozmajzl/miniconda3/envs/dagster/lib/python3.7/site-packages/dagster/core/definitions/handle.py"", line 161, in from_module_target
    module = importlib.import_module(module_name)
  File ""/Users/timrozmajzl/miniconda3/envs/dagster/lib/python3.7/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""&lt;frozen importlib._bootstrap&gt;"", line 1006, in _gcd_import
  File ""&lt;frozen importlib._bootstrap&gt;"", line 983, in _find_and_load
  File ""&lt;frozen importlib._bootstrap&gt;"", line 967, in _find_and_load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 677, in _load_unlocked
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 728, in exec_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 219, in _call_with_frames_removed
  File ""/Users/timrozmajzl/Projects/dagster/dagster/examples/dagster_examples/airline_demo/repository.py"", line 3, in &lt;module&gt;
    from .pipelines import define_airline_demo_ingest_pipeline, define_airline_demo_warehouse_pipeline
  File ""/Users/timrozmajzl/Projects/dagster/dagster/examples/dagster_examples/airline_demo/pipelines.py"", line 14, in &lt;module&gt;
    from .resources import postgres_db_info_resource, redshift_db_info_resource
  File ""/Users/timrozmajzl/Projects/dagster/dagster/examples/dagster_examples/airline_demo/resources.py"", line 3, in &lt;module&gt;
    from .types import DbInfo
  File ""/Users/timrozmajzl/Projects/dagster/dagster/examples/dagster_examples/airline_demo/types.py"", line 8, in &lt;module&gt;
    from dagster.core.types.runtime.runtime_type import create_string_type
ModuleNotFoundError: No module named 'dagster.core.types.runtime.runtime_type'; 'dagster.core.types.runtime' is not a package
</code></pre>
",1,1577504316,python;dagster,False,796,1,1589241533,https://stackoverflow.com/questions/59507906/error-dagster-core-types-runtime-is-not-a-package
59048161,Integrating Dagster with Django,"<p>Hi I am trying to integrate <a href=""https://dagster.readthedocs.io/en/0.6.4/index.html"" rel=""nofollow noreferrer"">Dagster</a> into ongoing Django project. I am kind of struggling with providing Django context (models, apps, ...) to Dagster. As of now I am just checking wether <code>dagit</code> is present in <code>sys.argv[0]</code> in <em><strong>init</strong>.py</em> of apps that are using Dagster.</p>

<pre><code>&lt;!-- language: python --&gt;
## project/app/__init__.py
import sys

import django

if 'dagit-cli' in sys.argv[0]:
    print('dagit')
    django.setup()
</code></pre>

<p>Can anyone help me with setup?</p>
",3,1574762148,python;django;dagster,True,1029,2,1583590830,https://stackoverflow.com/questions/59048161/integrating-dagster-with-django
60311714,Producing files in dagster without caring about the filename,"<p>In the dagster tutorial, in the <a href=""https://dagster.readthedocs.io/en/0.7.0/sections/tutorial/materializations.html"" rel=""nofollow noreferrer"">Materializiations section</a>, we choose a filename (<code>sorted_cereals_csv_path</code>) for our intermediate output, and then yield it as a materialization:</p>

<pre class=""lang-py prettyprint-override""><code>@solid
def sort_by_calories(context, cereals):

    # Sort the data (removed for brevity)

    sorted_cereals_csv_path = os.path.abspath(
        'calories_sorted_{run_id}.csv'.format(run_id=context.run_id)
    )
    with open(sorted_cereals_csv_path, 'w') as fd:
        writer = csv.DictWriter(fd, fieldnames)
        writer.writeheader()
        writer.writerows(sorted_cereals)
    yield Materialization(
        label='sorted_cereals_csv',
        description='Cereals data frame sorted by caloric content',
        metadata_entries=[
            EventMetadataEntry.path(
                sorted_cereals_csv_path, 'sorted_cereals_csv_path'
            )
        ],
    )
    yield Output(None)
</code></pre>

<p>However, this is relying on the fact that we can use the local filesystem (which may not be true), it will likely get overwritten by later runs (which is not what I want) and it's also forcing us to come up with a filename which will never be used.</p>

<p>What I'd like to do in most of my solids is just say ""here is a file object, please store it for me"", without concerning myself with <em>where</em> it's going to be stored. Can I materialize a file without considering all these things? Should I use python's <code>tempfile</code> facility for this?</p>
",0,1582162507,dagster,True,943,1,1582163197,https://stackoverflow.com/questions/60311714/producing-files-in-dagster-without-caring-about-the-filename
59905286,NoneType Error when trying to make a custom BeautifulSoup Dagster Type,"<p>I've been messing around with <code>@dagster_type</code> and was trying to make a custom <code>HtmlSoup</code> type. Basically a fancy <code>@dagster_type</code> wrapper around a BeautifulSoup Object. </p>

<pre class=""lang-py prettyprint-override""><code>import requests
from bs4 import BeautifulSoup
from dagster import (
    dagster_type,
    input_hydration_config,
    Selector,
    Field,
    String,
    TypeCheck,
    EventMetadataEntry
)

def max_depth(soup):
    if hasattr(soup, ""contents"") and soup.contents:
        return max([max_depth(child) for child in soup.contents]) + 1
    else:
        return 0

def html_soup_type_check(value):
    if not isinstance(value, BeautifulSoup):
        return TypeCheck(
            success=False,
            description=(
                'HtmlSoup should be a BeautifulSoup Object, got '
                '{type_}'
            ).format(type_=type(value))
        )

    if not hasattr(soup, ""contents""):
        return TypeCheck(
            success=False,
            description=(
                'HtmlSoup has no contents, check that the URL has content'
            )
        )

    return TypeCheck(
        success=True,
        description='HtmlSoup Summary Stats',
        metadata_entries=[
            EventMetadataEntry.text(
                str(max_depth(value)),
                'max_depth',
                'Max Nested Depth of the Page Soup'
            ),
            EventMetadataEntry.text(
                str(set(tag.name for tag in value.find_all())),
                'tag_names',
                'All available tags in the Page Soup'
            )
        ]
    )


@input_hydration_config(
    Selector(
        {
            'url': Field(
                String,
                is_optional=False,
                description=(
                    'URL to be ingested and converted to a Soup Object'
                )
            )
        }
    )
)
def html_soup_input_hydration_config(context, selector):
    url = selector['url']
    res = requests.get(url, params={
        'Content-type': 'text/html'
    })

    if (not res.status_code == 200):
        return TypeCheck(
            success=False,
            description=(
                '{status_code} ERROR, Check that URL: {url} is correct'
            ).format(status_code=res.status_code, url=url)
        )
    soup = BeautifulSoup(res.content, 'html.parser')
    return HtmlSoup(soup)

@dagster_type(
    name='HtmlSoup',
    description=(
        'The HTML extracted from a URL stored in '
        'a BeautifulSoup object.'
    ),
    type_check=html_soup_type_check,
    input_hydration_config=html_soup_input_hydration_config
)
class HtmlSoup(BeautifulSoup):
    pass
</code></pre>

<p>Is what I have been trying out but whenever I try to invoke a solid that uses takes an <code>HtmlSoup</code> type as input e.g. </p>

<pre class=""lang-py prettyprint-override""><code>@solid
def get_url(context, soup: HtmlSoup):
    return soup.contents
</code></pre>

<p>I get this error </p>

<p><strong>TypeError: 'NoneType' object is not callable</strong></p>

<pre><code>  File ""/Users/John/Documents/.../venv/lib/python3.7/site-packages/dagster/core/engine/engine_inprocess.py"", line 241, in dagster_event_sequence_for_step
    for step_event in check.generator(_core_dagster_event_sequence_for_step(step_context)):
  File ""/Users/John/Documents/.../venv/lib/python3.7/site-packages/dagster/core/engine/engine_inprocess.py"", line 492, in _core_dagster_event_sequence_for_step
    for input_name, input_value in _input_values_from_intermediates_manager(step_context).items():
  File ""/Users/John/Documents/.../venv/lib/python3.7/site-packages/dagster/core/engine/engine_inprocess.py"", line 188, in _input_values_from_intermediates_manager
    step_context, step_input.config_data
  File ""/Users/John/Documents/.../venv/lib/python3.7/site-packages/dagster/core/types/config_schema.py"", line 73, in construct_from_config_value
    return func(context, config_value)
  File ""/Users/John/Documents/.../custom_types/html_soup.py"", line 82, in html_soup_input_hydration_config
    return HtmlSoup(soup)
  File ""/Users/John/Documents/.../venv/lib/python3.7/site-packages/bs4/__init__.py"", line 286, in __init__
    markup = markup.read()
</code></pre>

<p>I get some additional info that says</p>

<pre class=""lang-py prettyprint-override""><code>An exception was thrown during execution that is likely a framework error, rather than an error in user code.
Original error message: TypeError: 'NoneType' object is not callable
</code></pre>

<p>I've been digging for a while now into the internals of the <code>@dagster_type</code> decorator and how the <code>@input_hydration_config</code> decorator works but have been at a bit of a loss so far. </p>

<p>Appreciate any and all help! </p>
",2,1579911888,python-3.x;beautifulsoup;dagster,True,220,1,1580226802,https://stackoverflow.com/questions/59905286/nonetype-error-when-trying-to-make-a-custom-beautifulsoup-dagster-type
59024980,No value for arguement in function call,"<p>I am very new to Python and am working through the <a href=""https://dagster.readthedocs.io/en/stable/sections/learn/tutorial/hello_cereal.html"" rel=""nofollow noreferrer"">Dagster hello tutorial</a> </p>

<p>I have set up the following from the tutorial</p>

<pre><code>import csv

from dagster import execute_pipeline, execute_solid, pipeline, solid


@solid
def hello_cereal(context):
    # Assuming the dataset is in the same directory as this file
    dataset_path = 'cereal.csv'
    with open(dataset_path, 'r') as fd:
        # Read the rows in using the standard csv library
        cereals = [row for row in csv.DictReader(fd)]

    context.log.info(
        'Found {n_cereals} cereals'.format(n_cereals=len(cereals))
    )

    return cereals


@pipeline
def hello_cereal_pipeline():
    hello_cereal()
</code></pre>

<p>However pylint shows </p>

<blockquote>
  <p>a no value for parameter</p>
</blockquote>

<p>message.</p>

<p>What have I missed?</p>

<p>When I try to execute the pipeline I get the following</p>

<blockquote>
  <p>D:\python\dag>dagster pipeline execute -f hello_cereal.py -n
  hello_cereal_pipeline 2019-11-25 14:47:09 - dagster - DEBUG -
  hello_cereal_pipeline - 96c575ae-0b7d-49cb-abf4-ce998865ebb3 -
  PIPELINE_START - Started execution of pipeline
  ""hello_cereal_pipeline"". 2019-11-25 14:47:09 - dagster - DEBUG -
  hello_cereal_pipeline - 96c575ae-0b7d-49cb-abf4-ce998865ebb3 -
  ENGINE_EVENT - Executing steps in process (pid: 11684) 
  event_specific_data = {""metadata_entries"": [[""pid"", null, [""11684""]],
  [""step_keys"", null, [""{'hello_cereal.compute'}""]]]} 2019-11-25
  14:47:09 - dagster - DEBUG - hello_cereal_pipeline -
  96c575ae-0b7d-49cb-abf4-ce998865ebb3 - STEP_START - Started execution
  of step ""hello_cereal.compute"".
                 solid = ""hello_cereal""
      solid_definition = ""hello_cereal""
              step_key = ""hello_cereal.compute"" 2019-11-25 14:47:10 - dagster - ERROR - hello_cereal_pipeline -
  96c575ae-0b7d-49cb-abf4-ce998865ebb3 - STEP_FAILURE - Execution of
  step ""hello_cereal.compute"" failed.
              cls_name = ""FileNotFoundError""
                 solid = ""hello_cereal""
      solid_definition = ""hello_cereal""
              step_key = ""hello_cereal.compute""</p>
  
  <p>File
  ""c:\users\kirst\appdata\local\programs\python\python38-32\lib\site-packages\dagster\core\errors.py"",
  line 114, in user_code_error_boundary
      yield   File ""c:\users\kirst\appdata\local\programs\python\python38-32\lib\site-packages\dagster\core\engine\engine_inprocess.py"",
  line 621, in _user_event_sequence_for_step_compute_fn
      for event in gen:   File ""c:\users\kirst\appdata\local\programs\python\python38-32\lib\site-packages\dagster\core\execution\plan\compute.py"",
  line 75, in _execute_core_compute
      for step_output in _yield_compute_results(compute_context, inputs, compute_fn):   File
  ""c:\users\kirst\appdata\local\programs\python\python38-32\lib\site-packages\dagster\core\execution\plan\compute.py"",
  line 52, in _yield_compute_results
      for event in user_event_sequence:   File ""c:\users\kirst\appdata\local\programs\python\python38-32\lib\site-packages\dagster\core\definitions\decorators.py"",
  line 418, in compute
      result = fn(context, **kwargs)   File ""hello_cereal.py"", line 10, in hello_cereal
      with open(dataset_path, 'r') as fd:</p>
  
  <p>2019-11-25 14:47:10 - dagster - DEBUG - hello_cereal_pipeline -
  96c575ae-0b7d-49cb-abf4-ce998865ebb3 - ENGINE_EVENT - Finished steps
  in process (pid: 11684) in 183ms  event_specific_data =
  {""metadata_entries"": [[""pid"", null, [""11684""]], [""step_keys"", null,
  [""{'hello_cereal.compute'}""]]]} 2019-11-25 14:47:10 - dagster - ERROR
  - hello_cereal_pipeline - 96c575ae-0b7d-49cb-abf4-ce998865ebb3 - PIPELINE_FAILURE - Execution of pipeline ""hello_cereal_pipeline""
  failed.</p>
</blockquote>

<p>[Update]
From Rahul's comment I realised I had not copied the whole example.
When I corrected that I got a FileNotFoundError</p>
",2,1574653458,python-3.x;dagster,True,2011,2,1575395098,https://stackoverflow.com/questions/59024980/no-value-for-arguement-in-function-call
59050671,Core compute for solid returned an output multiple times,"<p>I am very new to Dagster and I can't find answer to my question in the docs. </p>

<p>I have 2 solids: one thats yielding tuples(str, str) that are parsed from XML file, he other one just consumes tuples and stores objects in DB with according fields set. However I am running into an error <code>Core compute for solid returned an output multiple times</code>. I am pretty sure I made fundamental mistake in my design. Could someone explain to me how to design this pipeline in the right way or point me to the chapter in the docs that explains this error?</p>

<pre><code>
@solid(output_defs=[OutputDefinition(Tuple, 'classification_data')])
def extract_classification_from_file(context, xml_path: String) -&gt; Tuple:
    context.log.info(f""start"")
    root = ET.parse(xml_path).getroot()
    for code_node in root.findall('definition-item'):
        context.log.info(f""{code_node.find('classification-symbol').text} {code_node.find('definition-title').text}"")
        yield Output((code_node.find('classification-symbol').text, code_node.find('definition-title').text), 'classification_data')


@solid()
def load_classification(context, classification_data):
    cls = CPCClassification.objects.create(code=classification_data[0], description=classification_data[1]).save()

@pipeline
def define_classification_pipeline():
    load_classification(extract_classification_from_file())
</code></pre>
",2,1574770150,python;django;xml;dagster,True,2311,2,1574866879,https://stackoverflow.com/questions/59050671/core-compute-for-solid-returned-an-output-multiple-times
