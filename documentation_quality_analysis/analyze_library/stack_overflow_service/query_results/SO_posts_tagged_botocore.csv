post_id,title,body,score,creation_date,tags,is_answered,view_count,answer_count,last_activity_date,link
78334317,ERROR - Runtime.ImportModuleError: Unable to import module &#39;main&#39;: No module named &#39;botocore.vendored.six.moves&#39;,"<p>I am developing a lambda project using Python Version 3.12 as a programming language.
I developed a simple API and when I run the project locally it shows the following error:</p>
<pre><code>START RequestId: f4eac928-9658-44f0-9193-46bb4162f297 Version: $LATEST
LAMBDA_WARNING: Unhandled exception. The most likely cause is an issue in the function code. However, in rare cases, a Lambda runtime update can cause unexpected function behavior. For functions using managed runtimes, runtime updates can be triggered by a function change, or can be applied automatically. To determine if the runtime has been updated, check the runtime version in the INIT_START log entry. If this error correlates with a change in the runtime version, you may be able to mitigate this error by temporarily rolling back to the previous runtime version. For more information, see https://docs.aws.amazon.com/lambda/latest/dg/runtimes-update.html
[ERROR] Runtime.ImportModuleError: Unable to import module 'main': No module named 'botocore.vendored.six.moves'
Traceback (most recent call last):
END RequestId: 064be081-c61f-4494-ab47-ee1382c83196
REPORT RequestId: 064be081-c61f-4494-ab47-ee1382c83196  Init Duration: 0.09 ms  Duration: 370.09 ms     Billed Duration: 371 ms Memory Size: 128 MB     Max Memory Used: 128 MB
{&quot;errorMessage&quot;: &quot;Unable to import module 'main': No module named 'botocore.vendored.six.moves'&quot;, &quot;errorType&quot;: &quot;Runtime.ImportModuleError&quot;, &quot;requestId&quot;: &quot;064be081-c61f-4494-ab47-ee1382c83196&quot;, &quot;stackTrace&quot;: []}
</code></pre>
<p>I've updated boto3, botocore and awscli:</p>
<pre><code>pip install --upgrade boto3
pip install --upgrade botocore 
pip install --upgrade awscli
</code></pre>
<p>But it still shows the same error mentioned above.</p>
<p>Can someone help me?</p>
<p><strong>Sorry for my English, I used Google Translate</strong></p>
",0,1713267974,python;python-3.x;aws-lambda;botocore;python-3.12,False,29,0,1713273906,https://stackoverflow.com/questions/78334317/error-runtime-importmoduleerror-unable-to-import-module-main-no-module-nam
78238447,ERROR - Runtime.ImportModuleError: Unable to import module &#39;main&#39;: cannot import name &#39;DEPRECATED_SERVICE_NAMES&#39; from &#39;botocore.docs&#39;,"<p>I'm supporting a lambda project <strong>(python 3.8)</strong> and when I run the command <strong>&quot;sam local invoke LambdaFunction --event events/get_users.json&quot;</strong>, it gives the following error</p>
<pre><code>`Invoking main.lambda_handler (python3.8)
arn:aws:lambda:sa-east-1:061977363137:layer:pyodbc-layer-3_8:1 is already cached. Skipping download
arn:aws:lambda:sa-east-1:061977363137:layer:my-api-libs:3 is already cached. Skipping download
Local image is up-to-date
Using local image: samcli/lambda-python:3.8-x86_64-25626f11905fabd722693f3a7.

Mounting C:\Projects\my-api\api-main\.aws-sam\build\LambdaFunction as /var/task:ro,delegated, inside runtime container
START RequestId: 71de187e-ce8d-4fb0-9ee1-58b5d578b330 Version: $LATEST
LAMBDA_WARNING: Unhandled exception. The most likely cause is an issue in the function code. However, in rare cases, a Lambda runtime update can cause unexpected function behavior. For functions using managed runtimes, runtime updates can be triggered by a function change, or can be applied automatically. To determine if the runtime has been updated, check the runtime version in the INIT_START log entry. If this error correlates with a change in the runtime version, you may be able to mitigate this error by temporarily rolling back to the previous runtime version. For more information, see https://docs.aws.amazon.com/lambda/latest/dg/runtimes-update.html
[ERROR] Runtime.ImportModuleError: Unable to import module 'main': cannot import name 'DEPRECATED_SERVICE_NAMES' from 'botocore.docs' (/opt/python/botocore/docs/__init__.py)
Traceback (most recent call last):
END RequestId: 3df868d8-6059-49c0-b401-e6d20754a442
REPORT RequestId: 3df868d8-6059-49c0-b401-e6d20754a442  Init Duration: 0.16 ms  Duration: 638.28 ms     Billed Duration: 639 ms Memory Size: 128 MB     Max Memory Used: 128 MB
{&quot;errorMessage&quot;: &quot;Unable to import module 'main': cannot import name 'DEPRECATED_SERVICE_NAMES' from 'botocore.docs' (/opt/python/botocore/docs/__init__.py)&quot;, &quot;errorType&quot;: &quot;Runtime.ImportModuleError&quot;, &quot;stackTrace&quot;: []}`
</code></pre>
<p><strong>boto3</strong> and <strong>botocore</strong> are updated:</p>
<pre><code>PS C:\Users\UserGuest\AppData\Local\Programs\Python\Python38&gt; python.exe -m pip install --upgrade boto3
Requirement already satisfied: boto3 in c:\users\UserGuest\appdata\local\programs\python\python38\lib\site-packages (1.34.81)
Requirement already satisfied: botocore&lt;1.35.0,&gt;=1.34.81 in c:\users\UserGuest\appdata\local\programs\python\python38\lib\site-packages (from boto3) (1.34.81)
Requirement already satisfied: jmespath&lt;2.0.0,&gt;=0.7.1 in c:\users\UserGuest\appdata\roaming\python\python38\site-packages (from boto3) (1.0.1)
Requirement already satisfied: s3transfer&lt;0.11.0,&gt;=0.10.0 in c:\users\UserGuest\appdata\local\programs\python\python38\lib\site-packages (from boto3) (0.10.1)
Requirement already satisfied: python-dateutil&lt;3.0.0,&gt;=2.1 in c:\users\UserGuest\appdata\roaming\python\python38\site-packages (from botocore&lt;1.35.0,&gt;=1.34.81-&gt;boto3) (2.9.0.post0)
Requirement already satisfied: urllib3&lt;1.27,&gt;=1.25.4 in c:\users\UserGuest\appdata\roaming\python\python38\site-packages (from botocore&lt;1.35.0,&gt;=1.34.81-&gt;boto3) (1.26.18)
Requirement already satisfied: six&gt;=1.5 in c:\users\UserGuest\appdata\roaming\python\python38\site-packages (from python-dateutil&lt;3.0.0,&gt;=2.1-&gt;botocore&lt;1.35.0,&gt;=1.34.81-&gt;boto3) (1.16.0)
</code></pre>
<p>But it's still showing the error I mentioned above.</p>
<p>Can anyone help me?
Thanks!</p>
",0,1711631596,python;aws-lambda;boto;python-3.8;botocore,False,107,0,1713267630,https://stackoverflow.com/questions/78238447/error-runtime-importmoduleerror-unable-to-import-module-main-cannot-import
74093110,How to encrypt and decrypt files using aws asymetric key using python,"<p>I need to encrypt a large file using aws asymmetric key. How to encrypt and decrypt files using aws asymetric key using python</p>
",0,1665986661,python;amazon-web-services;botocore,True,134,1,1712691101,https://stackoverflow.com/questions/74093110/how-to-encrypt-and-decrypt-files-using-aws-asymetric-key-using-python
78276800,"Botocore generates incorrect S3 URLs, leading to a 403 Access Denied","<p>I'm using Boto3(v1.34) client to list objects from an S3 bucket, but I've encountered some odd behavior. <strong>Note</strong>- 403 is not a real issue but due to different root cause which i have explained below.</p>
<p><strong>Background(botocore internals):</strong> botocore generates s3 url based on bucket name and do http call to fetch objects. input for generating url contains bucket key along with several other keys like this - <code>DEBUG:botocore.regions:Calling endpoint provider with parameters:{'Bucket': 'mybucket', 'Prefix': 'myobjects',.....}</code>. it gets bucket name from the provided method param in <code>s3_client.list_objects_v2(Bucket=&quot;mybucket&quot;, Prefix=&quot;myobject&quot;))</code>, and the generated is <code>url:'url': '[https://mybucket.s3.amazonaws.com/?list-type=..'</code></p>
<p><strong>Issue/Root Cause:</strong>  for the cases where random failure happens, input for generating url doesn't contain bucket key and hence generated url misses bucket name in it which leads to creation of wrong s3 url and throws 403 access denied. input param in this case (you can see bucket key is missing in this case and then generated url will also miss it) - <code>DEBUG:botocore.regions:Calling endpoint provider with parameters:{'Prefix': 'myobjects',.....}</code>,  and generated url is - <code>url': 'https://s3.amazonaws.com/?list-type=..</code></p>
<pre><code>session = boto3.Session()
credentials = session.get_credentials()

s3_client = boto3.client(
    &quot;s3&quot;,
    region_name=&quot;eu-central-1&quot;,
    aws_access_key_id=credentials.access_key,
    aws_secret_access_key=credentials.secret_key,
    aws_session_token=credentials.token,
)



s3_client.list_objects_v2(Bucket=mybucket,
        Prefix=myobject)
</code></pre>
<p>What factors might be contributing to this behavior?</p>
",-1,1712267657,python;amazon-s3;boto3;boto;botocore,False,41,0,1712524288,https://stackoverflow.com/questions/78276800/botocore-generates-incorrect-s3-urls-leading-to-a-403-access-denied
78260685,AWS Bedrock Stream responses model,"<p>I am trying to call the invokeModelWithResponseStream API endpoint ( not using any boto3 library or sdk). As per the docs <a href=""https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModelWithResponseStream.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModelWithResponseStream.html</a></p>
<p>However I am getting error when decoding using Python.</p>
<pre><code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0xb4 in position 9: invalid start byte
</code></pre>
<p>An example data from Bedrock:</p>
<pre><code>b'\x00\x00\x02\x1f\x00\x00\x00K+T\xe6)\x0b:event-type\x07\x00\x05chunk\r:content-type\x07\x00\x10application/json\r:message-type\x07\x00\x05event{&quot;bytes&quot;:&quot;eyJvdXRwdXRUZXh0IjoiXG5JbmRpYSBpcyBhIGNvdW50cnkgaW4gU291dGggQXNpYSB0aGF0IGlzIGtub3duIGZvciBpdHMgcmljaCBhbmQgZGl2ZXJzZSBjdWx0dXJlLiBJbmRpYW4gY3VsdHVyZSBoYXMgYmVlbiBzaGFwZWQgb3ZlciB0aG91c2FuZHMgb2YgeWVhcnMgYnkgYSB2YXJpZXR5IG9mIGZhY3RvcnMsIGluY2x1ZGluZyByZWxpZ2lvbiwgbGFuZ3VhZ2UsIGV0aG5pY2l0eSwgYW5kIGdlb2dyYXBoeS4gIE8iLCJpbmRleCI6MCwidG90YWxPdXRwdXRUZXh0VG9rZW5Db3VudCI6bnVsbCwiY29tcGxldGlvblJlYXNvbiI6bnVsbCwiaW5wdXRUZXh0VG9rZW5Db3VudCI6OH0=&quot;} \x18\x12M\x00\x00\x03\x1f\x00\x00\x00K\xe0\x085\x8c\x0b:event-type\x07\x00\x05chunk\r:content-type\x07\x00\x10application/json\r:message-type\x07\x00\x05event{&quot;bytes&quot;:&quot;eyJvdXRwdXRUZXh0IjoibmUgb2YgdGhlIG1vc3QgaW1wb3J0YW50IGFzcGVjdHMgb2YgSW5kaWFuIGN1bHR1cmUgaXMgaXRzIHJlbGlnaW9uLCBIaW5kdWlzbS4gSGluZHVpc20gaXMgdGhlIGRvbWluYW50IHJlbGlnaW9uIGluIEluZGlhLCBwcmFjdGljZWQgYnkgb3ZlciA5MCUgb2YgdGhlIHBvcHVsYXRpb24sIGFuZCBpdCBoYXMgaGFkIGEgcHJvZm91bmQgaW1wYWN0IG9uIHRoZSBjb3VudHJ5J3MgYXJ0LCBsaXRlcmF0dXJlLCBtdXNpYywgYW5kIGN1aXNpbmUuICBJbmRpYW4gbXVzaWMgaXMgYW5vdGhlciBtYWpvciBjb21wb25lbnQgb2YgSW5kaWFuIGN1bHR1cmUsIGFuZCBpdCBpcyBrbm93biBmb3IgaXRzIGRpdmVyc2UgcmFuZ2Ugb2Ygc3R5bGVzIGFuZCBpbnN0cnVtZW50cy4gQ2xhc3NpY2FsIG11c2ljIGlzIHBhcnRpY3VsYXJseSIsImluZGV4IjowLCJ0b3RhbE91dHB1dFRleHRUb2tlbkNvdW50IjoxMjgsImNvbXBsZXRpb25SZWFzb24iOiJMRU5HVEgiLCJpbnB1dFRleHRUb2tlbkNvdW50IjpudWxsfQ==&quot;}W,\xf7\x01'
</code></pre>
<p>There is additional header data added at the beginning which I cannot figure out after spending hours trying and reading the documentations.</p>
<p>I have tried various decoding techniques. Manually removing them doesnâ€™t make sense</p>
",-1,1712054360,python;boto3;boto;botocore;amazon-bedrock,False,37,0,1712054360,https://stackoverflow.com/questions/78260685/aws-bedrock-stream-responses-model
78205682,Runtime.ImportModuleError unable to import module cannot import module name &quot;DEPRECETED_SERVICE_NAMES&quot; from botocore.docs,"<p>I am running my Python code of Lambda runtime Python3.9 and recently I made some changes in the code and deployed those changes, after deploying the changes I am now getting an error saying &quot;cannot import module name &quot;DEPRECETED_SERVICE_NAMES&quot; from botocore.docs&quot;. After reverting to the runtime version code is running fine but it is just a temporary fix, what is the permanent solution for this?</p>
",1,1711104670,python;aws-lambda;boto3;botocore,False,33,0,1711104670,https://stackoverflow.com/questions/78205682/runtime-importmoduleerror-unable-to-import-module-cannot-import-module-name-dep
78078290,botocore &gt;= 1.28.0 slower in multithread application,"<p>The official Boto3 docs recommends creating a new resource per thread: <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/guide/resources.html#multithreading-or-multiprocessing-with-resources"" rel=""nofollow noreferrer"">https://boto3.amazonaws.com/v1/documentation/api/latest/guide/resources.html#multithreading-or-multiprocessing-with-resources</a></p>
<p>Botocore 1.28.0 merged a feature which appears to generate a list of all possible endpoints on resource creation: <a href=""https://github.com/boto/botocore/pull/2785"" rel=""nofollow noreferrer"">https://github.com/boto/botocore/pull/2785</a></p>
<p>I have a test suite which uses <code>motoserver</code> and an application that relies heavily on parallelized downloads from / uploads to s3 from a process pool. With botocore 1.28.0, the test suite takes an extra 20 minutes to run as compared to the previous version.</p>
<p>I've done some work with <code>cProfile</code> and I can confirm that at least half of the additional time is spent inside of <code>botocore</code>'s <code>load_service_model</code> method called during botocore client creation. Haven't tracked down the other ~50% of extra time yet but it's somewhere in botocore usage.</p>
<p>What can I do to speed this up again with the version bump?</p>
",1,1709166611,python;amazon-web-services;amazon-s3;boto;botocore,False,32,0,1709166611,https://stackoverflow.com/questions/78078290/botocore-1-28-0-slower-in-multithread-application
73721274,Python : XML file downloaded from S3 full of string escaping characters,"<p>I have a number of XML files that I have added to S3 (localstack sever).  I can view these files through Cyberduck and they are valid xml files.  However, when I download the objects, the XML data is wrapped in double quotes, with each double quote in the document excaped, and each line having \n.  I have made sure the response content type is &quot;text/xml&quot;.</p>
<pre><code>s3 = boto3.client('s3',
                  config=s3_config,
                  endpoint_url=endpoint_url,
                  aws_access_key_id='foo',
                  aws_secret_access_key='bar',
                 )

try:
    r = s3.get_object(Bucket=bucket, Key=key)
    return Response(r['Body'].read().decode(&quot;utf-8&quot;))
except Exception as e:
    raise(e)
</code></pre>
<p>which results in a respose of</p>
<pre><code>&quot;
&lt;rpc-reply xmlns:....&quot;&gt;\n
    &lt;data&gt;\n
        &lt;configuration&gt;\n    
            &lt;server&gt;meanwhileinhell&lt;/server&gt;\n
            &lt;security&gt;\n  
                &lt;group&gt;\n  
                    &lt;name&gt;mih-&lt;/name&gt;\n
                    &lt;system&gt;\n            
                        &lt;scripts&gt;\n

             ...
             ...
             ...

        &lt;/configuration&gt;\n
    &lt;/data&gt;\n
&lt;/rpc-reply&gt;\n&quot;
</code></pre>
<p>I cannot seem to ensure this is a raw XML response body, with all of the escaping removed.  Here are some of the other implementations I have tried:</p>
<pre><code>from io import BytesIO

f = BytesIO()
s3.download_fileobj(bucket, key, f)
return Response(f.getvalue(), content_type=&quot;text/xml&quot;)
</code></pre>
<pre><code>from xml.etree import ElementTree

tree = ElementTree.fromstring(r['Body'].read())
return Response(tree)
</code></pre>
<p>I have also tried using <code>pickle</code> and <code>BeautifulSoup</code> with no further success.  I have not tried this with another type of file such as a jpg, but why can't I get the actual raw binary data from the objects?  The files I am downloading are &lt;50KB.</p>
",0,1663178107,python;amazon-s3;boto;localstack;botocore,True,666,1,1709138254,https://stackoverflow.com/questions/73721274/python-xml-file-downloaded-from-s3-full-of-string-escaping-characters
76247145,How to set Content-Type header with boto3 presigned url multipart upload,"<p>Is there any way to allow <code>Content-Type</code> header with multipart uploads to presigned s3 url?</p>
<p>Let's begin with the following code:</p>
<pre class=""lang-py prettyprint-override""><code>import boto3
import requests


BUCKET_NAME = &quot;foo&quot;

# No, it's global in this MRE only
client = boto3.client('s3')


def create(key):
    response = client.create_multipart_upload(Bucket=BUCKET_NAME, Key=key)
    return response['UploadId']


def get_url(key, upload_id, chunk_number):
    signed_url = client.generate_presigned_url(
        ClientMethod='upload_part',
        Params={
            &quot;Bucket&quot;: BUCKET_NAME,
            &quot;Key&quot;: key,
            &quot;PartNumber&quot;: chunk_number,
            &quot;UploadId&quot;: upload_id,
            # &quot;ContentType&quot;: &quot;application/x-www-form-urlencoded&quot;,
        },
        ExpiresIn=60 * 60,  # seconds
    )
    return signed_url


def complete(key, upload_id, parts):
    client.complete_multipart_upload(
        Bucket=BUCKET_NAME,
        Key=key,
        UploadId=upload_id,
        MultipartUpload={&quot;Parts&quot;: parts}
    )


def test_upload():
    key = 'data/foo.bar'
    upload_id = create(key)

    url = get_url(key, upload_id, 1)
    with open(&quot;/tmp/foo.bar&quot;, &quot;rb&quot;) as src:
        response = requests.put(url, data=src.read())

    etag = response.headers[&quot;ETag&quot;]
    complete(key, upload_id, {&quot;ETag&quot;: etag, &quot;PartNumber&quot;: 1})
</code></pre>
<p>Hooray, this works. However, let's try to do the same from frontend, replacing <code>requests</code> call with</p>
<pre><code>fetch(uploadTo, {
    method: 'PUT',
    body: blob,
})
</code></pre>
<p>(no matter how blob is defined here, this is irrelevant to our problem).</p>
<p>And this fails, returning 403 <code>SignatureDoesNotMatch</code>. Why? Because <code>Content-Type</code> header <em>is</em> set (and <code>fetch</code> <a href=""https://stackoverflow.com/questions/71670213/tell-fetch-to-not-send-a-content-type-header-at-all"">cannot do without it</a>), and this is part of S3-side backend signature verification. <code>Content-Type</code> is not a part of generated URL, so with any content type <code>fetch</code> tries to set this will not match. I know this is the case, because here's what response looks like (with only URL being different, ignore this incompatibilities, <code>uploadId=1</code> is just a fake - same thing happens with real URL; pay attention to <strong>StringToSign</strong> tag):</p>
<pre class=""lang-xml prettyprint-override""><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;Error&gt;&lt;Code&gt;SignatureDoesNotMatch&lt;/Code&gt;
  &lt;Message&gt;The request signature we calculated does not match the signature you
    provided. Check your key and signing method.&lt;/Message&gt;
  &lt;AWSAccessKeyId&gt;...&lt;/AWSAccessKeyId&gt;
  &lt;StringToSign&gt;PUT
    application/x-www-form-urlencoded
    1684064749
    x-amz-security-token:FwoGZXIvYXdzEOT//////////wEaDE+2gqVw4NTt1c2eOCKGAVTf4uDCD+GJ8P6lG2vBg8yQ2dqyU7/6aHg4hXMljyDFByT7hJ1/F/GPwBi84eAMDZqGzXpIySe8PhU80ak5C4vg7vcGOOSaB3cXk7TtQ2q0pWb8MB0AYb3LGAJ6sahySjHSdArFFADB60u6SskWhq9HHSijilW9hKIiUgdceZAPhLH1J59oKITngqMGMijigFpTERPtZLB+MOjIqJIpHvPJrrfRg4mzwAmZbk+rropyYha4rBNP
    /sim-cal-bucket/temp/foo.mp4?partNumber=1&amp;amp;uploadId=1&lt;/StringToSign&gt;
  &lt;SignatureProvided&gt;miwrnxtxoPdGnuAEqiP52ZMscBQ=&lt;/SignatureProvided&gt;
  &lt;StringToSignBytes&gt;50 55 54 0a 0a 61 70 70 6c 69 63 61 74 69 6f 6e 2f 78 2d
    77 77 77 2d 66 6f 72 6d 2d 75 72 6c 65 6e 63 6f 64 65 64 0a 31 36 38 34 30
    36 34 37 34 39 0a 78 2d 61 6d 7a 2d 73 65 63 75 72 69 74 79 2d 74 6f 6b 65
    6e 3a 46 77 6f 47 5a 58 49 76 59 58 64 7a 45 4f 54 2f 2f 2f 2f 2f 2f 2f 2f
    2f 2f 77 45 61 44 45 2b 32 67 71 56 77 34 4e 54 74 31 63 32 65 4f 43 4b 47
    41 56 54 66 34 75 44 43 44 2b 47 4a 38 50 36 6c 47 32 76 42 67 38 79 51 32
    64 71 79 55 37 2f 36 61 48 67 34 68 58 4d 6c 6a 79 44 46 42 79 54 37 68 4a
    31 2f 46 2f 47 50 77 42 69 38 34 65 41 4d 44 5a 71 47 7a 58 70 49 79 53 65
    38 50 68 55 38 30 61 6b 35 43 34 76 67 37 76 63 47 4f 4f 53 61 42 33 63 58
    6b 37 54 74 51 32 71 30 70 57 62 38 4d 42 30 41 59 62 33 4c 47 41 4a 36 73
    61 68 79 53 6a 48 53 64 41 72 46 46 41 44 42 36 30 75 36 53 73 6b 57 68 71
    39 48 48 53 69 6a 69 6c 57 39 68 4b 49 69 55 67 64 63 65 5a 41 50 68 4c 48
    31 4a 35 39 6f 4b 49 54 6e 67 71 4d 47 4d 69 6a 69 67 46 70 54 45 52 50 74
    5a 4c 42 2b 4d 4f 6a 49 71 4a 49 70 48 76 50 4a 72 72 66 52 67 34 6d 7a 77
    41 6d 5a 62 6b 2b 72 72 6f 70 79 59 68 61 34 72 42 4e 50 0a 2f 73 69 6d 2d
    63 61 6c 2d 62 75 63 6b 65 74 2f 74 65 6d 70 2f 66 6f 6f 2e 6d 70 34 3f 70
    61 72 74 4e 75 6d 62 65 72 3d 31 26 75 70 6c 6f 61 64 49 64 3d 31&lt;/StringToSignBytes&gt;
  &lt;RequestId&gt;073V6QJXMA0XAKWS&lt;/RequestId&gt;
  &lt;HostId&gt;1pR1Pz4RSnRilgjUbb0AVDcMWiqCq05dMrAVU+0t4a0HF5ytfXmNiIecxH80urVoiKtxtHhxS2o=&lt;/HostId&gt;
&lt;/Error&gt;
</code></pre>
<p>So, we need to pass a <code>Content-Type</code> to signed url somehow. Neither <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/generate_presigned_url.html"" rel=""nofollow noreferrer""><code>generate_signed_url</code></a> nor its <code>Params</code> (that must match params of <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/upload_part.html"" rel=""nofollow noreferrer""><code>upload_part</code></a>) accept <code>ContentType</code> option. This looks like a dead end... To double confirm, here's <a href=""https://stackoverflow.com/questions/24684959/pre-signed-s3-url-signature-does-not-match"">what works in JS</a> - <code>ContentType</code> is passed to the signer.</p>
<p>Well, for now I'm just monkeypatching botocore to allow passing <code>ContentType</code> parameter to <code>upload_part</code> (cloned <a href=""https://github.com/boto/botocore/blob/d95d441ef9b81ac4d0163fa97e736edf63a8b4b3/botocore/data/s3/2006-03-01/service-2.json"" rel=""nofollow noreferrer""><code>botocore/data/s3/2006-03-01/service-2.json</code></a> and added this parameter to <code>UploadPartRequest</code> definition, patching this file in venv in Dockerfile), but it's certainly not what I want. However, this confirms that I <em>really</em> need to pass <code>ContentType</code>, and no other solution can allow setting this header. After uncommenting <code>ContentType</code> key in the sample above, everything is fine.</p>
<p>Just to compare, below are urls without and with content type - this arg is included in url directly. The latter URL works with frontend <code>fetch</code> flawlessly.</p>
<pre><code>https://sim-cal-bucket.s3.amazonaws.com/temp/foo.mp4?partNumber=1&amp;uploadId=1&amp;AWSAccessKeyId=...&amp;Signature=miwrnxtxoPdGnuAEqiP52ZMscBQ%3D&amp;x-amz-security-token=FwoGZXIvYXdzEOT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDE%2B2gqVw4NTt1c2eOCKGAVTf4uDCD%2BGJ8P6lG2vBg8yQ2dqyU7%2F6aHg4hXMljyDFByT7hJ1%2FF%2FGPwBi84eAMDZqGzXpIySe8PhU80ak5C4vg7vcGOOSaB3cXk7TtQ2q0pWb8MB0AYb3LGAJ6sahySjHSdArFFADB60u6SskWhq9HHSijilW9hKIiUgdceZAPhLH1J59oKITngqMGMijigFpTERPtZLB%2BMOjIqJIpHvPJrrfRg4mzwAmZbk%2BrropyYha4rBNP&amp;Expires=1684064749
https://sim-cal-bucket.s3.amazonaws.com/temp/foo.mp4?partNumber=1&amp;uploadId=1&amp;AWSAccessKeyId=...&amp;Signature=1FeHhXi7QRtL0wCT7kJ%2BVEcBeso%3D&amp;content-type=application%2Fx-www-form-urlencoded&amp;x-amz-security-token=FwoGZXIvYXdzEOT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDE%2B2gqVw4NTt1c2eOCKGAVTf4uDCD%2BGJ8P6lG2vBg8yQ2dqyU7%2F6aHg4hXMljyDFByT7hJ1%2FF%2FGPwBi84eAMDZqGzXpIySe8PhU80ak5C4vg7vcGOOSaB3cXk7TtQ2q0pWb8MB0AYb3LGAJ6sahySjHSdArFFADB60u6SskWhq9HHSijilW9hKIiUgdceZAPhLH1J59oKITngqMGMijigFpTERPtZLB%2BMOjIqJIpHvPJrrfRg4mzwAmZbk%2BrropyYha4rBNP&amp;Expires=1684065651
</code></pre>
<p>Solutions suggesting to make a request without <code>Content-Type</code> are unacceptable, because this is a part of public API, and I do not want to make customers jump through hoops trying to send such request.</p>
",1,1684062356,python;amazon-s3;http-headers;boto3;botocore,True,1030,2,1708821516,https://stackoverflow.com/questions/76247145/how-to-set-content-type-header-with-boto3-presigned-url-multipart-upload
77677397,Increase s3 connection pool,"<p>I have the following code:</p>
<pre class=""lang-py prettyprint-override""><code>    client_config = botocore.config.Config(
        max_pool_connections=20
    )
    athena = boto3.client('athena')
    s3 = boto3.resource('s3',config=client_config)
    
    query_result = athena.start_query_execution(
        QueryString = query,
        ResultConfiguration = {
            'OutputLocation': s3_url
        }
    )
    queryExecutionId = query_result['QueryExecutionId']
    response = athena.get_query_execution(QueryExecutionId = queryExecutionId)
    pd.read_csv(f&quot;s3_bucket/{queryExecutionId}.csv&quot;)
    
    athena.close() 
</code></pre>
<p>It throws the following warning:</p>
<p><strong>Connection pool is full, discarding connection: x.s3.us-west-2.amazonaws.com. Connection pool size: 10</strong></p>
<p>How can I increase this pool size for s3 connection?</p>
<p>Thanks.</p>
",0,1702882700,python;amazon-web-services;amazon-s3;boto3;botocore,False,163,1,1702884809,https://stackoverflow.com/questions/77677397/increase-s3-connection-pool
77531410,How can I take the difference of two overlapping byte streams in Python?,"<p>I have an audio stream coming in from an API call which generated text-to-speech; I am currently generating overlapping chunks of consecutive texts and want to only write the difference between a stream chunk and the one preceding it (excluding the overlap), how can I do that when what I'm dealing with are byte streams and not actual audio/text streams?</p>
<p>Here's an example:</p>
<p>let's say at iteration 0, I make an API call with the text <code>I am feeling good</code> and that returns an audio stream which is written to a file</p>
<p>now at iteration 1, I make another API call with the text <code>feeling good because I</code> and that returns an audio stream too, but what I want to do is skip the overlapping <code>feeling good</code> and append the remaining part <code>because I</code> from the audio stream to the file. Any suggestions on how to do this when I'm working with byte streams instead of text?</p>
<p>here's what I have so far, which is storing the previous stream and the current one, but how can I take the difference here?</p>
<pre><code>for i in sentences:
   curr_audio_stream = API_call(i) #Returns a botocore.response.StreamingBody object
   with open(&quot;file.mp3&quot;, &quot;ab&quot;) as file:
      file.write(curr_audio_stream.read())
   prev_audio_stream = curr_audio_stream
</code></pre>
<p>Here's an example bunch of texts overlapping:</p>
<pre><code>1. Oh, that sounds like
2. a fun game!  Let's do it! So
3. Let's do it! So do you want to
4. do you want to start by giving me
5. start by giving me a clue about the
</code></pre>
",0,1700668550,python;audio;stream;byte;botocore,False,45,0,1700671599,https://stackoverflow.com/questions/77531410/how-can-i-take-the-difference-of-two-overlapping-byte-streams-in-python
42975609,How to capture botocore&#39;s NoSuchKey exception?,"<p>I'm trying to write ""good"" python and capture a S3 no such key error with this:</p>

<pre><code>session = botocore.session.get_session()
client = session.create_client('s3')
try:
    client.get_object(Bucket=BUCKET, Key=FILE)
except NoSuchKey as e:
    print &gt;&gt; sys.stderr, ""no such key in bucket""
</code></pre>

<p>But NoSuchKey isn't defined and I can't trace it to the import I need to have it defined.</p>

<p><code>e.__class__</code> is <code>botocore.errorfactory.NoSuchKey</code> but <code>from botocore.errorfactory import NoSuchKey</code> gives an error and <code>from botocore.errorfactory import *</code> doesn't work either and I don't want to capture a generic error. </p>
",201,1490270940,python;botocore,True,79515,4,1698693197,https://stackoverflow.com/questions/42975609/how-to-capture-botocores-nosuchkey-exception
77323410,S3FS fails when given aiobotocore session to initialize the File System,"<p>I am trying to use a combination of aiobotocore, botocore and s3fs to build an S3 client which refreshes its credentials automatically.</p>
<p>This is my code so far:</p>
<p>Code to create a refreshable session. It has two methods, one to generate a normal boto3 session and another one to generate an aiobotocore session object.</p>
<pre class=""lang-py prettyprint-override""><code>import boto3

from boto3 import Session
from botocore.session import get_session
from botocore.credentials import RefreshableCredentials
import random
import pytz
from datetime import datetime, timedelta
from dateutil.parser import parse

class RefreshableSession:

    def __init__(self):
        session = boto3.Session()
        self.sts_client = session.client(&quot;sts&quot;, region_name='eu-west-1')
        self.session_name = &quot;session&quot; + str(random.randint(1, 1000))
        self.duration = 3600
        self.session = None
        self.session_aio = None
        response = self.sts_client.get_caller_identity()
        arn_role = response['Arn']
        self.role_name = arn_role
        word_list = [':role', 'instance-profile', ':assumed-role']
        if [ele for ele in word_list if (ele in self.role_name)]:
            if ':assumed-role' in self.role_name:
                sts_to_iam = self.role_name.replace('arn:aws:sts::', '').replace(':assumed-role', '')[:-1].split('/')
                self.role_name = 'arn:aws:iam::' + sts_to_iam[0] + ':role/' + sts_to_iam[1]
            params = {
                &quot;RoleArn&quot;: self.role_name,
                &quot;RoleSessionName&quot;: self.session_name,
                &quot;DurationSeconds&quot;: self.duration,
            }
            response = self.sts_client.assume_role(**params).get(&quot;Credentials&quot;)
            self.metadata = {
                &quot;access_key&quot;: response.get(&quot;AccessKeyId&quot;),
                &quot;secret_key&quot;: response.get(&quot;SecretAccessKey&quot;),
                &quot;token&quot;: response.get(&quot;SessionToken&quot;),
                &quot;expiry_time&quot;: response.get(&quot;Expiration&quot;).isoformat(),
                &quot;expired&quot;: False
            }
        else:
            print(
                &quot;There is no role in the constructor or in the env, so creating a standard session with your &quot;
                &quot;env user credentials instead&quot;)
            self.session = session
            credentials = session.get_credentials().get_frozen_credentials()
            expiry_time = (pytz.utc.localize(datetime.utcnow()) + timedelta(hours=1)).isoformat()
            self.metadata = {
                &quot;access_key&quot;: credentials.access_key,
                &quot;secret_key&quot;: credentials.secret_key,
                &quot;token&quot;: credentials.token,
                &quot;expiry_time&quot;: expiry_time,
                &quot;expired&quot;: False
            }

    def get_metadata(self):
        &quot;&quot;&quot;
        If the credentials has not been automatically refreshed, this method will refresh.
        :return: credentials
        &quot;&quot;&quot;
        # Give a 10 minute interval to make sure the credentials are not expired
        utc_now = pytz.utc.localize(datetime.utcnow())
        expiry_time = parse(self.metadata['expiry_time']) - timedelta(minutes=10)

        if expiry_time &lt; utc_now:
            session = boto3.Session()
            # Refresh session
            print(&quot;Refreshing credentials&quot;)
            print(expiry_time, utc_now)
            params = {
                &quot;RoleArn&quot;: self.role_name,
                &quot;RoleSessionName&quot;: self.session_name + str(random.randint(1, 1000)),
                &quot;DurationSeconds&quot;: self.duration,
            }
            self.sts_client = session.client(&quot;sts&quot;, region_name='eu-west-1')
            response2 = self.sts_client.assume_role(**params).get(&quot;Credentials&quot;)
            self.metadata = {
                &quot;access_key&quot;: response2.get(&quot;AccessKeyId&quot;),
                &quot;secret_key&quot;: response2.get(&quot;SecretAccessKey&quot;),
                &quot;token&quot;: response2.get(&quot;SessionToken&quot;),
                &quot;expiry_time&quot;: response2.get(&quot;Expiration&quot;).isoformat()
            }
        return self.metadata

    def Session(self):
        &quot;&quot;&quot;
        Works as Singleton returning the existing session, if not, create a new one
        :return: if the session already exists return exisiting session
        &quot;&quot;&quot;
        if self.session is None:
            session_credentials = RefreshableCredentials.create_from_metadata(
                metadata=self.get_metadata(),
                refresh_using=self.get_metadata,
                method=&quot;sts-assume-role&quot;,
            )
            session = get_session()
            session._credentials = session_credentials
            session.set_config_variable(&quot;region&quot;, 'eu-west-1')
            autorefresh_session = Session(botocore_session=session)
            self.session = autorefresh_session
        return self.session

    def session_aiobotocore(self, region_name: str = 'eu-west-1', profile_name: str = None):
        &quot;&quot;&quot;
        Get a basic aiobotocore.session.AioSession session with use a vy_session.
        :param region_name -&gt; aws_region to use
        :param profile_name -&gt; local aws profile name to use to create the session
        :return: aiobotocore.session.AioSession
        &quot;&quot;&quot;
        from aiobotocore.session import get_session as get_session_aio
        from aiobotocore.credentials import AioRefreshableCredentials

        if self.session_aio is None:
            aio_session = get_session_aio()
            if region_name is not None:
                aio_session.set_config_variable(&quot;region&quot;, region_name)

            credentials = AioRefreshableCredentials.create_from_metadata(
                metadata=self.get_metadata(),
                refresh_using=self.get_metadata,
                method=&quot;sts-assume-role&quot;,
            )
            aio_session._credentials = credentials
            if region_name is not None:
                aio_session.set_config_variable(&quot;region&quot;, region_name)
            self.session_aio = aio_session
        return self.session_aio
</code></pre>
<p>Then, <strong>I use this class to give a session in the S3FileSystem creation</strong> so that the session can be refreshed automatically:</p>
<pre class=""lang-py prettyprint-override""><code>fs = s3fs.S3FileSystem(anon=False, session=AutoRefreshSession().session_aiobotocore(), asynchronous=async_mode)
</code></pre>
<p>This is a full example that can be used to reproduce the erorr:</p>
<pre class=""lang-py prettyprint-override""><code>import datetime as dt
import pandas as pd
import s3fs
import os


df = pd.DataFrame(data={'test': [1,2,3], 'test2': [4,5,6]})
bucket_name = 'xxxx'
bucket_path = 'xxxxxx'

ses = AutoRefreshSession().session_aiobotocore()
fs = s3fs.S3FileSystem(anon=False, session=ses, asynchronous=False)

# Set csv file name
folder_name_year = dt.datetime.utcnow().strftime('%Y/')
folder_name_month = dt.datetime.utcnow().strftime('%m/')
folder_name_day = dt.datetime.utcnow().strftime('%d/')
folder_name_hour = dt.datetime.utcnow().strftime('%H/')
file_name = dt.datetime.utcnow().strftime('%Y%m%d%H%M%S')
file = file_name + '.csv'
key_name = bucket_name + '/' + bucket_path + folder_name_year + folder_name_month + folder_name_day + folder_name_hour + file
table = bucket_path.split('/')
tmp_name = table[-2] + file
df.to_csv(tmp_name, index=False)

# THIS LINE HERE IS THE ONE FAILING
fs.put(tmp_name, key_name)
os.remove(tmp_name)
</code></pre>
<p>When i run the code above, I get the following error:</p>
<blockquote>
<p>Trace
back (most recent call last):
File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
File &quot;/usr/local/lib/python3.10/site-packages/fsspec/asyn.py&quot;, line
118, in wrapper
return sync(self.loop, func, *args, **kwargs)
File &quot;/usr/local/lib/python3.10/site-packages/fsspec/asyn.py&quot;, line 10
3, in sync
raise return_result
File &quot;/usr/local/lib/python3.10/site-packages/fsspec/asyn.py&quot;, line 56, in _runner
result[0] = aw
ait coro
File &quot;/usr/local/lib/python3.10/site-packages/fsspec/asyn.py&quot;, line 536, in _put
trailing_sep(rpath) or await self._isdir(rp
ath)
File &quot;/usr/local/lib/python3.10/site-packages/s3fs/core.py&quot;, line 1411, in _isdir
return bool(await self._lsdir(path))
File &quot;/usr/local/lib/python3.10/site-packages/s3fs/core.py&quot;, line 706, in _lsdir
async for c in self._iterdir(
File &quot;/usr/local/lib/python
3.10/site-packages/s3fs/core.py&quot;, line 737, in _iterdir
await self.set_session()
File &quot;/usr/local/lib/python3.10/site-packages/s3fs/c
ore.py&quot;, line 527, in set_session
self._s3 = await s3creator.<strong>aenter</strong>()
File &quot;/usr/local/lib/python3.10/site-packages/botocore/clie
nt.py&quot;, line 888, in <strong>getattr</strong>
raise AttributeError(
AttributeError: 'S3' object has no attribute '<strong>aenter</strong>'</p>
</blockquote>
<p>However, if I change the line <code>fs = s3fs.S3FileSystem(anon=False, session=ses, asynchronous=False)</code> and I remove the session I created <code>fs = s3fs.S3FileSystem(anon=False)</code> it works just fine.</p>
<p>What is wrong with my session? Is there something I am missing?</p>
<p>The package versions I am using are the following:
aiobotocore==2.5.4 boto3==1.28.17 botocore==1.31.17 s3fs==2023.9.2 s3transfer==0.6.2</p>
<p>Any help is much appreciated!</p>
",0,1697715238,python;boto;botocore;python-s3fs;aiobotocore,False,256,0,1697764622,https://stackoverflow.com/questions/77323410/s3fs-fails-when-given-aiobotocore-session-to-initialize-the-file-system
77293906,ERROR: botocore.exceptions.HTTPClientError: An HTTP Client raised an unhandled exception: &#39;NoneType&#39; object has no attribute &#39;request&#39;,"<p>So I am pretty new to asynchronous programming in python3. I am creating aws sqs client using types_aiobotocore_sqs and aiobotocore libraries. But I am getting the error mentioned in the title.</p>
<p>Following is the code implementation</p>
<pre><code>url.py file: 

@api_view([&quot;GET&quot;])
def root_view(request):
    async def run_async():
        sqs_helper = await Helper.create(QueueName.TEST_QUEUE.value)
        message = await sqs_helper.get_queue_url()
        return message
    response = asyncio.run(run_async())
    return Response(str(response))
</code></pre>
<p>sqs_helper file</p>
<pre><code>class Helper:
    def __init__(self, queue_name: str) -&gt; None:
        self.queue_name = queue_name
        self.access_key_id = env(&quot;AWS_ACCESS_KEY_ID&quot;)
        self.secret_access_key = env(&quot;AWS_SECRET_ACCESS_KEY&quot;)
        self.region_name = env(&quot;REGION_NAME&quot;)
        self.sqs = None

    @classmethod
    async def create(cls, queue_name):
        instance = cls(queue_name)
        await instance.setup()
        return instance

    async def setup(self):
        self.sqs_client = await self.create_sqs_client()
        self.sqs = Sqs(self.sqs_client, self.queue_name)

    async def create_sqs_client(self):
        session = get_session()
        async with session.create_client('sqs', region_name=self.region_name, aws_access_key_id=self.access_key_id, aws_secret_access_key=self.secret_access_key) as client:
            client: SQSClient
        print(&quot;TYPE&quot; + str(client))
        return client

     async def get_queue_url(self):
        return await self.sqs.get_queue_url()

</code></pre>
<p>Sqs client which initiating the queue</p>
<pre><code>class Sqs:
    def __init__(self, client: SQSClient, queue_name: str) -&gt; None:
        self.queue_name = queue_name
        self._queue_url = &quot;&quot;
        self.client = client

    async def get_queue_url(self) -&gt; str:
        if not self._queue_url:
            try:
                response = await self.client.get_queue_url(QueueName=self.queue_name)
                self._queue_url = response[&quot;QueueUrl&quot;]
            except ClientError as err:
                if (
                    err.response.get(&quot;Error&quot;, {}).get(&quot;Code&quot;)
                    == &quot;AWS.SimpleQueueService.NonExistentQueue&quot;
                ):
                    raise QueueDoesNotExist(
                        f&quot;Queue {self.queue_name} does not exist&quot;
                    ) from err

                raise err

        return self._queue_url
</code></pre>
<p>I am getting the particular error at</p>
<p>response = await self.client.get_queue_url(QueueName=self.queue_name) in Sqs class which in the third code snippet.</p>
",2,1697305759,python;python-3.x;python-asyncio;amazon-sqs;botocore,True,1015,1,1697309531,https://stackoverflow.com/questions/77293906/error-botocore-exceptions-httpclienterror-an-http-client-raised-an-unhandled-e
77237945,E ModuleNotFoundError: No module named &#39;botocore.compress&#39;,"<p>I had both <code>boto3</code> and <code>botocore</code> installed in my conda venv, but was getting this error during compilation:</p>
<pre class=""lang-bash prettyprint-override""><code>E ModuleNotFoundError: No module named 'botocore.compress'
</code></pre>
",0,1696515146,python;pip;boto3;modulenotfounderror;botocore,True,2056,1,1696515146,https://stackoverflow.com/questions/77237945/e-modulenotfounderror-no-module-named-botocore-compress
77153311,Issue with using Stubber with aibotocore in Pytest,"<p>I have the below fixture that creates a mock aibotocore client and activates the botocore stubber on this, this works as expected with my pytest below.</p>
<pre><code>import io
from contextlib import closing
import pytest
from aiobotocore.response import StreamingBody
from aiobotocore.session import get_session
from botocore.stub import ANY, Stubber
from contextlib import asynccontextmanager

@pytest.fixture(scope=&quot;function&quot;)
@asynccontextmanager
async def mock_client():
    with open(&quot;data.json&quot;, mode=&quot;rb&quot;) as f:
        encoded_message = f.read()
        body = AsyncBytesIO(encoded_message) #async version of io.BytesIO
        raw_stream = StreamingBody(body, len(encoded_message))

    response = {&quot;Body&quot;: raw_stream}

    expected_params = {&quot;Bucket&quot;: ANY, &quot;Key&quot;: ANY}

    async with get_session().create_client(
        &quot;s3&quot;, endpoint_url=endpoint
    ) as mock_client:
        stubber = Stubber(mock_client)
        stubber.add_response(&quot;get_object&quot;, response, expected_params)
        stubber.activate()
        yield mock_client

@pytest.mark.asyncio
async def test_func(mock_client):
    with patch(
        &quot;x.x.x.client&quot; #refers to the aibotocore client
    ) as mock:
        mock.return_value = mock_client
        await func_under_test() #this method has the actual AWS S3 calls
        assert XXX
</code></pre>
<p>But now, I want to decouple the client and the stubber as I have a bunch of cases to test with various stubber responses. When I try to remove the stubber and place it inside the pytest, I get below errors -</p>
<pre><code>@pytest.mark.asyncio
async def test_func(mock_client):
    with patch(
        &quot;x.x.x.client&quot; #refers to the aibotocore client
    ) as mock:
        stubber = Stubber(mock_client)
        stubber.add_response(&quot;get_object&quot;, response, expected_params)
        stubber.activate()
        mock.return_value = mock_client
        await func_under_test() #this method has the actual AWS S3 calls
        assert XXX

&gt;       operation_name = self.client.meta.method_to_api_mapping.get(method)
E       AttributeError: '_AsyncGeneratorContextManager' object has no attribute 'meta'
</code></pre>
<p>How can I make this work? I understand that fixture yields a '_AsyncGeneratorContextManager' object and if that's the case how are the tests working with the previous setup which also yields the same? If I remove the asynccontextmanager piece in total, I get the below errors -
'coroutine' object does not support the asynchronous context manager protocol
RuntimeWarning: coroutine 'mock_client' was never awaited</p>
",0,1695326508,python;asynchronous;pytest;botocore;pytest-asyncio,False,98,0,1695326508,https://stackoverflow.com/questions/77153311/issue-with-using-stubber-with-aibotocore-in-pytest
76838565,Mocking file-like gzipped csv for boto3&#39;s StreamingBody,"<p>My real S3 helper does the following:</p>
<pre><code>def read_gzipped_csv_from_s3(self, key):
    return self.bucket.Object(key).get()

obj = S3Helper().read_gzipped_csv_from_s3(key)
df = pd.read_csv(obj['Body'], compression='gzip')
</code></pre>
<p>I need to mock <code>read_gzipped_csv_from_s3()</code> method for unit tests. The problem is that the response should be a gzipped CSV which I must construct from a string because I cannot store anything as tests are running in a Gitlab's pipeline.</p>
<p>So I have some csv as a string:</p>
<pre><code>CSV_DATA = &quot;&quot;&quot;
name,value,control
ABC,1.0,1
DEF,2.0,0
GHI,3.0,-1
&quot;&quot;&quot;
</code></pre>
<p>Then I have some example code for using a regular CSV file to mock botocore.response.StreamingBody:</p>
<pre><code>body_encoded = open('accounts.csv').read().encode()
mock_stream = StreamingBody(io.BytesIO(body_encoded), len(body_encoded))
</code></pre>
<p>but I can't figure out how to create gzipped CSV in memory: there's the beginning I've found somewhere:</p>
<pre><code>import gzip

buffer = BytesIO()
with gzip.GzipFile(fileobj=buffer, mode='wb') as compressed:
    with TextIOWrapper(compressed, encoding='utf-8') as wrapper:
        &lt;can't figure out what's here&gt;
</code></pre>
<p>Help would be much appreciated.</p>
<p>Tried tons of other snippets from SO and modified them but no luck. What I expect: gzipped CSV file-like object to pass to StreamingBody</p>
",0,1691175705,python;boto3;python-unittest;botocore,True,191,1,1691178042,https://stackoverflow.com/questions/76838565/mocking-file-like-gzipped-csv-for-boto3s-streamingbody
76810349,Python unittest- mocking botocore exception,"<p>I am having issues mocking an exception in my python unit unit test I have simplified the code and the test here</p>
<pre><code>session = botocore.session.get_session()
client = session.create_client('stepfunctions', region_name='us-east-1')

    def execute(event, context):
        try:
            # some code here setting the parameters for start_execution
    
            response = client.start_execution(
                stateMachineArn=SM_ARN,
                input=json.dumps(queue_event),
                name=invocation_name
            )
            
        except client.exceptions.ExecutionAlreadyExists as e:
            error_message = f'Execution {invocation_name} already exists'
            logger.info(f&quot;{error_message}. Details: {e}&quot;)
            raise Exception(&quot;Exception already exists&quot;)
</code></pre>
<p>and the test I have written which passes, but when debugging is not entering the <code>except client.exceptions.ExecutionAlreadyExists as e:</code> block of code is this</p>
<pre><code>@mock.patch('lib.lambdas.my_lambda.index.client.start_execution')
    def test_execute_execution_already_exists(self, mock_start_execution):
        from lib.lambdas.my_lambda.index import execute

        # Set the side effect of the mock to raise the ExecutionAlreadyExists exception
        exception = client.exceptions.ExecutionAlreadyExists(operation_name='StartExecution', error_response={
            'Error': {'Code': 'ExecutionAlreadyExists', 'Message': 'Execution already exists'}})
        mock_start_execution.side_effect = exception
        mock_start_execution.return_value = {'ResponseMetadata': {'HTTPStatusCode': 400}}
        # Call the execute function and verify that it raises an exception with the expected message
        with self.assertRaises(Exception) as cm:
            execute(self.event, get_lambda_context())
        expected_message = &quot;Execution already exists&quot;
        actual_message = str(cm.exception)
        self.assertIn(expected_message, actual_message)
</code></pre>
",0,1690882778,python;amazon-web-services;unit-testing;aws-lambda;botocore,False,55,0,1690885734,https://stackoverflow.com/questions/76810349/python-unittest-mocking-botocore-exception
76258710,Boto3 - How to combine TransferConfig and Config?,"<p>I have this Python code and I am trying to find a way to combine two configs:</p>
<pre><code>...
from boto3.s3.transfer import TransferConfig
from botocore.client import Config
...

transfer_config = TransferConfig(max_concurrency=XXX, ...)
config = Config(retries=XX,region_name=XX, ...)

s3 = boto3.Session().resource(&quot;s3&quot;, config = [__HOW I can combibe &quot;transfer_config&quot; and &quot;config&quot; here?])
</code></pre>
<p>I need two configurations because, for example, <code>max_concurrency</code> can<code>t be applied to </code>Config<code>and</code>signature_version<code>to</code>TransferConfig`. I need all these paremeters (and more)</p>
",0,1684197661,python;boto3;botocore,True,207,1,1689233698,https://stackoverflow.com/questions/76258710/boto3-how-to-combine-transferconfig-and-config
76601099,How do I add a different header to every boto3 request?,"<p>I need to be able to dynamically add a custom header based on the object hash. I have previously been able to find examples of how to add a constant header to every request sent, but that's not adequate for my use case.</p>
",-2,1688339187,python;boto3;botocore,False,183,1,1688399682,https://stackoverflow.com/questions/76601099/how-do-i-add-a-different-header-to-every-boto3-request
76337978,Attributes and Dimensions from Segments are empty values when retrieving data from Pinpoint API,"<p>I am trying to get the column and rows for customers from an AWS Pinpoint segment. To do so I used the boto3 SDK. Its my understanding from the <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/pinpoint/client/get_segment.html"" rel=""nofollow noreferrer"">docs</a> that the get_segment method will contain the data for the segment in the attribute and dimension keys .When I try to load the attributes and dimension from an AWS Pinpoint segment using boto3 in an lambda function the dimensions and attributes show up as empty.</p>
<p>here is what my code looks like:</p>
<pre><code>client = boto3.client('pinpoint',region_name='us-east-1')

response = client.get_segment(
        ApplicationId=appid,
        SegmentId=segment_id
    )
    
    print(response)
</code></pre>
<p>the response I am getting is this:</p>
<pre><code>&quot;SegmentResponse&quot;:{
      &quot;ApplicationId&quot;:,
      &quot;Arn&quot;:,
      &quot;CreationDate&quot;:&quot;2023-05-25T13:48:41.464Z&quot;,
      &quot;Dimensions&quot;:{
         &quot;Attributes&quot;:{
            
         },
         &quot;Behavior&quot;:{
            
         },
         &quot;Demographic&quot;:{
            
         },
         &quot;Location&quot;:{
            
         },
         &quot;UserAttributes&quot;:{
            
         }
      },
      &quot;Id&quot;:,
      &quot;LastModifiedDate&quot;:&quot;2023-05-25T13:50:51.675Z&quot;,
      &quot;Name&quot;:&quot;test 1&quot;,
      &quot;SegmentGroups&quot;:{
         &quot;Groups&quot;:[
            {
               &quot;Dimensions&quot;:[
                  {
                     &quot;Attributes&quot;:{
                        
                     },
                     &quot;Behavior&quot;:{
                        
                     },
                     &quot;Demographic&quot;:{
                        
                     },
                     &quot;Location&quot;:{
                        
                     },
                     &quot;Metrics&quot;:{
                        
                     },
                     &quot;UserAttributes&quot;:{
                        
                     }
                  }
               ],
               &quot;SourceSegments&quot;:[
                  
               ],
               &quot;SourceType&quot;:&quot;ANY&quot;,
               &quot;Type&quot;:&quot;ANY&quot;
            }
         ],
         &quot;Include&quot;:&quot;ALL&quot;
      },
      &quot;SegmentType&quot;:&quot;DIMENSIONAL&quot;,
      &quot;tags&quot;:{
         
      },
      &quot;Version&quot;:3
   }

</code></pre>
<p>As you can see the dimensions and attributes are empty. When I create an export for the segment, the csv file populates properly and the correct data is given out with the correct format as well. The segment created is dynamic and uses the imported static segment as the base. If there is an error with the format then it wont take the csv file as an upload.</p>
<p>*ApplicationId, Id and Arn show up fine in the response. I took it off for security reasons.</p>
<p>When I tried pulling the data with the static up csv as the input, the Attributes and Dimension fields dont  show up. I created another segment using the static segment as the base since the new segment would end up being dynamic. The dynamic segment then at least had the dimensions and attribute fields show up, but it is still empty.</p>
",0,1685080276,python;aws-lambda;boto3;aws-pinpoint;botocore,False,51,0,1685081310,https://stackoverflow.com/questions/76337978/attributes-and-dimensions-from-segments-are-empty-values-when-retrieving-data-fr
76332198,aiobotocore - AttributeError: &#39;ClientCreatorContext&#39; object has no attribute &#39;send_message&#39;,"<p>I have a working application that interacts with SQS using python 3.6 and I am required to upgrade the same to Python3.8. Locally, I am using elasticmq as part of the development.</p>
<p>I have a SQSWrapper class that initializes queues and associates sqs_client with each queue. So if I have 10 queues, I will be creating 10 sqs_clients.</p>
<p>Here is the extract of the code, that creates an sqs_client</p>
<pre><code>from aiobotocore.session import get_session
def _create_sqs_client():
        '''
        Creates an SQS client using our botocore session.
        '''
        return get_session().create_client(
            'sqs', **_connection_details
        )

sqs_client = _create_sqs_client()
coro = sqs_client.send_message(
            QueueUrl=await self._get_queue_url(),
            MessageBody=self.dumps(message.data)
        )

result = await asyncio.wait_for(coro, self.API_TIMEOUT)
</code></pre>
<p>I am getting an error message here:</p>
<pre><code>&gt;       coro = sqs_client.send_message(
            QueueUrl=await self._get_queue_url(),
            MessageBody=self.dumps(message.data)
        )
E       AttributeError: 'ClientCreatorContext' object has no attribute 'send_message'
</code></pre>
<p>When I debugged, I came to see that sqs_client is</p>
<pre><code>&lt;aiobotocore.session.ClientCreatorContext object at 0x108cc3a30&gt;
</code></pre>
<p>I see a warning as well:</p>
<pre><code>sys:1: RuntimeWarning: coroutine 'AioSession._create_client' was never awaited
</code></pre>
<p>I am unsure what I am missing here, I would really appreciate if someone could help me to crack this one.</p>
",1,1685017125,python;aws-cli;amazon-sqs;python-3.8;botocore,False,307,0,1685017981,https://stackoverflow.com/questions/76332198/aiobotocore-attributeerror-clientcreatorcontext-object-has-no-attribute-se
76202267,python aiobotocore recursive deletion,"<p>I am using <code>aiobotocore</code> version 2.5.0 in python. (can't use any alternatives)</p>
<p>My bucket name is shop and it has multiple directories (keys)</p>
<pre><code>s3://shop/food/vegetables
</code></pre>
<p>I want to recursively delete all files that are in <strong>vegetables</strong> directory or delete the entire directory (both options work)</p>
<p>How can I do this? Could not find any examples for recursive deletion in documentation - <a href=""https://aiobotocore.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">https://aiobotocore.readthedocs.io/en/latest/</a></p>
<p>Currently, I can delete only specific file with full path of it.</p>
",0,1683561508,python;amazon-s3;botocore,False,58,0,1683561508,https://stackoverflow.com/questions/76202267/python-aiobotocore-recursive-deletion
49943938,UnrecognizedClientException when using botocore stubber,"<p>I am using <code>unittest</code> to test a function that makes a call to AWS using <code>boto3</code>.</p>

<p>The function looks like this:</p>

<pre><code>import boto3


def my_function():
    client = boto3.client('athena')
    res = client.start_query_exeuction(
        QueryString='SELECT * FROM logs',
        ResultConfiguration={'OutputLocation': 's3://mybucket'}
    )

    return res['QueryExecutionId']
</code></pre>

<p>I am using botocore stubber to stub this request in my unit tests like this:</p>

<pre><code>from botocore.stub import Stubber
import botocore.session

def test_my_function():    
    client = botocore.session.get_session().create_client('athena')
    client_res = {'QueryExecutionId': 'testid'}
    exp_params = {
        'QueryString': 'SELECT * FROM logs',
        'ResultConfiguration': {
            'OutputLocation': 's3://mybucket'
        }
    }
    with Stubber(client) as stubber:
        stubber.add_response('start_query_execution', client_res, exp_params)
        res = my_function()

    self.assertEqual(res, 'testid')
</code></pre>

<p>This test is failing with </p>

<blockquote>
  <p>botocore.exceptions.ClientError: An error occurred
  (UnrecognizedClientException) when calling the StartQueryExecution
  operation: The security token included in the request is invalid.</p>
</blockquote>

<p>Why would this be failing? Is it because I am creating a new client in <code>my_function()</code> which is different from the client used in the stubber? If so, how can I test this?</p>

<p>Any help is much appreciated.</p>
",2,1524234309,python;boto3;python-unittest;botocore,True,3717,3,1682001445,https://stackoverflow.com/questions/49943938/unrecognizedclientexception-when-using-botocore-stubber
48091874,Downloading multiple S3 objects in parallel in Python,"<p>Is there a way to concurrently download S3 files using boto3 in Python3?
I am aware of the <a href=""https://github.com/aio-libs/aiobotocore"" rel=""noreferrer"">aiobotocore</a> library, but I would like to know if there is a way to do it using the standard <a href=""https://github.com/boto/boto3"" rel=""noreferrer"">boto3</a> library.</p>
",15,1515056790,python;python-3.x;amazon-s3;boto3;botocore,True,20605,3,1678471147,https://stackoverflow.com/questions/48091874/downloading-multiple-s3-objects-in-parallel-in-python
52703985,Cannot mock method from boto3 (using botocore.stub.Stubber),"<p>In my <code>podcasts.py</code> I have at first lines:</p>

<pre><code>kms = boto3.client('kms')
access_key = kms.decrypt(
    CiphertextBlob=base64.b64decode(os.environ['access_key'])
)['Plaintext'].decode()
</code></pre>

<p>According to the <a href=""https://botocore.amazonaws.com/v1/documentation/api/latest/reference/stubber.html"" rel=""nofollow noreferrer"">docs</a>, I tried to stub it in my <code>podcasts_test.py</code>:</p>

<pre><code>import base64
import os

from botocore.stub import Stubber

os.environ['access_key'] = base64.b64encode('my_test_access_key'.encode()).decode()
client = boto3.client('kms')
stubber = Stubber(client)
stubber.add_response('decrypt', {'Plaintext': b'my_test_key'})
stubber.activate()

import podcasts_build
</code></pre>

<p>But I get:</p>

<pre><code>Traceback (most recent call last):
  File ""podcasts_build_test.py"", line 14, in &lt;module&gt;
    import podcasts_build
  File ""/Users/vitaly/intelligent-speaker/backend/lambdas/podcasts_build/podcasts_build.py"", line 23, in &lt;module&gt;
    CiphertextBlob=base64.b64decode(os.environ['access_key'])
  File ""/usr/local/lib/python3.7/site-packages/botocore/client.py"", line 320, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File ""/usr/local/lib/python3.7/site-packages/botocore/client.py"", line 623, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.errorfactory.InvalidCiphertextException: An error occurred (InvalidCiphertextException) when calling the Decrypt operation:
</code></pre>
",0,1539007270,python;boto3;amazon-kms;botocore,True,5420,1,1677611793,https://stackoverflow.com/questions/52703985/cannot-mock-method-from-boto3-using-botocore-stub-stubber
75415616,AWS Boto3 sts get_caller_identity - catching exceptions if credentials are invalid,"<p>With a Python app, using Boto3 v1.26.59 (and botocore of same version) about the first thing done is to try to get the username of the user. We have Identity Center (SSO) users. With aged credentials (token), two exceptions are thrown and I don't seem to be able to catch them. Here is a snippet:</p>
<pre class=""lang-py prettyprint-override""><code>import boto3  # type: ignore
import botocore.errorfactory as ef
import botocore.exceptions as bcexp


def profile_user_name(profile_name: str) -&gt; Optional[str]:
    session = boto3.Session(profile_name=profile_name)
    sts = session.client(&quot;sts&quot;)
    try:
        user_id = sts.get_caller_identity().get(&quot;UserId&quot;)
        return user_id.split(&quot;:&quot;)[-1].split(&quot;@&quot;)[0]
    except ef.UnauthorizedException as e:
        _logger.error(f'Not authenticated. Please execute:  aws sso login --profile {profile_name}')
        return None
    except bcexp.UnauthorizedSSOTokenError as e:
        _logger.error(f'Not authenticated. Please execute:  aws sso login --profile {profile_name}')
        return None
    except Exception as e:
        _logger.error(f&quot;Encountered exception '{str(e)}'!&quot;)
        return None
</code></pre>
<p>Exceptions thrown by the above code look like these:</p>
<pre><code>Refreshing temporary credentials failed during mandatory refresh period.
Traceback (most recent call last):
  File &quot;/Users/kevinbuchs/lib/python3.11/site-packages/botocore/credentials.py&quot;, line 2121, in _get_credentials
    response = client.get_role_credentials(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/kevinbuchs/lib/python3.11/site-packages/botocore/client.py&quot;, line 530, in _api_call
    return self._make_api_call(operation_name, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/kevinbuchs/lib/python3.11/site-packages/botocore/client.py&quot;, line 960, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.errorfactory.UnauthorizedException: An error occurred (UnauthorizedException) when calling the GetRoleCredentials operation: Session token not found or invalid

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/Users/kevinbuchs/lib/python3.11/site-packages/botocore/credentials.py&quot;, line 510, in _protected_refresh
    metadata = self._refresh_using()
               ^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/kevinbuchs/lib/python3.11/site-packages/botocore/credentials.py&quot;, line 657, in fetch_credentials
    return self._get_cached_credentials()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/kevinbuchs/lib/python3.11/site-packages/botocore/credentials.py&quot;, line 667, in _get_cached_credentials
    response = self._get_credentials()
               ^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/kevinbuchs/lib/python3.11/site-packages/botocore/credentials.py&quot;, line 2123, in _get_credentials
    raise UnauthorizedSSOTokenError()
botocore.exceptions.UnauthorizedSSOTokenError: The SSO session associated with this profile has expired or is otherwise invalid. To refresh this SSO session run aws sso login with the corresponding profile.
Encountered exception 'The SSO session associated with this profile has expired or is otherwise invalid. To refresh this SSO session run aws sso login with the corresponding profile.'!
The auth profile 'dev-devaccess-default' is not logged in. Login with 'aws sso login --profile dev-devaccess-default' and retry!
</code></pre>
<p>I thought I would check to see if I am missing some trick before submitting a GitHub issue.</p>
",0,1676059129,python;amazon-web-services;single-sign-on;boto;botocore,True,952,1,1677273002,https://stackoverflow.com/questions/75415616/aws-boto3-sts-get-caller-identity-catching-exceptions-if-credentials-are-inval
75367378,Is it possible to shrink botocore within site-packages?,"<p>I've just seen that my web applications docker image is enormous. A 600-MB reason is the packages I install for it. The biggest single offender is botocore with 77.7 MB.</p>
<p>Apparently this is known behavior: <a href=""https://github.com/boto/botocore/issues/1629"" rel=""noreferrer"">https://github.com/boto/botocore/issues/1629</a></p>
<p>Is it possible to redue that size?</p>
<h2>Analysis</h2>
<ul>
<li>The <code>tar.gz</code> distribution is just 10.8 MB: <a href=""https://pypi.org/project/botocore/#files"" rel=""noreferrer"">https://pypi.org/project/botocore/#files</a></li>
<li>75MB are in <a href=""https://github.com/boto/botocore/tree/develop/botocore/data"" rel=""noreferrer"">the <code>data</code> directory</a></li>
<li>For every single AWS service, there seem to be <a href=""https://github.com/boto/botocore/tree/develop/botocore/data/ec2"" rel=""noreferrer"">multiple folders</a> (some kind of versioning?) and <a href=""https://github.com/boto/botocore/blob/develop/botocore/data/ec2/2014-09-01/service-2.json"" rel=""noreferrer"">a <code>service-2.json</code></a></li>
<li>The <code>service-2.json</code> files probably use most of the space. They are not minified and they contain a lot of information that seems not to be necessary for running a production system (e.g. <code>description</code>).</li>
</ul>
<p>Is there a way to either completely avoid botocore or in any other way reduce botocores size for the Docker image? (I'm only using S3)</p>
",6,1675724071,python;amazon-web-services;docker;botocore,False,625,0,1675724071,https://stackoverflow.com/questions/75367378/is-it-possible-to-shrink-botocore-within-site-packages
75160609,Download Amazon AWS-S3 GEFS Ensenble DATA Loop,"<p>I would like to shorten the following code such that it downloads the bucket subfolder from the Amazon server</p>
<pre><code>import boto3
import botocore
import datetime
import xarray as xr
import matplotlib.pyplot as plt
import cfgrib

#cfs_bucket = 'noaa-cfs-pds'

cfs_bucket = 'noaa-gefs-pds'

# Want to get 5 days worth of forecast data = 5 days x 4 forecast cycles/day
cycles2get = 4*5 

client = boto3.client('s3', config=botocore.client.Config(signature_version=botocore.UNSIGNED))

paginator = client.get_paginator('list_objects')
result = paginator.paginate(Bucket=cfs_bucket, Delimiter='/')

    count = 0
    
    for prefix in result.search('CommonPrefixes'):
        print(prefix.get('Prefix'))
        count += 1 
        if(count&gt;49):
            break # skip the for loop after reporting 50 values

gefs.20170101/
gefs.20170102/
gefs.20170103/
gefs.20170104/
gefs.20170105/
gefs.20170106/
gefs.20170107/
gefs.20170108/
gefs.20170109/
gefs.20170110/
gefs.20170111/
gefs.20170112/
gefs.20170113/
gefs.20170114/
gefs.20170115/
gefs.20170116/
gefs.20170117/
gefs.20170118/
gefs.20170119/
gefs.20170120/
gefs.20170121/
gefs.20170122/
gefs.20170123/
gefs.20170124/
gefs.20170125/
gefs.20170126/
gefs.20170127/
gefs.20170128/
gefs.20170129/
gefs.20170130/
gefs.20170131/
gefs.20170201/
gefs.20170202/
gefs.20170203/
gefs.20170204/
gefs.20170205/
gefs.20170206/
gefs.20170207/
gefs.20170208/
gefs.20170209/
gefs.20170210/
gefs.20170211/
gefs.20170212/
gefs.20170213/
gefs.20170214/
gefs.20170215/
gefs.20170216/
gefs.20170217/
gefs.20170218/
gefs.20170219/

keys = []
date = datetime.datetime(2017,11,28,hour=0)
prefix = date.strftime('gefs.%Y%m%d/00/')

   print(prefix)

response = client.list_objects_v2(Bucket=cfs_bucket, Prefix=prefix)
response_meta = response.get('ResponseMetadata')

if response_meta.get('HTTPStatusCode') == 200:
    contents = response.get('Contents')
    if contents == None:
        print(&quot;No objects are available for %s&quot; % date.strftime('%B %d, %Y'))
    else:
        for obj in contents:
            keys.append(obj.get('Key'))
        print(&quot;There are %s objects available for %s\n--&quot; % (len(keys), date.strftime('%B %d, %Y')))
        
        count = 0
        for k in keys:
            print(k)
            
            count += 1 
            if(count&gt;1200):
                break # skip the for loop after reporting 50 values
else:
    print(&quot;There was an error with your request.&quot;)

gefs_filebase = 'gec'

sdate = date
ensnum = '00' # GEFS Ensemble number
tz = 't00z'
pgr = 'pgrb2af'
pga = 'pgrb2aanl'
fdate = sdate

flx_files = []
    
for i in range(cycles2get):
    
    sdatestr = sdate.strftime('%Y%m%d')
    fdatestr = fdate.strftime('%Y%m%d')
    
    flx_file_A   = gefs_filebase + ensnum + '.' + tz + '.' + pga
    flx_file_0   = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '000'
    flx_file_6   = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '006'
    flx_file_12  = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '012'
    flx_file_18  = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '018'
    flx_file_24  = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '024'
    flx_file_30  = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '030'
    flx_file_36  = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '036'
    flx_file_42  = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '042'
    flx_file_48  = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '048'
    flx_file_54  = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '054'
    flx_file_60  = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '060'
    flx_file_66  = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '066'
    flx_file_72  = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '072'
    flx_file_78  = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '078'
    flx_file_84  = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '084'
    flx_file_90  = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '090'
    flx_file_96  = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '096'
    flx_file_102 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '102'
    flx_file_108 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '108'
    flx_file_114 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '114'
    flx_file_120 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '120'
    flx_file_126 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '126'
    flx_file_132 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '132'
    flx_file_138 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '138'
    flx_file_144 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '144'
    flx_file_150 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '150'
    flx_file_156 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '156'
    flx_file_162 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '162'
    flx_file_168 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '168'
    flx_file_174 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '174'
    flx_file_180 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '180'
    flx_file_186 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '186'
    flx_file_192 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '192'
    flx_file_198 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '198'
    flx_file_204 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '204'
    flx_file_210 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '210'
    flx_file_216 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '216'
    flx_file_210 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '210'
    flx_file_216 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '216'
    flx_file_222 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '222'
    flx_file_228 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '228'
    flx_file_234 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '234'
    flx_file_240 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '240'
    flx_file_246 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '246'
    flx_file_252 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '252'
    flx_file_258 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '258'
    flx_file_264 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '264'
    flx_file_270 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '270'
    flx_file_276 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '276'
    flx_file_282 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '282'
    flx_file_288 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '288'
    flx_file_288 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '288'
    flx_file_294 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '294'
    flx_file_300 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '300'
    flx_file_306 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '306'
    flx_file_312 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '312'
    flx_file_318 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '318'
    flx_file_324 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '324'
    flx_file_330 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '330'
    flx_file_336 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '336'
    flx_file_342 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '342'
    flx_file_348 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '348'
    flx_file_354 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '354'
    flx_file_360 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '360'
    flx_file_366 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '366'
    flx_file_372 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '372'
    flx_file_378 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '378'
    flx_file_384 = gefs_filebase + ensnum + '.' + tz + '.' + pgr + '384'
    
#   flx_file = gefs_filebase + ensnum + '.' + tz + '.' + pgr +  f'{(fcst)}'
    flx_files.append(flx_file)
    
    # Update the valid datetime by adding six hours from the valid time just used
    fdate = fdate + datetime.timedelta(hours=6)

print('Downloading '+flx_file_A)
client.download_file(cfs_bucket,prefix+flx_file_A,flx_file_A)
print('Downloading '+flx_file_0)
client.download_file(cfs_bucket,prefix+flx_file_0,flx_file_0)
print('Downloading '+flx_file_6)
client.download_file(cfs_bucket,prefix+flx_file_6,flx_file_6)
print('Downloading '+flx_file_12)
client.download_file(cfs_bucket,prefix+flx_file_12,flx_file_12)
print('Downloading '+flx_file_18)
client.download_file(cfs_bucket,prefix+flx_file_18,flx_file_18)
print('Downloading '+flx_file_24)
client.download_file(cfs_bucket,prefix+flx_file_24,flx_file_24)
print('Downloading '+flx_file_30)
client.download_file(cfs_bucket,prefix+flx_file_30,flx_file_30)
print('Downloading '+flx_file_36)
client.download_file(cfs_bucket,prefix+flx_file_36,flx_file_36)
print('Downloading '+flx_file_42)
client.download_file(cfs_bucket,prefix+flx_file_42,flx_file_42)
print('Downloading '+flx_file_48)
client.download_file(cfs_bucket,prefix+flx_file_48,flx_file_48)
print('Downloading '+flx_file_54)
client.download_file(cfs_bucket,prefix+flx_file_54,flx_file_54)
print('Downloading '+flx_file_60)
client.download_file(cfs_bucket,prefix+flx_file_60,flx_file_60)
print('Downloading '+flx_file_66)
client.download_file(cfs_bucket,prefix+flx_file_66,flx_file_66)
print('Downloading '+flx_file_72)
client.download_file(cfs_bucket,prefix+flx_file_72,flx_file_72)
print('Downloading '+flx_file_78)
client.download_file(cfs_bucket,prefix+flx_file_78,flx_file_78)
print('Downloading '+flx_file_84)
client.download_file(cfs_bucket,prefix+flx_file_84,flx_file_84)
print('Downloading '+flx_file_90)
client.download_file(cfs_bucket,prefix+flx_file_90,flx_file_90)
print('Downloading '+flx_file_96)
client.download_file(cfs_bucket,prefix+flx_file_96,flx_file_96)
print('Downloading '+flx_file_102)
client.download_file(cfs_bucket,prefix+flx_file_102,flx_file_102)
print('Downloading '+flx_file_108)
client.download_file(cfs_bucket,prefix+flx_file_108,flx_file_108)
print('Downloading '+flx_file_114)
client.download_file(cfs_bucket,prefix+flx_file_114,flx_file_114)
print('Downloading '+flx_file_120)
client.download_file(cfs_bucket,prefix+flx_file_120,flx_file_120)
print('Downloading '+flx_file_126)
client.download_file(cfs_bucket,prefix+flx_file_126,flx_file_126)
print('Downloading '+flx_file_132)
client.download_file(cfs_bucket,prefix+flx_file_132,flx_file_132)
print('Downloading '+flx_file_138)
client.download_file(cfs_bucket,prefix+flx_file_138,flx_file_138)
print('Downloading '+flx_file_144)
client.download_file(cfs_bucket,prefix+flx_file_144,flx_file_144)
print('Downloading '+flx_file_150)
client.download_file(cfs_bucket,prefix+flx_file_150,flx_file_150)
print('Downloading '+flx_file_156)
client.download_file(cfs_bucket,prefix+flx_file_156,flx_file_156)
print('Downloading '+flx_file_162)
client.download_file(cfs_bucket,prefix+flx_file_162,flx_file_162)
print('Downloading '+flx_file_168)
client.download_file(cfs_bucket,prefix+flx_file_168,flx_file_168)
print('Downloading '+flx_file_174)
client.download_file(cfs_bucket,prefix+flx_file_174,flx_file_174)
print('Downloading '+flx_file_180)
client.download_file(cfs_bucket,prefix+flx_file_180,flx_file_180)
print('Downloading '+flx_file_186)
client.download_file(cfs_bucket,prefix+flx_file_186,flx_file_186)
print('Downloading '+flx_file_192)
client.download_file(cfs_bucket,prefix+flx_file_192,flx_file_192)
print('Downloading '+flx_file_198)
client.download_file(cfs_bucket,prefix+flx_file_198,flx_file_198)
print('Downloading '+flx_file_204)
client.download_file(cfs_bucket,prefix+flx_file_204,flx_file_204)
print('Downloading '+flx_file_210)
client.download_file(cfs_bucket,prefix+flx_file_210,flx_file_210)
print('Downloading '+flx_file_216)
client.download_file(cfs_bucket,prefix+flx_file_216,flx_file_216)
print('Downloading '+flx_file_222)
client.download_file(cfs_bucket,prefix+flx_file_222,flx_file_222)
print('Downloading '+flx_file_228)
client.download_file(cfs_bucket,prefix+flx_file_228,flx_file_228)
print('Downloading '+flx_file_234)
client.download_file(cfs_bucket,prefix+flx_file_234,flx_file_234)
print('Downloading '+flx_file_240)
client.download_file(cfs_bucket,prefix+flx_file_240,flx_file_240)
print('Downloading '+flx_file_246)
client.download_file(cfs_bucket,prefix+flx_file_246,flx_file_246)
print('Downloading '+flx_file_252)
client.download_file(cfs_bucket,prefix+flx_file_252,flx_file_252)
print('Downloading '+flx_file_258)
client.download_file(cfs_bucket,prefix+flx_file_258,flx_file_258)
print('Downloading '+flx_file_264)
client.download_file(cfs_bucket,prefix+flx_file_264,flx_file_264)
print('Downloading '+flx_file_270)
client.download_file(cfs_bucket,prefix+flx_file_270,flx_file_270)
print('Downloading '+flx_file_276)
client.download_file(cfs_bucket,prefix+flx_file_276,flx_file_276)
print('Downloading '+flx_file_282)
client.download_file(cfs_bucket,prefix+flx_file_282,flx_file_282)
print('Downloading '+flx_file_288)
client.download_file(cfs_bucket,prefix+flx_file_288,flx_file_288)
print('Downloading '+flx_file_294)
client.download_file(cfs_bucket,prefix+flx_file_294,flx_file_294)
print('Downloading '+flx_file_300)
client.download_file(cfs_bucket,prefix+flx_file_300,flx_file_300)
print('Downloading '+flx_file_306)
client.download_file(cfs_bucket,prefix+flx_file_306,flx_file_306)
print('Downloading '+flx_file_312)
client.download_file(cfs_bucket,prefix+flx_file_312,flx_file_312)
print('Downloading '+flx_file_318)
client.download_file(cfs_bucket,prefix+flx_file_318,flx_file_318)
print('Downloading '+flx_file_324)
client.download_file(cfs_bucket,prefix+flx_file_324,flx_file_324)
print('Downloading '+flx_file_330)
client.download_file(cfs_bucket,prefix+flx_file_330,flx_file_330)
print('Downloading '+flx_file_336)
client.download_file(cfs_bucket,prefix+flx_file_336,flx_file_336)
print('Downloading '+flx_file_342)
client.download_file(cfs_bucket,prefix+flx_file_342,flx_file_342)
print('Downloading '+flx_file_348)
client.download_file(cfs_bucket,prefix+flx_file_348,flx_file_348)
print('Downloading '+flx_file_354)
client.download_file(cfs_bucket,prefix+flx_file_354,flx_file_354)
print('Downloading '+flx_file_360)
client.download_file(cfs_bucket,prefix+flx_file_360,flx_file_360)
print('Downloading '+flx_file_366)
client.download_file(cfs_bucket,prefix+flx_file_366,flx_file_366)
print('Downloading '+flx_file_372)
client.download_file(cfs_bucket,prefix+flx_file_372,flx_file_372)
print('Downloading '+flx_file_378)
client.download_file(cfs_bucket,prefix+flx_file_378,flx_file_378)
print('Downloading '+flx_file_384)
client.download_file(cfs_bucket,prefix+flx_file_384,flx_file_384)
</code></pre>
<p>The above Code works to download the specific files. But I would like to shorten this code so that it loops the forecast time from 000 to 384, how can I modify the code above? And in this case direct these files to a specific directory with the name of the BUCKET</p>
",1,1674051412,python;amazon-web-services;amazon-s3;boto3;botocore,True,141,1,1674052808,https://stackoverflow.com/questions/75160609/download-amazon-aws-s3-gefs-ensenble-data-loop
75136438,Multithreading in AWS boto pagination Python not working,"<p>I tried multithreading with paginator but instead of running on multiple thread, it is running on a single thread.</p>
<ul>
<li>response: &lt;class 'botocore.paginate.PageIterator'&gt;</li>
<li>page.get('Contents') return a list of dict containing object name and object size</li>
<li>Since &quot;list_objects_v2&quot; only list 1000 object per response So thatâ€™s why we need Paginator to iterate over every page where each page should have 1000 object.</li>
</ul>
<p>I was giving the response iterator to the thread so that this iterator load can be distributed between different threads. But instead of that whole iterator works on single thread</p>
<pre><code>def list_s3_files_using_paginator_multithreading(bucket_name, prefix = None):
    import boto3
    from threading import Thread
    from queue import Queue   
    &quot;&quot;&quot;
    This functions list all files in s3 using paginator.
    Paginator is useful when you have 1000s of files in S3.
    S3 list_objects_v2 can list at max 1000 files in one go.
    :return: None
    &quot;&quot;&quot;
    def safe_dedicated_writing_thread(filepath, queue):
        with open(filepath, 'w') as f:
            while True:
                # Retrieve s3 ls object name from the queue
                line = queue.get()
                
                # check if we have reached end of s3 ls dump
                # then we close the file
                if line is None:
                    break
                
                # Write to file
                f.write(line)
                
                # In Python, files are automatically flushed
                # while closing them but, a programmer can flush
                # a file before closing it
                f.flush()
                
                # Mark the unit of work complete
                queue.task_done()
                
            # mark the exit signal as processed, after the file was closed
            queue.task_done()
            
    # Create a shared queue
    queue = Queue()
    
    # Path of the shared file
    filepath = 's3_ls_dump.txt'
    
    # Create And Start the file writer Thread
    writer_thread = Thread(target = safe_dedicated_writing_thread, args = (filepath, queue), daemon = True)
    writer_thread.start()
    
    
    # Creating a Paginator to list 1000 object per response
    s3_client = boto3.client(&quot;s3&quot;)
    paginator = s3_client.get_paginator(&quot;list_objects_v2&quot;)
    response = paginator.paginate(Bucket=bucket_name, Prefix = prefix,  PaginationConfig={&quot;PageSize&quot;: 1000})
 
        
    def list_object(page):
        files = page.get('Contents')
        for file in files:
            queue.put(file['Key'])
    
    # Configure worker Thread
    with concurrent.futures.ThreadPoolExecutor() as executor:
        executor.map(list_object, response)
    
    
    # Signal the file writer thread that we are done
    queue.put(None)
    
    # Wait for all the queue to be processed
    queue.join()
</code></pre>
<p>I also tried one more way to mutlithread AWS boto pagination. Is it even possible to multithread?</p>
<pre><code>def list_object(page, queue):
    files = page.get('Contents')
    for file in files:
        queue.put(file['Key'])


threads = [Thread(target = list_object, args = (page, queue)) for page in response]


for thread in threads:
    thread.start()
    

for thread in threads:
    thread.join()
</code></pre>
",1,1673883321,python;amazon-s3;python-multithreading;boto;botocore,False,371,0,1673883321,https://stackoverflow.com/questions/75136438/multithreading-in-aws-boto-pagination-python-not-working
54084758,Python Botocore remove unused files from data folder,"<p>I have a Python app running on an OpenWrt router (Chaos Calmer) that uses the boto3 package for creating and updating AWS IoT thing shadows. The app is packaged with its own Python and dependencies (including boto3 and botocore). boto3 depends on botocore, which has a data folder with a bunch of json files for other AWS services unused by my app (S3, dynamoDB, route53, lambda, etc. etc.). Out of all the app's resources, this folder takes up the most space by far. Eventually I will need to do OTA updates over cellular to many of these routers, meaning data usage is a concern. Therefore I would like to trim out as many unused json files from the data folder as possible without breaking the code.</p>
<p>These are the only three boto3 and botocore imports my app uses:</p>
<pre><code>from boto3 import client as boto3_client
from boto3.exceptions import ResourceNotExistsError
from botocore.exceptions import ClientError
</code></pre>
<p>How can I determine which json files I can get rid of?</p>
",9,1546917184,python;boto3;botocore,False,1084,1,1673664136,https://stackoverflow.com/questions/54084758/python-botocore-remove-unused-files-from-data-folder
37143597,Mocking boto3 S3 client method Python,"<p>I'm trying to mock a singluar method from the boto3 s3 client object to throw an exception. But I need all other methods for this class to work as normal.</p>

<p>This is so I can test a singular Exception test when and error occurs performing a <a href=""http://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.Client.upload_part_copy"" rel=""noreferrer"">upload_part_copy</a></p>

<p><strong>1st Attempt</strong></p>

<pre><code>import boto3
from mock import patch

with patch('botocore.client.S3.upload_part_copy', side_effect=Exception('Error Uploading')) as mock:
    client = boto3.client('s3')
    # Should return actual result
    o = client.get_object(Bucket='my-bucket', Key='my-key')
    # Should return mocked exception
    e = client.upload_part_copy()
</code></pre>

<p>However this gives the following error:</p>

<pre><code>ImportError: No module named S3
</code></pre>

<p><strong>2nd Attempt</strong></p>

<p>After looking at the botocore.client.py source code I found that it is doing something clever and the method <code>upload_part_copy</code> does not exist. I found that it seems to call <code>BaseClient._make_api_call</code> instead so I tried to mock that</p>

<pre><code>import boto3
from mock import patch

with patch('botocore.client.BaseClient._make_api_call', side_effect=Exception('Error Uploading')) as mock:
    client = boto3.client('s3')
    # Should return actual result
    o = client.get_object(Bucket='my-bucket', Key='my-key')
    # Should return mocked exception
    e = client.upload_part_copy()
</code></pre>

<p>This throws an exception... but on the <code>get_object</code> which I want to avoid.</p>

<p>Any ideas about how I can only throw the exception on the <code>upload_part_copy</code> method?</p>
",133,1462895975,python;mocking;boto;boto3;botocore,True,184113,8,1672835114,https://stackoverflow.com/questions/37143597/mocking-boto3-s3-client-method-python
34447304,Example of update_item in dynamodb boto3,"<p>Following <a href=""http://boto3.readthedocs.org/en/latest/reference/services/dynamodb.html#DynamoDB.Table.update_item"" rel=""noreferrer"">the documentation</a>, I'm trying to create an update statement that will update or add if not exists only one attribute in a dynamodb table.</p>

<p>I'm trying this</p>

<pre><code>response = table.update_item(
    Key={'ReleaseNumber': '1.0.179'},
    UpdateExpression='SET',
    ConditionExpression='Attr(\'ReleaseNumber\').eq(\'1.0.179\')',
    ExpressionAttributeNames={'attr1': 'val1'},
    ExpressionAttributeValues={'val1': 'false'}
)
</code></pre>

<p>The error I'm getting is:</p>

<p><code>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the UpdateItem operation: ExpressionAttributeNames contains invalid key: Syntax error; key: ""attr1""</code></p>

<p>If anyone has done anything similar to what I'm trying to achieve please share example.</p>
",77,1450931900,python;amazon-dynamodb;boto3;botocore,True,177712,10,1671906594,https://stackoverflow.com/questions/34447304/example-of-update-item-in-dynamodb-boto3
73641803,How can I query UserId for AWS SSO Users using Boto3,"<p>How can I get <code>UserId</code> for AWS SSO Users using Boto3.</p>
<p>I wanted to use it to assign permissions to a user for a specific aws account using below code, however, this requires <code>PrincipalId</code> which is some 16-20 digit number associated with each user and is called <code>User ID</code> in the AWS console.</p>
<p>You can read about it - <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sso-admin.html#SSOAdmin.Client.create_account_assignment"" rel=""nofollow noreferrer"">here</a></p>
<pre><code>response = client.create_account_assignment(
    InstanceArn='string',
    TargetId='string',
    TargetType='AWS_ACCOUNT',
    PermissionSetArn='string',
    PrincipalType='USER'|'GROUP',
    PrincipalId='string'
)
</code></pre>
",2,1662587545,python;amazon-web-services;boto3;botocore;aws-sso,True,1176,2,1671059290,https://stackoverflow.com/questions/73641803/how-can-i-query-userid-for-aws-sso-users-using-boto3
74745877,Why the does AWS CLI use different AWS services on darwin (mac) vs. linux? When does the rds describe-db-instances command use STS?,"<p>Running</p>
<pre class=""lang-bash prettyprint-override""><code>aws --profile=REDACTED --region=REDACTED rds describe-db-instances
</code></pre>
<p>fails for me on linux but succeeds on darwin (mac).</p>
<p>It seems that on my linux, a call is made to AWS's Simple Token Service that I don't have permissions to. But for some reason, that call is skipped on my mac.</p>
<p>Why might the first call run by the AWS CLI vary by system? Why would a call to the Simple Token Service be required when using the AWS CLI?</p>
<p>Is there perhaps something related to authentication or session management that I don't have configured properly on the linux machine I'm running on?</p>
<hr />
<p>I confirmed I'm using the same version, <code>aws-cli/1.25.76 Python/3.10.8 Linux/6.0.11 botocore/1.27.75</code>, on each machine.</p>
<p>At first, the error appeared permissions-related</p>
<pre><code>
An error occurred (AccessDenied) when calling the AssumeRole operation: User: REDACTED is not authorized to perform: sts.AssumeRole on resource: REDACTED
</code></pre>
<p>I confirmed I do not have <code>sts.AssumeRole</code>.</p>
<p>However, while investigating, I accidentally made a typo and noticed something strange: on darwin, the first call made by the above command appears to be to the <code>rds</code> service, while on linux the first call is to the <code>sts</code> service.</p>
<p>For example,</p>
<pre class=""lang-bash prettyprint-override""><code>aws --profile=REDACTED --region=typo-region rds describe-db-instances
</code></pre>
<p>yields this on my darwin machine</p>
<pre><code>
Could not connect to the endpoint URL: &quot;https://rds.typo-region.amazonaws.com/&quot;
</code></pre>
<p>and this on my linux machine</p>
<pre><code>
Could not connect to the endpoint URL: &quot;https://sts.typo-region.amazonaws.com/&quot;
</code></pre>
<p>I'm not yet familiar enough with how the <code>aws</code> command and it's <code>boto</code> internals work to understand why the extra call to <code>sts</code> is being made on one machine and not the other.</p>
",1,1670603832,python;amazon-web-services;amazon-rds;botocore;aws-sts,True,60,1,1670604302,https://stackoverflow.com/questions/74745877/why-the-does-aws-cli-use-different-aws-services-on-darwin-mac-vs-linux-when
74058094,How to run parallel lambdas from another lambda?,"<p>I'm trying to trigger several lambdas in parallel from another lambda. I'm using aiobotocore and this works fine locally but when I try to run it on AWSLambda, I have an error on the import modules:</p>
<p><code>Unable to import module 'lambda_function': cannot import name 'apply_request_checksum' from 'botocore.client' (/var/runtime/botocore/client.py)</code></p>
<p>I have tried to use aioboto3, but as it's a wrapper the same thing happens, I've checked the versions of the packages, they match. I've tried changing the python version, did not work.</p>
<p>Googling this specific error did not help either. If you need any precision, please let me know, any hint would be highly appreciated.</p>
",2,1665673823,python;amazon-web-services;aws-lambda;botocore,True,744,3,1668097727,https://stackoverflow.com/questions/74058094/how-to-run-parallel-lambdas-from-another-lambda
73955824,S3 boto - list objects in parallel,"<p>I want to quickly (in parallel?) list directory/prefix with 100K+ objects.</p>
<p>The pagination works (see below) but I'm unable to get <code>NextKeyMarker</code> until I download the last page - it breaks the idea of parallelization.
Ideally, I should be downloading all the pages in parallel.</p>
<pre><code>session = botocore.session.get_session()
client = session.create_client('s3')
pages = paginator.paginate(Bucket=my_bucket, Prefix=my_prefix,
      PaginationConfig={'StartingToken':None, 'MaxItems': 1200})

it = iter(pages)
p1 = next(it)
...
p2 = next(it)
...

marker = p2['NextKeyMarker']
pages = paginator.paginate(Bucket=my_bucket, Prefix=my_prefix,
      PaginationConfig={'StartingToken': marker, 'MaxItems': 1000})
</code></pre>
<p>PS: Are there any other tricks like separating the prefix to <code>my_prefix + '/[A-I]'</code>, <code>my_prefix + '/[J-Q]'</code>, ... for parallel download?</p>
",2,1664942682,python;amazon-s3;boto;botocore,False,311,0,1665166032,https://stackoverflow.com/questions/73955824/s3-boto-list-objects-in-parallel
69563718,How to Use SSL Verify with AWS Wrangler,"<p>AWS Wrangler provides a convenient interface for consuming S3 objects as pandas dataframes. I want to use this instead of boto3 clients, resources, nor sessions when getting objects. I also need to use SSL verification.</p>
<p>The following boto3 client code works with the SSL Aries Root cert (!)</p>
<pre class=""lang-py prettyprint-override""><code>import awswrangler as wr
import boto3
import os

aries_cert = os.environ['ARIES_CERT']

s3_session = boto3.Session(
    aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],
    aws_secret_access_key=os.environ[&quot;AWS_SECRET_ACCESS_KEY&quot;],
    region_name=&quot;us-east-1&quot;
)
s3_client = s3_session.client(
    service_name=&quot;s3&quot;,
    endpoint_url=&quot;https://MY-ENDPOINT.com&quot;,
    use_ssl=True,
    verify=aries_cert,
    aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
    aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),
    config=botocore.config.Config(
        read_timeout=600,
        connect_timeout=600,
        retries={&quot;max_attempts&quot;: 3}
    )
)

bucket, prefix = path.split('/', 1)
bucket = bucket if not bucket.startswith('s3://') else bucket.split('s3://')[1]
obj = s3_client.get_object(Bucket=bucket, Key=prefix)
# Do stuff with `obj['Body'].read()`
</code></pre>
<p>This aws wrangler code works too (without the TLS (SSL?) client cert):</p>
<pre class=""lang-py prettyprint-override""><code>import awswrangler as wr
import boto3
import botocore
import os

wr.config.s3_endpoint_url = &quot;https://MY-ENDPOINT.com&quot;

session = boto3.Session(
    aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],
    aws_secret_access_key=os.environ[&quot;AWS_SECRET_ACCESS_KEY&quot;],
    region_name=&quot;us-east-1&quot;
)
path = f's3://{path}' if not path.startswith('s3://') else path

df = wr.s3.read_parquet(
    path=path,
    dataset=True,
    boto3_session=session
)
</code></pre>
<p>But then when I include the TLS (SSL?) client cert, the read fails:</p>
<pre class=""lang-py prettyprint-override""><code>wr.config.botocore_config = botocore.config.Config(
    retries={&quot;max_attempts&quot;: 3},
    connect_timeout=600,
    read_timeout=600,
    client_cert=os.getenv(&quot;ARIES_CERT&quot;)
)
df = wr.s3.read_parquet(
    path=path,
    dataset=True,
    boto3_session=session
)
</code></pre>
<p>Error message:</p>
<blockquote>
<p>SSLError: SSL validation failed for <a href=""https://MY-ENDPOINT.com/MY-BUCKET?list-type=2&amp;prefix=MY-PREFIX-BLAH-BLAH.parquet%2F&amp;max-keys=1000&amp;encoding-type=url"" rel=""nofollow noreferrer"">https://MY-ENDPOINT.com/MY-BUCKET?list-type=2&amp;prefix=MY-PREFIX-BLAH-BLAH.parquet%2F&amp;max-keys=1000&amp;encoding-type=url</a> [SSL] PEM lib (_ssl.c:3524)</p>
</blockquote>
<p>Any idea what's going on here? I'm not finding the aws wrangler docs, nor those for boto3 and botocore very helpful:</p>
<p><a href=""https://aws-data-wrangler.readthedocs.io/en/latest/tutorials/002%20-%20Sessions.html"" rel=""nofollow noreferrer"">https://aws-data-wrangler.readthedocs.io/en/latest/tutorials/002%20-%20Sessions.html</a>
<a href=""https://aws-data-wrangler.readthedocs.io/en/latest/tutorials/021%20-%20Global%20Configurations.html#21---Global-Configurations"" rel=""nofollow noreferrer"">https://aws-data-wrangler.readthedocs.io/en/latest/tutorials/021%20-%20Global%20Configurations.html#21---Global-Configurations</a>
<a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html"" rel=""nofollow noreferrer"">https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html</a>
<a href=""https://botocore.amazonaws.com/v1/documentation/api/latest/reference/config.html"" rel=""nofollow noreferrer"">https://botocore.amazonaws.com/v1/documentation/api/latest/reference/config.html</a>
<a href=""https://botocore.amazonaws.com/v1/documentation/api/latest/tutorial/index.html"" rel=""nofollow noreferrer"">https://botocore.amazonaws.com/v1/documentation/api/latest/tutorial/index.html</a></p>
<p>Also this kind of question has been asked before, and if intuition can be provided on how to work with boto3 clients, resources, and sessions in different contexts, that would be appreciated.</p>
",0,1634170727,python;ssl;boto3;botocore;aws-data-wrangler,False,1323,1,1661497247,https://stackoverflow.com/questions/69563718/how-to-use-ssl-verify-with-aws-wrangler
73285212,Python Unit test with mocking boto3 S3 resource,"<p>I am new to Python and have written some code in python but not a lot of unit tests, especially that involves mocking.</p>
<p>I would like to write unit test for this python function which uses boto3.</p>
<p><strong>simpleput.py</strong></p>
<pre class=""lang-py prettyprint-override""><code>s3 = boto3.resource('s3')

def simpleput(bucket: str, filename: str):
    // s3 = boto3.resource('s3')
    s3bucket = s3.Bucket(bucket)
    s3.Object(bucket, filename).put(Body='one\ntwo')
</code></pre>
<p>I found <code>botocore stubber</code> and went onto use it: <a href=""https://botocore.amazonaws.com/v1/documentation/api/latest/reference/stubber.html"" rel=""nofollow noreferrer"">https://botocore.amazonaws.com/v1/documentation/api/latest/reference/stubber.html</a></p>
<p>Since I am using <code>boto3.resource</code> instead of <code>boto3.client</code> in my code, so based on <a href=""https://github.com/boto/botocore/issues/1125#issuecomment-452468870"" rel=""nofollow noreferrer"">this suggestion</a> I wrote following test:</p>
<pre class=""lang-py prettyprint-override""><code>import unittest
import boto3
from botocore.stub import Stubber
import simpleput


class TestModule(unittest.TestCase):
    def test_seed(self):
        s3_resource = boto3.resource('s3')
        client = s3_resource.meta.client
        stubber = Stubber(client)
        simpleput.s3 = s3_resource // Setting this on file I want to test 

        response = {&quot;Expiration&quot;: &quot;whatever&quot;, &quot;ETag&quot;: &quot;12345&quot;, &quot;VersionId&quot;: &quot;1.0&quot;}

        expected_params = {
            &quot;Body&quot;: 'one\ntwo',
            &quot;Bucket&quot;: 'mybucket',
            &quot;Key&quot;: 'mykey',
        }
        stubber.add_response('put_object', response, expected_params)

        with stubber:
            service_response = client.put_object(Body='one\ntwo', Bucket='mybucket', Key='mykey')
            simpleput.simpleput('mybucket', 'mykey')

        assert service_response == response
</code></pre>
<p>Now am unable to figure out how can I use this mock/stubber to injected/intercept the call to my actual function <code>simpleput(bucket, filename)</code> that I want to test.</p>
<p>If I do it as shown in my code, it gives me this error:</p>
<p><code>raise UnStubbedResponseError( botocore.exceptions.UnStubbedResponseError: Error getting response stub for operation PutObject: Unexpected API Call: A call was made but no additional calls expected. Either the API Call was not stubbed or it was called multiple times.</code></p>
",0,1660003354,python;unit-testing;mocking;boto3;botocore,False,2548,1,1660073290,https://stackoverflow.com/questions/73285212/python-unit-test-with-mocking-boto3-s3-resource
73285726,How to import/catch &quot;BatchEntryIdsNotDistinct&quot;,"<p>My code is throwing:</p>
<pre><code>botocore.errorfactory.BatchEntryIdsNotDistinct: An error occurred (AWS.SimpleQueueService.BatchEntryIdsNotDistinct) when calling the DeleteMessageBatch operation: Id 85990c40-xxxx-43f5-a66e-80618a734157 repeated.
</code></pre>
<p>I would like to catch the exception, but can't import it:</p>
<pre><code>from botocore.errorfactory import BatchEntryIdsNotDistinct
ImportError: cannot import name 'BatchEntryIdsNotDistinct' from 'botocore.errorfactory' 
</code></pre>
<p>It doesn't even seem to exist as python code, just as json:</p>
<pre><code>grep -r BatchEntryIdsNotDistinct .
./lib/python3.9/site-packages/botocore/data/sns/2010-03-31/service-2.json:        {&quot;shape&quot;:&quot;BatchEntryIdsNotDistinctException&quot;},
./lib/python3.9/site-packages/botocore/data/sns/2010-03-31/service-2.json:    &quot;BatchEntryIdsNotDistinctException&quot;:{
./lib/python3.9/site-packages/botocore/data/sns/2010-03-31/service-2.json:        &quot;code&quot;:&quot;BatchEntryIdsNotDistinct&quot;,
./lib/python3.9/site-packages/botocore/data/sqs/2012-11-05/service-2.json:        {&quot;shape&quot;:&quot;BatchEntryIdsNotDistinct&quot;},
./lib/python3.9/site-packages/botocore/data/sqs/2012-11-05/service-2.json:        {&quot;shape&quot;:&quot;BatchEntryIdsNotDistinct&quot;},
./lib/python3.9/site-packages/botocore/data/sqs/2012-11-05/service-2.json:        {&quot;shape&quot;:&quot;BatchEntryIdsNotDistinct&quot;},
./lib/python3.9/site-packages/botocore/data/sqs/2012-11-05/service-2.json:    &quot;BatchEntryIdsNotDistinct&quot;:{
./lib/python3.9/site-packages/botocore/data/sqs/2012-11-05/service-2.json:        &quot;code&quot;:&quot;AWS.SimpleQueueService.BatchEntryIdsNotDistinct&quot;,
</code></pre>
<p>This is my code (shortened):</p>
<pre><code>sqs = boto3.resource('sqs', region_name='us-west-2')
queue = sqs.get_queue_by_name(QueueName=&lt;queue name here&gt;)
queue.delete_messages(Entries=&lt;messages to delete&gt;)
</code></pre>
<p>Where do I get the exception <code>BatchEntryIdsNotDistinct</code> from to catch it?</p>
",0,1660009860,python;exception;boto3;botocore,True,597,1,1660014963,https://stackoverflow.com/questions/73285726/how-to-import-catch-batchentryidsnotdistinct
64965793,AttributeError: module &#39;botocore&#39; has no attribute &#39;session&#39; when following the boto3 quickstart,"<p>I was following the boto3 quickstart instructions and I can run <code>import boto3</code>, but when I try to execute any basic command like <code>db = boto3.resource('dynamodb')</code> I get</p>
<pre><code>Traceback (most recent call last):

  File &quot;&lt;ipython-input-144-424c27c1bae1&gt;&quot;, line 1, in &lt;module&gt;
    botocore.session.get_session()

AttributeError: module 'botocore' has no attribute 'session'
</code></pre>
<p>The credentials and config files look good and i tried to reinstall awscli and boto3 but is not helping. I don't understand what is the issue.</p>
",6,1606123951,python;boto3;aws-cli;botocore,True,10113,2,1658869090,https://stackoverflow.com/questions/64965793/attributeerror-module-botocore-has-no-attribute-session-when-following-the
73111567,How to correctly handle Botocore exceptions and AWS service exceptions,"<p>I'm new to the python and would like to learn it proper way rather than just solving the problem. So I'm just come up with this to learn something properly.</p>
<p>When reading <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/guide/error-handling.html"" rel=""nofollow noreferrer"">this</a> documentation, I understand there are two type of exceptions we need to handle when working with boto3 (Botocore exceptions and AWS service exceptions).</p>
<p>I'm writing a small Lambda function like below and it works well. I just want to know is there anything else I have to do to handle the errors/exceptions properly?. (Please note, I do not need any custom error messages to pass by using <code>if err.response['Error']['Code']</code>. I just only need to make sure my function handles both botocore exceptions and AWS service exceptions. Can someone please explain, what I did here is correct or advice any improvements to be made?</p>
<p>Also what is the different between when I use <code>from botocore.exceptions import ClientError</code> and <code>except botocore.exceptions.ClientError as error:</code> directly in my code. Is there best practices to do so?</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>import boto3
import logging
from botocore.exceptions import ClientError

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

client = boto3.client('resourcegroupstaggingapi')

try:
    logger.info('Starting Report Creation')
    response = client.start_report_creation(
        S3Bucket='my-bucket'
    )

except ClientError as e:
        logger.error(e)</code></pre>
</div>
</div>
</p>
",0,1658762160,python;boto3;botocore,True,2594,1,1658856983,https://stackoverflow.com/questions/73111567/how-to-correctly-handle-botocore-exceptions-and-aws-service-exceptions
51085539,Streaming in / chunking csv&#39;s from S3 to Python,"<p>I intend to perform some memory intensive operations on a very large csv file stored in S3 using Python with the intention of moving the script to AWS Lambda. I know I can read in the whole csv nto memory, but I will definitely run into Lambda's memory and storage limits with such a large filem is there any way to stream in or just read in chunks of a csv at a time into Python using boto3/botocore, ideally by spefifying row numbers to read in?</p>

<p>Here are some things I've already tried:</p>

<p>1) using the <code>range</code> parameter in <code>S3.get_object</code> to specify the range of bytes to read in. Unfortunately this means the last rows get cut off in the middle since there's no ways to specify the number of rows to read in. There are some messy workarounds like scanning for the last newline character, recording the index, and then using that as the starting point for the next bytes range, but I'd like to avoid this clunky solution if possible.</p>

<p>2) Using S3 select to write sql queries to selectively retrieve data from S3 buckets. Unfortunately the <code>row_numbers</code> SQL function isn't supported and it doesn't look like there's a way to read in a a subset of rows.</p>
",13,1530196406,python;amazon-s3;boto3;botocore;amazon-s3-select,True,17269,3,1657896341,https://stackoverflow.com/questions/51085539/streaming-in-chunking-csvs-from-s3-to-python
72981203,How to stub S3.Object.wait_until_exists?,"<p>I have been tasked with writing tests for an s3 uploading function which uses <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Object.wait_until_exists"" rel=""nofollow noreferrer"">S3.Object.wait_until_exists</a> to wait for upload to complete and get the content length of the upload to return it.</p>
<p>But so far I am failing to stub <code>head_object</code> for the waiter.</p>
<p>I have explored and found the waiter has two acceptors:</p>
<ul>
<li>if HTTP code == 200, accept</li>
<li>if HTTP code == 404, retry</li>
</ul>
<p>I don't know how to explain in text more so instead here is an MRE.</p>
<pre class=""lang-py prettyprint-override""><code>from datetime import datetime
from io import BytesIO

import boto3
import botocore
import botocore.stub

testing_bucket = &quot;bucket&quot;
testing_key = &quot;key/of/object&quot;
testing_data = b&quot;data&quot;

s3 = boto3.resource(&quot;s3&quot;)


def put():
    try:
        o = s3.Object(testing_bucket, testing_key)
        o.load()  # head_object * 1
    except botocore.exceptions.ClientError as e:
        if e.response[&quot;Error&quot;][&quot;Code&quot;] == &quot;NoSuchKey&quot;:
            etag = &quot;&quot;
        else:
            raise e
    else:
        etag = o.e_tag
    try:
        o.upload_fileobj(BytesIO(testing_data))  # put_object * 1
    except botocore.exceptions.ClientError as e:
        raise e
    else:
        o.wait_until_exists(IfNoneMatch=etag)  # head_object * n until accepted
        return o.content_length  # not sure if calling head_object again


with botocore.stub.Stubber(s3.meta.client) as s3_stub:
    s3_stub.add_response(
        method=&quot;head_object&quot;,
        service_response={
            &quot;ETag&quot;: &quot;fffffffe&quot;,
            &quot;ContentLength&quot;: 0,
        },
        expected_params={
            &quot;Bucket&quot;: testing_bucket,
            &quot;Key&quot;: testing_key,
        },
    )
    s3_stub.add_response(
        method=&quot;put_object&quot;,
        service_response={},
        expected_params={
            &quot;Bucket&quot;: testing_bucket,
            &quot;Key&quot;: testing_key,
            &quot;Body&quot;: botocore.stub.ANY,
        },
    )
    s3_stub.add_response(  # cause time to increase by 5 seconds per response
        method=&quot;head_object&quot;,
        service_response={
            &quot;ETag&quot;: &quot;ffffffff&quot;,
            &quot;AcceptRanges&quot;: &quot;bytes&quot;,
            &quot;ContentLength&quot;: len(testing_data),
            &quot;LastModified&quot;: datetime.now(),
            &quot;Metadata&quot;: {},
            &quot;VersionId&quot;: &quot;null&quot;,
        },
        expected_params={
            &quot;Bucket&quot;: testing_bucket,
            &quot;Key&quot;: testing_key,
            &quot;IfNoneMatch&quot;: &quot;fffffffe&quot;,
        },
    )
    print(put())  # should print 4
</code></pre>
<p>And running the above gives:</p>
<pre><code>time python mre.py
Traceback (most recent call last):
  File &quot;/tmp/mre.py&quot;, line 72, in &lt;module&gt;
    put()
  File &quot;/tmp/mre.py&quot;, line 30, in put
    o.wait_until_exists(IfNoneMatch=etag)  # head_object * 1
  File &quot;/tmp/.tox/py310/lib/python3.10/site-packages/boto3/resources/factory.py&quot;, line 413, in do_waiter
    waiter(self, *args, **kwargs)
  File &quot;/tmp/.tox/py310/lib/python3.10/site-packages/boto3/resources/action.py&quot;, line 215, in __call__
    response = waiter.wait(**params)
  File &quot;/tmp/.tox/py310/lib/python3.10/site-packages/botocore/waiter.py&quot;, line 55, in wait
    Waiter.wait(self, **kwargs)
  File &quot;/tmp/.tox/py310/lib/python3.10/site-packages/botocore/waiter.py&quot;, line 343, in wait
    response = self._operation_method(**kwargs)
  File &quot;/tmp/.tox/py310/lib/python3.10/site-packages/botocore/waiter.py&quot;, line 93, in __call__
    return self._client_method(**kwargs)
  File &quot;/tmp/.tox/py310/lib/python3.10/site-packages/botocore/client.py&quot;, line 508, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File &quot;/tmp/.tox/py310/lib/python3.10/site-packages/botocore/client.py&quot;, line 878, in _make_api_call
    request_dict = self._convert_to_request_dict(
  File &quot;/tmp/.tox/py310/lib/python3.10/site-packages/botocore/client.py&quot;, line 936, in _convert_to_request_dict
    api_params = self._emit_api_params(
  File &quot;/tmp/.tox/py310/lib/python3.10/site-packages/botocore/client.py&quot;, line 969, in _emit_api_params
    self.meta.events.emit(
  File &quot;/tmp/.tox/py310/lib/python3.10/site-packages/botocore/hooks.py&quot;, line 412, in emit
    return self._emitter.emit(aliased_event_name, **kwargs)
  File &quot;/tmp/.tox/py310/lib/python3.10/site-packages/botocore/hooks.py&quot;, line 256, in emit
    return self._emit(event_name, kwargs)
  File &quot;/tmp/.tox/py310/lib/python3.10/site-packages/botocore/hooks.py&quot;, line 239, in _emit
    response = handler(**kwargs)
  File &quot;/tmp/.tox/py310/lib/python3.10/site-packages/botocore/stub.py&quot;, line 376, in _assert_expected_params
    self._assert_expected_call_order(model, params)
  File &quot;/tmp/.tox/py310/lib/python3.10/site-packages/botocore/stub.py&quot;, line 352, in _assert_expected_call_order
    raise UnStubbedResponseError(
botocore.exceptions.UnStubbedResponseError: Error getting response stub for operation HeadObject: Unexpected API Call: A call was made but no additional calls expected. Either the API Call was not stubbed or it was called multiple times.
python mre.py  0.39s user 0.19s system 9% cpu 5.859 total
</code></pre>
<p>Or with 2 answer, same thing with <code>python mre.py  0.40s user 0.20s system 5% cpu 10.742 total</code>.</p>
",0,1657804764,python;amazon-s3;boto3;stubbing;botocore,True,1040,1,1657811119,https://stackoverflow.com/questions/72981203/how-to-stub-s3-object-wait-until-exists
72906036,ParamValidationError when mocking AWS SQS client calls,"<p>When trying to mock sqs client and adding response for the method &quot;send_message&quot;, I am facing ParamValidationError <code>botocore.exceptions.ParamValidationError: Parameter validation failed:</code></p>
<p>Using pytest and botocore.stub (following this <a href=""https://blog.milancermak.com/2019/02/14/unit-testing-aws-services-in-python/"" rel=""nofollow noreferrer"">blob</a> as well)
Here is the expected_response:</p>
<pre><code>expected_params = {
            &quot;QueueUrl&quot;: &quot;abc&quot;,
            &quot;MessageBody&quot;: &quot;string&quot;,
            &quot;MessageAttributes&quot;: {
                &quot;CallbackClass&quot;: {&quot;DataType&quot;: &quot;String&quot;, &quot;StringValue&quot;: None},
                &quot;CallbackMethod&quot;: {&quot;DataType&quot;: &quot;String&quot;, &quot;StringValue&quot;: None},
                &quot;Key&quot;: {&quot;DataType&quot;: &quot;String&quot;, &quot;StringValue&quot;: '{&quot;package_name&quot;: &quot;package_name&quot;, &quot;min_version&quot;: 0, &quot;max_version&quot;: 0}'},
            },
</code></pre>
<p>After changing the StringValues to <code>None</code>, StubAssertionError is resolved. Not sure how to avoid this in the validation. I tried stub.ANY but still same error.
<code>Invalid type for parameter MessageAttributes.CallbackClass.StringValue, value: None, type: &lt;class 'NoneType'&gt;, valid types: &lt;class 'str'&gt;</code>
Any workaround for this?</p>
<p>This is how I am mocking the sqs client and sqs response (following the documentation for response structure):</p>
<pre><code>    client = boto3.client(&quot;sqs&quot;, region_name=&quot;us-east-1&quot;, aws_access_key_id=&quot;...&quot;, aws_secret_access_key=&quot;...&quot;)
    mock_res = {
        &quot;MD5OfMessageBody&quot;: &quot;string&quot;,
        &quot;MD5OfMessageAttributes&quot;: &quot;string&quot;,
        &quot;MD5OfMessageSystemAttributes&quot;: &quot;string&quot;,
        &quot;MessageId&quot;: &quot;string&quot;,
        &quot;SequenceNumber&quot;: &quot;string&quot;,
    }
</code></pre>
",0,1657246923,python;unit-testing;amazon-sqs;botocore,False,496,0,1657564121,https://stackoverflow.com/questions/72906036/paramvalidationerror-when-mocking-aws-sqs-client-calls
72692906,Load a .mov video in python from S3 via Lambda,"<p>I am trying to load a .mov video file into a Lambda python script for processing.</p>
<pre><code>import json
import urllib.parse
import boto3
import os

print('Loading function')

s3 = boto3.client('s3')


def lambda_handler(event, context):

    # Get the object from the event and show its content type
    bucket = os.environ['SOURCE_BUCKET_NAME']
    key = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'], encoding='utf-8')

    try:
        video = s3.get_object(Bucket=bucket, Key=key)
        print(video)
        return event
        
    except Exception as e:
        print(e)
        print('Error getting object {} from bucket {}. Make sure they exist and your bucket is in the same region as this function.'.format(key, bucket))
        raise e
</code></pre>
<p>The 'video' object in-code is a dictionary, containing a 'Body' key with the value '&lt;botocore.response.StreamingBody object at 0x7f0c0d4ec310'. I have tried a number of approaches to read this, including:</p>
<p><code>video = video['Body'].read()</code></p>
<p>which returns Bytes. When I try to read the bytes with open(...) or BytesIO, I get the error: &quot;endswith first arg must be bytes or a tuple of bytes, not str&quot;.</p>
<p>I have also tried</p>
<p><code>video = video['Body'].read().decode()</code></p>
<p>which returns error: 'utf-8' codec can't decode byte 0x8e in position 29: invalid start byte.</p>
",0,1655758687,python;amazon-s3;aws-lambda;boto;botocore,False,418,0,1655758687,https://stackoverflow.com/questions/72692906/load-a-mov-video-in-python-from-s3-via-lambda
72620335,Boto3 version mismatch seemingly causing error when using create_function() method with AWSLambda,"<p>I am having an issue creating a AWS Lambda function programmatically. The error I get is</p>
<blockquote>
<p>botocore.exceptions.ParamValidationError: Parameter validation failed:
Unknown parameter in input: &quot;RunTime&quot;, must be one of: FunctionName, Runtime, Role, Handler, Code, Description, Timeout, MemorySize, Publish, VpcConfig, PackageType, DeadLetterConfig, Environment, KMSKeyArn, TracingConfig, Tags, Layers, FileSystemConfigs, ImageConfig, CodeSigningConfigArn, Architectures, EphemeralStorage</p>
</blockquote>
<p>After a bit of research I found this AWS page(<a href=""https://aws.amazon.com/premiumsupport/knowledge-center/lambda-python-runtime-errors/"" rel=""nofollow noreferrer"">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-python-runtime-errors/</a>) that suggests that I needed to upgrade boto3 and botocore, which I did to the latest version as shown in my pip list.</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>Package               Version
--------------------- ---------
awscli                1.25.8
boto                  2.49.0
boto3                 1.24.8
botocore              1.27.8</code></pre>
</div>
</div>
</p>
<p>However, there is a mismatch when I run the script locally and with the following commands:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>print(boto3.__version__)
print(""botocore version:""+botocore.__version__)

1.24.4
botocore version:1.27.4</code></pre>
</div>
</div>
</p>
<p>How do I force my script to run the latest version of boto3/botocore, assuming that is the reason I can't create the Lambda function in code.</p>
",0,1655223723,python;amazon-web-services;aws-lambda;boto3;botocore,True,1492,1,1655230549,https://stackoverflow.com/questions/72620335/boto3-version-mismatch-seemingly-causing-error-when-using-create-function-meth
72547811,"Getting error Invalid type for parameter, value: [&#39;db/&#39;, &#39;db1/&#39;], type: &lt;class &#39;list&#39;&gt;, valid types: &lt;class &#39;str&#39;&gt; while reading folders in S3 buckets","<p>I need to list subfolders of some specific folders in my S3 Bucket ('db', 'db1', 'db2')
I'm setting an array with the folders of interest, and not all of them inside the bucket.
I'm using this 'filter Prefix' method, but it doesn't accept array.</p>
<pre><code>import boto
s3 = boto3.resource('s3')
my_bucket = s3.Bucket('database_buckets')
for object_summary in my_bucket.objects.filter(Prefix=[&quot;db/&quot;, &quot;db1/&quot;, &quot;db2/&quot;]):
    print(object_summary.key)
</code></pre>
<p><strong>Error: botocore.exceptions.ParamValidationError: Parameter validation failed:
Invalid type for parameter Prefix, value: ['db/', 'db1/', 'db2/'], type: &lt;class 'list'&gt;, valid types: &lt;class 'str'&gt;</strong></p>
<p>Any ideas for me to list these files inside these specific folders?
As there are many folders (more than 30), it would be ideal to do with array.</p>
",0,1654699698,python;python-3.x;amazon-s3;boto3;botocore,False,674,0,1654700675,https://stackoverflow.com/questions/72547811/getting-error-invalid-type-for-parameter-value-db-db1-type-class
68914327,"No matching distribution found for botocore&lt;1.22.0,&gt;=1.21.0 (from awsebcli)","<p>CircleCi returns the following error:</p>
<p>No matching distribution found for botocore &lt;1.22.0,&gt; = 1.21.0 (from awsebcli).</p>
<p><a href=""https://i.stack.imgur.com/1BZJz.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/1BZJz.png"" alt=""enter image description here"" /></a></p>
<p>My config.yml file installs the following dependencies:</p>
<ul>
<li>sudo apt-get -y -qq update</li>
<li>sudo apt-get install python3-pip python-dev build-essential</li>
<li>sudo pip3 install --upgrade setuptools</li>
<li>sudo pip3 install awsebcli --upgrade pip3 install awscli</li>
</ul>
",5,1629840851,python;pip;circleci;botocore,True,8557,4,1652377492,https://stackoverflow.com/questions/68914327/no-matching-distribution-found-for-botocore1-22-0-1-21-0-from-awsebcli
71732270,Unable to locate credentials in boto3 AWS,"<p>I'm trying to view S3 bucket list through a python scripts using boto3. Credential file and config file is available in the C:\Users\user1.aws location. Secret access and access key available there for user &quot;vscode&quot;. But unable to run the script which return exception message as</p>
<pre><code>&quot;botocore.exceptions.NoCredentialsError: Unable to locate credentials&quot;.
</code></pre>
<p>Code sample follows,</p>
<pre><code>import boto3
s3 = boto3.resource('s3')
for bucket in s3.buckets.all():     
    print(bucket.name)
</code></pre>
<p>Do I need to specify user mentioned above (&quot;vscode&quot;) ?</p>
<p>Copied the credential and config file to folder of python script is running. But same exception occurs.</p>
",0,1649048863,python;amazon-s3;visual-studio-code;boto3;botocore,True,17156,2,1649065529,https://stackoverflow.com/questions/71732270/unable-to-locate-credentials-in-boto3-aws
71533781,botocore.exceptions.DataNotFoundError: Unable to load data for: sqs,"<p>I'm trying to run a function which purges an AWS SQS queue but I keep getting this error:</p>
<blockquote>
<p>DataNotFoundError(data_path=name) botocore.exceptions.DataNotFoundError: Unable to load data for: sqs</p>
</blockquote>
<p>I have <code>python 3.7</code>, <code>boto3-1.21.22</code>, and <code>botocore-1.24.22</code> installed. I also ran <code>pip install --upgrade botocore</code>, but I'm still getting the same error.</p>
<p><a href=""https://i.stack.imgur.com/n9uOS.png"" rel=""nofollow noreferrer"">See image</a></p>
<pre><code>sqs_client = boto3.client('sqs', AWS_DEFAULT_REGION, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)

def purge_queue(queue_url): 
    try:
        response = sqs_client.purge_queue(QueueUrl=queue_url)
    except ClientError as e:
        logger.exception(&quot;Unexpected exception! %s&quot;, e)
        raise
    else:
        return response
</code></pre>
",-1,1647642417,python;boto3;botocore,True,5784,1,1647652271,https://stackoverflow.com/questions/71533781/botocore-exceptions-datanotfounderror-unable-to-load-data-for-sqs
68824003,Validating Type or Class of EC2 Object,"<p>I would like to validate the type or class of a boto3 client object.</p>
<pre><code>&gt;&gt;&gt; import boto3
&gt;&gt;&gt; ec2 = boto3.client('ec2')
&gt;&gt;&gt; type(ec2)
&lt;class 'botocore.client.EC2'&gt;
&gt;&gt;&gt;
</code></pre>
<p>How does that translate back into something I can use for an <code>if</code> comparison? Statements like <code>if type(ec2) == 'botocore.client.EC2':</code> and <code>if isinstance(ec2, botocore.client.EC2):</code> don't work.</p>
<p>@gshpychka says I need to import the appropriate module first. So I need the botocore module? Still doesn't seem to be working.</p>
<pre><code>&gt;&gt;&gt; import boto3
&gt;&gt;&gt; import botocore
&gt;&gt;&gt; ec2 = boto3.client('ec2')
&gt;&gt;&gt; type(ec2)
&lt;class 'botocore.client.EC2'&gt;
&gt;&gt;&gt; type(ec2) == botocore.client.EC2
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
AttributeError: module 'botocore.client' has no attribute 'EC2'
&gt;&gt;&gt; type(ec2) == botocore.client
False
&gt;&gt;&gt; type(ec2) == botocore
False
&gt;&gt;&gt;
</code></pre>
",1,1629234250,python;amazon-web-services;boto3;botocore,False,373,1,1645199159,https://stackoverflow.com/questions/68824003/validating-type-or-class-of-ec2-object
70965298,Boto3 and S3: For loop finishing after iterating through only 6 out of 200 S3 buckets in the list,"<p>I'm trying to list and log all the S3 buckets PublicAccessBlockConfiguration in a list. The thing is, the loop finishes and prints 'No Public Access' after iterating over only 6 out of ~200 buckets.</p>
<p>I tested the code statically, writing the name of a bucket that I knew existed and had PublicAccessBlockConfiguration, and it worked.</p>
<p>But when iterating over the list, the same bucket doesn't show up. Why is that?</p>
<pre><code>def check_bucket_access_block():
    try:
        for bucket in filtered_buckets:
            response = s3client.get_public_access_block(Bucket=bucket['Name'])
            for key, value in response['PublicAccessBlockConfiguration'].items():
                logger.info('Bucket Name: {}, {}: {}'.format(bucket['Name'], key, value))
    except botocore.exceptions.ClientError as e:
        if e.response['Error']['Code'] == 'NoSuchPublicAccessBlockConfiguration':
            print('\t no Public Access')
        else:
            print(&quot;unexpected error: %s&quot; % (e.response))

check_bucket_access_block()
</code></pre>
",0,1643855823,python;amazon-s3;boto3;devops;botocore,True,473,1,1643859671,https://stackoverflow.com/questions/70965298/boto3-and-s3-for-loop-finishing-after-iterating-through-only-6-out-of-200-s3-bu
70286915,mocking S3 download file operation for gzipped files,"<p>I am trying to mock some S3 operations and after banging my head against the stubber object, I tried doing something as follows:</p>
<pre><code>def mock_make_api_call(self, operation_name, kwarg):
    if operation_name == &quot;ListObjectsV2&quot;:
        return {
            &quot;KeyCount&quot;: 1,
            &quot;Contents&quot;: [
                {&quot;Key&quot;: &quot;sensor_1&quot;, &quot;LastModified&quot;: &quot;2021-11-30T12:58:14+00:00&quot;}
            ],
        }
    elif operation_name == &quot;GetObjectTagging&quot;:
        return {&quot;TagSet&quot;: []}
    elif operation_name == &quot;HeadObject&quot;:
        return {
            &quot;ContentLength&quot;: 10,
            &quot;ContentType&quot;: &quot;gzip&quot;,
            &quot;ResponseMetadata&quot;: {
                &quot;Bucket&quot;: &quot;1&quot;,
            },
        }
    elif operation_name == &quot;GetObject&quot;:
        content = get_object_response()
        return {
            &quot;ContentLength&quot;: len(content),
            &quot;ContentType&quot;: &quot;xml&quot;,
            &quot;ContentEncoding&quot;: &quot;gzip&quot;,
            &quot;Body&quot;: content,
            &quot;ResponseMetadata&quot;: {
                &quot;Bucket&quot;: &quot;1&quot;,
            },
        }
</code></pre>
<p>Ot is the s3 <code>download_file</code>operation which is giving me a headache. As far as I can tell it generates, the <code>HeadObject</code>and <code>GetObject</code>calls.</p>
<p>My content generation method is as follows:</p>
<pre><code>def get_object_response():

    content = b&quot;&lt;some-valid-xml&gt;&quot;

    buf = BytesIO()
    compressed = gzip.GzipFile(fileobj=buf, mode=&quot;wb&quot;)
    compressed.write(content)
    compressed.close()

    return buf.getvalue()
</code></pre>
<p>The way it gets used is:</p>
<pre><code>with NamedTemporaryFile() as tmp:
    s3_client.download_file(Bucket=..., Key=..., Filename=tmp.name)
</code></pre>
<p>However, my test fails with:</p>
<pre><code>elf = &lt;s3transfer.utils.StreamReaderProgress object at 0x116a77820&gt;
args = (262144,), kwargs = {}

    def read(self, *args, **kwargs):
&gt;       value = self._stream.read(*args, **kwargs)
E       AttributeError: 'bytes' object has no attribute 'read'
</code></pre>
<p>I simply cannot figure out how to encode the response so that the generated content can be saved.</p>
",0,1639039597,python;mocking;boto3;botocore,False,240,0,1639039597,https://stackoverflow.com/questions/70286915/mocking-s3-download-file-operation-for-gzipped-files
32618216,Override S3 endpoint using Boto3 configuration file,"<h3>OVERVIEW:</h3>
<p>I'm trying to override certain variables in <code>boto3</code> using the configuration file (<code>~/aws/confg</code>).
In my use case I want to use <code>fakes3</code> service and send S3 requests to the localhost.</p>
<h3>EXAMPLE:</h3>
<p>In <code>boto</code> (not <code>boto3</code>), I can create a config in <code>~/.boto</code> similar to this one:</p>
<pre><code>[s3]
host = localhost
calling_format = boto.s3.connection.OrdinaryCallingFormat

[Boto]
is_secure = False
</code></pre>
<p>And the client can successfully pick up desired changes and instead of sending traffic to the real S3 service, it will send it to the localhost.</p>
<pre><code>&gt;&gt;&gt; import boto
&gt;&gt;&gt; boto.connect_s3()
S3Connection:localhost
&gt;&gt;&gt; 
</code></pre>
<h3>WHAT I TRIED:</h3>
<p>I'm trying to achieve a similar result using <code>boto3</code> library. By looking at the source code I found that I can use <code>~/aws/config</code> location. I've also found an example config in <code>unittests</code> folder of <code>botocore</code>.</p>
<p>I tried to modify the config to achieve the desired behaviour.  But unfortunately, it doesn't work.</p>
<p>Here is the config:</p>
<pre><code>[default]
aws_access_key_id = XXXXXXXXX
aws_secret_access_key = YYYYYYYYYYYYYY
region = us-east-1
is_secure = False
s3 =
    host = localhost
</code></pre>
<h3>QUESTION:</h3>
<ol>
<li>How to overwrite <code>clients</code> variables using config file?</li>
<li>Where can I find a complete list of allowed variables for the configuration?</li>
</ol>
",52,1442435808,python;amazon-web-services;boto;boto3;botocore,True,85416,6,1638232716,https://stackoverflow.com/questions/32618216/override-s3-endpoint-using-boto3-configuration-file
69971723,python boto3 error from botocore.compat import OrderedDict ImportError: cannot import name &#39;OrderedDict,"<pre><code>&gt;&gt;&gt; import boto3
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/usr/local/lib/python3.6/dist-packages/boto3/__init__.py&quot;, line 16, in &lt;module&gt;
    from boto3.session import Session
  File &quot;/usr/local/lib/python3.6/dist-packages/boto3/session.py&quot;, line 17, in &lt;module&gt;
    import botocore.session
  File &quot;/usr/local/lib/python3.6/dist-packages/botocore/session.py&quot;, line 28, in &lt;module&gt;
    import botocore.configloader
  File &quot;/usr/local/lib/python3.6/dist-packages/botocore/configloader.py&quot;, line 19, in &lt;module&gt;
    from botocore.compat import six
  File &quot;/usr/local/lib/python3.6/dist-packages/botocore/compat.py&quot;, line 28, in &lt;module&gt;
    from urllib3 import exceptions
  File &quot;/usr/local/lib/python3.6/dist-packages/urllib3/__init__.py&quot;, line 13, in &lt;module&gt;
    from .connectionpool import HTTPConnectionPool, HTTPSConnectionPool, connection_from_url
  File &quot;/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py&quot;, line 37, in &lt;module&gt;
    from .packages.six.moves import queue
  File &quot;/usr/local/lib/python3.6/dist-packages/urllib3/packages/six.py&quot;, line 96, in __get__
    result = self._resolve()
  File &quot;/usr/local/lib/python3.6/dist-packages/urllib3/packages/six.py&quot;, line 118, in _resolve
    return _import_module(self.mod)
  File &quot;/usr/local/lib/python3.6/dist-packages/urllib3/packages/six.py&quot;, line 87, in _import_module
    __import__(name)
  File &quot;/home/gti/pysys/req_static/queue.py&quot;, line 3, in &lt;module&gt;
    from botocore.config import Config
  File &quot;/usr/local/lib/python3.6/dist-packages/botocore/config.py&quot;, line 14, in &lt;module&gt;
    from botocore.compat import OrderedDict
ImportError: cannot import name 'OrderedDict'
</code></pre>
",0,1636966731,python;boto3;ubuntu-18.04;botocore,False,440,0,1637166797,https://stackoverflow.com/questions/69971723/python-boto3-error-from-botocore-compat-import-ordereddict-importerror-cannot-i
69822501,Stubbing an AWS Resource to raise a ClientError,"<p>I have the following python code snippet (assuming the update_item arguments are valid):</p>
<pre><code>#foo.py
def update_table(value):
 dynamodb = boto3.resource('dynamodb')
     try:
         table = dynamodb.Table(&quot;test-table&quot;)
         response = table.update_item(Key=('value' : value)
     except botocore.exceptions.ClientError as err:
         return None
     return response
</code></pre>
<p>Now, I would like to write a unit test that test whether this function returns None when a valid arugments are passed but a ClientError is thrown.</p>
<p>I mocked out a DynamoDB Table in a setUp function as shown:</p>
<pre><code>#test_foo.py
@mock_dynamodb2
class TestUpdateTable(unittest.TestCase):
    

    def setUp(self):
        self.dynamodb = boto3.resource('dynamodb')
        
        # Creates a Mock Table
        self.table = self.dynamodb.create_table(
            TableName='test-table',
            KeySchema=[
                {
                    'AttributeName': 'value',
                    'KeyType': 'HASH'
                },
            ],
            AttributeDefinitions=[
                {
                    'AttributeName': 'value',
                    'AttributeType': 'S'
                },
            ],
            ProvisionedThroughput={
                'ReadCapacityUnits': 5,
                'WriteCapacityUnits': 5
        }
        )
</code></pre>
<p>and in my test function under this class:</p>
<pre><code>def test_returns_none(self):
        stubber = Stubber(self.dynamodb.meta.client)
        stubber.add_client_error('update_item')
        
        stubber.activate()
        
        response = foo.update_table(&quot;123&quot;)
        stubber.deactivate()

        self.assertEqual(response, None)
</code></pre>
<p>Now, I expect this to properly stub out the table and raise a ClientError when calling the function to update the table. However, it seems like a ClientError is not raised and the response from the function that I'm testing is not = None as a ClientError is not raised.</p>
<p>Anyone knows the reason for that? And how can I implement this test in a way so that the mock table will raise a ClientError and return None?</p>
",1,1635931892,python;mocking;python-unittest;stubbing;botocore,False,484,0,1635931892,https://stackoverflow.com/questions/69822501/stubbing-an-aws-resource-to-raise-a-clienterror
69100510,Boto SQS fetching large numbers of messages,"<p>I have a python app that is supposed to read all the messages from the queue, and then process them after all the messages have been added to a list.
Once the process is complete it deletes the messages in batch.</p>
<p>The app works perfectly for up to 100K messages however while doing a load test for more than 500K messages.</p>
<p>I got the following error</p>
<pre><code>botocore.errorfactory.OverLimit: An error occurred (OverLimit) when calling the ReceiveMessage operation: 

Too many messages have been received without being deleted
</code></pre>
<p>I understand that this is because I am trying to fetch a large number of messages without deleting them, What I want to know is if there is a way to bypass this.</p>
",0,1631093597,python;amazon-sqs;boto;botocore,True,560,1,1631111367,https://stackoverflow.com/questions/69100510/boto-sqs-fetching-large-numbers-of-messages
69029082,boto3: Is aws_session_token necessary when trying to list objects in a bucket?,"<p>I'm writing a function currently to fetch objects from a bucket using boto3. While connecting, I have to connect like this:</p>
<pre><code>session = boto3.session.Session(
    aws_access_key_id=&quot;XXXXXXXXXX&quot;,
    aws_secret_access_key=&quot;YYYYYYYYYYYYYY&quot;)
</code></pre>
<p>or like this:</p>
<pre><code>client = boto3.client('s3',aws_access_key_id=&quot;XXXXXXXXXXXX&quot;,
aws_secret_access_key=&quot;YYYYYYYYYYYYYYY&quot;
</code></pre>
<p>i.e, there are only two arguments with me: aws_access_key_id and aws_secret_access_key. And using a shared credential file is not an option. Though I am able to connect without the aws_session_token, whenever I run:</p>
<pre><code>for obj in first_bucket.objects.filter(Prefix=&quot;my filter&quot;):
        print(obj.key)
        response = obj.key
</code></pre>
<p>this error is shown:</p>
<pre><code>botocore.exceptions.ClientError: An error occurred (InvalidAccessKeyId) when calling the ListObjectsV2 operation: The AWS Access Key
Id you provided does not exist in our records.
</code></pre>
<p>When I went through the credentials documentation <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html"" rel=""nofollow noreferrer"">here</a> and went through a few other questions on SO, there are only suggestions to use aws_session_token when other credentials are temporary. There is also mention that sessions can be created by boto3 itself and that there's no need for the programmer to handle it. In that case, why am I being forced to add a aws_session_token mandatorily when accessing objects? Is aws_session_token something that is mandatory when objects are being accessed?</p>
",2,1630580916,python;amazon-web-services;amazon-s3;boto3;botocore,False,1630,1,1630676504,https://stackoverflow.com/questions/69029082/boto3-is-aws-session-token-necessary-when-trying-to-list-objects-in-a-bucket
63011353,AWS Lambda calling another lambda and then parsing &lt;botocore.response.StreamingBody problem,"<p>The first line is part of response logged after invoking another lambda in 'RequestResponse' type.</p>
<pre><code>'StatusCode': 200, 'ExecutedVersion': '$LATEST', 'Payload': &lt;botocore.response.StreamingBody object at 0x7f62887d1410&gt;}
</code></pre>
<p>The following code is returning null</p>
<pre><code>json.loads(lambda_response['Payload'].read().decode('utf-8'))
</code></pre>
<p>The call lambda which is returning response is doing correctly.</p>
<p>Any help is highly appreciated.</p>
<p>Thanks,</p>
",0,1595323289,python;lambda;aws-lambda;botocore;aws-lambda-layers,True,399,2,1630211527,https://stackoverflow.com/questions/63011353/aws-lambda-calling-another-lambda-and-then-parsing-botocore-response-streamingb
68914738,Error with botocore in apache airflow-windows,"<p>I have set the variables and connections in apache airflow webUI, so when i try to import the s3 hook from aiflow hooks,so i use to get the keys in the s3 bucket, but it fails, and i get the following error:</p>
<pre><code>[2021-08-24 21:59:11,720] {{dagbag.py:246}} ERROR - Failed to import: 
/usr/local/airflow/dags/lesson4.py
webserver_1  | Traceback (most recent call last):
webserver_1  |   File &quot;/usr/local/lib/python3.7/site-packages/airflow/models/dagbag.py&quot;, line 
243, in process_file
webserver_1  |     m = imp.load_source(mod_name, filepath)
webserver_1  |   File &quot;/usr/local/lib/python3.7/imp.py&quot;, line 171, in load_source
webserver_1  |     module = _load(spec)
webserver_1  |   File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 696, in _load
webserver_1  |   File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
webserver_1  |   File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
webserver_1  |   File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
webserver_1  |   File &quot;/usr/local/airflow/dags/lesson4.py&quot;, line 9, in &lt;module&gt;
webserver_1  |     from airflow.hooks.S3_hook import S3hook
webserver_1  |   File &quot;/usr/local/lib/python3.7/site-packages/airflow/hooks/S3_hook.py&quot;, line 
19, in &lt;module&gt;
webserver_1  |     from botocore.exceptions import ClientError
webserver_1  | ModuleNotFoundError: No module named 'botocore'


import datetime
import logging

from airflow import DAG
from airflow.models import Variable
from airflow.operators.python_operator import PythonOperator
#import airflow.hooks.S3_hook as S3Hook
from airflow.hooks.S3_hook import S3Hook


def list_keys():
    hook = S3Hook(aws_conn_id='aws_credentials')
    bucket = Variable.get('s3_bucket')
    prefix = Variable.get('s3_prefix')
    logging.info(f&quot;Listing Keys from {bucket}/{prefix}&quot;)
    keys = hook.list_keys(bucket, prefix=prefix)
    for key in keys:
        logging.info(f&quot;- s3://{bucket}/{key}&quot;)


dag = DAG(
        'lesson1.exercise4',
        start_date=datetime.datetime.now())

list_task = PythonOperator(
    task_id=&quot;list_keys&quot;,
    python_callable=list_keys,
    dag=dag
</code></pre>
<p>)</p>
<p>Can anyone show me how to resolve this</p>
",0,1629844043,python;amazon-s3;airflow;botocore,False,484,1,1629847992,https://stackoverflow.com/questions/68914738/error-with-botocore-in-apache-airflow-windows
68864939,s3fs suddenly stopped working in Google Colab with error &quot;AttributeError: module &#39;aiobotocore&#39; has no attribute &#39;AioSession&#39;&quot;,"<p>Yesterday the following cell sequence in Google Colab would work.</p>
<p><a href=""https://i.stack.imgur.com/0zkMw.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/0zkMw.png"" alt=""enter image description here"" /></a></p>
<p>(I am using <a href=""https://github.com/apolitical/colab-env"" rel=""noreferrer"">colab-env</a> to import environment variables from Google Drive.)</p>
<p>This morning, when I run the same code, I get the following error.</p>
<p><a href=""https://i.stack.imgur.com/5s6QM.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/5s6QM.png"" alt=""enter image description here"" /></a></p>
<p>It appears to be a new issue with s3fs and aiobotocore.  I have some experience with Google Colab and library version dependency issues that I have previously solved by upgrading libraries in a particular order:</p>
<pre><code>!pip install --upgrade library_name
</code></pre>
<p>But I am a bit stuck this morning with this one.  It is affecting all of my Google Colab notebooks so I thought that perhaps it is affecting others who are using data stored in Amazon AWS S3 with Google Colab.</p>
<p>The version of s3fs that gets installed is 2021.07.0, which appears to be the latest.</p>
<p><a href=""https://i.stack.imgur.com/P8XdK.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/P8XdK.png"" alt=""enter image description here"" /></a></p>
",14,1629474861,python;google-colaboratory;botocore;python-s3fs,True,12673,1,1629574708,https://stackoverflow.com/questions/68864939/s3fs-suddenly-stopped-working-in-google-colab-with-error-attributeerror-module
56704360,BotoCore/Boto3 Stubber Operation Not Found for generate_presigned_post,"<p>I'm trying to use the <code>botocore.stub.Stubber</code> to mock my <code>boto3.client</code>, and I am getting a <code>botocore.model.OperationNotFoundError</code> when trying to add the mocked <code>generate_presigned_post</code> response:</p>

<pre><code>class S3FileTestCase(TestCase):

    @classmethod
    def setUpTestData(cls):
        cls.s3 = botocore.session.get_session().create_client('s3')
        cls.region_name = 'eu-west-2'

    @staticmethod
    def _mock__get_s3(region_name):
        client = boto3.client('s3', config=boto3.session.Config(signature_version='s3v4'), region_name=region_name)
        stubber = Stubber(client)

        stubber.add_response('generate_presigned_post', {'test':1}, {'bucket_name': 'test_bucket', 'region_name': region_name, 'object_name': 'test.csv'})
        return stubber

    @patch('uploader.models.s3_file.S3File._get_s3', new=_mock__get_s3)
    def test_create_presigned_post(self):
        response = S3File.create_presigned_post('stuart-special-testing-bucket', self.region_name, 'test.csv')
        print(response)
</code></pre>

<p>When I run <code>test_create_presigned_post</code> I get the <code>OperationNotFoundError</code> in the <code>add_reponse</code>. Does anyone have any idea why this might be? </p>

<p>Note: <code>S3File.create_presigned_post</code> is taken directly from the <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-presigned-urls.html#generating-a-presigned-url-to-upload-a-file"" rel=""noreferrer"">docs</a> with the only change being that the client is moved into a function so it can be mocked (and adding a region parameter). </p>
",5,1561123425,python;django;mocking;boto3;botocore,True,1440,1,1628273556,https://stackoverflow.com/questions/56704360/botocore-boto3-stubber-operation-not-found-for-generate-presigned-post
57830582,"Versions of boto3, aiobotocore, awscli, and botocore incompatible; can&#39;t be resolved","<p>If I try to install the latest version of aiobotocore <code>pip3 install aiobotocore==0.10.3</code> it says my version of botocore is incompatible and I need an older version of it. </p>

<pre><code>ERROR: aiobotocore 0.10.3 has requirement botocore&lt;1.12.190,&gt;=1.12.189, but you'll have botocore 1.12.224 which is incompatible.
</code></pre>

<p>So if I change my version of botocore to 1.12.189, it breaks <code>awscli</code> and <code>boto3</code></p>

<pre><code>ERROR: boto3 1.9.224 has requirement botocore&lt;1.13.0,&gt;=1.12.224, but you'll have botocore 1.12.189 which is incompatible.
ERROR: awscli 1.16.234 has requirement botocore==1.12.224, but you'll have botocore 1.12.189 which is incompatible.
</code></pre>

<p>It doesn't seem like there are versions of the packages that would work with them all. Can someone tell me what versions are all compatible? And is there any way for pip for automatically determine this?</p>
",8,1567829149,python;amazon-web-services;boto3;aws-cli;botocore,True,14001,1,1628242597,https://stackoverflow.com/questions/57830582/versions-of-boto3-aiobotocore-awscli-and-botocore-incompatible-cant-be-reso
32361173,python boto3 connection error with no apparent cause,"<p>I'm hitting an error with code that connects to AWS using boto3. The error just started yesterday afternoon, and between the last time I didn't get the error and the first time I got the error I don't see anything that changed.</p>

<p>The error is:</p>

<pre><code>botocore.exceptions.EndpointConnectionError: Could not connect to the endpoint URL:
</code></pre>

<p>In .aws/config I have:</p>

<pre><code>$ cat ~/.aws/config
[default]
region=us-east-1
</code></pre>

<p>Here's what I know:</p>

<ul>
<li>Using the same AWS credentials and config on another machine, I don't see the error.</li>
<li>Using different AWS credentials and config on the same machine, I do see the error.</li>
<li>I'm the only one in our group that has this issue for any credentials on any machine.</li>
</ul>

<p>I don't think I changed anything that would affect this between the last time this worked and the first time it didn't. It seems like I'd have had to change some AWS specific configuration on my side or some low level libraries, and I didn't make any such change. I was talking with a colleague for 30-45 minutes and when I returned and picked up where I left off the issue first appeared.</p>

<p>Any thoughts or ideas on troubleshooting this?</p>

<p>Full exception dump follows.</p>

<pre><code>$ python
Python 2.7.10 (default, Jul 14 2015, 19:46:27) 
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import boto3
&gt;&gt;&gt; boto3.client('ec2').describe_regions()
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/Library/Python/2.7/site-packages/botocore/client.py"", line 200, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File ""/Library/Python/2.7/site-packages/botocore/client.py"", line 244, in _make_api_call
    operation_model, request_dict)
  File ""/Library/Python/2.7/site-packages/botocore/endpoint.py"", line 173, in make_request
    return self._send_request(request_dict, operation_model)
  File ""/Library/Python/2.7/site-packages/botocore/endpoint.py"", line 203, in _send_request
    success_response, exception):
  File ""/Library/Python/2.7/site-packages/botocore/endpoint.py"", line 267, in _needs_retry
    caught_exception=caught_exception)
  File ""/Library/Python/2.7/site-packages/botocore/hooks.py"", line 226, in emit
    return self._emit(event_name, kwargs)
  File ""/Library/Python/2.7/site-packages/botocore/hooks.py"", line 209, in _emit
    response = handler(**kwargs)
  File ""/Library/Python/2.7/site-packages/botocore/retryhandler.py"", line 183, in __call__
    if self._checker(attempts, response, caught_exception):
  File ""/Library/Python/2.7/site-packages/botocore/retryhandler.py"", line 250, in __call__
    caught_exception)
  File ""/Library/Python/2.7/site-packages/botocore/retryhandler.py"", line 273, in _should_retry
    return self._checker(attempt_number, response, caught_exception)
  File ""/Library/Python/2.7/site-packages/botocore/retryhandler.py"", line 313, in __call__
    caught_exception)
  File ""/Library/Python/2.7/site-packages/botocore/retryhandler.py"", line 222, in __call__
    return self._check_caught_exception(attempt_number, caught_exception)
  File ""/Library/Python/2.7/site-packages/botocore/retryhandler.py"", line 355, in _check_caught_exception
    raise caught_exception
botocore.exceptions.EndpointConnectionError: Could not connect to the endpoint URL: ""https://ec2.us-east-1.amazonaws.com/""
</code></pre>
",2,1441220663,python;exception;boto3;botocore,True,8481,3,1628138793,https://stackoverflow.com/questions/32361173/python-boto3-connection-error-with-no-apparent-cause
60703127,How to catch `botocore.errorfactory.UserNotFoundException`?,"<p>I am using <code>AWS Cognito</code> to make <code>OAuth</code> server. I am now creating the exception handler in case use does not exist, but <code>requests</code> intended to get one</p>

<pre class=""lang-sh prettyprint-override""><code>ipdb&gt; pk
'David'
ipdb&gt; res = self.cognito_client.admin_get_user(
            UserPoolId=settings.AWS_USER_POOL_ID,
            Username=pk
        )
*** botocore.errorfactory.UserNotFoundException: An error occurred (UserNotFoundException) when calling the AdminGetUser operation: User does not exist.
Traceback (most recent call last):
  File ""/Users/sarit/.pyenv/versions/futuready-titan/lib/python3.8/site-packages/botocore/client.py"", line 316, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File ""/Users/sarit/.pyenv/versions/futuready-titan/lib/python3.8/site-packages/botocore/client.py"", line 626, in _make_api_call
    raise error_class(parsed_response, operation_name)
</code></pre>

<pre><code>boto3==1.12.15            # via -r el.in
botocore==1.15.15         # via boto3, s3transfer
django==3.0.3
python3.8.1
</code></pre>

<p>I had checked with <a href=""https://github.com/boto/botocore/blob/develop/botocore/exceptions.py"" rel=""noreferrer"">botocore source code</a> <code>UserNotFoundException</code></p>

<p><strong>Question:</strong><br>
How can I specifically <code>catch</code> this <code>exception</code>?</p>
",33,1584349933,python;boto3;botocore,True,15673,3,1627240305,https://stackoverflow.com/questions/60703127/how-to-catch-botocore-errorfactory-usernotfoundexception
68394929,botocore.response.StreamingBody gives JSONDecodeError,"<p>I am trying to load a payload returned by a lambda invocation but getting <code>JSONDecodeError</code>
Below is my lambda code.</p>
<pre><code>from datetime import datetime
metadata={}
metadata[&quot;execution_info&quot;] = {
            &quot;aws_lambda_request_id&quot;: &quot;test&quot;,
            &quot;aws_lambda_instance_id&quot;: &quot;test&quot;,
            &quot;started_timestamp&quot;: datetime.utcnow().isoformat()
            }
metadata[&quot;execution_info&quot;][&quot;completed_timestamp&quot;] = datetime.utcnow().isoformat()
def lambda_handler(event, context):
    return {
    &quot;statusCode&quot;: 200,
    &quot;headers&quot;: {
        &quot;Content-Type&quot;: &quot;application/json&quot;
    },
    &quot;body&quot;: json.dumps({
        &quot;body&quot;: metadata
    })
}
</code></pre>
<p>Then in my calling function when I try to load the reponse, like below</p>
<pre><code>response=lambda_mock.invoke(
    FunctionName='test-lambda',
    InvocationType='RequestResponse'
)
response=json.load(response['Payload'])
</code></pre>
<p>It throws</p>
<pre><code>raise JSONDecodeError(&quot;Expecting value&quot;, s, err.value) from None
E           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
/usr/local/lib/python3.8/json/decoder.py:355: JSONDecodeError
</code></pre>
<p>Tried this as well- <code>response=json.loads(response['Payload'].read().decode(&quot;utf-8&quot;))</code> but still same error.</p>
<p>What is going wrong here ?</p>
<p>Update 1-</p>
<p>print(response['Payload']) gives &lt;botocore.response.StreamingBody object at 0x7f1907af1400&gt;</p>
<p><strong>Root Cause</strong></p>
<p>I think I have found the issue. Looks like the mock lambda is returning bad data. It's returning the expected response + lambda logs (start request/end request details). It's somehow combining them both and returning them as one unit. I have no clue why is this happening.</p>
<p><code>print(response['Payload'].read())</code> generates below responses in my local and container. It's strange that same code produces two different output.</p>
<h2 id=""local"">local</h2>
<pre><code>{'statusCode': 200, 'headers': {'Content-Type': 'application/json'}, 'body': '{&quot;body&quot;: {&quot;execution_info&quot;: {&quot;aws_lambda_request_id&quot;: &quot;test&quot;, &quot;aws_lambda_instance_id&quot;: &quot;test&quot;, &quot;started_timestamp&quot;: &quot;some-time&quot;, &quot;completed_timestamp&quot;: &quot;some-time&quot;}}}'}
</code></pre>
<h2 id=""container"">container</h2>
<pre><code>b'\x1b[32mSTART RequestId: 14b675fe-bea2-1072-ceb7-53f0a72ac92c Version: 1\x1b[0m\n\x1b[32mEND RequestId: 14b675fe-bea2-1072-ceb7-53f0a72ac92c\x1b[0m\n\x1b[32mREPORT RequestId: 14b675fe-bea2-1072-ceb7-53f0a72ac92c\tInit Duration: 432.19 ms\tDuration: 3.07 ms\tBilled Duration: 4 ms\tMemory Size: 1000 MB\tMax Memory Used: 23 MB\t\x1b[0m\n\n{&quot;statusCode&quot;:200,&quot;headers&quot;:{&quot;Content-Type&quot;:&quot;application/json&quot;},&quot;body&quot;:&quot;{\\&quot;body\\&quot;: {\\&quot;execution_info\\&quot;: {\\&quot;aws_lambda_request_id\\&quot;: \\&quot;test\\&quot;, \\&quot;aws_lambda_instance_id\\&quot;: \\&quot;test\\&quot;, \\&quot;started_timestamp\\&quot;: \\&quot;some-time\\&quot;, \\&quot;completed_timestamp\\&quot;: \\&quot;some-time\\&quot;}}}&quot;}'
</code></pre>
<p><strong>Phew !</strong>
Moto 1.3.14 has this bug. Degraded moto version on my local and now I am getting the same error.</p>
",0,1626356304,python;json;amazon-web-services;boto;botocore,True,692,1,1626376521,https://stackoverflow.com/questions/68394929/botocore-response-streamingbody-gives-jsondecodeerror
68285179,"awscli 1.18.34 has requirement botocore==1.15.34, but you&#39;ll have botocore 1.15.49 which is incompatible","<p>I am getting this error after upgrading to python version 3.7 from 2.7.</p>
<p>How to solve this error?</p>
<pre><code>ERROR: tox 3.14.6 has requirement importlib-metadata&lt;2,&gt;=0.12; python_version &lt; &quot;3.8&quot;, but you'll have importlib-metadata 4.6.1 which is incompatible.
ERROR: awscli 1.18.34 has requirement botocore==1.15.34, but you'll have botocore 1.15.49 which is incompatible.
</code></pre>
",0,1625657437,python;python-3.x;tox;botocore,True,871,1,1625659256,https://stackoverflow.com/questions/68285179/awscli-1-18-34-has-requirement-botocore-1-15-34-but-youll-have-botocore-1-15
68097671,How to install botocore in yocto,"<p>I need boto3 in yocto and I cannot find a recipe that install botocore.</p>
<p>I have found the recipe for boto3, see below, but the boto3-xxx.tar.gz installs botocore through requirements.txt. Yocto does not seems to run it.</p>
<p>Finally, boto3 is installed but it is completely missing botocore.</p>
<pre><code>HOMEPAGE = &quot;https://github.com/boto/boto&quot;
SUMMARY = &quot;Amazon Web Services API&quot;
DESCRIPTION = &quot;\
  Boto3 is the Amazon Web Services (AWS) Software Development Kit (SDK) for Python, \
  which allows Python developers to write software that makes use of services like \
  Amazon S3 and Amazon EC2. \
  &quot;
SECTION = &quot;devel/python&quot;
LICENSE = &quot;MIT&quot;
LIC_FILES_CHKSUM = &quot;file://LICENSE;md5=2ee41112a44fe7014dce33e26468ba93&quot;


SRCNAME = &quot;boto3&quot;
SRC_URI = &quot;https://pypi.python.org/packages/source/b/${SRCNAME}/${SRCNAME}-${PV}.tar.gz&quot;


SRC_URI[md5sum] = &quot;fb8b77d4ac10a971570419dd3613196e&quot;
SRC_URI[sha256sum] = &quot;4856c8cb4150b900cc7dccbdf16f542fb8c12e97b17639979e58760847f7cf35&quot;

inherit pypi setuptools3
</code></pre>
<p><a href=""https://i.stack.imgur.com/sprr6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sprr6.png"" alt=""enter image description here"" /></a></p>
<p>Regards
Marco</p>
",2,1624442502,python;boto3;yocto;botocore,True,672,1,1624447441,https://stackoverflow.com/questions/68097671/how-to-install-botocore-in-yocto
67899815,Failed invoking lambda with reason Connection was closed before we received a valid response from endpoint,"<p>Trying to invoke a AWS Lambda from AWS EC2 instance running a python program. Both the EC2 instance and lambda are on same VPC.</p>
<pre><code>from botocore.config import Config

adv_client_config = Config(read_timeout=60,connect_timeout=60,retries={'max_attempts': 0})

self.client = boto3.client(
            'lambda',
            region_name='us-east-1',
            config=adv_client_config
        )
response = self.client.invoke(
              FunctionName=function_name,
              InvocationType=invoke_type,
              Payload=json.dumps(request)
           )
</code></pre>
<p>InvocationType is &quot;RequestResponse&quot;. This behavior is seen only when the 'max_attempts' is set to 0. If it is set to some value this behavior is not seen.</p>
<p>The call is failing within 100ms of placing call to the lambda.</p>
",0,1623225017,python;amazon-web-services;aws-lambda;boto3;botocore,False,768,1,1623411632,https://stackoverflow.com/questions/67899815/failed-invoking-lambda-with-reason-connection-was-closed-before-we-received-a-va
67567374,&#39;S3&#39; object has no attribute &#39;get_object_lock_configuration&#39;,"<p>I'm trying to implement the object lock feature but functions (get/put_object_lock_configuration) are not available :</p>
<pre><code>&gt;&gt;&gt; import boto3
&gt;&gt;&gt; boto3.__version__
'1.17.64'
&gt;&gt;&gt; client = boto3.client('s3')
&gt;&gt;&gt; client.get_object_lock_configuration
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/usr/lib/python3.6/site-packages/botocore/client.py&quot;, line 553, in __getattr__
    self.__class__.__name__, item)
AttributeError: 'S3' object has no attribute 'get_object_lock_configuration'

&gt;&gt;&gt; client.get_object_lock_configuration(Bucket='tst', ExpectedBucketOwner='tst')
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/usr/lib/python3.6/site-packages/botocore/client.py&quot;, line 553, in __getattr__
    self.__class__.__name__, item)
AttributeError: 'S3' object has no attribute 'get_object_lock_configuration'
</code></pre>
<p><strong>Edit:</strong>
object lock functions not showing in python (tab tab) :</p>
<pre><code>&gt;&gt;&gt; client.get_object_
client.get_object_acl(      client.get_object_tagging(  client.get_object_torrent(

&gt;&gt;&gt; client.put_object
client.put_object(          client.put_object_acl(      client.put_object_tagging(

</code></pre>
",0,1621244680,python;amazon-s3;boto3;botocore,True,1127,2,1621254236,https://stackoverflow.com/questions/67567374/s3-object-has-no-attribute-get-object-lock-configuration
67057881,boto3 gives InvalidBucketName error for valid bucket names on S3 with custom url,"<p>I am trying to write a python script for basic get/put/delete/list operations on S3. I am using a cloudian S3 object storage and not AWS. To set the boto3 resource, I set the endpoint and keys like this -</p>
<pre><code>URL = 'http://ip:80'

s3_resource = boto3.resource ('s3', endpoint_url=URL,
   aws_access_key_id = ACCESS_KEY,
   aws_secret_access_key = SECRET_KEY,
   region_name='region1')
</code></pre>
<p>I have created some test buckets MANUALLY with following names that pass valid S3 bucket names constraints:</p>
<ul>
<li>test-bucket-0</li>
<li>test-bucket-1</li>
<li>sample-bucket</li>
<li>testbucket</li>
</ul>
<p>However, when I try to create a bucket from python code, I get the following error repeatedly -</p>
<pre><code># &gt;&gt;&gt; client.list_buckets()
# Traceback (most recent call last):
#   File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
#   File &quot;/usr/local/lib/python3.8/site-packages/botocore/client.py&quot;, line 357, in _api_call
#     return self._make_api_call(operation_name, kwargs)
#   File &quot;/usr/local/lib/python3.8/site-packages/botocore/client.py&quot;, line 676, in _make_api_call
#     raise error_class(parsed_response, operation_name)
# botocore.exceptions.ClientError: An error occurred (InvalidBucketName) when calling the ListBuckets operation: The specified bucket is not valid.
</code></pre>
<p>Being very new to boto3, I am really not sure what boto3 is expecting. I have tried various combinations for creating connections to the S3 service such as using <code>client</code> instead of <code>resource</code>, but the problem is consistent.</p>
<p>A few other S3 connections I tried are these :</p>
<pre><code>s3 = boto3.resource('s3',
        endpoint_url='http://10.43.235.193:80',
        aws_access_key_id = 'aaa',                                                                                                                                              
        aws_secret_access_key = 'sss',
        config=Config(signature_version='s3v4'),
        region_name='region1')
</code></pre>
<pre><code>conn = boto3.connect_s3(                                                                                                                                                       
    aws_access_key_id = 'aaa',                                                                                                                                              
    aws_secret_access_key = 'sss',                                                                                                                                       
    host = '10.43.235.193',                                                                                                                                                       
    port = 80,                                                                                                                                                              
    is_secure = False,                                                                                                                                                        
) 
</code></pre>
<pre><code>from boto3.session import Session
session = Session(
    aws_access_key_id='aaa',
    aws_secret_access_key='sss',
    region_name='region1'
)

s3 = session.resource('s3')
client = session.client('s3', endpoint_url='http://10.43.235.193:80') # s3-region1.example.com
</code></pre>
<pre><code>s3_client = boto3.client ('s3', 
   endpoint_url=s3_endpoint,
   aws_access_key_id = 'aaa',
   aws_secret_access_key = 'sss',
   region_name='region1')
</code></pre>
<p>The python-script is running inside a container and the same pod that runs s3 container. Therefore IP is accessible from 1 container to another. How should I solve this problem?</p>
",0,1618228451,python;amazon-web-services;amazon-s3;boto3;botocore,True,1696,1,1618587054,https://stackoverflow.com/questions/67057881/boto3-gives-invalidbucketname-error-for-valid-bucket-names-on-s3-with-custom-url
67067680,botocore.exceptions.SSLError (CERTIFICATE_VERIFY_FAILED) inside a docker container,"<p>I have been trying for quite some time to figure out what is going wrong when I am making an API call to my AWS account using boto3 library inside a Docker container. The error I see is:</p>
<pre><code>docker run --rm -ti -v ${HOME}/.aws/credentials:/root/.aws/credentials:ro boto3_test
Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.8/site-packages/urllib3/connectionpool.py&quot;, line 699, in urlopen
    httplib_response = self._make_request(
  File &quot;/usr/local/lib/python3.8/site-packages/urllib3/connectionpool.py&quot;, line 382, in _make_request
    self._validate_conn(conn)
  File &quot;/usr/local/lib/python3.8/site-packages/urllib3/connectionpool.py&quot;, line 1010, in _validate_conn
    conn.connect()
  File &quot;/usr/local/lib/python3.8/site-packages/urllib3/connection.py&quot;, line 411, in connect
    self.sock = ssl_wrap_socket(
  File &quot;/usr/local/lib/python3.8/site-packages/urllib3/util/ssl_.py&quot;, line 428, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
  File &quot;/usr/local/lib/python3.8/site-packages/urllib3/util/ssl_.py&quot;, line 472, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File &quot;/usr/local/lib/python3.8/ssl.py&quot;, line 500, in wrap_socket
    return self.sslsocket_class._create(
  File &quot;/usr/local/lib/python3.8/ssl.py&quot;, line 1040, in _create
    self.do_handshake()
  File &quot;/usr/local/lib/python3.8/ssl.py&quot;, line 1309, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1125)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.8/site-packages/botocore/httpsession.py&quot;, line 314, in send
    urllib_response = conn.urlopen(
  File &quot;/usr/local/lib/python3.8/site-packages/urllib3/connectionpool.py&quot;, line 755, in urlopen
    retries = retries.increment(
  File &quot;/usr/local/lib/python3.8/site-packages/urllib3/util/retry.py&quot;, line 507, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File &quot;/usr/local/lib/python3.8/site-packages/urllib3/packages/six.py&quot;, line 734, in reraise
    raise value.with_traceback(tb)
  File &quot;/usr/local/lib/python3.8/site-packages/urllib3/connectionpool.py&quot;, line 699, in urlopen
    httplib_response = self._make_request(
  File &quot;/usr/local/lib/python3.8/site-packages/urllib3/connectionpool.py&quot;, line 382, in _make_request
    self._validate_conn(conn)
  File &quot;/usr/local/lib/python3.8/site-packages/urllib3/connectionpool.py&quot;, line 1010, in _validate_conn
    conn.connect()
  File &quot;/usr/local/lib/python3.8/site-packages/urllib3/connection.py&quot;, line 411, in connect
    self.sock = ssl_wrap_socket(
  File &quot;/usr/local/lib/python3.8/site-packages/urllib3/util/ssl_.py&quot;, line 428, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
  File &quot;/usr/local/lib/python3.8/site-packages/urllib3/util/ssl_.py&quot;, line 472, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File &quot;/usr/local/lib/python3.8/ssl.py&quot;, line 500, in wrap_socket
    return self.sslsocket_class._create(
  File &quot;/usr/local/lib/python3.8/ssl.py&quot;, line 1040, in _create
    self.do_handshake()
  File &quot;/usr/local/lib/python3.8/ssl.py&quot;, line 1309, in do_handshake
    self._sslobj.do_handshake()
urllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1125)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/src/main.py&quot;, line 51, in &lt;module&gt;
    print(dynamodb_ss.get_all_records())
  File &quot;/src/main.py&quot;, line 25, in get_all_records
    response = self.table.scan()
  File &quot;/usr/local/lib/python3.8/site-packages/boto3/resources/factory.py&quot;, line 520, in do_action
    response = action(self, *args, **kwargs)
  File &quot;/usr/local/lib/python3.8/site-packages/boto3/resources/action.py&quot;, line 83, in __call__
    response = getattr(parent.meta.client, operation_name)(*args, **params)
  File &quot;/usr/local/lib/python3.8/site-packages/botocore/client.py&quot;, line 357, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File &quot;/usr/local/lib/python3.8/site-packages/botocore/client.py&quot;, line 662, in _make_api_call
    http, parsed_response = self._make_request(
  File &quot;/usr/local/lib/python3.8/site-packages/botocore/client.py&quot;, line 682, in _make_request
    return self._endpoint.make_request(operation_model, request_dict)
  File &quot;/usr/local/lib/python3.8/site-packages/botocore/endpoint.py&quot;, line 102, in make_request
    return self._send_request(request_dict, operation_model)
  File &quot;/usr/local/lib/python3.8/site-packages/botocore/endpoint.py&quot;, line 136, in _send_request
    while self._needs_retry(attempts, operation_model, request_dict,
  File &quot;/usr/local/lib/python3.8/site-packages/botocore/endpoint.py&quot;, line 253, in _needs_retry
    responses = self._event_emitter.emit(
  File &quot;/usr/local/lib/python3.8/site-packages/botocore/hooks.py&quot;, line 356, in emit
    return self._emitter.emit(aliased_event_name, **kwargs)
  File &quot;/usr/local/lib/python3.8/site-packages/botocore/hooks.py&quot;, line 228, in emit
    return self._emit(event_name, kwargs)
  File &quot;/usr/local/lib/python3.8/site-packages/botocore/hooks.py&quot;, line 211, in _emit
    response = handler(**kwargs)
  File &quot;/usr/local/lib/python3.8/site-packages/botocore/retryhandler.py&quot;, line 183, in __call__
    if self._checker(attempts, response, caught_exception):
  File &quot;/usr/local/lib/python3.8/site-packages/botocore/retryhandler.py&quot;, line 250, in __call__
    should_retry = self._should_retry(attempt_number, response,
  File &quot;/usr/local/lib/python3.8/site-packages/botocore/retryhandler.py&quot;, line 277, in _should_retry
    return self._checker(attempt_number, response, caught_exception)
  File &quot;/usr/local/lib/python3.8/site-packages/botocore/retryhandler.py&quot;, line 316, in __call__
    checker_response = checker(attempt_number, response,
  File &quot;/usr/local/lib/python3.8/site-packages/botocore/retryhandler.py&quot;, line 222, in __call__
    return self._check_caught_exception(
  File &quot;/usr/local/lib/python3.8/site-packages/botocore/retryhandler.py&quot;, line 359, in _check_caught_exception
    raise caught_exception
  File &quot;/usr/local/lib/python3.8/site-packages/botocore/endpoint.py&quot;, line 200, in _do_get_response
    http_response = self._send(request)
  File &quot;/usr/local/lib/python3.8/site-packages/botocore/endpoint.py&quot;, line 269, in _send
    return self.http_session.send(request)
  File &quot;/usr/local/lib/python3.8/site-packages/botocore/httpsession.py&quot;, line 341, in send
    raise SSLError(endpoint_url=request.url, error=e)
botocore.exceptions.SSLError: SSL validation failed for https://dynamodb.us-west-2.amazonaws.com/ [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1125)
</code></pre>
<p>My Dockerfile contains the following:</p>
<pre><code>FROM python:3.8-slim

RUN pip install --upgrade pip
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY /src/ /src/

RUN chmod +x /src/main.py

ENTRYPOINT [&quot;python&quot;, &quot;/src/main.py&quot;]
</code></pre>
<p>Here is my <code>requirements.txt</code> file:</p>
<pre><code>awscli==1.19.50
boto3==1.17.50
botocore==1.20.50
certifi==2020.12.5
cffi==1.14.5
colorama==0.4.3
cryptography==3.4.7
docutils==0.15.2
jmespath==0.10.0
pyasn1==0.4.8
pycparser==2.20
pyOpenSSL==20.0.1
python-dateutil==2.8.1
PyYAML==5.4.1
rsa==4.7.2
s3transfer==0.3.6
six==1.15.0
urllib3==1.26.4
</code></pre>
<p>Essentially I'm just trying to retrieve a list of records in DynamoDB. This script works fine locally, but fails inside a Docker container.</p>
<p>Do I have to configure SSL certs? Any help is greatly appreciated!</p>
<p>Thanks,
Brian</p>
<p>EDIT: here is the Python code</p>
<pre><code>import boto3
from botocore.exceptions import ClientError


def gen_session_obj(profile_name='dynamodb', region_name='us-west-2'):
    return boto3.Session(profile_name=profile_name, region_name=region_name)


def gen_client(session, service):
    client = session.resource(service)
    return client


class DynamoDbStateStore:

    def __init__(self, dynamo_db_resource, table):
        self.dynamodb_session = dynamo_db_resource
        self.table = self.dynamodb_session.Table(table)

    def get_all_records(self, project_expression=''):
        try:
            if project_expression:
                response = self.table.scan(ProjectionExpression=project_expression)
            else:
                response = self.table.scan()
            data = response.get('Items')

            while 'LastEvaluatedKey' in response:
                if project_expression:
                    response = self.table.scan(
                        ExclusiveStartKey=response['LastEvaluatedKey'],
                        ProjectionExpression=project_expression
                    )
                else:
                    response = self.table.scan(
                        ExclusiveStartKey=response['LastEvaluatedKey']
                    )
                data.extend(response['Items'])
        except ClientError as e:
            print(e.response['Error']['Message'])
            raise
        return data


if __name__ == '__main__':
    session = gen_session_obj()
    dynamodb_client = gen_client(session, 'dynamodb')

    dynamodb_ss = DynamoDbStateStore(dynamodb_client, 'user_mgr_audit_log')
    print(dynamodb_ss.get_all_records())
</code></pre>
",1,1618278341,python;python-3.x;docker;boto3;botocore,False,583,1,1618450034,https://stackoverflow.com/questions/67067680/botocore-exceptions-sslerror-certificate-verify-failed-inside-a-docker-contain
46063138,How can I import the boto3 ssm ParameterNotFound exception?,"<p>I would like to <code>import</code> the <code>exception</code> that occurs when a <code>boto3</code> <code>ssm</code> parameter is not found with <code>get_parameter</code>.  I'm trying to add some extra <code>ssm</code> functionality to the <code>moto</code> library, but I am stumped at this point.</p>

<pre><code>&gt;&gt;&gt; import boto3
&gt;&gt;&gt; ssm = boto3.client('ssm')
&gt;&gt;&gt; try:
        ssm.get_parameter(Name='not_found')
    except Exception as e:
        print(type(e))
&lt;class 'botocore.errorfactory.ParameterNotFound'&gt;
&gt;&gt;&gt; from botocore.errorfactory import ParameterNotFound
ImportError: cannot import name 'ParameterNotFound'
&gt;&gt;&gt; import botocore.errorfactory.ParameterNotFound
ModuleNotFoundError: No module named 'botocore.errorfactory.ParameterNotFound'; 'botocore.errorfactory' is not a package
</code></pre>

<p>However, the <code>Exception</code> cannot be imported, and does not appear to exist in the botocore code.  How can I import this exception?</p>
",27,1504644448,python;amazon-web-services;boto3;botocore;moto,True,15810,2,1618307004,https://stackoverflow.com/questions/46063138/how-can-i-import-the-boto3-ssm-parameternotfound-exception
48808956,Search specific file in AWS S3 bucket using python,"<p>I have AWS S3 access and the bucket has nearly 300 files inside the bucket. I need to download single file from this bucket by pattern matching or search because i do not know the exact filename (Say files ends with .csv format).<br>
Here is my sample code which shows all files inside the bucket</p>

<pre><code>def s3connection(credentialsdict):
    """"""
    :param access_key: Access key for AWS to establish S3 connection
    :param secret_key: Secret key for AWS to establish S3 connection
    :param file_name: file name of the billing file(csv file)
    :param bucket_name: Name of the bucket which consists of billing files
    :return: status, billing_bucket, billing_key
    """"""
    os.environ['S3_USE_SIGV4'] = 'True'
    conn = S3Connection(credentialsdict[""access_key""], credentialsdict[""secret_key""], host='s3.amazonaws.com')
    billing_bucket = conn.get_bucket(credentialsdict[""bucket_name""], validate=False)
    try:
        billing_bucket.get_location()
    except S3ResponseError as e:
        if e.status == 400 and e.error_code == 'AuthorizationHeaderMalformed':
            conn.auth_region_name = ET.fromstring(e.body).find('./Region').text
    billing_bucket = conn.get_bucket(credentialsdict[""bucket_name""])
    print billing_bucket

    if not billing_bucket:
        raise Exception(""Please Enter valid bucket name. Bucket %s does not exist""
                        % credentialsdict.get(""bucket_name""))
    for key in billing_bucket.list():
        print key.name
    del os.environ['S3_USE_SIGV4']
</code></pre>

<p>Can I pass search string to retrieve the exact matched filenames?</p>
",3,1518702890,python;amazon-s3;boto;boto3;botocore,True,19668,3,1617253383,https://stackoverflow.com/questions/48808956/search-specific-file-in-aws-s3-bucket-using-python
66821778,python verify AWS S3 canned ACL / get list of valid canned ACLs,"<p>Is there away to validate a canned AWS S3 ACL or is there a list of valid ACLs in <code>boto3</code>/<code>botocore</code> available?</p>
<p><a href=""https://i.stack.imgur.com/nFBSu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nFBSu.png"" alt=""enter image description here"" /></a></p>
<p>I'd like to verify a user input ACL. I could just compile a list myself to verify against, but I figured it might be somewhere within <code>boto3</code> or <code>botocore</code>. I've checked, but found nothing.</p>
",0,1616780135,python;amazon-web-services;boto3;botocore,True,64,1,1617043265,https://stackoverflow.com/questions/66821778/python-verify-aws-s3-canned-acl-get-list-of-valid-canned-acls
66679157,Error code for try/catch with boto3 S3 upload,"<p>I have a function that I use to upload files/folders to S3 using <code>boto3</code>. I want to have a <code>try/catch</code> that handles errors if something fails to upload.:</p>
<pre><code>import os
import boto3
import botocore

def upload_files(key, secret, bucket, s3_path, source_path):

    session = boto3.Session(
        aws_access_key_id=key,
        aws_secret_access_key=secret,
        region_name='us-east-1'
    )

    s3 = session.resource('s3')
    bucket = s3.Bucket(bucket)
    
    try:
        for subdir, dirs, files in os.walk(source_path):
            for file in files:
                full_path = os.path.join(subdir, file)
                with open(full_path, 'rb') as data:
                    bucket.put_object(Key=s3_path + full_path, Body=data, ServerSideEncryption='AES256')
    except botocore.exceptions.ClientError as e:
        if e.response['Error']['Code'] == &quot;404&quot;:
            print(&quot;The object does not exist.&quot;)
        else:
            raise
</code></pre>
<p>This I believe is the wrong error code that I'm looking for. I checked all the possible ones:</p>
<pre><code>import botocore
import boto3
[e for e in dir(botocore.exceptions) if e.endswith('Error')]

['AliasConflictParameterError',
 'ApiVersionNotFoundError',
 'BaseEndpointResolverError',
 'BotoCoreError',
 'CapacityNotAvailableError',
 'ChecksumError',
 'ClientError',
 'ConfigParseError',
 'ConnectTimeoutError',
 'ConnectionClosedError',
 'ConnectionError',
 'CredentialRetrievalError',
 'DataNotFoundError',
 'EndpointConnectionError',
 'EventStreamError',
 'HTTPClientError',
 'IncompleteReadError',
 'InfiniteLoopConfigError',
 'InvalidConfigError',
 'InvalidDNSNameError',
 'InvalidEndpointDiscoveryConfigurationError',
 'InvalidExpressionError',
 'InvalidHostLabelError',
 'InvalidIMDSEndpointError',
 'InvalidMaxRetryAttemptsError',
 'InvalidProxiesConfigError',
 'InvalidRegionError',
 'InvalidRetryConfigurationError',
 'InvalidRetryModeError',
 'InvalidS3AddressingStyleError',
 'InvalidS3UsEast1RegionalEndpointConfigError',
 'InvalidSTSRegionalEndpointsConfigError',
 'MD5UnavailableError',
 'MetadataRetrievalError',
 'MissingParametersError',
 'MissingServiceIdError',
 'NoCredentialsError',
 'NoRegionError',
 'OperationNotPageableError',
 'PaginationError',
 'ParamValidationError',
 'PartialCredentialsError',
 'ProxyConnectionError',
 'RangeError',
 'ReadTimeoutError',
 'RefreshWithMFAUnsupportedError',
 'SSLError',
 'SSOError',
 'SSOTokenLoadError',
 'ServiceNotInRegionError',
 'StubAssertionError',
 'StubResponseError',
 'UnStubbedResponseError',
 'UnauthorizedSSOTokenError',
 'UndefinedModelAttributeError',
 'UnknownClientMethodError',
 'UnknownCredentialError',
 'UnknownEndpointError',
 'UnknownKeyError',
 'UnknownParameterError',
 'UnknownServiceError',
 'UnknownSignatureVersionError',
 'UnseekableStreamError',
 'UnsupportedOutpostResourceError',
 'UnsupportedS3AccesspointConfigurationError',
 'UnsupportedS3ArnError',
 'UnsupportedS3ControlArnError',
 'UnsupportedS3ControlConfigurationError',
 'UnsupportedSignatureVersionError',
 'ValidationError',
 'WaiterConfigError',
 'WaiterError']
</code></pre>
<p>Given this, I'm not sure what to use from that list in my case and how to add that to the function that I have right now. Any help is appreciated!</p>
",1,1616005791,python;python-3.x;amazon-s3;boto3;botocore,False,1160,0,1616005791,https://stackoverflow.com/questions/66679157/error-code-for-try-catch-with-boto3-s3-upload
66663807,Invalid Header Value Error when creating boto3 sts client,"<p>I was authenticating with an internal IDP and then using the SAML assertion to assume role using with boto3 sts client. Interaction with IDP was fine and able to generate the SAML assertion after successful authentication but when I tried to generate the sts client &quot;client = boto3.client('sts')&quot; botocore threw Invalid header value error.</p>
<p>Error was coming from our egress proxy server.</p>
<pre><code>File &quot;/usr/local/lib/python3.8/dist-packages/aws_authentication/credentials.py&quot;, line 219, in decode_saml_assertion
    client = boto3.client('sts')
  File &quot;/usr/local/lib/python3.8/dist-packages/boto3/__init__.py&quot;, line 93, in client
    return _get_default_session().client(*args, **kwargs)
  File &quot;/usr/local/lib/python3.8/dist-packages/boto3/session.py&quot;, line 258, in client
    return self._session.create_client(
  File &quot;/usr/local/lib/python3.8/dist-packages/botocore/session.py&quot;, line 826, in create_client
    credentials = self.get_credentials()
  File &quot;/usr/local/lib/python3.8/dist-packages/botocore/session.py&quot;, line 430, in get_credentials
    self._credentials = self._components.get_component(
  File &quot;/usr/local/lib/python3.8/dist-packages/botocore/credentials.py&quot;, line 1975, in load_credentials
    creds = provider.load()
  File &quot;/usr/local/lib/python3.8/dist-packages/botocore/credentials.py&quot;, line 1028, in load
    metadata = fetcher.retrieve_iam_role_credentials()
  File &quot;/usr/local/lib/python3.8/dist-packages/botocore/utils.py&quot;, line 486, in retrieve_iam_role_credentials
    role_name = self._get_iam_role(token)
  File &quot;/usr/local/lib/python3.8/dist-packages/botocore/utils.py&quot;, line 518, in _get_iam_role
    return self._get_request(
  File &quot;/usr/local/lib/python3.8/dist-packages/botocore/utils.py&quot;, line 427, in _get_request
    response = self._session.send(request.prepare())
  File &quot;/usr/local/lib/python3.8/dist-packages/botocore/httpsession.py&quot;, line 356, in send
    raise HTTPClientError(error=e)

botocore.exceptions.HTTPClientError: An HTTP Client raised an unhandled exception: Invalid header value b'---- proxy error response ----'
</code></pre>
",0,1615930908,python;amazon-web-services;boto3;botocore,True,773,1,1615930908,https://stackoverflow.com/questions/66663807/invalid-header-value-error-when-creating-boto3-sts-client
66210193,s3fs/botocore import error: InvalidIMDSEndpointError,"<p>I was trying to run some python code in docker and export a .csv file to S3, but got the same error as in <a href=""https://stackoverflow.com/questions/65688584/"">aiobotocore - ImportError: cannot import name &#39;InvalidIMDSEndpointError&#39;</a> (asking here because I don't have enough reputation to comment under that thread..)</p>
<pre><code>  File &quot;/opt/conda/lib/python3.7/site-packages/s3fs/__init__.py&quot;, line 1, in &lt;module&gt;
    from .core import S3FileSystem, S3File
  File &quot;/opt/conda/lib/python3.7/site-packages/s3fs/core.py&quot;, line 14, in &lt;module&gt;
    import aiobotocore
  File &quot;/opt/conda/lib/python3.7/site-packages/aiobotocore/__init__.py&quot;, line 1, in &lt;module&gt;
    from .session import get_session, AioSession
  File &quot;/opt/conda/lib/python3.7/site-packages/aiobotocore/session.py&quot;, line 6, in &lt;module&gt;
    from .client import AioClientCreator, AioBaseClient
  File &quot;/opt/conda/lib/python3.7/site-packages/aiobotocore/client.py&quot;, line 12, in &lt;module&gt;
    from .utils import AioS3RegionRedirector
  File &quot;/opt/conda/lib/python3.7/site-packages/aiobotocore/utils.py&quot;, line 10, in &lt;module&gt;
    from botocore.exceptions import (
ImportError: cannot import name 'InvalidIMDSEndpointError' from 'botocore.exceptions' (/opt/conda/lib/python3.7/site-packages/botocore/exceptions.py)
</code></pre>
<p>I tried to use the versions of libraries as in the comment:
<code>botocore==1.19.52</code>
<code>s3fs==0.5.1</code>
<code>boto3==1.16.52</code>
<code>aiobotocore==1.2.0</code>
However, these don't solve the problem and I still get the same error.</p>
<p>Could anyone here give me some hints how to solve this?
Thanks!</p>
",1,1613400662,python;amazon-web-services;amazon-s3;botocore;python-s3fs,False,1072,1,1613409448,https://stackoverflow.com/questions/66210193/s3fs-botocore-import-error-invalidimdsendpointerror
66074136,UpdateItem Dynamodb Boto3 Says Wrong Query,"<p>I am trying to increment values of specific columns.
I am using Boto3.</p>
<pre><code>update_query = '#clicks.#OS.#os = #clicks.#OS.#os + :inc, #clicks.#Brands.#brand = #clicks.#Brands.#brand + :inc, #clicks.#RAMs.#ram = #clicks.#RAMs.#ram + :inc'
table_urls.update_item(
            Key={
                'urlid': urlid
            },
            UpdateExpression=update_query,
            ExpressionAttributeValues={
                ':inc' : Decimal(1)
            },
            ExpressionAttributeNames={
                &quot;#os&quot;   : os,
                &quot;#brand&quot;: brand,
                &quot;#ram&quot;  : ram,
                &quot;#clicks&quot;: &quot;clicks&quot;,
                &quot;#OS&quot;   : &quot;OS&quot;,
                &quot;#Brands&quot;: &quot;Brands&quot;,
                &quot;#RAMs&quot;  : &quot;RAMs&quot;
            },
            ReturnValues='NONE'
        )
</code></pre>
<p>Here is Error StackTrace:
Error:</p>
<pre><code>An error occurred (ValidationException) when calling the UpdateItem operation: Invalid UpdateExpression: Syntax error; token: &quot;#clicks&quot;, near: &quot;#clicks.&quot;
QUERY::&gt;&gt; #clicks.#OS.#os = #clicks.#OS.#os + :inc, #clicks.#Brands.#brand = #clicks.#Brands.#brand + :inc, #clicks.#RAMs.#ram = #clicks.#RAMs.#ram + :inc
</code></pre>
",0,1612592355,python;amazon-web-services;amazon-dynamodb;boto3;botocore,True,89,1,1612641048,https://stackoverflow.com/questions/66074136/updateitem-dynamodb-boto3-says-wrong-query
58564394,mock boto3 response for downloading file from S3,"<p>I've got code that downloads a file from an S3 bucket using <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Bucket.download_file"" rel=""noreferrer"">boto3</a>.</p>

<pre class=""lang-py prettyprint-override""><code># foo.py
def dl(src_f, dest_f):
  s3 = boto3.resource('s3')
  s3.Bucket('mybucket').download_file(src_f, dest_f)
</code></pre>

<p>I'd now like to write a unit test for <code>dl()</code> using pytest and by mocking the interaction with AWS using the stubber available in <a href=""https://botocore.amazonaws.com/v1/documentation/api/latest/reference/stubber.html"" rel=""noreferrer"">botocore</a>.</p>

<pre class=""lang-py prettyprint-override""><code>@pytest.fixture
def s3_client():
    yield boto3.client(""s3"")

from foo import dl
def test_dl(s3_client):

  with Stubber(s3_client) as stubber:
    params = {""Bucket"": ANY, ""Key"": ANY}
    response = {""Body"": ""lorem""}
    stubber.add_response(SOME_OBJ, response, params)
    dl('bucket_file.txt', 'tmp/bucket_file.txt')
    assert os.path.isfile('tmp/bucket_file.txt')
</code></pre>

<p>I'm not sure about the right approach for this. How do I add <code>bucket_file.txt</code> to the stubbed reponse? What object do I need to <code>add_response()</code> to (shown as <code>SOME_OBJ</code>)?</p>
",8,1572030521,python;amazon-web-services;amazon-s3;boto3;botocore,True,11126,1,1610532421,https://stackoverflow.com/questions/58564394/mock-boto3-response-for-downloading-file-from-s3
49603710,Botocore fails to read credentials when run as daemon.service,"<p>I had my script running smoothly from command line, however, when I start it as a systemd.service, I get the following error:</p>

<pre><code>iot_local.service - My iot_local Service
   Loaded: loaded (/lib/systemd/system/iot_local.service; enabled; vendor preset: enabled)
   Active: failed (Result: exit-code) since Sun 2018-04-01 23:06:45 UTC; 5s ago
  Process: 2436 ExecStart=/usr/bin/python /home/ubuntu/myTemp/iot_local.py (code=exited, status=1/FAILURE)
 Main PID: 2436 (code=exited, status=1/FAILURE)

Apr 01 23:06:45 ip-172-31-29-45 python[2436]:   File ""/usr/local/lib/python2.7/dist-packages/botocore/client.py"", line 358, in resolve
Apr 01 23:06:45 ip-172-31-29-45 python[2436]:     service_name, region_name)
Apr 01 23:06:45 ip-172-31-29-45 python[2436]:   File ""/usr/local/lib/python2.7/dist-packages/botocore/regions.py"", line 122, in construct_endpoint
Apr 01 23:06:45 ip-172-31-29-45 python[2436]:     partition, service_name, region_name)
Apr 01 23:06:45 ip-172-31-29-45 python[2436]:   File ""/usr/local/lib/python2.7/dist-packages/botocore/regions.py"", line 135, in _endpoint_for_partition
Apr 01 23:06:45 ip-172-31-29-45 python[2436]:     raise NoRegionError()
Apr 01 23:06:45 ip-172-31-29-45 python[2436]: botocore.exceptions.NoRegionError: You must specify a region.
Apr 01 23:06:45 ip-172-31-29-45 systemd[1]: iot_local.service: Main process exited, code=exited, status=1/FAILURE
Apr 01 23:06:45 ip-172-31-29-45 systemd[1]: iot_local.service: Unit entered failed state.
Apr 01 23:06:45 ip-172-31-29-45 systemd[1]: iot_local.service: Failed with result 'exit-code'.
</code></pre>

<p>it seems to fail on this line:</p>

<pre><code>DB=boto3.resourse('dynamodb')
</code></pre>

<p>If I add the region as an argument, the script still fails later because can not find credentials. So, when I provide region, id and a key as argument, everything works:</p>

<pre><code>boto3.resource('dynamodb', region_name='us-west-2', aws_access_key_id=ACCESS_ID, aws_secret_access_key=ACCESS_KEY)
</code></pre>

<p>The obvious problem is that when this script is run as service, it fails to obtain the info from the <code>~/.aws/config</code> and <code>~/.aws/credentials</code>, which I made sure to contain all the necessary information by running <code>aws configure</code> as mentioned <a href=""https://stackoverflow.com/questions/33297172/boto3-error-botocore-exceptions-nocredentialserror-unable-to-locate-credential?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa"">here</a>.</p>

<pre><code>[default]
aws_access_key_id=XXXXXXXXXXXXXX
aws_secret_access_key=YYYYYYYYYYYYYYYYYYYYYYYYYYY
</code></pre>

<p>I also tried this:</p>

<pre><code>export AWS_CONFIG_FILE=""/home/ubuntu/.aws/config""
</code></pre>

<p>and this</p>

<pre><code>sudo chown root:root ~/.aws
</code></pre>

<p>but it did not help. Any ideas why .service does not ""see"" the credentials files?</p>
",1,1522626441,python;amazon-web-services;boto3;credentials;botocore,True,1179,3,1607671905,https://stackoverflow.com/questions/49603710/botocore-fails-to-read-credentials-when-run-as-daemon-service
65212063,dynamodb conditional update item (client),"<p>I am referring to this sample of update item: <a href=""https://stackoverflow.com/a/62030403/13967222"">https://stackoverflow.com/a/62030403/13967222</a>.</p>
<pre><code>
    for key, val in body.items():
        update_expression.append(f&quot; {key} = :{key},&quot;)
        update_values[f&quot;:{key}&quot;] = val

    return &quot;&quot;.join(update_expression)[:-1], update_values
</code></pre>
<p>I am trying to achieve the same but using dynamodb client.</p>
<p>Is there way I add check if the value is available using dynamodb client?</p>
",2,1607497376,python;amazon-dynamodb;boto3;botocore,True,645,1,1607511520,https://stackoverflow.com/questions/65212063/dynamodb-conditional-update-item-client
55795236,Downloading files from AWS S3 Bucket with boto3 results in ClientError: An error occurred (403): Forbidden,"<p>I am trying to download files from a s3 bucket by using the Access Key ID and Secret Access Key provided by <a href=""https://db.humanconnectome.org"" rel=""noreferrer"">https://db.humanconnectome.org</a>. However, even though I am able to navigate the database and find the files (as I have configured my credentials via aws cli), attempting to download them results in the following error:
""botocore.exceptions.ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden""</p>

<p>With the same credentials, I can browse the same database and download the files manually via a cloud storage browser such as Cyberduck, so how Cyberduck accesses the data does not invoke a  403 Forbidden error.</p>

<p>I have also verified that boto3 is able to access my aws credentials, and also tried by hardcoding them.</p>

<p>How I am attempting to download the data is very straightforward, and replicates the boto3 docs example: <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-example-download-file.html"" rel=""noreferrer"">https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-example-download-file.html</a></p>

<pre><code>s3 = boto3.client('s3',
    aws_access_key_id=ACCESS_KEY_ID,
    aws_secret_access_key=ACCESS_KEY,)

s3.download_file(Bucket=BUCKET_NAME, Key=FILE_KEY, Filename=FILE_NAME)

</code></pre>

<p>This should download the file to the location and file given by FILE_NAME, but instead invokes the 403 Forbidden error.</p>
",6,1555940069,python;amazon-web-services;amazon-s3;boto3;botocore,True,8762,2,1606442473,https://stackoverflow.com/questions/55795236/downloading-files-from-aws-s3-bucket-with-boto3-results-in-clienterror-an-error
64925806,Heroku still accessing old jobs in the queue and not using the updated code,"<p>I'm trying to build a python app on Heroku. I want to upload an excel file to via botocore to a bucket hosted on AWS, but I get the following error:</p>
<pre><code>2020-11-19T20:21:20.616363+00:00 app[web.1]: botocore.exceptions.NoCredentialsError: Unable to locate credentials
</code></pre>
<p>I modified the code to include the credentials, cleared the cache, restarted the dynos and then started the app again. Now, I get the following error (along with the credentials error described above):</p>
<pre><code>2020-11-19T20:21:20.616193+00:00 app[web.1]: Started at: 2020-11-10 15:35:54.303420
2020-11-19T20:21:20.616295+00:00 app[web.1]: Ended at: 2020-11-10 15:35:57.484733
2020-11-19T20:21:20.616331+00:00 app[web.1]: Exception status: Traceback (most recent call last):
</code></pre>
<p>I had 2 questions here:</p>
<ol>
<li>Why is heroku not using the updated code?</li>
<li>Why is it still accessing an old job even when I've cleared the cache, restarted the dynos, etc.?</li>
</ol>
<p>Any help would be much appreciated -Thanks!</p>
",0,1605859425,python;heroku;botocore,False,58,1,1605945778,https://stackoverflow.com/questions/64925806/heroku-still-accessing-old-jobs-in-the-queue-and-not-using-the-updated-code
54249396,Should we close botocore&#39;s StreamingBody?,"<p>The <code>close()</code> method of <code>StreamingBody</code> is <a href=""https://botocore.amazonaws.com/v1/documentation/api/latest/reference/response.html"" rel=""noreferrer"">clearly documented</a>. However, it is unclear whether we should proactively close it after use.</p>

<p>As of <a href=""https://stackoverflow.com/questions/31976273/open-s3-object-as-a-string-with-boto3"">the few examples</a> I have seen, the <code>close()</code> method is never explicitly called.</p>

<p>My concern/question is, if we don't explicitly close it, wouldn't there be the risk of leaking connections.</p>
",9,1547796229,python;amazon-s3;boto3;botocore,False,536,0,1604681076,https://stackoverflow.com/questions/54249396/should-we-close-botocores-streamingbody
53745860,Download latest file in S3 bucket folder,"<p>I am writing a Python script to download the latest file from inside an S3 Bucket's folder. I understand how to download the latest file object from my S3 Bucket, however the files I want to download are in a <strong>folder</strong> inside the bucket. I am at a complete loss on how to do it and where it may be added within my code. I tried putting the path at the end of my bucket link but that did not seem to work.</p>

<pre><code># AWS Credentials 
client = boto3.client('athena',aws_access_key_id=aws_server_access_key, aws_secret_access_key=aws_server_secret_key,region_name='us-east-1')
ba = boto3.client('s3',aws_access_key_id=aws_server_access_key, aws_secret_access_key=aws_server_secret_key,region_name='us-east-1')
# Get latest modified file
get_last_modified = lambda obj: int(obj['LastModified'].strftime('%s'))

objs = ba.list_objects_v2(Bucket=BUCKET_NAME)['Contents']
last_added = [obj['Key'] for obj in sorted(objs, key=get_last_modified)][0]

s3 = boto3.resource('s3', aws_access_key_id= aws_server_access_key,aws_secret_access_key= aws_server_secret_key)
try:
    s3.Bucket(BUCKET_NAME).download_file(last_added, last_added)
except botocore.exceptions.ClientError as e:
    if e.response['Error']['Code'] == ""404"":
        print(""The object does not exist."")
    else:
        raise
</code></pre>
",1,1544627048,python;amazon-s3;boto3;botocore,False,1632,1,1603200341,https://stackoverflow.com/questions/53745860/download-latest-file-in-s3-bucket-folder
63489753,UnrecognizedClientException after updating to new version of boto libraries,"<p>After updating <code>boto3</code> and <code>botocore</code> this code calling the <code>describe_trails</code> function of CloudTrail client objects now errors.
The library versions that were changed were as follows:</p>
<p><code>boto3</code>: <code>1.9.44</code> -&gt; <code>1.14.45</code></p>
<p><code>botocore</code>: <code>1.12.44</code> -&gt; <code>1.17.45</code></p>
<pre class=""lang-py prettyprint-override""><code>def get_region_list():
    '''
    Get the list of regions covered by CloudTrail from AWS
    '''
    return boto3.session.Session().get_available_regions(
        service_name='cloudtrail',
        partition_name='aws',
        allow_non_regional=False
    )


def generate_cloudtrail_clients(region_list, access_key, secret_key):
    '''
    Generates client objects that interact with CloudTrail in Amazon AWS.
    Each client object corresponds to a different region in region_list
    '''
    for region in region_list:
        yield boto3.client(
            'cloudtrail',
            aws_access_key_id=access_key,
            aws_secret_access_key=secret_key,
            region_name=region
        )
        
clients = generate_cloudtrail_clients(get_region_list(), access_key, secret_key)

for client in clients:
    print(client.describe_trails())
</code></pre>
<p>The error it gives me:</p>
<pre class=""lang-none prettyprint-override""><code>ClientError                               Traceback (most recent call last)
&lt;ipython-input-31-31c1b228c022&gt; in &lt;module&gt;
     30 
     31 for client in clients:
---&gt; 32     print(client.describe_trails())

/opt/anaconda3/lib/python3.7/site-packages/botocore/client.py in _api_call(self, *args, **kwargs)
    314                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)
    315             # The &quot;self&quot; in this scope is referring to the BaseClient.
--&gt; 316             return self._make_api_call(operation_name, kwargs)
    317 
    318         _api_call.__name__ = str(py_operation_name)

/opt/anaconda3/lib/python3.7/site-packages/botocore/client.py in _make_api_call(self, operation_name, api_params)
    633             error_code = parsed_response.get(&quot;Error&quot;, {}).get(&quot;Code&quot;)
    634             error_class = self.exceptions.from_code(error_code)
--&gt; 635             raise error_class(parsed_response, operation_name)
    636         else:
    637             return parsed_response

ClientError: An error occurred (UnrecognizedClientException) when calling the DescribeTrails operation: The security token included in the request is invalid.
</code></pre>
<p>From what I've found looking up about this error is that it often occurs if the access_key, secret_access_key credentials do not have the correct permissions to access the relevant objects. In this case I do have the correct permissions, as I can access these objects on an older version of the boto libraries and it is clear in the attached permission JSON.</p>
<pre><code>{
    &quot;Effect&quot;: &quot;Allow&quot;,
    &quot;Action&quot;: [
        &quot;cloudtrail:DescribeTrails&quot;
    ],
    &quot;Resource&quot;: &quot;*&quot;
}
</code></pre>
<p>Any idea what is going wrong here for this error to occur?</p>
",0,1597848949,python;boto3;botocore,True,474,1,1597935539,https://stackoverflow.com/questions/63489753/unrecognizedclientexception-after-updating-to-new-version-of-boto-libraries
63154781,Generate the AWS HTTP signature from boto3,"<p>I am working with the <a href=""https://docs.aws.amazon.com/transcribe/latest/dg/how-streaming.html"" rel=""noreferrer"">AWS Transcribe streaming</a> service that boto3 does not support yet, so to make HTTP/2 requests, I need to manually setup the <code>authorization</code> header with the &quot;AWS Signature Version 4&quot;</p>
<p>I've found some example <a href=""https://docs.aws.amazon.com/general/latest/gr/sigv4-signed-request-examples.html"" rel=""noreferrer"">implementation</a>, but I was hoping to just call whatever function boto3/botocore have implemented using the same configuration object.</p>
<p>Something like</p>
<pre><code>    session = boto3.Session(...)
    auth = session.generate_signature('POST', '/stream-transcription', ...)
</code></pre>
<p>Any pointers in that direction?</p>
",4,1596029486,python;amazon-web-services;boto3;boto;botocore,True,8275,3,1596202464,https://stackoverflow.com/questions/63154781/generate-the-aws-http-signature-from-boto3
62929695,Use botocore to list content of S3 bucket,"<p>I need to list files of the S3 bucket. But in the project we don't want to create new dependencies and use boto3. Instead, we want to use botocore.</p>
<p>Do you know how to implement this using Python if it is possible?</p>
",0,1594884724,python;python-3.x;amazon-s3;boto3;botocore,True,42,1,1594887590,https://stackoverflow.com/questions/62929695/use-botocore-to-list-content-of-s3-bucket
62192098,Lambda Python and Boto3 - Passing Exception,"<p>I am now getting to the end of resolving a lambda exception issue.  After getting some help debugging the recent exceptions and resolving them, there is one i cannot resolve, so would like to pass it.</p>

<pre><code>import boto3

sts_client = boto3.client('sts')
assumed_role_object=sts_client.assume_role(
    RoleArn=""arn:aws:iam::11111111:role/role"",
    RoleSessionName=""AssumedRoleSession2""
)
credentials=assumed_role_object['Credentials']

def lambda_handler(context,event):
    client                  = boto3.client(
        'iam',
        aws_access_key_id=credentials['AccessKeyId'],
        aws_secret_access_key=credentials['SecretAccessKey'],
        aws_session_token=credentials['SessionToken'],
    )
    sns                     = boto3.client('sns')
    response                = client.list_users()
    userVirtualMfa          = client.list_virtual_mfa_devices()
    mfaNotEnabled           = []
    virtualEnabled          = []
    physicalString          = ''

    # loop through virtual mfa to find users that actually have it
    for virtual in userVirtualMfa['VirtualMFADevices']:
        if 'User' not in virtual or 'UserName' not in virtual['User']:
     # Catch the exception
           raise Exception(""Invalid virtual %s"" % virtual)

        virtualEnabled.append(virtual['User']['UserName'])

    # loop through users to find physical MFA
    for user in response['Users']:
        userMfa  = client.list_mfa_devices(UserName=user['UserName'])

        if len(userMfa['MFADevices']) == 0:
            if user['UserName'] not in virtualEnabled:
                mfaNotEnabled.append(user['UserName']) 


    if len(mfaNotEnabled) &gt; 0:
        physicalString = 'Physical &amp; Virtual MFA is not enabled for the following users: \n\n' + '\n'.join(mfaNotEnabled)
    else:
        physicalString = 'All Users have Physical and Virtual MFA enabled'

    response = sns.publish(
        TopicArn='arn:aws:sns:eu-west-2:222222222:sns',
        Message= physicalString,
        Subject='Enable MFA',
    )

    return mfaNotEnabled
</code></pre>

<p>The exception is reporting correctly, but i would like the function to continue</p>

<pre><code>Response:
{
  ""stackTrace"": [
    [
      ""/var/task/lambda_mfa_function.py"",
      27,
      ""lambda_handler"",
      ""raise Exception(\""Invalid virtual %s\"" % virtual)""
    ]
  ],
  ""errorType"": ""Exception"",
  ""errorMessage"": ""Invalid virtual {u'SerialNumber': 'arn:aws:iam::11111111:mfa/blah-mfa-device', u'EnableDate': datetime.datetime(2016, 05, 16, 01, 6, 35, tzinfo=tzlocal()), u'User': {u'PasswordLastUsed': datetime.datetime(2018, 5, 1, 02, 35, 27, tzinfo=tzlocal()), u'CreateDate': datetime.datetime(2014, 7, 17, 13, 43, 27, tzinfo=tzlocal()), u'UserId': '11111111', u'Arn': 'arn:aws:iam::11111111:blah'}}""
}

Request ID:
""c11a70c9-3a59-486a-9aa9-7286a0cb0b94""

Function Logs:
START RequestId: c11a70c9-3a59-486a-9aa9-7286a0cb0b94 Version: $LATEST
Invalid virtual {u'SerialNumber': 'arn:aws:iam::11111111:mfa/blah-mfa-device', u'EnableDate': datetime.datetime(2016, 11, 16, 22, 6, 35, tzinfo=tzlocal()), u'User': {u'PasswordLastUsed': datetime.datetime(2020, 5, 1, 14, 35, 27, tzinfo=tzlocal()), u'CreateDate': datetime.datetime(2015, 7, 17, 13, 43, 27, tzinfo=tzlocal()), u'UserId': '265742304136', u'Arn': 'arn:aws:iam::11111111:blah'}}: Exception
Traceback (most recent call last):
  File ""/var/task/lambda_mfa_function.py"", line 27, in lambda_handler
    raise Exception(""Invalid virtual %s"" % virtual)
Exception: Invalid virtual {u'SerialNumber': 'arn:aws:iam::11111111:mfa/blah-mfa-device', u'EnableDate': datetime.datetime(2014, 12, 12, 02, 6, 35, tzinfo=tzlocal()), u'User': {u'PasswordLastUsed': datetime.datetime(2016, 7, 2, 1, 15, 27, tzinfo=tzlocal()), u'CreateDate': datetime.datetime(2014, 2, 7, 3, 33, 17, tzinfo=tzlocal()), u'UserId': '11111111', u'Arn': 'arn:aws:iam::11111111:blah'}}
</code></pre>

<p>Its throwing a key error on the root MFA i expect due to the nature in which the root name is held</p>

<pre><code>  File ""/var/task/lambda_mfa_function.py"", line 26, in lambda_handler
    virtualEnabled.append(virtual['User']['UserName'])
KeyError: 'UserName'
</code></pre>

<p>Addin an </p>

<pre><code>except:
    pass
</code></pre>

<p>Is causing the lambda to fail.
Can someone kindly point me into the right direction?</p>

<p>thanks
Nick</p>
",0,1591265414,python;amazon-web-services;aws-lambda;boto3;botocore,True,479,1,1591267827,https://stackoverflow.com/questions/62192098/lambda-python-and-boto3-passing-exception
51074577,python: I get InvalidToken error while using bucket.objects.filter(Prefi=myPrefixString) in boto3 1.7.21,"<p>I am trying to access objects from my s3 bucket. Iam using boto3 1.7.21. First I created a session using </p>

<pre><code>session = boto3.session.Session(aws_access_key_id=aws_access_key_id,
                  aws_secret_access_key=aws_secret_access_key,
                  region_name=region_name)
</code></pre>

<p>then trying to get objects by a filter using</p>

<pre><code>session.resources(""s3"").Bucket(myBucketName)
.objects.filter(Prefix=myPrefix)
</code></pre>

<p>I get the below error</p>

<pre><code>ClientError: An error occurred (InvalidToken) when calling the ListObjects operation: The provided token is malformed or otherwise invalid.
</code></pre>

<p>My dev versions:</p>

<ul>
<li>python - 2.7, </li>
<li>boto3 - 1.7.21, </li>
<li>botocore - 1.10.35</li>
</ul>
",0,1530158849,python;amazon-s3;boto;boto3;botocore,False,1547,2,1588162185,https://stackoverflow.com/questions/51074577/python-i-get-invalidtoken-error-while-using-bucket-objects-filterprefi-myprefi
61267718,Can I use the python s3fs library over aiobotocore?,"<p><a href=""https://pypi.org/project/s3fs/"" rel=""nofollow noreferrer"">s3fs</a> is a convenient Python filesystem-like interface for S3, built on top of <a href=""https://botocore.amazonaws.com/v1/documentation/api/latest/index.html"" rel=""nofollow noreferrer"">botocore</a>.  To access S3 using asyncio, <a href=""https://aiobotocore.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">aiobotocore</a> is an <a href=""https://stackoverflow.com/a/50394548/974555"">alternative</a> to <a href=""https://botocore.amazonaws.com/v1/documentation/api/latest/index.html"" rel=""nofollow noreferrer"">botocore</a>.  Is it possible to use s3fs with asyncio/<a href=""https://aiobotocore.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">aiobotocore</a> rather than vanilla <a href=""https://botocore.amazonaws.com/v1/documentation/api/latest/index.html"" rel=""nofollow noreferrer"">botocore</a>?</p>

<p>My use case is that I need to collect three items: one is coming from a tape archive, one from S3, and one is calculated with CPU.  They all take 10-30 minutes but concurrency should be possible here.</p>
",2,1587113940,python;python-asyncio;botocore;python-s3fs,False,543,0,1587115091,https://stackoverflow.com/questions/61267718/can-i-use-the-python-s3fs-library-over-aiobotocore
52436835,How to set tags for AWS EC2 instance in boto3,"<p>I am new to <code>Boto3</code>, and wanted to create a VPC, subnets, and some ec2 instances. The basic architecture is having a VPC, 2 subnets within 2 different availability zones (us-east-1a and b), and applying a security group which allows <code>SSH</code> and <code>ping</code>. </p>

<p>My problem is how to specify <strong>additional options</strong> for each resources. The Python SDK (unlike how <code>Javadoc</code> works) doesn't show the required arguments and example options, so I'm confused. </p>

<p>How can I specify <code>tags</code> for resources? (e.g. ec2 instance). I need to set <code>name</code>, <code>owner</code>, etc.</p>

<pre><code>instances2 = ec2.create_instances(ImageId='ami-095575c1a372d21db', InstanceType='t2.micro', MaxCount=1, MinCount=1, NetworkInterfaces=[{'SubnetId': subnet2.id, 'DeviceIndex': 0, 'AssociatePublicIpAddress': True, 'Groups': [sec_group.group_id]}])
instances2[0].wait_until_running()
print(instances1[0].id)
</code></pre>
",3,1537505722,python;amazon-web-services;aws-sdk;boto3;botocore,True,5289,2,1586833376,https://stackoverflow.com/questions/52436835/how-to-set-tags-for-aws-ec2-instance-in-boto3
60664637,SSLError using boto,"<p>We are using a proxy + profile when using the <code>aws s3</code> commands to browse our buckets in CLI.</p>

<pre><code>export HTTPS_PROXY=https://ourproxyhost.com:3128
aws s3 ls s3://our_bucket/.../ --profile dev
</code></pre>

<p>And we can work with our buckets and objects fine.</p>

<p>Because I need to write Python code for this, I translated this using boto3:</p>

<pre><code># python 2.7.12
import boto3                        # v1.5.18
from botocore.config import Config  # v1.8.32

s3 = boto3.Session(profile_name='dev').resource('s3', config=Config(proxies={'https': 'ourproxyhost.com:3128'})).meta.client
obj = s3.get_object(Bucket='our_bucket', Key='dir1/dir2/.../file')
</code></pre>

<p>What I get is this:</p>

<pre><code>botocore.vendored.requests.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590)
</code></pre>

<p>Why is this working in CLI, but not in Python?</p>
",9,1584071398,python;python-2.7;amazon-s3;boto3;botocore,True,15749,1,1584175219,https://stackoverflow.com/questions/60664637/sslerror-using-boto
60437201,Python Boto 3 version incompatible,"<p>Need steps for downloading the correct versions botocore and s3transfer.
I did pip install awscli and got the following error because of which no module found shows up while running the .py script.
ERROR: boto3 1.10.50 has requirement botocore&lt;1.14.0,>=1.13.50, but you'll have botocore 1.15.8 which is incompatible. ERROR: boto3 1.10.50 has requirement s3transfer&lt;0.3.0,>=0.2.0, but you'll have s3transfer 0.3.3 which is incompatible</p>
",0,1582819264,python;amazon-s3;botocore,False,3754,0,1582884682,https://stackoverflow.com/questions/60437201/python-boto-3-version-incompatible
51258497,"flask installed, but ModuleNotFoundError: No module named &#39;Flask&#39;","<p>i have flask installed in my Mac, </p>

<pre><code>pip list | grep Flask
Flask       1.0.2   
</code></pre>

<p>but when i execute a python file, it shows error like </p>

<pre><code>from Flask import Flask, render_template
ModuleNotFoundError: No module named 'Flask'
</code></pre>

<p>the directories used for python and flask are</p>

<pre><code>which python
/usr/bin/python

which flask
/Library/Frameworks/Python.framework/Versions/3.5/bin/flask
</code></pre>

<p>so I thought of running virtualenv, but l cant even install it with error messages </p>

<pre><code>s3transfer 0.1.8 requires botocore&lt;2.0.0,&gt;=1.3.0, which is not installed.
</code></pre>

<p>then l try </p>

<pre><code>sudo pip install ""botocore&lt;2.0.0,&gt;=1.3.0""
Cannot uninstall 'six'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.
</code></pre>

<p>Is this happening because of conflicting python versions? can anyone help please?</p>
",2,1531204085,python;flask;virtualenv;six;botocore,True,14601,4,1582857383,https://stackoverflow.com/questions/51258497/flask-installed-but-modulenotfounderror-no-module-named-flask
44828156,How to catch exceptions from botocore,"<p>I am getting the below error while using <code>boto3</code> with <code>Amazon SNS</code>. I want to catch <code>InvalidParameterException</code> only, how can I do the same? </p>

<pre><code>Traceback (most recent call last):
  File ""D:\Logger\Notification.py"", line 279, in &lt;module&gt;
    Push.subscribe(token1, 'android')
  File ""D:\Logger\Notification.py"", line 119, in subscribe
    'Enabled': b'True'
  File ""C:\Python27\lib\site-packages\botocore\client.py"", line 310, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File ""C:\Python27\lib\site-packages\botocore\client.py"", line 599, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.errorfactory.InvalidParameterException: An error occurred (InvalidParameter) when calling the CreatePlatformEndpoint operation: Invalid parameter: Token Reason: Endpoint arn:aws:sns:us-west-2:252285631092:endpoint/GCM/Test/06c4448e-545b-312a-978f-98af5d5829e4 already exists with the same Token, but different attributes.
</code></pre>

<p>If I try to catch <code>InvalidParameterException</code>, it shows </p>

<pre><code>NameError: global name 'InvalidParameterException' is not defined
</code></pre>

<p>I have imported <code>botocore</code>. Now if I try to catch <code>botorcore.errorfactory.InvalidParameterException</code> it shows.</p>

<pre><code>AttributeError: 'module' object has no attribute 'InvalidParameterException'
</code></pre>
",4,1498746978,python;python-2.7;exception;botocore,True,3943,1,1579439060,https://stackoverflow.com/questions/44828156/how-to-catch-exceptions-from-botocore
57793635,Python Boto3 &#39;StreamingBody&#39; object has no attribute &#39;iter_lines&#39;,"<p>I am using Boto3 to read the results of my Athena query in a python script.</p>

<p>I have the following code that works fine in AWS Lambda.</p>

<pre><code>def get_athena_results(s3_bucket, s3_output_path, execution_id):
    s3client = boto3.client('s3')
    key = s3_output_path + '/' + execution_id + '.csv'
    obj = s3client.get_object(Bucket=s3_bucket, Key=key)
    results_iterator = obj['Body'].iter_lines()
    results = [r for r in results_iterator]
    return results
</code></pre>

<p>When I run the same function in AWS Glue <strong>Python Shell</strong> (Not a Spark job), I get the error:</p>

<pre><code>Unexpected error: &lt;class 'AttributeError'&gt;
'StreamingBody' object has no attribute 'iter_lines'
</code></pre>

<p>This doesn't make sense to me as the <code>botocore.response.StreamingBody</code> class has an <code>iter_lines</code> method, and it works fine in AWS Lambda.</p>

<p><a href=""https://botocore.amazonaws.com/v1/documentation/api/latest/reference/response.html"" rel=""nofollow noreferrer"">https://botocore.amazonaws.com/v1/documentation/api/latest/reference/response.html</a></p>

<p>Any idea why this is happening in AWS Glue Python Shell?</p>

<p>Thanks</p>
",3,1567619921,python;aws-lambda;boto3;aws-glue;botocore,True,4659,1,1578950425,https://stackoverflow.com/questions/57793635/python-boto3-streamingbody-object-has-no-attribute-iter-lines
52675027,Why do I sometimes get Key Error using SQS client,"<p>I am using boto3 SQS client to receive messages from AWS SQS FIFO queue.</p>
<pre><code>def consume_msgs():
    sqs = None
    try:
        sqs = boto3.client('sqs',
                       region_name=S3_BUCKET_REGION,
                       aws_access_key_id=AWS_ACCESS_KEY_ID,
                       aws_secret_access_key=AWS_SECRET_ACCESS_KEY)
    except Exception:
        logger.warning('SQS client error {}'.format(sys.exc_info()[0]))
        logger.error(traceback.format_exc())

  ### more code to process message
</code></pre>
<p>The application is set up as service on EC2 using <code>upstart</code>. It works fine most of time. But sometimes when I restart the service after code change, the app would exit with the following error</p>
<pre><code>2018-10-06 01:29:38,654 WARNING SQS client error &lt;class 'KeyError'&gt;
2018-10-06 01:29:38,658 WARNING SQS client error &lt;class 'KeyError'&gt;
2018-10-06 01:29:38,663 ERROR Traceback (most recent call last):
  File &quot;/home/ec2-user/aae_client/app/run.py&quot;, line 194, in consume_msgs
    aws_secret_access_key=AWS_SECRET_ACCESS_KEY)
  File &quot;/home/ec2-user/aae_client/env/lib64/python3.6/dist-packages/boto3/__init__.py&quot;, line 83, in client
    return _get_default_session().client(*args, **kwargs)
  File &quot;/home/ec2-user/aae_client/env/lib64/python3.6/dist-packages/boto3/session.py&quot;, line 263, in client
    aws_session_token=aws_session_token, config=config)
  File &quot;/home/ec2-user/aae_client/env/lib64/python3.6/dist-packages/botocore/session.py&quot;, line 851, in create_client
    endpoint_resolver = self.get_component('endpoint_resolver')
  File &quot;/home/ec2-user/aae_client/env/lib64/python3.6/dist-packages/botocore/session.py&quot;, line 726, in get_component
    return self._components.get_component(name)
  File &quot;/home/ec2-user/aae_client/env/lib64/python3.6/dist-packages/botocore/session.py&quot;, line 926, in get_component
    del self._deferred[name]
KeyError: 'endpoint_resolver'
</code></pre>
<p>Restarting the service usually fixes it. It doesn't happen every time I restart the service. What is confusing is the <code>KeyError</code> warning leading the actual error traceback. What exactly does this <code>KeyError</code> refer to? It can't be the <code>AWS_SECRET_ACCESS_KEY</code> since this key is never changed and it works just fine most of the time. The issue happens quite randomly and comes and goes. Therefore it is hard to debug. And I don't understand how this error escaped the <code>try..except</code> block</p>
<h2>EDIT</h2>
<p>Based on comments, this seem to be related to multithreading. <code>consume_msg</code> is indeed run by multiple threads
def process_msgs():</p>
<pre><code>for i in range(NUM_WORKERS):
    t = threading.Thread(target=consume_msgs, name='worker-%s' % i)
    t.setDaemon(True)
    t.start()
while True:
    time.sleep(MAIN_PROCESS_SLEEP_INTERVAL)
</code></pre>
",10,1538790239,python;amazon-web-services;boto3;amazon-sqs;botocore,True,8727,3,1578429914,https://stackoverflow.com/questions/52675027/why-do-i-sometimes-get-key-error-using-sqs-client
59402057,Null values passed to cloudformation module - ansible,"<p>Ansible cloudformation module uses these environment variables of  shell:</p>

<pre><code>$ export AWS_PROFILE=djangoapp
$ export AWS_DEFAULT_REGION=ca-central-1
$ aws configure list
      Name                    Value             Type    Location
      ----                    -----             ----    --------
   profile                djangoapp           manual    --profile
access_key     ****************WKWG shared-credentials-file    
secret_key     ****************/I4Z shared-credentials-file    
    region             ca-central-1              env    AWS_DEFAULT_REGION
$ cat ~/.aws/config
[djangoapp]
region = ca-central-1
$ cat ~/.aws/credentials 
[djangoapp]
aws_access_key_id = ****************KWG
aws_secret_access_key = ********************/I4Z
$
$ aws ec2 describe-vpcs
{
    ""Vpcs"": [
        {
            ""CidrBlock"": ""172.31.0.0/16"",
            ""DhcpOptionsId"": ""dopt-aaaaa8"",
            ""State"": ""available"",
            ""VpcId"": ""vpc-ccccccc"",
            ""OwnerId"": ""444444444"",
            ""InstanceTenancy"": ""default"",
            ""CidrBlockAssociationSet"": [
                {
                    ""AssociationId"": ""vpc-cidr-assoc-fffff"",
                    ""CidrBlock"": ""172.31.0.0/16"",
                    ""CidrBlockState"": {
                        ""State"": ""associated""
                    }
                }
            ],
            ""IsDefault"": true
        }
    ]
}
</code></pre>

<hr>

<h1>./site.yml</h1>

<pre><code>---
- name: Todobackend deployment playbook
  hosts: localhost
  connection: local
  gather_facts: no
  vars_files:
    - secrets.yml
  environment:
    AWS_DEFAULT_REGION: ""{{ lookup('env', 'AWS_DEFAULT_VERSION') | default('ca-central-1', true) }}""
  tasks:
    - include: tasks/create_stack.yml
    - include: tasks/deploy_app.yml
</code></pre>

<hr>

<h1>./tasks/create_stack.yml</h1>

<pre><code>---
- name: task to create/update stack
  cloudformation:
    stack_name: todobackend
    state: present
    template: templates/stack.yml
    template_format: yaml
    template_parameters:
      VpcId: ""{{ vpc_id }}""
      SubnetId: ""{{ subnet_id }}""
      KeyPair: ""{{ ec2_keypair }}""
      InstanceCount: ""{{ instance_count | default(1) }}""
      DbSubnets: ""{{ db_subnets | join(',') }}""
      DbAvailabilityZone: ""{{ db_availability_zone }}""
      DbUsername: ""{{ db_username }}""
      DbPassword: ""{{ db_password }}""
    tags:
      Environment: test
  register: cf_stack

- name: Debug output
  debug: msg=""{{ cf_stack }}""
  when: debug is defined
</code></pre>

<hr>

<h1>./templates/stack.yml</h1>

<pre><code>AWSTemplateFormatVersion: ""2010-09-09""
Description: ""Todobackend Stack""

# Stack Parameters
Parameters:
  VpcId:
    Type: ""AWS::EC2::VPC::Id""
    Description: ""The target VPC Id""
  SubnetId:
    Type: ""AWS::EC2::Subnet::Id""
    Description: ""The target Subnet Id in availability zone a""
  KeyPair:
    Type: ""String""
    Description: ""The key pair that is allowed SSH access""
  InstanceCount:
    Type: ""Number""
    Description: ""The desired number of application instances""
  DbSubnets:
    Type: ""List&lt;AWS::EC2::Subnet::Id&gt;""
    Description: ""The target DB Subnet Group subnet Ids""
  DbAvailabilityZone:
    Type: ""String""
    Description: ""The target availability zone for the database instance""
  DbUsername:
    Type: ""String""
    Description: ""The RDS database username""
  DbPassword:
    Type: ""String""
    Description: ""The RDS database password""
    NoEcho: ""true""

# Stack Resources
Resources:
  # Configure auto scaing group
  AutoScalingGroup:
    Type: ""AWS::AutoScaling::AutoScalingGroup""
    Properties:
      VPCZoneIdentifier: [ { ""Ref"": ""SubnetId"" } ]
      LaunchConfigurationName: { ""Ref"": ""AutoScalingLaunchConfiguration"" }
      MinSize: 0
      MaxSize: 2
      DesiredCapacity: { ""Ref"": ""InstanceCount"" }
      Tags:
        - Key: ""Name""
          Value: { ""Fn::Join"": ["""", [ { ""Ref"": ""AWS::StackName"" }, "" -instance"" ] ] }
          PropagateAtLaunch: ""true""

  AutoScalingLaunchConfiguration:
    Type: ""AWS::AutoScaling::LaunchConfiguration""
    Properties:
      ImageId: ami-05958d7635caa4d04
      InstanceType: t2.micro
      keyName: { ""Ref"": ""KeyPair"" }
      IamInstanceProfile: { ""Ref"": ""EC2InstanceProfile"" }
      SecurityGroups:
        - { ""Ref"": ""EC2InstanceSecurityGroup"" }
      UserData: {
        ""Fn::Base64"": { ""Fn::Join"": ["""", [
          ""#!/bin/bash\n"",
          ""echo ECS_CLUSTER="", { ""Ref"": ""EcsCluster""}, "" &gt;&gt; /etc/ecs/ecs.config\n""
        ] ] }
      }

  EC2InstanceSecurityGroup:
    Type: ""AWS::EC2::SecurityGroup""
    Properties:
      GroupDescription: ""todobackend-sg""
      VpcId: { ""Ref"": ""VpcId"" }
      SecurityGroupIngress:
        - IpProtocol: ""tcp""
          FromPort: ""8080""
          ToPort: ""8080""
          SourceSecurityGroupId: { ""Ref"": ""ElbSecurityGroup"" }
        - IpProtocol: ""tcp""
          FromPort: ""22""
          ToPort: ""22""
          CidrIp: ""0.0.0.0/0""
      Tags:
        - Key: ""Name""
          Value: { ""Fn::Join"": ["""", [ { ""Ref"": ""AWS::StackName"" }, ""-instance-sg"" ] ] }

  EC2InstanceProfile:
    Type: ""AWS::IAM::InstanceProfile""
    Properties:
      Path: ""/""
      Roles: [ { ""Ref"": ""EC2InstanceRole"" } ]

  EC2InstanceRole:
    Type: ""AWS::IAM::Role""
    Properties:
      AssumeRolePolicyDocument: {
        ""Version"": ""2012-10-17"",
        ""Statement"": [
          {
            ""Effect"": ""Allow"",
            ""Principal"": { ""Service"": [ ""ec2.amazonaws.com"" ] },
            ""Action"": [ ""sts:AssumeRole""]
          }
        ]
      }
      Path: ""/""
      ManagePolicyArns:
        - ""arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceForEc2Role""

  # Configure RDS
  DbInstance:
    Type: ""AWS::RDS::DBInstance""
    Properties:
      DBSubnetGroupName: { ""Ref"": ""DbSubnetGroup"" }
      MultiAZ: ""false""
      AvailabilityZone: { ""Ref"": ""DBAvailabilityZone"" }
      AllocatedStorage: 8
      StorageType: ""gp2""
      DBInstanceClass: ""db.t2.micro""
      DBName: ""todobackend""
      Engine: ""MySQL""
      EngineVersion: ""5.6""
      MasterUserName: { ""Ref"": ""DbUserName"" }
      MasterUserPassword: { ""Ref"": ""DbPassword"" }
      VPCSecurityGroups:
        - { ""Ref"": ""DbSecurityGroup"" }
      Tags:
        - Key: ""Name""
          Value: { ""Fn::Join"": ["""", [ { ""Ref"": ""AWS::Stackname"" }, ""-db"" ] ] }

  DbSecurityGroup:
    Type: ""AWS::EC2::SecurityGroup""
    Properties:
      GroupDescription: ""Todobackend DB Security Group""
      VpcId: { ""Ref"": ""VpcId"" }
      SecurityGroupIngress:
        - IpProtocol: ""tcp""
          FromPort: ""3306""
          ToPort: ""3306""
          SourceSecurityGroupId: { ""Ref"": ""EC2InstanceSecurityGroup"" }

  DbSubnetGroup:
    Type: ""AWS::RDS::DBSubnetGroup""
    Properties:
      DBSubnetGroupDescription: ""Todobackend DB Subnet Group""
      SubnetIds: { ""Ref"": ""DbSubnets"" }
      Tags:
        - Key: ""Name""
          Value: { ""Fn::Join"": ["""", [ { ""Ref"": ""AWS::StackName"" }, ""-db-subnet-group"" ] ] }


  # Configure ELB
  ElasticLoadBalancer:
    Type: ""AWS::ElasticLoadBalancing::LoadBalancer""
    Properties:
      CrossZone: ""false""
      SecurityGroups: [ { ""Ref"": ""ElbSecurityGroup"" } ]
      Listeners:
        - LoadBalancerPort: ""80""
          InstancePort: ""8000""
          Protocol: ""http""
      HealthCheck:
        Target: ""HTTP:8000/todos""
        HealthyThreshold: ""2""
        UnhealthyThreshold: ""10""
        Interval: ""30""
        Timeout: ""5""
      Subnets: [ { ""Ref"": ""SubnetId"" } ]
      Tags:
        - Key: ""Name""
          Value: { ""Fn::Join"": ["""", [ { ""Ref"": ""AWS::StackName"" }, ""-elb"" ] ] }

  ElbSecurityGroup:
    Type: ""AWS::EC2::SecurityGroup""
    Properties:
      GroupDescription: ""Todobackend ELB Security Group""
      VpcId: { ""Ref"": ""VpcId"" }
      SecurityGroupIngress:
        - IpProtocol: ""tcp""
          FromPort: ""80""
          ToPort: ""80""
          CidrIp: ""0.0.0.0/0""
      Tags:
        - Key: ""Name""
          Value: { ""Fn::Join"": ["""", [ { ""Ref"": ""AWS::StackName"" }, ""-elb-sg"" ] ] }

  # Configure ECS
  EcsCluster:
    Type: ""AWS::ECS::EcsCluster""

  TodobackendTaskDefinition:
    Type: ""AWS::ECS::TaskDefinition""
    Properties:
      ContainerDefinitions:

        - Name: todobackend
          Image: shamdockerhub/todobackend
          Memory: 450
          Environment:
            - Name: DJANGO_SETTINGS_MODULE
              Value: todobackend.settings.release
            - Name: MYSQL_HOST
              Value: { ""Fn::GetAtt"": [""DbInstance"", ""Endpoint.Address""] }
            - Name: MYSQL_USER
              Value: { ""Ref"": ""DbUsername"" }
            - Name: MYSQL_PASSWORD
              Value: { ""Ref"": ""DbPassword"" }
          MountPoints:
            - ContainerPath: /var/www/todobackend
              SourceVolume: webroot
          Command:
            - uwsgi
            - ""--socket /var/www/todobackend/todobackend.sock""
            - ""--chmod-socket=666""
            - ""--module todobackend.wsgi""
            - ""--master""
            - ""--die-on-term""

        - Name: nginx
          Image: shamdockerhub/todobackend-nginx
          Memory: 300
          PortMappings:
            - ContainerPort: ""8000""
              HostPort: ""8000""
          MountPoints:
            - ContainerPath: /var/www/todobackend
              SourceVolume: webroot

      Volumes:
        - Name: webroot
          Host:
            SourcePath: /ecs/webroot

  TodobackendService:
    Type: ""AWS::ECS::Service""
    Properties:
      TaskDefinition: { ""Ref"": ""TodobackendTaskDefinition"" }
      Cluster: { ""Ref"": ""EcsCluster"" }
      LoadBalancers:
        - ContainerName: ""nginx""
          ContainerPort: ""8000""
          LoadBalancerName: { ""Ref"": ""ElasticLoadBalancer"" }
      Role: { ""Ref"": ""EcsServiceRole"" }
      DesiredCount: 0

  EcsServiceRole:
    Type: ""AWS::IAM::Role""
    Properties:
      AssumeRolePolicyDocument: {
        ""Version"": ""2012-10-17"",
        ""Statement"": [
          {
            ""Effect"": ""Allow"",
            ""Principal"": {
              ""Service"": { ""ecs.amazonaws.com"" }
            },
            ""Action"": [ ""sts:AssumeRole"" ]
          }
        ]
      }
      Path: ""/""
      ManagePolicyArns:
        - arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceRole

  TodobackendAdhocTaskDefinition: # Application management task
    Type: ""AWS::ECS::TaskDefinition""
    Properties:
      ContainerDefinitions:
        - Name: todobackend
          Image: shamdockerhub/todobackend
          Memory: 245
          Environment:
            - Name: DJANGO_SETTINGS_MODULE
              Value: todobackend.settings.release
            - Name: MYSQL_HOST
              Value: { ""Fn::GetAtt"": [""DbInstance"", ""Endpoint.Address""] }
            - Name: MYSQL_USER
              Value: { ""Ref"": ""DbUsername"" }
            - Name: MYSQL_PASSWORD
              Value: { ""Ref"": ""DbPassword"" }
          MountPoints:
            - ContainerPath: /var/www/todobackend
              SourcePath: webroot

      Volumes:
        - Name: webroot
          Host:
            SourcePath: /ecs/webroot


  # Stack outputs
  Outputs:
    ElbDomainName:
      Description: ""Public DNS name of Elastic Load Balancer""
      Value: { ""Fn:GetAtt"": [ ""ElasticLoadBalancer"", ""DNSName"" ] }
    EcsCluster:
      Description: ""Amazon resource name (ARN) of Todobackend Ecs Cluster""
      Value: { ""Ref"": ""EcsCluster"" }
    TodobackendTaskDefinition:
      Description: ""Amazon resource name (ARN) of Todobackend Task definition""
      Value: { ""Ref"": ""TodobackendTaskDefinition""}
    TodobackendAdhocTaskDefinition:
      Description: ""Amazon resource name(ARN) of Todobackend Adhoc Task Definition""
      Value: { ""Ref"": ""TodobackendAdhocTaskDefinition"" }
    TodobackendService:
      Description: ""Amazon resource name (ARN) of Todobackend service""
      Value: { ""Ref"": ""TodobackendService"" }
</code></pre>

<hr>

<p>Below is the <code>CreateStack</code> operation error:</p>

<pre><code>$ ansible-playbook  site.yml --ask-vault-pass -e debug=true -vvv
ansible-playbook 2.5.1
  config file = /etc/ansible/ansible.cfg
  configured module search path = [u'/home/user1/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules']
  ansible python module location = /usr/lib/python2.7/dist-packages/ansible
  executable location = /usr/bin/ansible-playbook
  python version = 2.7.15+ (default, Oct  7 2019, 17:39:04) [GCC 7.4.0]
Using /etc/ansible/ansible.cfg as config file
Vault password: 
Parsed /etc/ansible/hosts inventory source with ini plugin
 [WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match
'all'

Read vars_file 'secrets.yml'
statically imported: /home/user1/git/ContDelivery_course/DjangoApp/todobackend-deploy/tasks/create_stack.yml
Read vars_file 'secrets.yml'
 [WARNING]: file /home/user1/git/ContDelivery_course/DjangoApp/todobackend-deploy/tasks/deploy_app.yml is
empty and had no tasks to include


PLAYBOOK: site.yml *****************************************************************************************************
1 plays in site.yml
Read vars_file 'secrets.yml'
Read vars_file 'secrets.yml'

PLAY [Todobackend deployment playbook] *********************************************************************************
META: ran handlers
Read vars_file 'secrets.yml'

TASK [task to create/update stack] *************************************************************************************
task path: /home/user1/git/ContDelivery_course/DjangoApp/todobackend-deploy/tasks/create_stack.yml:2
Using module file /usr/lib/python2.7/dist-packages/ansible/modules/cloud/amazon/cloudformation.py
&lt;127.0.0.1&gt; ESTABLISH LOCAL CONNECTION FOR USER: mohet01-ubuntu
&lt;127.0.0.1&gt; EXEC /bin/sh -c 'echo ~ &amp;&amp; sleep 0'
&lt;127.0.0.1&gt; EXEC /bin/sh -c '( umask 77 &amp;&amp; mkdir -p ""` echo /home/user1/.ansible/tmp/ansible-tmp-1576716480.56-111176828564019 `"" &amp;&amp; echo ansible-tmp-1576716480.56-111176828564019=""` echo /home/user1/.ansible/tmp/ansible-tmp-1576716480.56-111176828564019 `"" ) &amp;&amp; sleep 0'
&lt;127.0.0.1&gt; PUT /home/user1/.ansible/tmp/ansible-local-7506yaa0Y9/tmpl7pqXl TO /home/user1/.ansible/tmp/ansible-tmp-1576716480.56-111176828564019/cloudformation.py
&lt;127.0.0.1&gt; EXEC /bin/sh -c 'chmod u+x /home/user1/.ansible/tmp/ansible-tmp-1576716480.56-111176828564019/ /home/user1/.ansible/tmp/ansible-tmp-1576716480.56-111176828564019/cloudformation.py &amp;&amp; sleep 0'
&lt;127.0.0.1&gt; EXEC /bin/sh -c 'AWS_DEFAULT_REGION=ca-central-1 /usr/bin/python2 /home/user1/.ansible/tmp/ansible-tmp-1576716480.56-111176828564019/cloudformation.py &amp;&amp; sleep 0'
&lt;127.0.0.1&gt; EXEC /bin/sh -c 'rm -f -r /home/user1/.ansible/tmp/ansible-tmp-1576716480.56-111176828564019/ &gt; /dev/null 2&gt;&amp;1 &amp;&amp; sleep 0'
The full traceback is:
Traceback (most recent call last):
  File ""/tmp/ansible_bfmm8l/ansible_module_cloudformation.py"", line 314, in create_stack
    cfn.create_stack(**stack_params)
  File ""/tmp/ansible_bfmm8l/ansible_modlib.zip/ansible/module_utils/cloud.py"", line 150, in retry_func
    raise e
ClientError: An error occurred (ValidationError) when calling the CreateStack operation: [/Resources/EcsServiceRole/Type/AssumeRolePolicyDocument/Statement/0/Principal/Service/ecs.amazonaws.com] 'null' values are not allowed in templates

fatal: [localhost]: FAILED! =&gt; {
    ""changed"": false, 
    ""invocation"": {
        ""module_args"": {
            ""aws_access_key"": null, 
            ""aws_secret_key"": null, 
            ""changeset_name"": null, 
            ""create_changeset"": false, 
            ""disable_rollback"": false, 
            ""ec2_url"": null, 
            ""notification_arns"": null, 
            ""profile"": null, 
            ""region"": null, 
            ""role_arn"": null, 
            ""security_token"": null, 
            ""stack_name"": ""todobackend"", 
            ""stack_policy"": null, 
            ""state"": ""present"", 
            ""tags"": {
                ""Environment"": ""test""
            }, 
            ""template"": ""templates/stack.yml"", 
            ""template_body"": null, 
            ""template_format"": ""yaml"", 
            ""template_parameters"": {
                ""DbAvailabilityZone"": ""ca-central-1a"", 
                ""DbPassword"": ""ccccc"", 
                ""DbSubnets"": ""subnet-22222,subnet-33333"", 
                ""DbUsername"": ""todobackend"", 
                ""InstanceCount"": ""1"", 
                ""KeyPair"": ""admin"", 
                ""SubnetId"": ""subnet-33333"", 
                ""VpcId"": ""vpc-111111""
            }, 
            ""template_url"": null, 
            ""termination_protection"": null, 
            ""validate_certs"": true
        }
    }, 
    ""msg"": ""Failed to create stack todobackend: An error occurred (ValidationError) when calling the CreateStack operation: [/Resources/EcsServiceRole/Type/AssumeRolePolicyDocument/Statement/0/Principal/Service/ecs.amazonaws.com] 'null' values are not allowed in templates An error occurred (ValidationError) when calling the CreateStack operation: [/Resources/EcsServiceRole/Type/AssumeRolePolicyDocument/Statement/0/Principal/Service/ecs.amazonaws.com] 'null' values are not allowed in templates - &lt;class 'botocore.exceptions.ClientError'&gt;.""
}
    to retry, use: --limit @/home/user1/git/ContDelivery_course/DjangoApp/todobackend-deploy/site.retry

PLAY RECAP *************************************************************************************************************
localhost                  : ok=0    changed=0    unreachable=0    failed=1   

$
</code></pre>

<hr>

<p>Why <code>module_args</code> dictionary have null values?  How to resolve this error?</p>

<p>Ansible 2.5.1 is using Python 2.7</p>
",1,1576717625,python;ansible;aws-cloudformation;botocore;ansible-api,False,1298,0,1576801622,https://stackoverflow.com/questions/59402057/null-values-passed-to-cloudformation-module-ansible
57613895,ConnectionRefusedError: [Errno 61] Connection refused &amp; Could not connect to the endpoint URL: http://127.0.0.1:3001,"<p>I am trying to run integration tests using boto3 and aws. It seems the tests are unable to connect to the localhost endpoint so the tests are failing. I'm quite sure I've configured my aws environment variables correctly. I suspect that maybe there is no local server running but I'm still unsure how to fix the problem. Any help would be greatly appreciated. </p>

<p>I'm running on Mac Mojave, I've tried changing the ports that the server is attempting to connect to localhost:3000. I've also tried connecting to a server that I'm running using AWS Step Functions (<a href=""https://docs.aws.amazon.com/step-functions/latest/dg/sfn-local-config-options.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/step-functions/latest/dg/sfn-local-config-options.html</a>) but with no luck. </p>

<p>Error messages from Log:</p>

<pre><code>ConnectionRefusedError: [Errno 61] Connection refused
</code></pre>

<p>and </p>

<pre><code>raise EndpointConnectionError(endpoint_url=request.url, error=e)
botocore.exceptions.EndpointConnectionError: Could not connect to the endpoint URL: ""http://127.0.0.1:3001/2015-03-31/functions/AthenaDatabaseProvider/invocations""
</code></pre>

<p>Complete Log: </p>

<pre><code>Traceback (most recent call last):
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/urllib3/connection.py"", line 160, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/urllib3/util/connection.py"", line 80, in create_connection
    raise err
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/urllib3/util/connection.py"", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/botocore/httpsession.py"", line 262, in send
    chunked=self._chunked(request.headers),
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 641, in urlopen
    _stacktrace=sys.exc_info()[2])
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/urllib3/util/retry.py"", line 344, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/urllib3/packages/six.py"", line 686, in reraise
    raise value
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 603, in urlopen
    chunked=chunked)
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 355, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File ""/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py"", line 1244, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/botocore/awsrequest.py"", line 125, in _send_request
    method, url, body, headers, *args, **kwargs)
  File ""/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py"", line 1290, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File ""/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py"", line 1239, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/botocore/awsrequest.py"", line 152, in _send_output
    self.send(msg)
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/botocore/awsrequest.py"", line 236, in send
    return super(AWSConnection, self).send(str)
  File ""/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py"", line 966, in send
    self.connect()
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/urllib3/connection.py"", line 183, in connect
    conn = self._new_conn()
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/urllib3/connection.py"", line 169, in _new_conn
    self, ""Failed to establish a new connection: %s"" % e)
urllib3.exceptions.NewConnectionError: &lt;botocore.awsrequest.AWSHTTPConnection object at 0x11ea50d90&gt;: Failed to establish a new connection: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/patrickward/repositories/JAM/hobbes/transformations/tests/integration/lambdas/athena_database_provider_test.py"", line 97, in test_drop_with_stack_false_keeps_database_on_delete
    invoke_fn(cloud_formation_event('Delete', DatabaseName=existent_database, DropWithStack='False'))
  File ""/Users/patrickward/repositories/JAM/hobbes/transformations/tests/integration/lambdas/athena_database_provider_test.py"", line 54, in _exec
    Payload=json.dumps(event)
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/botocore/client.py"", line 357, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/botocore/client.py"", line 648, in _make_api_call
    operation_model, request_dict, request_context)
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/botocore/client.py"", line 667, in _make_request
    return self._endpoint.make_request(operation_model, request_dict)
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/botocore/endpoint.py"", line 102, in make_request
    return self._send_request(request_dict, operation_model)
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/botocore/endpoint.py"", line 137, in _send_request
    success_response, exception):
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/botocore/endpoint.py"", line 231, in _needs_retry
    caught_exception=caught_exception, request_dict=request_dict)
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/botocore/hooks.py"", line 356, in emit
    return self._emitter.emit(aliased_event_name, **kwargs)
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/botocore/hooks.py"", line 228, in emit
    return self._emit(event_name, kwargs)
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/botocore/hooks.py"", line 211, in _emit
    response = handler(**kwargs)
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/botocore/retryhandler.py"", line 183, in __call__
    if self._checker(attempts, response, caught_exception):
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/botocore/retryhandler.py"", line 251, in __call__
    caught_exception)
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/botocore/retryhandler.py"", line 277, in _should_retry
    return self._checker(attempt_number, response, caught_exception)
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/botocore/retryhandler.py"", line 317, in __call__
    caught_exception)
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/botocore/retryhandler.py"", line 223, in __call__
    attempt_number, caught_exception)
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/botocore/retryhandler.py"", line 359, in _check_caught_exception
    raise caught_exception
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/botocore/endpoint.py"", line 200, in _do_get_response
    http_response = self._send(request)
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/botocore/endpoint.py"", line 244, in _send
    return self.http_session.send(request)
  File ""/Users/patrickward/.local/share/virtualenvs/hobbes-iOpBOaJm/lib/python3.7/site-packages/botocore/httpsession.py"", line 282, in send
    raise EndpointConnectionError(endpoint_url=request.url, error=e)
botocore.exceptions.EndpointConnectionError: Could not connect to the endpoint URL: ""http://127.0.0.1:3001/2015-03-31/functions/AthenaDatabaseProvider/invocations""
</code></pre>
",0,1566492542,python;amazon-web-services;amazon-s3;aws-lambda;botocore,True,3240,1,1566588969,https://stackoverflow.com/questions/57613895/connectionrefusederror-errno-61-connection-refused-could-not-connect-to-the
57420377,Issue with Glue get_databases API,"<p>When I use get_databases() API in boto3, I only get the first 100 databases. How do I get the list of all databases from Glue catalog? </p>
",0,1565297169,python;amazon-web-services;aws-glue;botocore,True,689,1,1565456354,https://stackoverflow.com/questions/57420377/issue-with-glue-get-databases-api
57060540,need help configuring python boto3,"<p><strong><em>what ive been doing:</em></strong>
ive been reading a lot on the documentation on boto3 but im still struggling to get it working the way i am wanting as this is my first time using AWS.</p>

<p>im trying to use boto3 to access microsoft excel files that are uploaded to a S3 bucket... im able to use boto3.session() to give my ""hard coded"" credentials and from there print the names of the files that are in my bucket</p>

<p>however, im trying to figure out how to access the contents of that excel file in that bucket...</p>

<p><strong><em>My end goal/what im trying to do:</em></strong>
The end goal of this project is to have people upload excel files into the S3 bucket with zip codes in them(organized in the cells)... and then have that file sent to an EC2 instance to have a program i wrote read from the file one zip code at a time and process certain things...</p>

<p>any help is greatly appreciated as this is all new and overwhelming</p>

<p>This is the code i am trying:</p>

<pre><code>    import boto3
session = boto3.Session(
    aws_access_key_id='put key here',
    aws_secret_access_key='put key here',
)

s3 = session.resource('s3')
bucket = s3.Bucket('bucket name')

for f in bucket.objects.all():
   print(f.key)
   f.download_file('testrun')
</code></pre>
",-1,1563290260,python;amazon-web-services;amazon-s3;boto3;botocore,False,132,2,1563344893,https://stackoverflow.com/questions/57060540/need-help-configuring-python-boto3
56748577,Unable to get data from botocore.response.StreamingBody even using data.read().decode(),"<p>I am writing an AWS Lambda function that invokes another Lambda function. Both Lambda functions are in python. The called function returns a JSON, that gets wrapped into <code>botocore.response.streamingbody</code>. I am unable to convert this back to JSON to access the required fields.</p>

<p>Tried different links including:</p>

<pre class=""lang-py prettyprint-override""><code>  p = r['Payload'].read()
    print p #Prints an empty string
    print(r['Payload'].read()) #Prints Just a string
    invoke_lambda(payload)

and r['Payload'].read().decode()
r['Payload'].read().decode(""utf-8"")
</code></pre>

<p>Sample output from called Lambda function (got from printing the response before returning):</p>

<blockquote>
  <p>{'statusCode': 200, 'headers': {'Access-Control-Allow-Origin': '<em>'}, 'body1': [{'statusCode': 200, 'body': {'featureId': 'testFeature', 'version': '2.1', 'active': True}, 'headers': {'Access-Control-Allow-Origin': '</em>'}}]}</p>
</blockquote>

<p>Response returned after calling this lambda from called Lambda function:</p>

<blockquote>
  <p>{u'Payload': , u'ExecutedVersion': '$LATEST', 'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 200, 'RequestId': '0f10c74d-ebd7-4a8a-8992-6211a969ceb1', 'HTTPHeaders': {'x-amzn-requestid': '0f10c74d-ebd7-4a8a-8992-6211a969ceb1', 'content-length': '213', 'x-amz-executed-version': '$LATEST', 'x-amzn-trace-id': 'root=1-5d11c78f-f4d70afef2f04f3d061136fa;sampled=0', 'x-amzn-remapped-content-length': '0', 'connection': 'keep-alive', 'date': 'Tue, 25 Jun 2019 07:04:47 GMT', 'content-type': 'application/json'}}, u'StatusCode': 200}</p>
</blockquote>
",3,1561446514,python;amazon-web-services;aws-lambda;botocore,False,4291,0,1561508372,https://stackoverflow.com/questions/56748577/unable-to-get-data-from-botocore-response-streamingbody-even-using-data-read-d
55765219,Reading multiple &quot;bulked&quot; jsons from s3 asynchronously. Is there a better way?,"<p>The goal is to try to load a large amount of ""bulked"" jsons from s3. I found <code>aiobotocore</code> and felt urged to try in hope to get more efficiency and at the same time familiarise myself with <code>asyncio</code>. I gave it a shot, and it works but I know basically nada about asynchronous programming. Therefore, I was hoping for some improvements/comments. Maybe there are some kind souls out there that can spot some obvious mistakes. </p>

<p>The problem is that boto3 only supports one http request at a time. By utilising <code>Threadpool</code> I managed to get significant improvements, but I'm hoping for a more efficient way. </p>

<p>Here is the code: </p>

<p>Imports: </p>

<pre class=""lang-py prettyprint-override""><code>import os 
import asyncio
import aiobotocore
from itertools import chain
import json
from json.decoder import WHITESPACE
</code></pre>

<p>Some helper generator I found somewhere to return decoded jsons from string with multiple jsons.</p>

<pre class=""lang-py prettyprint-override""><code>def iterload(string_or_fp, cls=json.JSONDecoder, **kwargs):
    '''helper for parsing individual jsons from string of jsons (stolen from somewhere)'''
    string = str(string_or_fp)

    decoder = cls(**kwargs)
    idx = WHITESPACE.match(string, 0).end()
    while idx &lt; len(string):
        obj, end = decoder.raw_decode(string, idx)
        yield obj
        idx = WHITESPACE.match(string, end).end()
</code></pre>

<p>This function gets keys from an s3 bucket with a given prefix:</p>

<pre class=""lang-py prettyprint-override""><code># Async stuff starts here
async def get_keys(loop, bucket, prefix):
    '''Get keys in bucket based on prefix'''

    session = aiobotocore.get_session(loop=loop)
    async with session.create_client('s3', region_name='us-west-2',
                                   aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
                                   aws_access_key_id=AWS_ACCESS_KEY_ID) as client:
        keys = []
        # list s3 objects using paginator
        paginator = client.get_paginator('list_objects')
        async for result in paginator.paginate(Bucket=bucket, Prefix=prefix):
            for c in result.get('Contents', []):
                keys.append(c['Key'])
        return keys
</code></pre>

<p>This function gets the content for a provided key. Untop of that it flattens the list of decoded content: </p>

<pre class=""lang-py prettyprint-override""><code>async def get_object(loop,bucket, key):
    '''Get json content from s3 object'''
    session = aiobotocore.get_session(loop=loop)
    async with session.create_client('s3', region_name='us-west-2',
                                   aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
                                   aws_access_key_id=AWS_ACCESS_KEY_ID) as client:


        # get object from s3
        response = await client.get_object(Bucket=bucket, Key=key)
        async with response['Body'] as stream:
            content = await stream.read()    

    return list(iterload(content.decode()))       
</code></pre>

<p>Here is the main function which gathers the contents for all the found keys and flattens the list of contents.</p>

<pre class=""lang-py prettyprint-override""><code>async def go(loop, bucket, prefix):
    '''Returns list of dicts of object contents'''
    session = aiobotocore.get_session(loop=loop)
    async with session.create_client('s3', region_name='us-west-2',
                                   aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
                                   aws_access_key_id=AWS_ACCESS_KEY_ID) as client:

        keys = await get_keys(loop, bucket, prefix)

        contents = await asyncio.gather(*[get_object(loop, bucket, k) for k in keys])     

        return list(chain.from_iterable(contents))

</code></pre>

<p>Finally, I run this and the result list of dicts ends up  nicely in <code>result</code></p>

<pre><code>loop = asyncio.get_event_loop()
result = loop.run_until_complete(go(loop, 'some-bucket', 'some-prefix'))
</code></pre>

<ul>
<li><p>One thing that I think might be a bit wierd is that I create a client in each async function. Probably that can be lifted out. Note sure about how <code>aiobotocore</code> works with multiple clients.</p></li>
<li><p>Furthermore, I think that you would not need to await that all keys are loaded before loading the objects for the keys, which I think is the case in this implementation. I'm assuming that as soon as a key is found you could call <code>get_object</code>. So, maybe it should be an <code>async generator</code>. But I'm not completely in the clear here. </p></li>
</ul>

<p>Thank you in advance! Hope this helps someone in a similar situation.</p>
",3,1555693137,python;amazon-s3;boto3;python-asyncio;botocore,True,3337,1,1555827979,https://stackoverflow.com/questions/55765219/reading-multiple-bulked-jsons-from-s3-asynchronously-is-there-a-better-way
43400797,How to upgrade botocore1.2.6 to 1.4 or above present with boto3?,"<p>I have boto3 installed on my linux machine. When I pass below command, I get botocore version as 1.2.6</p>

<pre><code>&gt;&gt;&gt; import boto3
&gt;&gt;&gt; print boto3.__version__
1.2.6
</code></pre>

<p>How to upgrade to botocore present within boto3 to 1.4 or latest version? In case I need to downgrade botocore 1.4 to botocore 1.2.6 , what are the steps ?</p>
",14,1492113686,python;boto3;botocore,True,42345,1,1552041097,https://stackoverflow.com/questions/43400797/how-to-upgrade-botocore1-2-6-to-1-4-or-above-present-with-boto3
54035588,S3 boto connection resulting into ClientError even with the right keys,"<p>I am running a command:</p>

<pre><code>aws s3 cp s3://full-key .
</code></pre>

<p>I have made sure that the same keys are on both the pcs and yet, one pc is able to access the object while the other receives an error:</p>

<pre><code>/usr/local/lib/python2.7/dist-packages/urllib3/util/ssl_.py:354: SNIMissingWarning: An HTTPS request has been made, but the SNI (Server Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate, which can cause validation failures. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  SNIMissingWarning
fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden
</code></pre>

<p>It should be noted that my instance's timezone is UTC
Command: </p>

<pre><code>timedatectl status                   
      Local time: Fri 2019-01-04 09:04:39 UTC
  Universal time: Fri 2019-01-04 09:04:39 UTC
        Timezone: Etc/UTC (UTC, +0000)
     NTP enabled: yes
NTP synchronized: no
 RTC in local TZ: no
      DST active: n/a
</code></pre>

<p>But when I type <code>date</code> I get <code>Fri Jan  4 09:05:44 UTC 2019</code>
However, google displays utc time as: <code>8:52 am Friday, 4 January 2019 Coordinated Universal Time (UTC)</code></p>
",0,1546591535,python;amazon-web-services;amazon-s3;boto;botocore,True,180,1,1546637846,https://stackoverflow.com/questions/54035588/s3-boto-connection-resulting-into-clienterror-even-with-the-right-keys
53211433,Athena InvalidRequestException from Python,"<p>I am trying to read CSV file from S3 bucket and create table in Athena through Python. But I am getting below on executing it -</p>

<hr>

<p>Start of DB Query </p>

<p>{'QueryExecutionId': '9cc82243-4220-47d0-8b63-0aa4f01fd590', 'ResponseMetadata': {'RequestId': '1c74bec6-663a-42ef-b9d1-73c7372eb4e1', 'HTTPStatusCode': 200, 'HTTPHeaders': {'content-type': 'application/x-amz-json-1.1', 'date': 'Thu, 08 Nov 2018 15:37:11 GMT', 'x-amzn-requestid': '1c74bec6-663a-42ef-b9d1-73c7372eb4e1', 'content-length': '59', 'connection': 'keep-alive'}, 'RetryAttempts': 0}} </p>

<p>Start of table creation </p>

<p>Traceback (most recent call last):   </p>

<p>File ""C:/Users/Doc/PycharmProjects/aws-athena-repo/athena/app.py"", line 61, in 
    QueryExecutionContext={'Database': 'athenadb'})   </p>

<p>File ""C:\Program Files\Python37\lib\site-packages\botocore\client.py"", line 320, in _api_call
    return self._make_api_call(operation_name, kwargs)   </p>

<p>File ""C:\Program Files\Python37\lib\site-packages\botocore\client.py"", line 623, in _make_api_call
    raise error_class(parsed_response, operation_name) </p>

<p>botocore.errorfactory.InvalidRequestException: An error occurred (InvalidRequestException) when calling the StartQueryExecution operation: line 1:8: no viable alternative at input 'CREATE EXTERNAL'</p>

<hr>

<p>Here is my code sample --</p>

<pre><code>print(""Start of DB Query"")
# Create a new database
db_query = 'CREATE DATABASE IF NOT EXISTS athenadb;'
response = client.start_query_execution(
    QueryString=db_query,
    ResultConfiguration={'OutputLocation': 's3://mybucket'})
print(response)

table_query = '''
CREATE EXTERNAL TABLE IF NOT EXISTS `athenadb.testtable`(
    `id` int,
    `ident` string,
    `type` string,
    `name` string,
    `latitude_deg` double,
    `longitude_deg` double,
    `continent` string,
    `iso_country` string,
    `iso_region` string,
    `municipality` string,
    `scheduled_service` string,
    `gps_code` string,
    `iata_code` string,
    `local_code` string,
    `home_link` string,
    `wikipedia_link` string,
    `keywords` string 
)
ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY ',' 
  LINES TERMINATED BY '\n' 
WITH SERDEPROPERTIES ( 
  'escape.delim'='\\')
STORED AS TEXTFILE
LOCATION 's3://mybucket/folder/' ;'''

print(""Start of table creation"")

response1 = client.start_query_execution(
    QueryString=table_query,
    ResultConfiguration={'OutputLocation': 's3://mybucket'},
    QueryExecutionContext={'Database': 'athenadb'})
print(response1)
</code></pre>

<p>I am not sure if the problem is with ROW FORMAT DELIMITED or else.
I think my code is fine.</p>

<p>Detailed steps would be appreciated!</p>

<p>Thanks in adavace!</p>
",0,1541692502,python;amazon-web-services;boto3;amazon-athena;botocore,True,3520,1,1542126073,https://stackoverflow.com/questions/53211433/athena-invalidrequestexception-from-python
37120281,Accessing AWS through PynamoDB vs. low-level botocore,"<p>I have configured the AWS CLI properly following instructions and I want to access a DynamoDB table from the high-level package <a href=""https://github.com/jlafon/PynamoDB"" rel=""nofollow"">PynamoDB</a> rather than boto3. </p>

<p>If I try to access my AWS cluster through the low-level <em>botocore</em> package, which is used by both boto3 and PynamoDB, and connect to a DynamoDB table as </p>

<pre><code>import botocore.session
session = botocore.session.get_session()
client = session.create_client('dynamodb')
client.describe_table(TableName='my_table_name')
</code></pre>

<p>all is OK, the table gets accessed fine.</p>

<p>But, I am trying to access through PynamoDB by following the tutorial in the documentation and creating a model for the table, as</p>

<pre><code>from pynamodb.models import Model

class MyTableModel(Model):

    class Meta:
        table_name = 'my_table_name'

    pk_field = UnicodeAttribute(hash_key=True)
    field1 = UnicodeAttribute()
    field2 = UnicodeAttribute()
</code></pre>

<p>and again describing the table through the appropriate method on the model</p>

<pre><code>print MyTableModel.describe_table()
</code></pre>

<p>I get error </p>

<blockquote>
  <p>pynamodb.exceptions.TableDoesNotExist: Table does not exist:
  <code>Requested resource not found: Table: my_table_name not found</code></p>
</blockquote>

<p>I don't understand why as I have digged into the PynamoDB code and what I seem to understand is that it should call the same code from botocore and the configuration should be implicit.</p>
",4,1462809444,python;amazon-web-services;amazon-dynamodb;botocore,True,3573,2,1541576013,https://stackoverflow.com/questions/37120281/accessing-aws-through-pynamodb-vs-low-level-botocore
52017742,AWS Translation - Excel,"<p>I have an excel file that contains multiple columns .The data inside it is in english language , I want to translate all of them into French(fr) and get the new excel .. </p>

<p>The problem is that <code>translate_txt</code> is not accepting dataframe , Is there any way to fix this ?</p>

<pre><code>import boto3
import pandas as pd

translate = boto3.client(service_name='translate', region_name='us-east-1', use_ssl=True)

df = pd.read_excel('data.xlsx')
result = translate.translate_text(Text=df,SourceLanguageCode=""en"", TargetLanguageCode=""fr"")
</code></pre>
",0,1535203355,python;python-3.x;amazon-web-services;boto3;botocore,True,538,1,1535207326,https://stackoverflow.com/questions/52017742/aws-translation-excel
51687990,Renew STS credentials for dynamodb during scan operation,"<p>I'm new to python and need to migrate data from one dynamo db into another.
the problem is that we have multiple accounts for different environments for example one for dev and one for prod. I have/use Ec2 instance to migrate this data. To access prod account I have STS.</p>

<pre><code>stsCredentials = boto3.client('sts').assume_role(
    RoleArn=STS_ROLE_TO_ACCESS_DEV_ACCOUNT_ARN,
    RoleSessionName='cross-account-dynamodb-role',
    DurationSeconds=STS_TOKEN_TIME_OUT
)

prodAccount = boto3.client(
    'dynamodb',
    region_name=AWS_REGION,
    aws_access_key_id=stsCredentials['Credentials']['AccessKeyId'],
    aws_secret_access_key=stsCredentials['Credentials']['SecretAccessKey'],
    aws_session_token=stsCredentials['Credentials']['SessionToken']
)
devAccount = boto3.client('dynamodb', region_name=AWS_REGION)
</code></pre>

<p>Then I need to scan production table(in reality I create backup and restored another table from this backup to reduce impact on prod table)</p>

<pre><code>paginator = prodAccount.get_paginator('scan')
pageIterator = paginator.paginate(
    TableName=backupTableName,
    Select='ALL_ATTRIBUTES',
    ReturnConsumedCapacity='TOTAL',
    ConsistentRead=False
)
for page in pageIterator :
    for item in page['Items']:
        // Process response, store data to different files and migration to dev account
</code></pre>

<p>And the problem is that my prod db has too much data and STS token expires too quickly(Because of some restrictions I can not create sts credentials for more than 1 hour). As a result my pageIterator throws exception at some point with token expired exception. <strong>May be it is possible to update this token for this pageIterator and continue scan from point where it failed.</strong></p>
",3,1533403428,python;amazon-web-services;boto3;botocore,False,724,0,1533405671,https://stackoverflow.com/questions/51687990/renew-sts-credentials-for-dynamodb-during-scan-operation
50478807,Using boto3 as a base class,"<p>With boto2, I could write a class that extended.
Say S3, with some custom functions but still allowed me to use any of the built-in ones.
Something like:</p>

<pre><code>class MyS3(S3Connection):
    def bucket_exists(self, bucket_name):
        . . .


s3_connection = MyS3()
s3_connection.create_bucket('mybucket')  # built-in function
s3_connection.bucket_exists('mybucket')  # custom
</code></pre>

<p>But, in boto3, all of the resource/client classes are generated at runtime so I can't inherit directly.</p>

<p>I found the documentation on <a href=""http://boto3.readthedocs.io/en/latest/guide/events.html"" rel=""nofollow noreferrer"">extending boto3</a> but I don't know, how one could subclass it?</p>
",0,1527040983,python;python-2.7;boto3;botocore,False,2007,1,1527493554,https://stackoverflow.com/questions/50478807/using-boto3-as-a-base-class
48419066,Detect execution in Amazon cloud from a Python script using boto3,"<p>How can I find that my python script is being executed in AWS (EC2 instance, ECS container or Lambda) and not for example from my own workstation, within the script using ideally <code>boto3</code>.</p>

<p>I would expect that <code>boto3</code> has some way to say: ""<code>True</code> - yep, you're in AWS"" or ""<code>False</code> - nope, you're not"". However I could not find anything like this.</p>

<p>Something relatively close seems to be <code>boto.utils.get_instance_metadata</code> - this would I guess at least fail when executed on my workstation, but apparently such functionality does not even exist in <code>boto3</code>:
<a href=""https://github.com/boto/boto3/issues/313"" rel=""nofollow noreferrer"">https://github.com/boto/boto3/issues/313</a></p>
",-1,1516786505,python;aws-sdk;boto3;botocore,True,188,1,1516822460,https://stackoverflow.com/questions/48419066/detect-execution-in-amazon-cloud-from-a-python-script-using-boto3
48307919,Unknown service error botocore,"<p>I'm trying to use the recent Amazon transcribe service with:</p>

<pre><code>transcribe = boto3.client('transcribe')
</code></pre>

<p>and I get the following error:</p>

<pre><code>botocore.exceptions.UnknownServiceError: Unknown service: 'transcribe'. Valid service names are: ...
</code></pre>

<p>I've tried upgrading boto3 and botocore using:</p>

<pre><code>pip install botocore --upgrade
pip install boto3 --upgrade
</code></pre>
",5,1516214455,python;amazon-web-services;aws-sdk;boto3;botocore,True,6166,1,1516214816,https://stackoverflow.com/questions/48307919/unknown-service-error-botocore
47905855,How can I be sure to catch all possible exceptions when uploading to Glacier with boto3 (AWS),"<p>I am writing a small python application that uploads archives to AWS Glacier. During the upload process, I call the following methods:</p>

<ul>
<li>client.initiate_multipart_upload()</li>
<li>client.upload_multipart_part()</li>
<li>client.complete_multipart_upload()</li>
</ul>

<p>All of these have the potential to raise many different exceptions if the connection is lost or the request takes too long.</p>

<p>I was using:</p>

<pre><code> except (botocore.exceptions.EndpointConnectionError, client.exceptions.RequestTimeoutException):
</code></pre>

<p>to try and catch them, retry after a set period of time, and give up after a set number of attempts, but I still sometimes get other exceptions that go uncaught. Right now I have switched to using a blanket </p>

<pre><code>except:
</code></pre>

<p>statement, which I am not very happy about. My application checks the AWS account credentials, vault name and connection to AWS at another stage, so these do not need to be considered. Is there a way I can catch all boto3 and botocore exceptions that might occur while carrying out these three methods?</p>
",1,1513772072,python;amazon-web-services;boto3;botocore,True,1445,1,1514123329,https://stackoverflow.com/questions/47905855/how-can-i-be-sure-to-catch-all-possible-exceptions-when-uploading-to-glacier-wit
47823925,Streaming huge gzip file from s3 using boto3 python,"<p>I have a 4GB gzip file on s3, I am trying to read the gzip file and write the decompressed contents to another file. How would I do this using boto3 python without loading the whole file into memory?</p>
",4,1513295891,python;python-2.7;boto;boto3;botocore,False,731,0,1513295891,https://stackoverflow.com/questions/47823925/streaming-huge-gzip-file-from-s3-using-boto3-python
47519548,botocore s3 put has issue hashing file due to encoding?,"<p>I'm having trouble figuring out why the file, the contents of which are ""DELETE ME LATER"", which is loaded with encoding <code>utf-8</code> causes an exception in botocore when it's being hashed.</p>

<pre><code>with io.open('deleteme','r', encoding='utf-8') as f:
  try:
    resp=client.put_object(
    Body=f,
    Bucket='s3-bucket-actual-name-for-real',
    Key='testing/a/put'
    )
    print('deleteme exists')
    print(resp)
  except:
    print('deleteme could not put')
    raise
</code></pre>

<p>Produces:</p>

<blockquote>
  <p>deleteme could not put<br>
  Traceback (most recent call last):   File<br>
  ""./test_operator.py"", line 41, in <br>
      Key='testing/a/put'   File ""/Users/lamblin/VEnvs/awscli/lib/python3.6/site-packages/botocore/client.py"",<br>
  line 312, in _api_call<br>
      return self._make_api_call(operation_name, kwargs)   File ""/Users/lamblin/VEnvs/awscli/lib/python3.6/site-packages/botocore/client.py"",<br>
  line 582, in _make_api_call
      request_signer=self._request_signer, context=request_context)   File<br>
  ""/Users/lamblin/VEnvs/awscli/lib/python3.6/site-packages/botocore/hooks.py"",<br>
  line 242, in emit_until_response<br>
      responses = self._emit(event_name, kwargs, stop_on_response=True)   File<br>
  ""/Users/lamblin/VEnvs/awscli/lib/python3.6/site-packages/botocore/hooks.py"",<br>
  line 210, in _emit<br>
      response = handler(**kwargs)   File ""/Users/lamblin/VEnvs/awscli/lib/python3.6/site-packages/botocore/handlers.py"",<br>
  line 201, in conditionally_calculate_md5<br>
      calculate_md5(params, **kwargs)   File ""/Users/lamblin/VEnvs/awscli/lib/python3.6/site-packages/botocore/handlers.py"",<br>
  line 179, in calculate_md5<br>
      binary_md5 = _calculate_md5_from_file(body)   File ""/Users/lamblin/VEnvs/awscli/lib/python3.6/site-packages/botocore/handlers.py"",<br>
  line 193, in _calculate_md5_from_file
      md5.update(chunk)<br>
  TypeError: Unicode-objects must be encoded before hashing</p>
</blockquote>

<p>Now this can be avoided by opening the file with 'rb' but, isn't the file object <code>f</code> clearly using an encoding?</p>
",6,1511815675,python;amazon-s3;botocore,True,3542,1,1511821602,https://stackoverflow.com/questions/47519548/botocore-s3-put-has-issue-hashing-file-due-to-encoding
46645664,botocore - Tags are missing from &#39;describe_images&#39; response,"<p>When I do something like:</p>

<p><code>ec2_client.describe_images(ImageIds=['ami-123456'])</code></p>

<p>The response I get is missing the 'Tags'. This is not the case when I do the same call using aws cli:</p>

<p><code>aws ec2 describe-images --image-ids ami-123456</code></p>
",0,1507549485,python;amazon-web-services;amazon-ec2;botocore,False,146,1,1507566510,https://stackoverflow.com/questions/46645664/botocore-tags-are-missing-from-describe-images-response
46398666,Is it possible to use an HTTPS proxy with the aiobotocore Python module?,"<p>Using the example from the Aiobotocore website and a HTTPS proxy like this:</p>

<pre><code>import asyncio
import aiobotocore
from aiobotocore.config import AioConfig as Config

AWS_ACCESS_KEY_ID = ""xxx""
AWS_SECRET_ACCESS_KEY = ""xxx""


async def go(loop):
    bucket = 'dataintake'
    filename = 'dummy.bin'
    folder = 'aiobotocore'
    key = '{}/{}'.format(folder, filename)

    session = aiobotocore.get_session(loop=loop)
    conf = Config(proxies={'http': '&lt;http_proxy&gt;:&lt;http_proxy_port&gt;', 'https': '&lt;https_proxy&gt;:&lt;https_proxy_port&gt;'})
    async with session.create_client('s3', region_name='us-west-2',
                                   aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
                                   aws_access_key_id=AWS_ACCESS_KEY_ID,
                                   config=conf) as client:
        # upload object to amazon s3
        data = b'\x01'*1024
        resp = await client.put_object(Bucket=bucket,
                                            Key=key,
                                            Body=data)
        print(resp)

        # getting s3 object properties of file we just uploaded
        resp = await client.get_object_acl(Bucket=bucket, Key=key)
        print(resp)

        # get object from s3
        response = await client.get_object(Bucket=bucket, Key=key)
        # this will ensure the connection is correctly re-used/closed
        async with response['Body'] as stream:
            assert await stream.read() == data

        # list s3 objects using paginator
        paginator = client.get_paginator('list_objects')
        async for result in paginator.paginate(Bucket=bucket, Prefix=folder):
            for c in result.get('Contents', []):
                print(c)

        # delete object from s3
        resp = await client.delete_object(Bucket=bucket, Key=key)
        print(resp)

loop = asyncio.get_event_loop()
loop.run_until_complete(go(loop))
</code></pre>

<p>I get the following error:</p>

<pre><code>ValueError: Only http proxies are supported
</code></pre>

<p>Is it possible to somehow use HTTPS proxies with aiobotocore in another way or would it be easy to modify the source code to also support HTTPS proxies?</p>
",0,1506320008,python;boto3;aiohttp;botocore,False,1431,1,1506382374,https://stackoverflow.com/questions/46398666/is-it-possible-to-use-an-https-proxy-with-the-aiobotocore-python-module
33461877,botocore: how to close or clean up a session or client,"<p>While doing some automation around AWS-EC2 with the <code>botocore</code> library in Python, I noticed a lot of HTTPS connections remained established that were no longer needed by processes that were busy doing other things (so killing them or recoding them to exit is not an option).  I think the <code>botocore</code> session and/or client object is leaving the connections to AWS endpoints established.  The <code>botocore</code> documentation shows how to start or create them, but <em>not</em> how to close them or clean things up.  I tried a <code>.close</code> method but it did not exist.  <strong>How can I get these connections to gracefully close without killing the processes?</strong></p>
",13,1446378971,python;botocore,True,10598,1,1505204919,https://stackoverflow.com/questions/33461877/botocore-how-to-close-or-clean-up-a-session-or-client
45704697,Botocore object issue.,"<p>I'm attempting to create custom configurations for my boto3.client. The first two parameters work (maxpoolconnect and readtimeout) but the third one retries is giving me a  <em>Got unexpected keyword argument 'retries'</em> heres the link to the documentation for reference. </p>

<p><a href=""http://botocore.readthedocs.io/en/latest/reference/config.html"" rel=""nofollow noreferrer"">http://botocore.readthedocs.io/en/latest/reference/config.html</a></p>

<p><code>config = botocore.config.Config(max_pool_connections=50,read_timeout=1,retries={'max_attempts':0})</code></p>
",1,1502854390,python;config;botocore,False,1608,0,1502854390,https://stackoverflow.com/questions/45704697/botocore-object-issue
42939155,"boto3 giving AccessDenied, is there a way to lookup the missing permissions?","<p>I've been trying to figure out why this step in my script isn't working, as far as I know, my user has permissions to do everything on our AWS account, so I'm a bit confused. I was wondering if there was a way you could get a return from botocore or boto3 itself identifying the missing iam permission for your user rather than figure out exactly the right permissions for the job you need to do it yourself?</p>

<p>Initially I had zone_id configured <a href=""http://docs.aws.amazon.com/general/latest/gr/rande.html#elasticbeanstalk_region"" rel=""nofollow noreferrer"">from here</a></p>

<pre><code>&gt;&gt;&gt; dns_record = connection.change_resource_record_sets(
...     HostedZoneId=zone_id,
...     ChangeBatch={
...         'Changes': [
...             {
...                 'Action': action,
...                 'ResourceRecordSet': {
...                     'Name': name,
...                     'Type': record_type,
...                     'TTL': 60,
...                     'AliasTarget': {
...                         'HostedZoneId': zone_id,
...                         'DNSName': destination,
...                         'EvaluateTargetHealth': eval_health
...                     },
...                 }
...             },
...         ]
...     }
... )

Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 14, in &lt;module&gt;
  File ""/usr/lib/python2.7/site-packages/botocore/client.py"", line 253, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File ""/usr/lib/python2.7/site-packages/botocore/client.py"", line 543, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.exceptions.ClientError: An error occurred (AccessDenied) when calling the ChangeResourceRecordSets operation: User: arn:aws:iam::1337:user/rumbles is not authorized to access this resource
&gt;&gt;&gt; 
&gt;&gt;&gt; print connection, zone_id, action, name, record_type, destination, eval_health
&lt;botocore.client.Route53 object at 0x7f9049d64590&gt; Z117KPS5GTRQ2G UPSERT test-rumbles.domain.com A some-elb-1337.us-east-1.elb.amazonaws.com False
</code></pre>

<p>If I change zone_id to the Zone of our AWS domain I get:</p>

<pre><code>&gt;&gt;&gt; zone_id = ""Z2401337RNHANU""
&gt;&gt;&gt; 
&gt;&gt;&gt; try:
...
An error occurred (InvalidInput) when calling the ChangeResourceRecordSets operation: Invalid request
{'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 400, 'RequestId': '2bfde962-0e81-11e7-b312-53961dec3e91', 'HTTPHeaders': {'x-amzn-requestid': '2bfde962-0e81-11e7-b312-53961dec3e91', 'date': 'Tue, 21 Mar 2017 21:56:00 GMT', 'content-length': '259', 'content-type': 'text/xml', 'connection': 'close'}}, 'Error': {'Message': 'Invalid request', 'Code': 'InvalidInput', 'Type': 'Sender'}}
</code></pre>

<p>I've had to look in to these issues before, I know that I just have to find the right IAM permission for this particular job (assuming I am on the right path), but I was wondering if there is some way to get this information returned from botocore? Rather than spending my time manually having to look it up, can't I get the iam rule that denied me access returned by my except somehow?</p>

<p>Or am I talking about a feature request I haven't found/created yet?</p>
",2,1490134500,python;amazon-web-services;boto3;botocore,True,3169,1,1490213799,https://stackoverflow.com/questions/42939155/boto3-giving-accessdenied-is-there-a-way-to-lookup-the-missing-permissions
40203620,aiobotocore-aiohttp - Get S3 file content and stream it in the response,"<p>I want to get the content of an uploaded file on S3 using botocore and aiohttp service. As the files may have a huge size:</p>

<ul>
<li>I don't want to store the whole file content in memory,</li>
<li>I want to be able to handle other requests while downloading files from S3 (aiobotocore, aiohttp),</li>
<li>I want to be able to apply modifications on the files I download, so I want to treat it line by line and stream the response to the client</li>
</ul>

<p>For now, I have the following code in my aiohttp handler:</p>

<pre><code>import asyncio                                  
import aiobotocore                              

from aiohttp import web                         

@asyncio.coroutine                              
def handle_get_file(loop):                      

    session = aiobotocore.get_session(loop=loop)

    client = session.create_client(             
        service_name=""s3"",                      
        region_name="""",                         
        aws_secret_access_key="""",               
        aws_access_key_id="""",                   
        endpoint_url=""http://s3:5000""           
    )                                           

    response = yield from client.get_object(    
        Bucket=""mybucket"",                      
        Key=""key"",                              
    )                                           
</code></pre>

<p>Each time I read one line from the given file, I want to send the response. Actually, get_object() returns a dict with a Body (ClientResponseContentProxy object) inside. Using the method read(), how can I get a chunk of the expected response and stream it to the client ?</p>

<p>When I do :</p>

<pre><code>for content in response['Body'].read(10):
    print(""----"")                        
    print(content)          
</code></pre>

<p>The code inside the loop is never executed.</p>

<p>But when I do :</p>

<pre><code>result = yield from response['Body'].read(10)
</code></pre>

<p>I get the content of the file in result. I am a little bit confused about how to use read() here.</p>

<p>Thanks</p>
",2,1477230390,python;amazon-s3;aiohttp;botocore,True,3238,1,1477234408,https://stackoverflow.com/questions/40203620/aiobotocore-aiohttp-get-s3-file-content-and-stream-it-in-the-response
39457457,Not able to get the resources attached with route table,"<p>I am  using python for AWS  infrastructure automation.
I need to get the resources attached with the Route Table for which API given is </p>

<pre><code>ec2  = boto3.resource('ec2')
route_table_association = ec2.RouteTableAssociation('rtb-**********')
response=route_table_association.get_available_subresources()
</code></pre>

<p>Here the return type of response is giving me the empty list all the time. and <code>response=route_table_association.delete()</code> gives the exception</p>

<pre><code>An error occurred (InvalidAssociationID.NotFound) when calling the `DisassociateRouteTable operation: The association ID 'rtb-*********' does not exist.`
</code></pre>

<p>But the route tebale exist and is attached to a subnet explicitly</p>
",0,1473708613,python;amazon-web-services;amazon-ec2;boto3;botocore,False,944,2,1473770530,https://stackoverflow.com/questions/39457457/not-able-to-get-the-resources-attached-with-route-table
28629022,Not able to connect the amazon DynamoDb Local using python boto sdk,"<p>I want to connect the db available inside DynamoDbLocal using the boto sdk.I followed the documentation as per the below link.</p>

<p><a href=""http://boto.readthedocs.org/en/latest/dynamodb2_tut.html#dynamodb-local"" rel=""nofollow"">http://boto.readthedocs.org/en/latest/dynamodb2_tut.html#dynamodb-local</a></p>

<p>This is the official documentation provided by the amazon.But when I am executing the snippet available in the document, I am unable to connect the db and I can't get the tables available inside the db. The dbname is  ""dummy_us-east-1.db"". And my snippet is:</p>

<pre><code>from boto.dynamodb2.layer1 import DynamoDBConnection
con = DynamoDBConnection(host='localhost', port=8000,
    aws_access_key_id='dummy',
    aws_secret_access_key='dummy',
    is_secure=False,
    )

print con.list_tables()
</code></pre>

<p>I have a 8 tables available inside the db. But I am getting empty list, after executing the list_tables() command.</p>

<pre><code>output:
{u'TableNames':[]}
</code></pre>

<p>Instead of accessing the required database, it creating and accessing the new database.
Old database : <strong>dummy_us-east-1.db</strong>
New database : <strong>dummy_localhost.db</strong>
        How to resolve this.
Please give me some suggestions regarding to the DynamoDbLocal access. Thanks in advance.</p>
",2,1424435247,python;amazon-web-services;amazon-dynamodb;boto;botocore,True,2020,2,1453369545,https://stackoverflow.com/questions/28629022/not-able-to-connect-the-amazon-dynamodb-local-using-python-boto-sdk
30808297,Python PEP 273 and Amazon BotoCore,"<p>On a small embedded Linux device with limited space, I am trying to place the large [10 Mb] Amazon (AWS) BotoCore library (<a href=""https://github.com/boto/botocore"" rel=""nofollow noreferrer"">https://github.com/boto/botocore</a>) in a zip file to compress it and then import it in my Python Scripts using zipimport as described in PEP273 (<a href=""https://www.python.org/dev/peps/pep-0273/"" rel=""nofollow noreferrer"">https://www.python.org/dev/peps/pep-0273/</a>).</p>

<p>I modified my script to have the following lines at the beginning:</p>

<pre><code>## Use zip imports
import sys
sys.path.insert(0, '/usr/lib/python2.7/site-packages/site-packages.zip') 
</code></pre>

<p>The site-packages zip file only has botocore in it and site-packages directory itself has the other modules I use, but excluding botocore, in it.</p>

<p>Here is a listing of that directory:</p>

<pre><code>    /usr/lib/python2.7/site-packages &gt;&gt; ls -rlt
    total 1940
-rw-rw-r-- 1 root root   32984 Jun  8 12:22 six.pyc
-rw-r--r-- 1 root root     119 Jun 11 07:43 README
drwxrwxr-x 2 root root    4096 Jun 11 07:43 requests-2.4.3-py2.7.egg-info
drwxrwxr-x 2 root root    4096 Jun 11 07:43 six-1.9.0-py2.7.egg-info
drwxrwxr-x 2 root root    4096 Jun 11 07:43 python_dateutil-2.4.2-py2.7.egg-info
drwxrwxr-x 2 root root    4096 Jun 11 07:43 jmespath-0.7.0-py2.7.egg-info
-rw-rw-r-- 1 root root    2051 Jun 11 07:44 pygtk.pyc
-rw-rw-r-- 1 root root    1755 Jun 11 07:44 pygtk.pyo
-rw-rw-r-- 1 root root       8 Jun 11 07:44 pygtk.pth
drwxrwxr-x 2 root root    4096 Jun 11 07:44 futures-2.2.0-py2.7.egg-info
drwxrwxr-x 3 root root    4096 Jun 11 07:44 gtk-2.0
drwxrwxr-x 3 root root    4096 Jun 11 07:44 requests
drwxrwxr-x 3 root root    4096 Jun 11 07:44 dbus
drwxrwxr-x 3 root root    4096 Jun 11 07:44 dateutil
drwxrwxr-x 2 root root    4096 Jun 11 07:44 jmespath
drwxrwxr-x 3 root root    4096 Jun 11 07:44 concurrent
drwxrwxr-x 2 root root    4096 Jun 11 07:44 futures
drwxrwxr-x 2 root root    4096 Jun 12 10:42 gobject
drwxrwxr-x 2 root root    4096 Jun 12 10:42 glib
-rwxr-xr-x 1 root root    5800 Jun 12 10:42 _dbus_glib_bindings.so
-rwxr-xr-x 1 root root   77680 Jun 12 10:42 _dbus_bindings.so
-rwxr-xr-x 1 root root 1788623 Jun 12 11:39 site-packages.zip
</code></pre>

<p>And here are the contents of that zipfile:
<img src=""https://i.stack.imgur.com/oKkqL.png"" alt=""enter image description here""></p>

<p>My problem is that I can import boto3 and import botocore just find, but when I try to use some API methods contained therein, I get exceptions like this:</p>

<pre><code>&gt;&gt; Unknown component: enpoint_resolver
</code></pre>

<p>or </p>

<pre><code>&gt;&gt; Unable to load data for: aws/_endpoints!
</code></pre>

<p>If I remove the zip file after uncompressing it in the site-packages directory and reboot - my script works fine.</p>

<p>How can I leverage zipfile imports to compress this huge library? Thanks!</p>
",10,1434127676,python;python-2.7;python-import;pep;botocore,True,290,1,1445524913,https://stackoverflow.com/questions/30808297/python-pep-273-and-amazon-botocore
28996265,How to extend a Boto3 resource?,"<p>On boto3, how can I extend <code>ResourceModel</code>? What I wan't to do is subclass <code>boto3.resources.factory.ec2.Instance</code> and add a <code>run</code> method to it. That method would be used to remotely run commands on the EC2 instance represented by the Python object, via SSH. I wish to do this in a clean way, i.e., without resorting to monkey patches or other obscure techniques.</p>

<p><strong>Update</strong></p>

<p>Based on <a href=""https://stackoverflow.com/a/29637081"">Daniel's answer</a>, I came up with the following code. Requires a recent version of Boto 3, and <a href=""https://github.com/mwilliamson/spur.py"" rel=""nofollow noreferrer"">Spur</a> for the SSH connection (<code>pip install spur boto3</code>).</p>

<pre><code>from boto3 import session
from shlex import split
from spur import SshShell

# Customize here.
REGION = 'AWS-REGION'
INSTID = 'AWS-INSTANCE-ID'
USERID = 'SSH-USER'

def hook_ssh(class_attributes, **kwargs):
    def run(self, command):
        '''Run a command on the EC2 instance via SSH.'''

        # Create the SSH client.
        if not hasattr(self, '_ssh_client'):
            self._ssh_client = SshShell(self.public_ip_address, USERID)

        print(self._ssh_client.run(split(command)).output.decode())

    class_attributes['run'] = run

if __name__ == '__main__':
    b3s = session.Session()
    ec2 = b3s.resource('ec2', region_name=REGION)

    # Hook the ""run"" method to the ""ec2.Instance"" resource class.
    b3s.events.register('creating-resource-class.ec2.Instance', hook_ssh)

    # Run some commands.
    ec2.Instance(INSTID).run('uname -a')
    ec2.Instance(INSTID).run('uptime')
</code></pre>
",6,1426104205,python;boto;boto3;botocore,True,2503,1,1443212176,https://stackoverflow.com/questions/28996265/how-to-extend-a-boto3-resource
31728741,max_clients limit reached error on tornado-botocore server,"<p>I've developed a Tornado server using the tornado-botocore package for interacting with Amazon SQS service.
When I'm trying to load test the server i get the following log:
[simple_httpclient:137:fetch_impl] max_clients limit reached, request queued. 10 active, 89 queued requests.
I assume it's from the ASyncHTTPClient used by the botocore package.
I've tried to set the max_clients to an higher number but with no success:</p>

<pre><code>    def _connect(self, operation):
    sqs_connection = Botocore(
        service='sqs', operation=operation,
        region_name=options.aws_sqs_region_name,
        session=session)
    sqs_connection.http_client.configure(None, defaults=dict(max_clients=5000))
</code></pre>

<p>what am i doing wrong?</p>

<p>Thanks.</p>
",1,1438272144,python;tornado;boto;botocore,True,1254,1,1438295529,https://stackoverflow.com/questions/31728741/max-clients-limit-reached-error-on-tornado-botocore-server
30820377,AttributeError: &#39;Table&#39; object has no attribute &#39;update_item&#39; - DynamoDB v2 API,"<p>I am trying to conditionally update an item in DynamoDB using the following code:</p>

<pre><code>from boto.dynamodb2.table import Table

conn = get_layer1_ddb_connection()
values_table = Table(table_name, connection=conn)
attrs = { 'values' : new_values,
          'version' : existing_item['version'] + 1}
condition_expression = 'version = :v'
values_table.update_item(table_name, key=customer_id, attribute_updates=attrs, condition_expression=condition_expression, expression_attribute_values={':v': existing_item['version'],}, return_values='ALL_OLD',)
</code></pre>

<p>where, layer1 connection is created like this:</p>

<pre><code>from boto.dynamodb2.layer1 import DynamoDBConnection

def get_layer1_ddb_connection(self):
    return DynamoDBConnection(region=self.region, aws_access_key_id=self.creds[CRED_ACCESS_KEY], aws_secret_access_key=self.creds[CRED_SECRET_KEY])
</code></pre>

<p><code>self.region</code> is of type RegionInfo and self.creds have always worked perfectly  for other high level API calls.</p>
",0,1434209464,python;python-2.7;amazon-dynamodb;boto;botocore,False,6556,2,1435337691,https://stackoverflow.com/questions/30820377/attributeerror-table-object-has-no-attribute-update-item-dynamodb-v2-api
29929540,How to view Boto3 HTTPS request string,"<p>I have been able to view the attributes of the PreparedRequest that botocore sends, but I'm wondering how I can view the exact request string that is sent to AWS.  I need the exact request string to be able to compare it to another application I'm testing AWS calls with.</p>
",35,1430253825,python;boto3;botocore,True,26891,2,1431650031,https://stackoverflow.com/questions/29929540/how-to-view-boto3-https-request-string
24805608,AWS-BOTO security group error,"<p>I have the following code to spin up new instances:</p>

<pre><code>conn = boto.ec2.connect_to_region(""us-east-1"", security_token = ""xx"", aws_access_key_id= ""xx"",  aws_secret_access_key= ""xx"")


security_groups = conn.get_all_security_groups()
for security_group in security_groups:
    if str(security_group)[14:] == ""xx"":
        conn.run_instances(
                'ami-da2cd5b2',
                key_name='fornax_keypair',
                instance_type='c1.xlarge',
                security_groups=security_group)
    else:
        continue
</code></pre>

<p>It finds the security group and then gives the error :</p>

<pre><code>TypeError: 'SecurityGroup' object is not iterable
</code></pre>

<p>If I change it to str(security_group), it then gives the error:</p>

<pre><code>&lt;Response&gt;&lt;Errors&gt;&lt;Error&gt;&lt;Code&gt;InvalidGroup.NotFound&lt;/Code&gt;&lt;Message&gt;The security groups 'f', 'g', 'd', 'e', 'c', 'n', 'o', 'j', '.', 'i', 'v', 'u', 't', 's', 'r', 'p', '
:', 'y' do not exist&lt;/Message&gt;&lt;/Error&gt;&lt;/Errors&gt;&lt;RequestID&gt;c96afd3c-de3f-4441-be65-c6a85fbe7868&lt;/RequestID&gt;&lt;/Response&gt;
</code></pre>

<p><strong>Also how do I attach the connection to an already established vpc connection and subnet ?</strong></p>
",0,1405605469,python;python-2.7;amazon-ec2;boto;botocore,True,741,1,1405607159,https://stackoverflow.com/questions/24805608/aws-boto-security-group-error
24762106,Passing the security_token for ec2 creation - BOTO,"<p>I have the accesskey, secret access key and security token for my session and am trying to create a ec2 instance using BOTO: </p>

<pre><code>import boto
from boto.vpc import VPCConnection

conn = boto.ec2.connect_to_region(""us-east-1"", aws_security_token = ""xx"", aws_access_key_id= ""xx"",  aws_secret_access_key= ""xx"")
</code></pre>

<p>as I run the script, I get the error = </p>

<blockquote>
  <p>return self.connection_cls(region=self, **kw_params) TypeError:
  <strong>init</strong>() got an unexpected keyword argument 'aws_security_token'</p>
</blockquote>

<p>Not sure why I am getting this error, is there an alternate way of passing the security token?</p>
",2,1405437785,python;python-2.7;python-3.x;boto;botocore,False,1355,0,1405437785,https://stackoverflow.com/questions/24762106/passing-the-security-token-for-ec2-creation-boto
21737883,Changing an objects metadata with botocore,"<p>I'm using botocore because I'm using python 3 and can't use boto. So I followed the example here and can upload a file</p>

<pre><code>import botocore.session

session = botocore.session.get_session()
s3 = session.get_service('s3')
operation = s3.get_operation('PutObject')
endpoint = s3.get_endpoint('us-east-1')
fp = open('my_large_local_file', 'rb')
res, res_data = operation.call(endpoint, bucket='my-bucket',key='/my/key', body=fp, acl='public-read')
</code></pre>

<p>But I have no idea how to set the metadata and it isn't documented ANYWHERE. At random I tried just adding a metadata kwarg</p>

<pre><code>, metadata={key:value} 
</code></pre>

<p>but then it threw this error</p>

<pre><code>File ""/usr/local/lib/python3.2/dist-packages/botocore/auth.py"", line 382, in &lt;genexpr&gt;
custom_headers[lk] = ','.join(v.strip() for v in
AttributeError: 'dict' object has no attribute 'strip'
</code></pre>

<p>But when I try to change the type from a dict so, say, a string like</p>

<pre><code>metadata=""{key:value}""
</code></pre>

<p>I then get a different error, basically saying it expects a dict</p>

<pre><code>File ""/usr/local/lib/python3.2/dist-packages/botocore/parameters.py"", line 408, in validate
type_name='map', param=self)
botocore.exceptions.ValidationError: Invalid value (key:value) for param map:Metadata of type map
</code></pre>
",0,1392233805,python;amazon-s3;aws-cli;botocore,True,470,1,1401901144,https://stackoverflow.com/questions/21737883/changing-an-objects-metadata-with-botocore
