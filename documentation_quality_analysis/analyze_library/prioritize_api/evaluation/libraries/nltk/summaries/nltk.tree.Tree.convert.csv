,id,has_api,HAS_API,DISCUSSES_API,Unnamed: 4,Unnamed: 5,discusses_api_result,lexrank_summary,LSA_summary
0,24002485,False,False,False,,https://stackoverflow.com/questions/24002485,True,"python: How to use POS (part of speech) features in scikit learn classfiers (SVM) etc I want to use part of speech (POS) returned from nltk.pos_tag for sklearn classifier, How can I convert them to vector and use it? e.g. sent = ""This is POS example"" tok=nltk.tokenize.word_tokenize(sent) pos=nltk.pos_tag(tok) print (pos) This returns following [('This', 'DT'), ('is', 'VBZ'), ('POS', 'NNP'), ('example', 'NN')] Now I am unable to apply any of the vectorizer (DictVectorizer, or FeatureHasher, CountVectorizer from scikitlearn to use in classifier Pls suggest","python: How to use POS (part of speech) features in scikit learn classfiers (SVM) etc I want to use part of speech (POS) returned from nltk.pos_tag for sklearn classifier, How can I convert them to vector and use it? e.g. sent = ""This is POS example"" tok=nltk.tokenize.word_tokenize(sent) pos=nltk.pos_tag(tok) print (pos) This returns following [('This', 'DT'), ('is', 'VBZ'), ('POS', 'NNP'), ('example', 'NN')] Now I am unable to apply any of the vectorizer (DictVectorizer, or FeatureHasher, CountVectorizer from scikitlearn to use in classifier Pls suggest"
1,39241709,False,False,False,,https://stackoverflow.com/questions/39241709,False,"How to generate bi/tri-grams using spacy/nltk The input text are always list of dish names where there are 1~3 adjectives  and a noun Inputs outputs: Basically, I am looking to parse the sentence tree and try to generate bi-grams by pairing an adjective with the noun. And I would like to achieve this with spacy or nltk","How to generate bi/tri-grams using spacy/nltk The input text are always list of dish names where there are 1~3 adjectives  and a noun Inputs outputs: Basically, I am looking to parse the sentence tree and try to generate bi-grams by pairing an adjective with the noun. And I would like to achieve this with spacy or nltk"
2,19057837,False,False,False,,https://stackoverflow.com/questions/19057837,False,How to generate multiple parse trees for an ambiguous sentence in NLTK? I have the following code in Python.,How to generate multiple parse trees for an ambiguous sentence in NLTK? I have the following code in Python.
3,50828262,False,False,False,,https://stackoverflow.com/questions/50828262,False,"I looked at the source and found it was just using nltk and wrapping that, so I used that instead, and it worked. The structure for nltk training set needed to be a list of tuples, with the first part was a Counter of words in the text and frequency of appearance.","I looked at the source and found it was just using nltk and wrapping that, so I used that instead, and it worked. The structure for nltk training set needed to be a list of tuples, with the first part was a Counter of words in the text and frequency of appearance."
4,11071901,False,False,False,,https://stackoverflow.com/questions/11071901,False,"stuck in using Megam in Python ( nltk.classify.MaxentClassifier) I'm using ubuntu x64, after two days and searching all the net, still i've not been able to install Megam, i've read all information in this page http://www.cs.utah.edu/~hal/megam/ and installed x64 version of o'calm from http://packages.ubuntu.com/precise/ocaml but when i want to use ""megam"" as a classifier in python, it says: ""NLTK was unable to find the megam file! Use software specific configuration paramaters or set the MEGAM environment variable.","stuck in using Megam in Python ( nltk.classify.MaxentClassifier) I'm using ubuntu x64, after two days and searching all the net, still i've not been able to install Megam, i've read all information in this page http://www.cs.utah.edu/~hal/megam/ and installed x64 version of o'calm from http://packages.ubuntu.com/precise/ocaml but when i want to use ""megam"" as a classifier in python, it says: ""NLTK was unable to find the megam file! Use software specific configuration paramaters or set the MEGAM environment variable."
5,19373296,False,False,False,,https://stackoverflow.com/questions/19373296,False,"Consequences of abusing nltk's word_tokenize(sent) I'm attempting to split a paragraph into words. I've got the lovely nltk.tokenize.word_tokenize(sent) on hand, but help(word_tokenize) says, ""This tokenizer is designed to work on a sentence at a time.""","Consequences of abusing nltk's word_tokenize(sent) I'm attempting to split a paragraph into words. I've got the lovely nltk.tokenize.word_tokenize(sent) on hand, but help(word_tokenize) says, ""This tokenizer is designed to work on a sentence at a time."""
6,42394335,False,False,False,,https://stackoverflow.com/questions/42394335,False,"Many of the solutions listed on SO are simple path configs as can be found here but I think this issue related to pathing in Lambda: How to config nltk data directory from code? NLTK stores a lot of it's data in a nltk_data folder locally, however including this folder within the lambda zip for upload, it doesn't seem to find it.","Many of the solutions listed on SO are simple path configs as can be found here but I think this issue related to pathing in Lambda: How to config nltk data directory from code? NLTK stores a lot of it's data in a nltk_data folder locally, however including this folder within the lambda zip for upload, it doesn't seem to find it."
7,25072167,False,False,False,,https://stackoverflow.com/questions/25072167,False,Split Text into paragraphs NLTK - usage of nltk.tokenize.texttiling? I was looking at methods to split documents into paragraphs and I came across texttiling as one possible way to do this.,Split Text into paragraphs NLTK - usage of nltk.tokenize.texttiling? I was looking at methods to split documents into paragraphs and I came across texttiling as one possible way to do this.
8,39144991,False,False,False,,https://stackoverflow.com/questions/39144991,False,"Code: Output: ['U', '. ', 'A', 'Count', 'U', '.","Code: Output: ['U', '. ', 'A', 'Count', 'U', '."
9,32830867,False,False,False,,https://stackoverflow.com/questions/32830867,False,Converting Unicoded text to readable text in Python I have Unicode text as follows How do I change this to a readable format by converting the codes '\u0___' in to the relevant readable characters. Same issue have with the output from where grammar1 is nltk.grammar.CFG Output is as follows.,Converting Unicoded text to readable text in Python I have Unicode text as follows How do I change this to a readable format by converting the codes '\u0___' in to the relevant readable characters. Same issue have with the output from where grammar1 is nltk.grammar.CFG Output is as follows.
10,43193018,False,False,False,,https://stackoverflow.com/questions/43193018,False,How to split text into paragraphs using NLTK nltk.tokenize.texttiling? How can I return these 5 sections separately using nltk texttiling?,How to split text into paragraphs using NLTK nltk.tokenize.texttiling? How can I return these 5 sections separately using nltk texttiling?
11,29416355,False,False,False,,https://stackoverflow.com/questions/29416355,False,In nltk tree how can I access a parent from a child? Suppose I had a variable that holds a tree of nltk tree class.,In nltk tree how can I access a parent from a child? Suppose I had a variable that holds a tree of nltk tree class.
12,36516697,False,False,False,,https://stackoverflow.com/questions/36516697,False,Not able to install nltk data on django app on elastic beanstalk I am using nltk_tokenize in an django app . To do the same I need to do nltk data download so that I can use it for stemming .,Not able to install nltk data on django app on elastic beanstalk I am using nltk_tokenize in an django app . To do the same I need to do nltk data download so that I can use it for stemming .
13,33901232,True,False,False,,https://stackoverflow.com/questions/33901232,True,How to convert from Tree type to String type in Python by nltk? Using this code I able to extract the leaves of the tree.,How to convert from Tree type to String type in Python by nltk? Using this code I able to extract the leaves of the tree.
14,34535591,False,False,False,,https://stackoverflow.com/questions/34535591,False,"I would like to implement something like the following. I would also like to get the childs of a node, for example, if I have get_childs(positionOf('ADJP')) it should return position('RB') and positionOf('JJ').","I would like to implement something like the following. I would also like to get the childs of a node, for example, if I have get_childs(positionOf('ADJP')) it should return position('RB') and positionOf('JJ')."
15,38681698,False,False,False,,https://stackoverflow.com/questions/38681698,False,"HMM loaded from pickle looks untrained I am trying to serialise nltk.tag.hmm.HiddenMarkovModelTagger into a pickle to use it when needed without re-training. However, after loading from .pkl my HMM looks untrained.","HMM loaded from pickle looks untrained I am trying to serialise nltk.tag.hmm.HiddenMarkovModelTagger into a pickle to use it when needed without re-training. However, after loading from .pkl my HMM looks untrained."
16,27203429,False,False,False,,https://stackoverflow.com/questions/27203429,False,"Extending a class in nltk. - python The aim is to add additional functions to the wordnet class in nltk, e.g.: but it gives an error: So I tried with nltk.corpus.reader.WordNetCorpusReader (http://www.nltk.org/_modules/nltk/corpus/reader/wordnet.html#WordNetCorpusReader): Still it seems like if I'm using WordNetCorpusReader, I need to instantiate it, so I got: Then I tried: [out]: How do I extend the nltk wordnet API with new functions?","Extending a class in nltk. - python The aim is to add additional functions to the wordnet class in nltk, e.g.: but it gives an error: So I tried with nltk.corpus.reader.WordNetCorpusReader (http://www.nltk.org/_modules/nltk/corpus/reader/wordnet.html#WordNetCorpusReader): Still it seems like if I'm using WordNetCorpusReader, I need to instantiate it, so I got: Then I tried: [out]: How do I extend the nltk wordnet API with new functions?"
17,35690892,False,False,False,,https://stackoverflow.com/questions/35690892,False,"Great, so I just have to change the variables. (Here we go) __step2_suffixes – Suffixes to be deleted in step 2 of the algorithm.","Great, so I just have to change the variables. (Here we go) __step2_suffixes – Suffixes to be deleted in step 2 of the algorithm."
