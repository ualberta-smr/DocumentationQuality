"Paragraph","Tasks"
"The most likely cause of these errors is that one or more of the important jar files is missing. If it occurs when loading the models, make sure the current models file is in the classpath. The basic models file has a name like stanford-corenlp-V.V.V-models.jar, depending on the version. For other language models, you may also need additional models jars, which will have the language name in them. If you encounter this exception when trying to produce XML output, make sure xom.jar is included. Finally, if it seems to occur when loading SUTime, be sure to include joda-time.jar, etc.","load models
produce XML output
include xom.jar
load SUTime
include joda-time.jar"
"Basically, you want to include all of the jar files in the download directory unless you are sure a particular jar is not needed.","include  in download directory"
"A brief demo program included with the download will demonstrate how to load the tool and start processing text. When using this demo program, be sure to include all of the appropriate jar files in the classpath.","load tool
include  with download
use demo program"
"Once you have tried this, there is quite a bit of information on the CoreNLP home page describing what Annotators are available, what annotations they add to the text, and what options they support.","support options
add  to text"
"By default, it uses Unicode’s UTF-8. You can change the encoding used when reading files by either setting the Java encoding property or more simply by supplying the program with the command line flag -encoding FOO (or including the corresponding property in a properties file that you are using).","use UTF-8 by default
change encoding
set Java encoding property by supplying
encode FOO"
"Either give CoreNLP more memory, use fewer annotators, or give CoreNLP smaller documents. Nearly all our annotators load large model files which use lots of memory. Running the full CoreNLP pipeline requires the sum of all these memory requirements. Typically, this means that CoreNLP needs about 2GB to run the entire pipeline. Additionally, the coreference module operates over an entire document. Unless things are size-limited, as either sentence length or document size increases, processing time and space increase without bound.","use fewer annotators
load large model files
use lots of memory
use large model files of memory
run full CoreNLP pipeline
run entire pipeline"
"Running from the command line, you need to supply a flag like -Xmx2g. If running CoreNLP from within Eclipse, follow these instructions to increase the memory given to a program being run from inside Eclipse. Increasing the amount of memory given to Eclipse itself won’t help.","run  from command line
run CoreNLP
run  from inside eclipse"
"This is part of SUTime. It applies to repeating events such as “every other week” or “every two weeks”. SET is not the best name for such an event, but it matches the TIMEX3 standard (see section 2.3 of the linked document)","repeat events such_as other week
repeat events such_as weeks
match timex3 standard"
"Other than English, we currently provide trained CoreNLP models for Chinese. To run CoreNLP on Chinese text, you first have to download the models, which can be found in our release history. Include this .jar in your classpath, and use the StanfordCoreNLP-chinese.properties file it contains to process Chinese. For example, if you put the .jar in your distribution directory, you could run (adjusting the .jar version file extensions to your current release): java -cp stanford-corenlp-VV.jar:stanford-chinese-corenlp-VV-models.jar -Xmx3g edu.stanford.nlp.pipeline.StanfordCoreNLP -props StanfordCoreNLP-chinese.properties -file your-chinese-file.txt","provide trained CoreNLP models for chinese
run CoreNLP on Chinese text
download models
use StanfordCoreNLP-chinese.properties file
process chinese"
"The straightforward case is if you have an older version of a Stanford NLP tool. For example, you may still have a version of Stanford NER on your classpath that was released in 2009. In this case, you should upgrade, or at least use matching versions. For any releases from 2011 on, just use tools released at the same time – such as the most recent version of everything :) – and they will all be compatible and play nicely together.","release version of Stanford NER
use matching versions
release  at same time
release  for releases
play  at same time
play  for releases"
"The tricky case of this is when people distribute jar files that hide other people’s classes inside them. People think this will make it easy for users, since they can distribute one jar that has everything you need, but, in practice, as soon as people are building applications using multiple components, this results in a particular bad form of jar hell. People just shouldn’t do this. The only way to check that other jar files do not contain conflicting versions of Stanford tools is to look at what is inside them (for example, with the jar -tf command).","hide classes
hide jar files
build applications
use multiple components"
"In practice, if you’re having problems, the most common cause (in 2013-2014) is that you have ark-tweet-nlp on your classpath. The jar file in their github download hides old versions of many other people’s jar files, including Apache commons-codec (v1.4), commons-lang, commons-math, commons-io, Lucene; Twitter commons; Google Guava (v10); Jackson; Berkeley NLP code; Percy Liang’s fig; GNU trove; and an outdated version of the Stanford POS tagger (from 2011). You should complain to them for creating you and us grief. But you can then fix the problem by using their jar file from Maven Central. It doesn’t have all those other libraries stuffed inside.","hide old versions including twitter commons
hide old versions including GNU trove
hide old versions including google Guava
hide old versions including Apache lucene
hide old versions including jackson
hide old versions including outdated version
hide old versions including berkeley NLP code
hide old versions including percy liang s fig
hide old versions of jar files
use jar file from Maven central"
"You need to add the flag -parse.flags """" (or the corresponding property parse.flags:   ). It’s sort of a misfeature/bug that the default properties of CoreNLP turn this option on by default, because it is useful for English, but it isn’t defined for other languages, and so you get an error.)","add flag -parse.flags
get error
define  for other languages"
"The parser can be instructed to keep certain sets of tokens together as a single constituent. If you do this, it will try to make a parse which contains a subtree where the exact set of tokens in that subtree are the ones specified in the constraint.","specify  in constraint"
"For any sentence where you want to add constraints, attach the ParserAnnotations.ConstraintAnnotation to that sentence. This annotation is a List, where ParserConstraint specifies the start (inclusive) and end (exclusive) of the range and a pattern which the enclosing constituent must match. However, there is a bug in the way patterns are handled in the parser, so it is strongly recommended to use .* for the matching pattern.","add constraints
attach ParserAnnotations.ConstraintAnnotation for sentence
attach ParserAnnotations.ConstraintAnnotation to sentence
specify start of range
specify start of pattern
specify end of range
specify end of pattern
specify list of range
specify list of pattern
match start of range
match start of pattern
match end of range
match end of pattern
handle patterns in parser
handle way in parser"
"If you want CoreNLP to output the original Stanford Dependencies instead of the new Universal Dependencies, simply add the option -parse.originalDependencies or the property (""parse.originalDependencies"", true) to your command or code, respectively.","add option -parse.originalDependencies to command
add option -parse.originalDependencies to code
add property to command
add property to code"
"Note, however, that some annotators that use dependencies such as natlog might not function properly if you use this option. In case you are using the Neural Network Dependency Parser, use the following model to get Stanford Dependencies:","use option
use neural Network dependency Parser
use following model
get Stanford dependencies"
