Paragraph,Example,Page
Some uses of CoreNLP don’t need much time or space. It can just tokenize and sentence split text using very little time and space. It can do this on the sample text while giving Java just 20MB of memory:,"java -mx20m -cp ""$STANFORD_CORENLP_HOME/*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit -file James-Joyce-Ulysses-ch13.txt -outputFormat text",https://stanfordnlp.github.io/CoreNLP/memory-time.html
"Even at the command-line, if you have one thousand paragraph-long files named para1.txt, para2.txt, … then you will get much faster processing by doing this:","ls -1 para*.txt > all-files.txt
java -cp ""$STANFORD_CORENLP_HOME/*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -filelist all-files.txt -outputFormat json",https://stanfordnlp.github.io/CoreNLP/memory-time.html
"Even at the command-line, if you have one thousand paragraph-long files named para1.txt, para2.txt, … then you will get much faster processing by doing this:","java -cp ""$STANFORD_CORENLP_HOME/*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -file para1.txt -outputFormat json
java -cp ""$STANFORD_CORENLP_HOME/*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -file para2.txt -outputFormat json
...
java -cp ""$STANFORD_CORENLP_HOME/*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -file para1000.txt -outputFormat json",https://stanfordnlp.github.io/CoreNLP/memory-time.html
"Many people run Stanford CoreNLP with its default annotators, by just using a simple command-line like:","java -cp ""$STANFORD_CORENLP_HOME/*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -file James-Joyce-Ulysses-ch13.txt -outputFormat json",https://stanfordnlp.github.io/CoreNLP/memory-time.html
"Because of different default annotator choices, if you you try to process this file with the default annotation pipeline from earlier releases of CoreNLP v.3, most likely, it will just fail from lack of memory or your patience will run out before it finishes. In v.3.6, you need more than 20GB of RAM to run the default model with default options on this text. The default statistical coreference in v.3.6 was just too slow to run fully on large documents like this!","java -cp ""$STANFORD_CORENLP_v360_HOME/*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators ""tokenize,ssplit,pos,lemma,ner,parse"" -file James-Joyce-Ulysses-ch13.txt -outputFormat json",https://stanfordnlp.github.io/CoreNLP/memory-time.html
"Returning to v.3.7.0 and continuing with turning annotators off, if all you need are parts of speech and named entities, you should run a pipeline like this:","java -cp ""$STANFORD_CORENLP_HOME/*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators ""tokenize,ssplit,pos,lemma,ner"" -file James-Joyce-Ulysses-ch13.txt -outputFormat json",https://stanfordnlp.github.io/CoreNLP/memory-time.html
"Returning to v.3.7.0 and continuing with turning annotators off, if all you need are parts of speech and named entities, you should run a pipeline like this:","java -cp ""$STANFORD_CORENLP_HOME/*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,ner -ner.model edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz -ner.useSUTime false -ner.applyNumericClassifiers false -file James-Joyce-Ulysses-ch13.txt -outputFormat json",https://stanfordnlp.github.io/CoreNLP/memory-time.html
"For 2., the only thing you can do is to either remove annotators that you do not need or to make choices for smaller annotators. These models are what fills the large models jar. They are even larger when they are uncompressed and represented in memory. Here are some examples.","java -mx2g -cp ""$STANFORD_CORENLP_HOME/*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators ""tokenize,ssplit,pos,lemma,ner,depparse"" -file James-Joyce-Ulysses-ch13.txt -outputFormat text",https://stanfordnlp.github.io/CoreNLP/memory-time.html
"For 2., the only thing you can do is to either remove annotators that you do not need or to make choices for smaller annotators. These models are what fills the large models jar. They are even larger when they are uncompressed and represented in memory. Here are some examples.","java -mx4g -cp ""$STANFORD_CORENLP_HOME/*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators ""tokenize,ssplit,pos,lemma,ner,depparse,mention,coref"" -file James-Joyce-Ulysses-ch13.txt -outputFormat text",https://stanfordnlp.github.io/CoreNLP/memory-time.html
"For 2., the only thing you can do is to either remove annotators that you do not need or to make choices for smaller annotators. These models are what fills the large models jar. They are even larger when they are uncompressed and represented in memory. Here are some examples.","java -mx1g -cp ""$STANFORD_CORENLP_HOME/*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators ""tokenize,ssplit,pos,lemma,ner"" -file James-Joyce-Ulysses-ch13.txt -outputFormat text",https://stanfordnlp.github.io/CoreNLP/memory-time.html
