Paragraph,Ground truth tasks,Program tasks,Partial Ratio
"A processing interface for assigning a tag to each token in a list. Tags are case sensitive strings that identify some property of each token, such as its part of speech or its sense.","assign tag, identify property such as part of speech",assign tag,100
"A processing interface for assigning a tag to each token in a list. Tags are case sensitive strings that identify some property of each token, such as its part of speech or its sense.","assign tag, identify property such as part of speech",identify property such_as part,97
"A processing interface for assigning a tag to each token in a list. Tags are case sensitive strings that identify some property of each token, such as its part of speech or its sense.","assign tag, identify property such as part of speech",identify case sensitive strings such_as part,52
"Score the accuracy of the tagger against the gold standard. Strip the tags from the gold standard text, retag it using the tagger, then compute the accuracy score.",compute accuracy score,use tagger,50
"Score the accuracy of the tagger against the gold standard. Strip the tags from the gold standard text, retag it using the tagger, then compute the accuracy score.",compute accuracy score,compute accuracy score,100
"Determine the most appropriate tag sequence for the given token sequence, and return a corresponding list of tagged tokens. A tagged token is encoded as a tuple (token, tag).","determine appropriate tag sequence for given token sequence, return corresponding list of tagged tokens, encode tagged token as tuple",determine appropriate tag sequence for given token sequence,100
"Determine the most appropriate tag sequence for the given token sequence, and return a corresponding list of tagged tokens. A tagged token is encoded as a tuple (token, tag).","determine appropriate tag sequence for given token sequence, return corresponding list of tagged tokens, encode tagged token as tuple",return corresponding list of tagged tokens,100
"Determine the most appropriate tag sequence for the given token sequence, and return a corresponding list of tagged tokens. A tagged token is encoded as a tuple (token, tag).","determine appropriate tag sequence for given token sequence, return corresponding list of tagged tokens, encode tagged token as tuple",encode tagged token as tuple (token,80
"Determine the most appropriate tag sequence for the given token sequence, and return a corresponding list of tagged tokens. A tagged token is encoded as a tuple (token, tag).","determine appropriate tag sequence for given token sequence, return corresponding list of tagged tokens, encode tagged token as tuple", tag),80
"Brill taggers can be created directly, from an initial tagger and a list of transformational rules; but more often, Brill taggers are created by learning rules from a training corpus, using one of the TaggerTrainers available.",create brill tagger,learn rules from training corpus,37
"Brill taggers can be created directly, from an initial tagger and a list of transformational rules; but more often, Brill taggers are created by learning rules from a training corpus, using one of the TaggerTrainers available.",create brill tagger,create brill taggers from initial tagger,100
"Brill taggers can be created directly, from an initial tagger and a list of transformational rules; but more often, Brill taggers are created by learning rules from a training corpus, using one of the TaggerTrainers available.",create brill tagger,create brill taggers from list,100
"Brill taggers can be created directly, from an initial tagger and a list of transformational rules; but more often, Brill taggers are created by learning rules from a training corpus, using one of the TaggerTrainers available.",create brill tagger,create brill taggers,100
"Print a list of all templates, ranked according to efficiency.",print list of templates,print list of templates,100
"If test_stats is available, the templates are ranked according to their relative contribution (summed for all rules created from a given template, weighted by score) to the performance on the test set. If no test_stats, then statistics collected during training are used instead. There is also an unweighted measure (just counting the rules). This is less informative, though, as many low-score rules will appear towards end of training.",use statistics ,use statistics,100
"printunused (bool) – if True, print a list of all unused templates",print unused templates,print list of unused templates,82
Return the ordered list of transformation rules that this tagger has learnt,order list of transformation rules,order list of transformation rules,100
"Determine the most appropriate tag sequence for the given token sequence, and return a corresponding list of tagged tokens. A tagged token is encoded as a tuple (token, tag).","determine appropriate tag sequence for given token sequence, return corresponding list of tagged tokens, encode tagged token as tuple",determine appropriate tag sequence for given token sequence,100
"Determine the most appropriate tag sequence for the given token sequence, and return a corresponding list of tagged tokens. A tagged token is encoded as a tuple (token, tag).","determine appropriate tag sequence for given token sequence, return corresponding list of tagged tokens, encode tagged token as tuple",return corresponding list of tagged tokens,100
"Determine the most appropriate tag sequence for the given token sequence, and return a corresponding list of tagged tokens. A tagged token is encoded as a tuple (token, tag).","determine appropriate tag sequence for given token sequence, return corresponding list of tagged tokens, encode tagged token as tuple",encode tagged token as tuple (token,80
"Determine the most appropriate tag sequence for the given token sequence, and return a corresponding list of tagged tokens. A tagged token is encoded as a tuple (token, tag).","determine appropriate tag sequence for given token sequence, return corresponding list of tagged tokens, encode tagged token as tuple", tag),80
"Return a named statistic collected during training, or a dictionary of all available statistics if no name given","return named statistic of available statistics, return dictionary of available statistics",return named statistic of available statistics,100
"Return a named statistic collected during training, or a dictionary of all available statistics if no name given","return named statistic of available statistics, return dictionary of available statistics",return dictionary of available statistics,100
"Print the available template sets in this demo, with a short description”",print available templates,set  with short description,32
"Print the available template sets in this demo, with a short description”",print available templates,set  in demo,33
min_score (int) – stop training when no rules better than min_score can be found,find rules,find rules,100
the learned tagger,learn tagger,learn tagger,100
Train a new model using ``train’’ function,use train,use train,100
Use the pre-trained model which is set via ``set_model_file’’ function,"use pre-trained model, set pre-trained model via set_model_file function",use pre-trained model,100
Use the pre-trained model which is set via ``set_model_file’’ function,"use pre-trained model, set pre-trained model via set_model_file function",set pre-trained model via set_model_file function,100
Train a new model using ``train’’ function,use train,use train,100
Use the pre-trained model which is set via ``set_model_file’’ function,"use pre-trained model, set pre-trained model via set_model_file function",use pre-trained model,100
Use the pre-trained model which is set via ``set_model_file’’ function,"use pre-trained model, set pre-trained model via set_model_file function",set pre-trained model via set_model_file function,100
"Train the CRF tagger using CRFSuite :params train_data : is the list of annotated sentences. :type train_data : list (list(tuple(str,str))) :params model_file : the model will be saved to this file.","use crfsuite, save model to file",use crfsuite,100
"Train the CRF tagger using CRFSuite :params train_data : is the list of annotated sentences. :type train_data : list (list(tuple(str,str))) :params model_file : the model will be saved to this file.","use crfsuite, save model to file",save model to file,100
Returns the state sequence of the optimal (most probable) path through the HMM. Uses the Viterbi algorithm to calculate this part by dynamic programming.,return state sequence of optimal path,return state sequence through hmm,76
Returns the state sequence of the optimal (most probable) path through the HMM. Uses the Viterbi algorithm to calculate this part by dynamic programming.,return state sequence of optimal path,return state sequence of optimal path,100
Returns the state sequence of the optimal (most probable) path through the HMM. Uses the Viterbi algorithm to calculate this part by dynamic programming.,return state sequence of optimal path,calculate part by dynamic programming,31
Returns the entropy over labellings of the given sequence. This is given by:,return entropy over labellings of given sequence,return entropy over labellings,100
"The order of summation for the log terms can be flipped, allowing dynamic programming to be used to calculate the entropy. Specifically, we use the forward and backward probabilities (alpha, beta) giving:",calculate entropy,calculate entropy,100
"This simply uses alpha and beta to find the probabilities of partial sequences, constrained to include the given state(s) at some point in time.",find probabilities of partial sequences,find probabilities of partial sequences,100
"This simply uses alpha and beta to find the probabilities of partial sequences, constrained to include the given state(s) at some point in time.",find probabilities of partial sequences,include given state at point,39
"Returns the log-probability of the given symbol sequence. If the sequence is labelled, then returns the joint log-probability of the symbol, state sequence. Otherwise, uses the forward algorithm to find the log-probability over all label sequences.","return log-probability of given symbol sequence, return joint log-probability of symbol, use forward algorithm, find log-probability over label sequences",return log-probability of given symbol sequence,100
"Returns the log-probability of the given symbol sequence. If the sequence is labelled, then returns the joint log-probability of the symbol, state sequence. Otherwise, uses the forward algorithm to find the log-probability over all label sequences.","return log-probability of given symbol sequence, return joint log-probability of symbol, use forward algorithm, find log-probability over label sequences",return joint log-probability of symbol,100
"Returns the log-probability of the given symbol sequence. If the sequence is labelled, then returns the joint log-probability of the symbol, state sequence. Otherwise, uses the forward algorithm to find the log-probability over all label sequences.","return log-probability of given symbol sequence, return joint log-probability of symbol, use forward algorithm, find log-probability over label sequences",use forward algorithm,100
"Returns the log-probability of the given symbol sequence. If the sequence is labelled, then returns the joint log-probability of the symbol, state sequence. Otherwise, uses the forward algorithm to find the log-probability over all label sequences.","return log-probability of given symbol sequence, return joint log-probability of symbol, use forward algorithm, find log-probability over label sequences",find log-probability over label sequences,100
"Returns the pointwise entropy over the possible states at each position in the chain, given the observation sequence.",return pointwise entropy over possible states,return pointwise entropy over possible states,100
"Returns the probability of the given symbol sequence. If the sequence is labelled, then returns the joint probability of the symbol, state sequence. Otherwise, uses the forward algorithm to find the probability over all label sequences.","return probability of given symbol sequence, return joint probability of symbol, find probability over label sequence",return probability of given symbol sequence,100
"Returns the probability of the given symbol sequence. If the sequence is labelled, then returns the joint probability of the symbol, state sequence. Otherwise, uses the forward algorithm to find the probability over all label sequences.","return probability of given symbol sequence, return joint probability of symbol, find probability over label sequence",return joint probability of symbol,100
"Returns the probability of the given symbol sequence. If the sequence is labelled, then returns the joint probability of the symbol, state sequence. Otherwise, uses the forward algorithm to find the probability over all label sequences.","return probability of given symbol sequence, return joint probability of symbol, find probability over label sequence",use forward algorithm,38
"Returns the probability of the given symbol sequence. If the sequence is labelled, then returns the joint probability of the symbol, state sequence. Otherwise, uses the forward algorithm to find the probability over all label sequences.","return probability of given symbol sequence, return joint probability of symbol, find probability over label sequence",find probability over label sequences,99
"Randomly sample the HMM to generate a sentence of a given length. This samples the prior distribution then the observation distribution and transition distribution for each subsequent observation and state. This will mostly generate unintelligible garbage, but can provide some amusement.","generate sentence of given length, generate unintelligible garbage",generate sentence of given length,100
"Randomly sample the HMM to generate a sentence of a given length. This samples the prior distribution then the observation distribution and transition distribution for each subsequent observation and state. This will mostly generate unintelligible garbage, but can provide some amusement.","generate sentence of given length, generate unintelligible garbage",generate unintelligible garbage,100
"Randomly sample the HMM to generate a sentence of a given length. This samples the prior distribution then the observation distribution and transition distribution for each subsequent observation and state. This will mostly generate unintelligible garbage, but can provide some amusement.","generate sentence of given length, generate unintelligible garbage",provide amusement,41
Tags the sequence with the highest probability state sequence. This uses the best_path method to find the Viterbi path.,"tag highest probability state sequence, find viterbi path",find viterbi path,100
verbose (bool) – boolean flag indicating whether training should be verbose or include printed output,include printed output,include printed output,100
Train a new HiddenMarkovModelTagger using the given labeled and unlabeled training instances. Testing will be performed if test instances are provided.,"use given labeled, use unlabeled training instances, perform testing, provide test instances",use given labeled,100
Train a new HiddenMarkovModelTagger using the given labeled and unlabeled training instances. Testing will be performed if test instances are provided.,"use given labeled, use unlabeled training instances, perform testing, provide test instances",use unlabeled training instances,100
Train a new HiddenMarkovModelTagger using the given labeled and unlabeled training instances. Testing will be performed if test instances are provided.,"use given labeled, use unlabeled training instances, perform testing, provide test instances",perform testing,100
Train a new HiddenMarkovModelTagger using the given labeled and unlabeled training instances. Testing will be performed if test instances are provided.,"use given labeled, use unlabeled training instances, perform testing, provide test instances",provide test instances,100
verbose (bool) – boolean flag indicating whether training should be verbose or include printed output,include printed output,include printed output,100
"Creates an HMM trainer to induce an HMM with the given states and output symbol alphabet. A supervised and unsupervised training method may be used. If either of the states or symbols are not given, these may be derived from supervised training.",use supervised unsupervised training method ,use supervised unsupervised training method,100
Trains the HMM using both (or either of) supervised and unsupervised techniques.,use supervised unsupervised techniques,use supervised unsupervised techniques,100
estimator – a function taking a FreqDist and a number of bins and returning a CProbDistI; otherwise a MLE estimate is used,"return cprobdistl, use mle estimate",return cprobdisti,94
estimator – a function taking a FreqDist and a number of bins and returning a CProbDistI; otherwise a MLE estimate is used,"return cprobdistl, use mle estimate",use mle estimate,100
"Trains the HMM using the Baum-Welch algorithm to maximise the probability of the data sequence. This is a variant of the EM algorithm, and is unsupervised in that it doesn’t need the state sequences for the symbols. The code is based on ‘A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition’, Lawrence Rabiner, IEEE, 1989.",use baum-welch algorithm,use baum-welch algorithm,100
Retrieve the mapping dictionary between tagsets.,retrieve mapping dictionary between tagsets,retrieve mapping dictionary between tagsets,100
Dot-product the features and current weights and return the best label.,return best label,return best label,100
Save the pickled model weights.,save pickled model weights,save pickled model weights,100
Use the pretrain model (the default constructor),use pretrain model,use pretrain model,100
"Train a model from sentences, and save it at save_loc. nr_iter controls the number of Perceptron training iterations.",save at save_loc,save  at save_loc,94
"Applies the tag method over a list of sentences. This method will return for each sentence a list of tuples of (word, tag).","return list of tuples, return list for sentence",return list of tuples,100
"Applies the tag method over a list of sentences. This method will return for each sentence a list of tuples of (word, tag).","return list of tuples, return list for sentence",return list for sentence,100
"Applies the tag method over a list of sentences. This method will return for each sentence a list of tuples of (word, tag).","return list of types, return list for sentence",return list of tuples,90
"Applies the tag method over a list of sentences. This method will return for each sentence a list of tuples of (word, tag).","return list of types, return list for sentence",return list for sentence,100
"Applies the tag method over a list of sentences. This method will return for each sentence a list of tuples of (word, tag).","return list of types, return list for sentence",return list of tuples,90
"Applies the tag method over a list of sentences. This method will return for each sentence a list of tuples of (word, tag).","return list of types, return list for sentence",return list for sentence,100
affix_length – The length of the affixes that should be considered during training and tagging. Use negative numbers for suffixes.,use negative numbers for suffixes,use negative numbers for suffixes,100
min_stem_length – Any words whose length is less than min_stem_length+abs(affix_length) will be assigned a tag of None by this tagger.,"assign tag of none, assign min_stem_length+abs(affix_length) of none",assign tag of none,100
min_stem_length – Any words whose length is less than min_stem_length+abs(affix_length) will be assigned a tag of None by this tagger.,"assign tag of none, assign min_stem_length+abs(affix_length) of none",assign min_stem_length + abs(affix_length) of none,98
Return the feature detector that this tagger uses to generate featuresets for its classifier. The feature detector is a function with the signature:,"return feature detector, generate futuresets for classifier ",return feature detector,100
Return the feature detector that this tagger uses to generate featuresets for its classifier. The feature detector is a function with the signature:,"return feature detector, generate futuresets for classifier ",generate featuresets for classifier,94
"feature_detector – A function used to generate the featureset input for the classifier:: feature_detector(tokens, index, history) -> featureset",generate featureset input for classifier,generate featureset input for classifier,100
"classifier – The classifier that should be used by the tagger. This is only useful if you want to manually construct the classifier; normally, you would use train instead.","use classifier, use train",use classifier,100
"classifier – The classifier that should be used by the tagger. This is only useful if you want to manually construct the classifier; normally, you would use train instead.","use classifier, use train",use train,100
"backoff – A backoff tagger, used if this tagger is unable to determine a tag for a given token.",determine tag for given token,determine tag for given token,100
index (int) – The index of the word whose tag should be returned.,"return index, return word",return tag,70
index (int) – The index of the word whose tag should be returned.,"return index, return word",return word,100
Return the classifier that this tagger uses to choose a tag for each word in a sentence. The input for this classifier is generated using this tagger’s feature detector. See feature_detector(),"return classifier, choose tag for word, use feature detector, generate input for classifier",return classifier,100
Return the classifier that this tagger uses to choose a tag for each word in a sentence. The input for this classifier is generated using this tagger’s feature detector. See feature_detector(),"return classifier, choose tag for word, use feature detector, generate input for classifier",choose tag for word,100
Return the classifier that this tagger uses to choose a tag for each word in a sentence. The input for this classifier is generated using this tagger’s feature detector. See feature_detector(),"return classifier, choose tag for word, use feature detector, generate input for classifier",use feature detector,100
Return the classifier that this tagger uses to choose a tag for each word in a sentence. The input for this classifier is generated using this tagger’s feature detector. See feature_detector(),"return classifier, choose tag for word, use feature detector, generate input for classifier",generate input for classifier,100
Return the feature detector that this tagger uses to generate featuresets for its classifier. The feature detector is a function with the signature:,"return classifier, generate featuresets for classifier",return feature detector,61
Return the feature detector that this tagger uses to generate featuresets for its classifier. The feature detector is a function with the signature:,"return classifier, generate featuresets for classifier",generate featuresets for classifier,100
index (int) – The index of the word whose tag should be returned.,"return index, return word",return tag,70
index (int) – The index of the word whose tag should be returned.,"return index, return word",return word,100
"cutoff – If the most likely tag for a context occurs fewer than cutoff times, then exclude it from the context-to-tag table for the new tagger.",exclude from context-to-tag table,exclude  from context-to-tag table,97
index (int) – The index of the word whose tag should be returned.,"return index, return word",return tag,70
index (int) – The index of the word whose tag should be returned.,"return index, return word",return word,100
index (int) – The index of the word whose tag should be returned.,"return index, return word",return tag,70
index (int) – The index of the word whose tag should be returned.,"return index, return word",return word,100
index (int) – The index of the word whose tag should be returned.,"return index, return word",return tag,70
index (int) – The index of the word whose tag should be returned.,"return index, return word",return word,100
