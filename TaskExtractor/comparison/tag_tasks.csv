Paragraph,Ground truth tasks,Program tasks,Partial Ratio
"A processing interface for assigning a tag to each token in a list. Tags are case sensitive strings that identify some property of each token, such as its part of speech or its sense.",assign a tag for each token in a list,assign tag,80
"A processing interface for assigning a tag to each token in a list. Tags are case sensitive strings that identify some property of each token, such as its part of speech or its sense.",assign a tag for each token in a list,identify property such_as part,37
"A processing interface for assigning a tag to each token in a list. Tags are case sensitive strings that identify some property of each token, such as its part of speech or its sense.",assign a tag for each token in a list,identify case sensitive strings such_as part,38
"Score the accuracy of the tagger against the gold standard. Strip the tags from the gold standard text, retag it using the tagger, then compute the accuracy score.",score the accuracy of the tagger against the gold standard,use tagger,80
"Score the accuracy of the tagger against the gold standard. Strip the tags from the gold standard text, retag it using the tagger, then compute the accuracy score.",score the accuracy of the tagger against the gold standard,compute accuracy score,68
"Determine the most appropriate tag sequence for the given token sequence, and return a corresponding list of tagged tokens. A tagged token is encoded as a tuple (token, tag).",determine most appropriate tagging sequence for token sequence,determine appropriate tag sequence for given token sequence,85
"Determine the most appropriate tag sequence for the given token sequence, and return a corresponding list of tagged tokens. A tagged token is encoded as a tuple (token, tag).",determine most appropriate tagging sequence for token sequence,return corresponding list of tagged tokens,45
"Determine the most appropriate tag sequence for the given token sequence, and return a corresponding list of tagged tokens. A tagged token is encoded as a tuple (token, tag).",determine most appropriate tagging sequence for token sequence,encode tagged token as tuple,54
"Brill taggers can be created directly, from an initial tagger and a list of transformational rules; but more often, Brill taggers are created by learning rules from a training corpus, using one of the TaggerTrainers available.",how to use brill tagger,learn rules from training corpus,39
"Brill taggers can be created directly, from an initial tagger and a list of transformational rules; but more often, Brill taggers are created by learning rules from a training corpus, using one of the TaggerTrainers available.",how to use brill tagger,create brill taggers from initial tagger,65
"Brill taggers can be created directly, from an initial tagger and a list of transformational rules; but more often, Brill taggers are created by learning rules from a training corpus, using one of the TaggerTrainers available.",how to use brill tagger,create brill taggers from list,65
"Brill taggers can be created directly, from an initial tagger and a list of transformational rules; but more often, Brill taggers are created by learning rules from a training corpus, using one of the TaggerTrainers available.",how to use brill tagger,create brill taggers,77
"Print a list of all templates, ranked according to efficiency.",print templates ranked by efficiency,print list of templates,65
"If test_stats is available, the templates are ranked according to their relative contribution (summed for all rules created from a given template, weighted by score) to the performance on the test set. If no test_stats, then statistics collected during training are used instead. There is also an unweighted measure (just counting the rules). This is less informative, though, as many low-score rules will appear towards end of training.",rank templates,use statistics,36
"printunused (bool) – if True, print a list of all unused templates",print unused templates,print list of unused templates,82
Return the ordered list of transformation rules that this tagger has learnt,return ordered list of transformation rules,order list of transformation rules,94
"Determine the most appropriate tag sequence for the given token sequence, and return a corresponding list of tagged tokens. A tagged token is encoded as a tuple (token, tag).",determine most appropriate tag sequence,determine appropriate tag sequence for given token sequence,87
"Determine the most appropriate tag sequence for the given token sequence, and return a corresponding list of tagged tokens. A tagged token is encoded as a tuple (token, tag).",determine most appropriate tag sequence,return corresponding list of tagged tokens,44
"Determine the most appropriate tag sequence for the given token sequence, and return a corresponding list of tagged tokens. A tagged token is encoded as a tuple (token, tag).",determine most appropriate tag sequence,encode tagged token as tuple,39
"Return a named statistic collected during training, or a dictionary of all available statistics if no name given",return a named statistic,return named statistic of available statistics,92
"Return a named statistic collected during training, or a dictionary of all available statistics if no name given",return a named statistic,return dictionary of available statistics,58
"Print the available template sets in this demo, with a short description”",print available template sets,set  with short description,30
"Print the available template sets in this demo, with a short description”",print available template sets,set  in demo,33
"Trains the Brill tagger on the corpus train_sents, producing at most max_rules transformations, each of which reduces the net number of errors in the corpus by at least min_score, and each of which has accuracy not lower than min_acc.",train brill tagger,produce  at most max_rules transformations,39
min_score (int) – stop training when no rules better than min_score can be found,specify min score for all rules,find rules,60
Train a new model using ``train’’ function,train new model,use train,22
Use the pre-trained model which is set via ``set_model_file’’ function,use pre-trained model,use pre-trained model,100
Use the pre-trained model which is set via ``set_model_file’’ function,use pre-trained model,set pre-trained model via set_model_file function,95
Train a new model using ``train’’ function,train new model,use train,22
Use the pre-trained model which is set via ``set_model_file’’ function,user pre-trained model,use pre-trained model,95
Use the pre-trained model which is set via ``set_model_file’’ function,user pre-trained model,set pre-trained model via set_model_file function,91
"Train the CRF tagger using CRFSuite :params train_data : is the list of annotated sentences. :type train_data : list (list(tuple(str,str))) :params model_file : the model will be saved to this file.",train crf tagger using crfsuite,use crfsuite,75
"Train the CRF tagger using CRFSuite :params train_data : is the list of annotated sentences. :type train_data : list (list(tuple(str,str))) :params model_file : the model will be saved to this file.",train crf tagger using crfsuite,save model to file,33
outputs (ConditionalProbDistI) – output probabilities; Pr(o_k | s_i) is the probability of emitting symbol k when entering state i,get output probabilities,enter state i.,43
Returns the state sequence of the optimal (most probable) path through the HMM. Uses the Viterbi algorithm to calculate this part by dynamic programming.,return state sequence of optimal path through hmm,return state sequence through hmm,76
Returns the state sequence of the optimal (most probable) path through the HMM. Uses the Viterbi algorithm to calculate this part by dynamic programming.,return state sequence of optimal path through hmm,return state sequence of optimal path,100
Returns the state sequence of the optimal (most probable) path through the HMM. Uses the Viterbi algorithm to calculate this part by dynamic programming.,return state sequence of optimal path through hmm,calculate part by dynamic programming,36
"Returns the state sequence of the optimal (most probable) path through the HMM. Uses the Viterbi algorithm to calculate this part by dynamic programming. This uses a simple, direct method, and is included for teaching purposes.",get state sequence of optimal path through hmm,return state sequence through hmm,64
"Returns the state sequence of the optimal (most probable) path through the HMM. Uses the Viterbi algorithm to calculate this part by dynamic programming. This uses a simple, direct method, and is included for teaching purposes.",get state sequence of optimal path through hmm,return state sequence of optimal path,89
"Returns the state sequence of the optimal (most probable) path through the HMM. Uses the Viterbi algorithm to calculate this part by dynamic programming. This uses a simple, direct method, and is included for teaching purposes.",get state sequence of optimal path through hmm,calculate part by dynamic programming,36
"Returns the state sequence of the optimal (most probable) path through the HMM. Uses the Viterbi algorithm to calculate this part by dynamic programming. This uses a simple, direct method, and is included for teaching purposes.",get state sequence of optimal path through hmm,use simple direct method,46
"Returns the state sequence of the optimal (most probable) path through the HMM. Uses the Viterbi algorithm to calculate this part by dynamic programming. This uses a simple, direct method, and is included for teaching purposes.",get state sequence of optimal path through hmm,include  for teaching purposes,40
Returns the entropy over labellings of the given sequence. This is given by:,returns entropy over labellings of the given sequence,return entropy over labellings,97
"The order of summation for the log terms can be flipped, allowing dynamic programming to be used to calculate the entropy. Specifically, we use the forward and backward probabilities (alpha, beta) giving:",calculate the entropy,calculate entropy,76
"This simply uses alpha and beta to find the probabilities of partial sequences, constrained to include the given state(s) at some point in time.",find probabilities of partial sequences,find probabilities of partial sequences,100
"This simply uses alpha and beta to find the probabilities of partial sequences, constrained to include the given state(s) at some point in time.",find probabilities of partial sequences,include given state at point,39
"Returns the log-probability of the given symbol sequence. If the sequence is labelled, then returns the joint log-probability of the symbol, state sequence. Otherwise, uses the forward algorithm to find the log-probability over all label sequences.",get log-probability of given symbol sequence,return log-probability of given symbol sequence,93
"Returns the log-probability of the given symbol sequence. If the sequence is labelled, then returns the joint log-probability of the symbol, state sequence. Otherwise, uses the forward algorithm to find the log-probability over all label sequences.",get log-probability of given symbol sequence,return joint log-probability of symbol,74
"Returns the log-probability of the given symbol sequence. If the sequence is labelled, then returns the joint log-probability of the symbol, state sequence. Otherwise, uses the forward algorithm to find the log-probability over all label sequences.",get log-probability of given symbol sequence,use forward algorithm,38
"Returns the log-probability of the given symbol sequence. If the sequence is labelled, then returns the joint log-probability of the symbol, state sequence. Otherwise, uses the forward algorithm to find the log-probability over all label sequences.",get log-probability of given symbol sequence,find log-probability over label sequences,78
"Returns the pointwise entropy over the possible states at each position in the chain, given the observation sequence.",get pointwise entropy,return pointwise entropy over possible states,86
"Returns the probability of the given symbol sequence. If the sequence is labelled, then returns the joint probability of the symbol, state sequence. Otherwise, uses the forward algorithm to find the probability over all label sequences.",get probability of given symbol sequence,return probability of given symbol sequence,92
"Returns the probability of the given symbol sequence. If the sequence is labelled, then returns the joint probability of the symbol, state sequence. Otherwise, uses the forward algorithm to find the probability over all label sequences.",get probability of given symbol sequence,return joint probability of symbol,71
"Returns the probability of the given symbol sequence. If the sequence is labelled, then returns the joint probability of the symbol, state sequence. Otherwise, uses the forward algorithm to find the probability over all label sequences.",get probability of given symbol sequence,use forward algorithm,33
"Returns the probability of the given symbol sequence. If the sequence is labelled, then returns the joint probability of the symbol, state sequence. Otherwise, uses the forward algorithm to find the probability over all label sequences.",get probability of given symbol sequence,find probability over label sequences,76
"Randomly sample the HMM to generate a sentence of a given length. This samples the prior distribution then the observation distribution and transition distribution for each subsequent observation and state. This will mostly generate unintelligible garbage, but can provide some amusement.",randomly sample hmm to generate a sentence,generate sentence of given length,52
"Randomly sample the HMM to generate a sentence of a given length. This samples the prior distribution then the observation distribution and transition distribution for each subsequent observation and state. This will mostly generate unintelligible garbage, but can provide some amusement.",randomly sample hmm to generate a sentence,generate unintelligible garbage,42
"Randomly sample the HMM to generate a sentence of a given length. This samples the prior distribution then the observation distribution and transition distribution for each subsequent observation and state. This will mostly generate unintelligible garbage, but can provide some amusement.",randomly sample hmm to generate a sentence,provide amusement,47
Tags the sequence with the highest probability state sequence. This uses the best_path method to find the Viterbi path.,tag sequence with highest probability state sequence,find viterbi path,41
verbose (bool) – boolean flag indicating whether training should be verbose or include printed output,set verbose,include printed output,36
Train a new HiddenMarkovModelTagger using the given labeled and unlabeled training instances. Testing will be performed if test instances are provided.,train hidenmarkovmodeltagger,use given labeled,35
Train a new HiddenMarkovModelTagger using the given labeled and unlabeled training instances. Testing will be performed if test instances are provided.,train hidenmarkovmodeltagger,use unlabeled training instances,39
Train a new HiddenMarkovModelTagger using the given labeled and unlabeled training instances. Testing will be performed if test instances are provided.,train hidenmarkovmodeltagger,perform testing,40
Train a new HiddenMarkovModelTagger using the given labeled and unlabeled training instances. Testing will be performed if test instances are provided.,train hidenmarkovmodeltagger,provide test instances,36
verbose (bool) – boolean flag indicating whether training should be verbose or include printed output,make verbose,include printed output,33
"Creates an HMM trainer to induce an HMM with the given states and output symbol alphabet. A supervised and unsupervised training method may be used. If either of the states or symbols are not given, these may be derived from supervised training.",create hmm trainer,use supervised unsupervised training method,44
Trains the HMM using both (or either of) supervised and unsupervised techniques.,train hmm ,use supervised unsupervised techniques,30
estimator – a function taking a FreqDist and a number of bins and returning a CProbDistI; otherwise a MLE estimate is used,estimate the cprobdistl,return cprobdisti,71
estimator – a function taking a FreqDist and a number of bins and returning a CProbDistI; otherwise a MLE estimate is used,estimate the cprobdistl,use mle estimate,50
"Trains the HMM using the Baum-Welch algorithm to maximise the probability of the data sequence. This is a variant of the EM algorithm, and is unsupervised in that it doesn’t need the state sequences for the symbols. The code is based on ‘A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition’, Lawrence Rabiner, IEEE, 1989.",train hmm using baum-welch algorithm,use baum-welch algorithm,88
Retrieve the mapping dictionary between tagsets.,get mapping dictionary between tagsets,retrieve mapping dictionary between tagsets,95
Dot-product the features and current weights and return the best label.,dot-product the features and current weights,return best label,53
Save the pickled model weights.,save pickled model weights,save pickled model weights,100
Use the pretrain model (the default constructor),use pretrained model,use pretrain model,89
"Train a model from sentences, and save it at save_loc. nr_iter controls the number of Perceptron training iterations.",train a model and save it,save  at save_loc,48
"Applies the tag method over a list of sentences. This method will return for each sentence a list of tuples of (word, tag).",apply tag method over list of sentences,return list of tuples,67
"Applies the tag method over a list of sentences. This method will return for each sentence a list of tuples of (word, tag).",apply tag method over list of sentences,return list for sentence,75
"Applies the tag method over a list of sentences. This method will return for each sentence a list of tuples of (word, tag).",apply tag method over list of sentences,return list of tuples,67
"Applies the tag method over a list of sentences. This method will return for each sentence a list of tuples of (word, tag).",apply tag method over list of sentences,return list for sentence,75
"Applies the tag method over a list of sentences. This method will return for each sentence a list of tuples of (word, tag).",apply tag method over list of sentences,return list of tuples,67
"Applies the tag method over a list of sentences. This method will return for each sentence a list of tuples of (word, tag).",apply tag method over list of sentences,return list for sentence,75
affix_length – The length of the affixes that should be considered during training and tagging. Use negative numbers for suffixes.,get length of affixes considered during training and tagging,use negative numbers for suffixes,42
min_stem_length – Any words whose length is less than min_stem_length+abs(affix_length) will be assigned a tag of None by this tagger.,define minimum stem length,assign tag of none,33
min_stem_length – Any words whose length is less than min_stem_length+abs(affix_length) will be assigned a tag of None by this tagger.,define minimum stem length,assign min_stem_length + abs(affix_length) of none,62
Return the feature detector that this tagger uses to generate featuresets for its classifier. The feature detector is a function with the signature:,get the feature detector for this tagger,return feature detector,83
Return the feature detector that this tagger uses to generate featuresets for its classifier. The feature detector is a function with the signature:,get the feature detector for this tagger,generate featuresets for classifier,57
"feature_detector – A function used to generate the featureset input for the classifier:: feature_detector(tokens, index, history) -> featureset",generate featureset input for classifier,generate featureset input for classifier,100
"classifier – The classifier that should be used by the tagger. This is only useful if you want to manually construct the classifier; normally, you would use train instead.",specify classifier that should be used by the tagger,use classifier,79
"classifier – The classifier that should be used by the tagger. This is only useful if you want to manually construct the classifier; normally, you would use train instead.",specify classifier that should be used by the tagger,use train,56
"backoff – A backoff tagger, used if this tagger is unable to determine a tag for a given token.",specify a backoff tagger,determine tag for given token,38
index (int) – The index of the word whose tag should be returned.,get index of word whose tag should be returned,return tag,60
index (int) – The index of the word whose tag should be returned.,get index of word whose tag should be returned,return word,64
Return the classifier that this tagger uses to choose a tag for each word in a sentence. The input for this classifier is generated using this tagger’s feature detector. See feature_detector(),get classifier for the tagger,return classifier,76
Return the classifier that this tagger uses to choose a tag for each word in a sentence. The input for this classifier is generated using this tagger’s feature detector. See feature_detector(),get classifier for the tagger,choose tag for word,42
Return the classifier that this tagger uses to choose a tag for each word in a sentence. The input for this classifier is generated using this tagger’s feature detector. See feature_detector(),get classifier for the tagger,use feature detector,45
Return the classifier that this tagger uses to choose a tag for each word in a sentence. The input for this classifier is generated using this tagger’s feature detector. See feature_detector(),get classifier for the tagger,generate input for classifier,48
Return the feature detector that this tagger uses to generate featuresets for its classifier. The feature detector is a function with the signature:,get feature detector that the tagger uses,return feature detector,83
Return the feature detector that this tagger uses to generate featuresets for its classifier. The feature detector is a function with the signature:,get feature detector that the tagger uses,generate featuresets for classifier,51
index (int) – The index of the word whose tag should be returned.,get index of word whose tag should be returned,return tag,60
index (int) – The index of the word whose tag should be returned.,get index of word whose tag should be returned,return word,64
index (int) – The index of the word whose tag should be returned.,get index of word whose tag should be returned,return tag,60
index (int) – The index of the word whose tag should be returned.,get index of word whose tag should be returned,return word,64
"cutoff – If the most likely tag for a context occurs fewer than cutoff times, then exclude it from the context-to-tag table for the new tagger.",define a cutoff to use the backoff,exclude  from context-to-tag table,32
index (int) – The index of the word whose tag should be returned.,get index of word whose tag should be returned,return tag,60
index (int) – The index of the word whose tag should be returned.,get index of word whose tag should be returned,return word,64
index (int) – The index of the word whose tag should be returned.,get index of word whose tag should be returned,return tag,60
index (int) – The index of the word whose tag should be returned.,get index of word whose tag should be returned,return word,64
index (int) – The index of the word whose tag should be returned.,get index of word whose tag should be returned,return tag,60
index (int) – The index of the word whose tag should be returned.,get index of word whose tag should be returned,return word,64
