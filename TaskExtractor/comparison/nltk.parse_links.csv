Paragraph,Ground Truth link,Program link,Has Tasks,Partial ratio
Tag a list of tokens.,">>> parser = corenlpparser(url='http://localhost:9000', tagtype='ner')
>>> tokens = 'rami eid is studying at stony brook university in ny'.split()
>>> parser.tag(tokens)
[('rami', 'person'), ('eid', 'person'), ('is', 'o'), ('studying', 'o'), ('at', 'o'), ('stony', 'organization'),
('brook', 'organization'), ('university', 'organization'), ('in', 'o'), ('ny', 'o')]",">>> parser = corenlpparser(url='http://localhost:9000', tagtype='ner')
>>> tokens = 'rami eid is studying at stony brook university in ny'.split()
>>> parser.tag(tokens)
[('rami', 'person'), ('eid', 'person'), ('is', 'o'), ('studying', 'o'), ('at', 'o'), ('stony', 'organization'),
('brook', 'organization'), ('university', 'organization'), ('in', 'o'), ('ny', 'o')]",FALSE,99
Tag a list of tokens.,">>> parser = corenlpparser(url='http://localhost:9000', tagtype='ner')
>>> tokens = 'rami eid is studying at stony brook university in ny'.split()
>>> parser.tag(tokens)
[('rami', 'person'), ('eid', 'person'), ('is', 'o'), ('studying', 'o'), ('at', 'o'), ('stony', 'organization'),
('brook', 'organization'), ('university', 'organization'), ('in', 'o'), ('ny', 'o')]",">>> parser = corenlpparser(url='http://localhost:9000', tagtype='pos')
>>> tokens = ""what is the airspeed of an unladen swallow ?"".split()
>>> parser.tag(tokens)
[('what', 'wp'), ('is', 'vbz'), ('the', 'dt'),
('airspeed', 'nn'), ('of', 'in'), ('an', 'dt'),
('unladen', 'jj'), ('swallow', 'vb'), ('?', '.')]",FALSE,68
Tag a list of tokens.,">>> parser = corenlpparser(url='http://localhost:9000', tagtype='pos')
>>> tokens = ""what is the airspeed of an unladen swallow ?"".split()
>>> parser.tag(tokens)
[('what', 'wp'), ('is', 'vbz'), ('the', 'dt'),
('airspeed', 'nn'), ('of', 'in'), ('an', 'dt'),
('unladen', 'jj'), ('swallow', 'vb'), ('?', '.')]",">>> parser = corenlpparser(url='http://localhost:9000', tagtype='ner')
>>> tokens = 'rami eid is studying at stony brook university in ny'.split()
>>> parser.tag(tokens)
[('rami', 'person'), ('eid', 'person'), ('is', 'o'), ('studying', 'o'), ('at', 'o'), ('stony', 'organization'),
('brook', 'organization'), ('university', 'organization'), ('in', 'o'), ('ny', 'o')]",FALSE,68
Tag a list of tokens.,">>> parser = corenlpparser(url='http://localhost:9000', tagtype='pos')
>>> tokens = ""what is the airspeed of an unladen swallow ?"".split()
>>> parser.tag(tokens)
[('what', 'wp'), ('is', 'vbz'), ('the', 'dt'),
('airspeed', 'nn'), ('of', 'in'), ('an', 'dt'),
('unladen', 'jj'), ('swallow', 'vb'), ('?', '.')]",">>> parser = corenlpparser(url='http://localhost:9000', tagtype='pos')
>>> tokens = ""what is the airspeed of an unladen swallow ?"".split()
>>> parser.tag(tokens)
[('what', 'wp'), ('is', 'vbz'), ('the', 'dt'),
('airspeed', 'nn'), ('of', 'in'), ('an', 'dt'),
('unladen', 'jj'), ('swallow', 'vb'), ('?', '.')]",FALSE,99
Tokenize a string of text.,>>> parser = corenlpparser(url='http://localhost:9000'),>>> parser = corenlpparser(url='http://localhost:9000'),FALSE,100
Tokenize a string of text.,>>> parser = corenlpparser(url='http://localhost:9000'),">>> text = 'good muffins cost $3.88\nin new york.  please buy me\ntwo of them.\nthanks.'
>>> list(parser.tokenize(text))
['good', 'muffins', 'cost', '$', '3.88', 'in', 'new', 'york', '.', 'please', 'buy', 'me', 'two', 'of', 'them', '.', 'thanks', '.']",FALSE,20
Tokenize a string of text.,>>> parser = corenlpparser(url='http://localhost:9000'),">>> s = ""the colour of the wall is blue.""
>>> list(
...     parser.tokenize(
...         'the colour of the wall is blue.',
...             properties={'tokenize.options': 'americanize=true'},
...     )
... )
['the', 'color', 'of', 'the', 'wall', 'is', 'blue', '.']",FALSE,23
Tokenize a string of text.,">>> text = 'good muffins cost $3.88\nin new york.  please buy me\ntwo of them.\nthanks.'
>>> list(parser.tokenize(text))
['good', 'muffins', 'cost', '$', '3.88', 'in', 'new', 'york', '.', 'please', 'buy', 'me', 'two', 'of', 'them', '.', 'thanks', '.']",>>> parser = corenlpparser(url='http://localhost:9000'),FALSE,20
Tokenize a string of text.,">>> text = 'good muffins cost $3.88\nin new york.  please buy me\ntwo of them.\nthanks.'
>>> list(parser.tokenize(text))
['good', 'muffins', 'cost', '$', '3.88', 'in', 'new', 'york', '.', 'please', 'buy', 'me', 'two', 'of', 'them', '.', 'thanks', '.']",">>> text = 'good muffins cost $3.88\nin new york.  please buy me\ntwo of them.\nthanks.'
>>> list(parser.tokenize(text))
['good', 'muffins', 'cost', '$', '3.88', 'in', 'new', 'york', '.', 'please', 'buy', 'me', 'two', 'of', 'them', '.', 'thanks', '.']",FALSE,100
Tokenize a string of text.,">>> text = 'good muffins cost $3.88\nin new york.  please buy me\ntwo of them.\nthanks.'
>>> list(parser.tokenize(text))
['good', 'muffins', 'cost', '$', '3.88', 'in', 'new', 'york', '.', 'please', 'buy', 'me', 'two', 'of', 'them', '.', 'thanks', '.']",">>> s = ""the colour of the wall is blue.""
>>> list(
...     parser.tokenize(
...         'the colour of the wall is blue.',
...             properties={'tokenize.options': 'americanize=true'},
...     )
... )
['the', 'color', 'of', 'the', 'wall', 'is', 'blue', '.']",FALSE,47
Tokenize a string of text.,">>> s = ""the colour of the wall is blue.""
>>> list(
...     parser.tokenize(
...         'the colour of the wall is blue.',
...             properties={'tokenize.options': 'americanize=true'},
...     )
... )
['the', 'color', 'of', 'the', 'wall', 'is', 'blue', '.']",>>> parser = corenlpparser(url='http://localhost:9000'),FALSE,23
Tokenize a string of text.,">>> s = ""the colour of the wall is blue.""
>>> list(
...     parser.tokenize(
...         'the colour of the wall is blue.',
...             properties={'tokenize.options': 'americanize=true'},
...     )
... )
['the', 'color', 'of', 'the', 'wall', 'is', 'blue', '.']",">>> text = 'good muffins cost $3.88\nin new york.  please buy me\ntwo of them.\nthanks.'
>>> list(parser.tokenize(text))
['good', 'muffins', 'cost', '$', '3.88', 'in', 'new', 'york', '.', 'please', 'buy', 'me', 'two', 'of', 'them', '.', 'thanks', '.']",FALSE,47
Tokenize a string of text.,">>> s = ""the colour of the wall is blue.""
>>> list(
...     parser.tokenize(
...         'the colour of the wall is blue.',
...             properties={'tokenize.options': 'americanize=true'},
...     )
... )
['the', 'color', 'of', 'the', 'wall', 'is', 'blue', '.']",">>> s = ""the colour of the wall is blue.""
>>> list(
...     parser.tokenize(
...         'the colour of the wall is blue.',
...             properties={'tokenize.options': 'americanize=true'},
...     )
... )
['the', 'color', 'of', 'the', 'wall', 'is', 'blue', '.']",FALSE,99
Return a dot representation suitable for using with Graphviz.,">>> dg = dependencygraph(
...     'john n 2\n'
...     'loves v 0\n'
...     'mary n 2'
... )
>>> print(dg.to_dot())
digraph g{
edge [dir=forward]
node [shape=plaintext]

0 [label=""0 (none)""]
0 -> 2 [label=""root""]
1 [label=""1 (john)""]
2 [label=""2 (loves)""]
2 -> 1 [label=""""]
2 -> 3 [label=""""]
3 [label=""3 (mary)""]
}",">>> dg = dependencygraph(
...     'john n 2\n'
...     'loves v 0\n'
...     'mary n 2'
... )
>>> print(dg.to_dot())
digraph g{
edge [dir=forward]
node [shape=plaintext]

0 [label=""0 (none)""]
0 -> 2 [label=""root""]
1 [label=""1 (john)""]
2 [label=""2 (loves)""]
2 -> 1 [label=""""]
2 -> 3 [label=""""]
3 [label=""3 (mary)""]
}",FALSE,97
Class for measuring labelled and unlabelled attachment score for dependency parsing. Note that the evaluation ignores punctuation.,">>> from nltk.parse import dependencygraph, dependencyevaluator",">>> from nltk.parse import dependencygraph, dependencyevaluator",FALSE,100
Class for measuring labelled and unlabelled attachment score for dependency parsing. Note that the evaluation ignores punctuation.,">>> from nltk.parse import dependencygraph, dependencyevaluator",">>> gold_sent = dependencygraph(""""""
... pierre  nnp     2       nmod
... vinken  nnp     8       sub
... ,       ,       2       p
... 61      cd      5       nmod
... years   nns     6       amod
... old     jj      2       nmod
... ,       ,       2       p
... will    md      0       root
... join    vb      8       vc
... the     dt      11      nmod
... board   nn      9       obj
... as      in      9       vmod
... a       dt      15      nmod
... nonexecutive    jj      15      nmod
... director        nn      12      pmod
... nov.    nnp     9       vmod
... 29      cd      16      nmod
... .       .       9       vmod
... """""")",FALSE,19
Class for measuring labelled and unlabelled attachment score for dependency parsing. Note that the evaluation ignores punctuation.,">>> from nltk.parse import dependencygraph, dependencyevaluator",">>> parsed_sent = dependencygraph(""""""
... pierre  nnp     8       nmod
... vinken  nnp     1       sub
... ,       ,       3       p
... 61      cd      6       nmod
... years   nns     6       amod
... old     jj      2       nmod
... ,       ,       3       amod
... will    md      0       root
... join    vb      8       vc
... the     dt      11      amod
... board   nn      9       object
... as      in      9       nmod
... a       dt      15      nmod
... nonexecutive    jj      15      nmod
... director        nn      12      pmod
... nov.    nnp     9       vmod
... 29      cd      16      nmod
... .       .       9       vmod
... """""")",FALSE,19
Class for measuring labelled and unlabelled attachment score for dependency parsing. Note that the evaluation ignores punctuation.,">>> from nltk.parse import dependencygraph, dependencyevaluator",">>> de = dependencyevaluator([parsed_sent],[gold_sent])
>>> las, uas = de.eval()
>>> las
0.6...
>>> uas
0.8...
>>> abs(uas - 0.8) < 0.00001
true",FALSE,32
Class for measuring labelled and unlabelled attachment score for dependency parsing. Note that the evaluation ignores punctuation.,">>> gold_sent = dependencygraph(""""""
... pierre  nnp     2       nmod
... vinken  nnp     8       sub
... ,       ,       2       p
... 61      cd      5       nmod
... years   nns     6       amod
... old     jj      2       nmod
... ,       ,       2       p
... will    md      0       root
... join    vb      8       vc
... the     dt      11      nmod
... board   nn      9       obj
... as      in      9       vmod
... a       dt      15      nmod
... nonexecutive    jj      15      nmod
... director        nn      12      pmod
... nov.    nnp     9       vmod
... 29      cd      16      nmod
... .       .       9       vmod
... """""")",">>> from nltk.parse import dependencygraph, dependencyevaluator",FALSE,18
Class for measuring labelled and unlabelled attachment score for dependency parsing. Note that the evaluation ignores punctuation.,">>> gold_sent = dependencygraph(""""""
... pierre  nnp     2       nmod
... vinken  nnp     8       sub
... ,       ,       2       p
... 61      cd      5       nmod
... years   nns     6       amod
... old     jj      2       nmod
... ,       ,       2       p
... will    md      0       root
... join    vb      8       vc
... the     dt      11      nmod
... board   nn      9       obj
... as      in      9       vmod
... a       dt      15      nmod
... nonexecutive    jj      15      nmod
... director        nn      12      pmod
... nov.    nnp     9       vmod
... 29      cd      16      nmod
... .       .       9       vmod
... """""")",">>> gold_sent = dependencygraph(""""""
... pierre  nnp     2       nmod
... vinken  nnp     8       sub
... ,       ,       2       p
... 61      cd      5       nmod
... years   nns     6       amod
... old     jj      2       nmod
... ,       ,       2       p
... will    md      0       root
... join    vb      8       vc
... the     dt      11      nmod
... board   nn      9       obj
... as      in      9       vmod
... a       dt      15      nmod
... nonexecutive    jj      15      nmod
... director        nn      12      pmod
... nov.    nnp     9       vmod
... 29      cd      16      nmod
... .       .       9       vmod
... """""")",FALSE,98
Class for measuring labelled and unlabelled attachment score for dependency parsing. Note that the evaluation ignores punctuation.,">>> gold_sent = dependencygraph(""""""
... pierre  nnp     2       nmod
... vinken  nnp     8       sub
... ,       ,       2       p
... 61      cd      5       nmod
... years   nns     6       amod
... old     jj      2       nmod
... ,       ,       2       p
... will    md      0       root
... join    vb      8       vc
... the     dt      11      nmod
... board   nn      9       obj
... as      in      9       vmod
... a       dt      15      nmod
... nonexecutive    jj      15      nmod
... director        nn      12      pmod
... nov.    nnp     9       vmod
... 29      cd      16      nmod
... .       .       9       vmod
... """""")",">>> parsed_sent = dependencygraph(""""""
... pierre  nnp     8       nmod
... vinken  nnp     1       sub
... ,       ,       3       p
... 61      cd      6       nmod
... years   nns     6       amod
... old     jj      2       nmod
... ,       ,       3       amod
... will    md      0       root
... join    vb      8       vc
... the     dt      11      amod
... board   nn      9       object
... as      in      9       nmod
... a       dt      15      nmod
... nonexecutive    jj      15      nmod
... director        nn      12      pmod
... nov.    nnp     9       vmod
... 29      cd      16      nmod
... .       .       9       vmod
... """""")",FALSE,94
Class for measuring labelled and unlabelled attachment score for dependency parsing. Note that the evaluation ignores punctuation.,">>> gold_sent = dependencygraph(""""""
... pierre  nnp     2       nmod
... vinken  nnp     8       sub
... ,       ,       2       p
... 61      cd      5       nmod
... years   nns     6       amod
... old     jj      2       nmod
... ,       ,       2       p
... will    md      0       root
... join    vb      8       vc
... the     dt      11      nmod
... board   nn      9       obj
... as      in      9       vmod
... a       dt      15      nmod
... nonexecutive    jj      15      nmod
... director        nn      12      pmod
... nov.    nnp     9       vmod
... 29      cd      16      nmod
... .       .       9       vmod
... """""")",">>> de = dependencyevaluator([parsed_sent],[gold_sent])
>>> las, uas = de.eval()
>>> las
0.6...
>>> uas
0.8...
>>> abs(uas - 0.8) < 0.00001
true",FALSE,26
Class for measuring labelled and unlabelled attachment score for dependency parsing. Note that the evaluation ignores punctuation.,">>> parsed_sent = dependencygraph(""""""
... pierre  nnp     8       nmod
... vinken  nnp     1       sub
... ,       ,       3       p
... 61      cd      6       nmod
... years   nns     6       amod
... old     jj      2       nmod
... ,       ,       3       amod
... will    md      0       root
... join    vb      8       vc
... the     dt      11      amod
... board   nn      9       object
... as      in      9       nmod
... a       dt      15      nmod
... nonexecutive    jj      15      nmod
... director        nn      12      pmod
... nov.    nnp     9       vmod
... 29      cd      16      nmod
... .       .       9       vmod
... """""")",">>> from nltk.parse import dependencygraph, dependencyevaluator",FALSE,18
Class for measuring labelled and unlabelled attachment score for dependency parsing. Note that the evaluation ignores punctuation.,">>> parsed_sent = dependencygraph(""""""
... pierre  nnp     8       nmod
... vinken  nnp     1       sub
... ,       ,       3       p
... 61      cd      6       nmod
... years   nns     6       amod
... old     jj      2       nmod
... ,       ,       3       amod
... will    md      0       root
... join    vb      8       vc
... the     dt      11      amod
... board   nn      9       object
... as      in      9       nmod
... a       dt      15      nmod
... nonexecutive    jj      15      nmod
... director        nn      12      pmod
... nov.    nnp     9       vmod
... 29      cd      16      nmod
... .       .       9       vmod
... """""")",">>> gold_sent = dependencygraph(""""""
... pierre  nnp     2       nmod
... vinken  nnp     8       sub
... ,       ,       2       p
... 61      cd      5       nmod
... years   nns     6       amod
... old     jj      2       nmod
... ,       ,       2       p
... will    md      0       root
... join    vb      8       vc
... the     dt      11      nmod
... board   nn      9       obj
... as      in      9       vmod
... a       dt      15      nmod
... nonexecutive    jj      15      nmod
... director        nn      12      pmod
... nov.    nnp     9       vmod
... 29      cd      16      nmod
... .       .       9       vmod
... """""")",FALSE,94
Class for measuring labelled and unlabelled attachment score for dependency parsing. Note that the evaluation ignores punctuation.,">>> parsed_sent = dependencygraph(""""""
... pierre  nnp     8       nmod
... vinken  nnp     1       sub
... ,       ,       3       p
... 61      cd      6       nmod
... years   nns     6       amod
... old     jj      2       nmod
... ,       ,       3       amod
... will    md      0       root
... join    vb      8       vc
... the     dt      11      amod
... board   nn      9       object
... as      in      9       nmod
... a       dt      15      nmod
... nonexecutive    jj      15      nmod
... director        nn      12      pmod
... nov.    nnp     9       vmod
... 29      cd      16      nmod
... .       .       9       vmod
... """""")",">>> parsed_sent = dependencygraph(""""""
... pierre  nnp     8       nmod
... vinken  nnp     1       sub
... ,       ,       3       p
... 61      cd      6       nmod
... years   nns     6       amod
... old     jj      2       nmod
... ,       ,       3       amod
... will    md      0       root
... join    vb      8       vc
... the     dt      11      amod
... board   nn      9       object
... as      in      9       nmod
... a       dt      15      nmod
... nonexecutive    jj      15      nmod
... director        nn      12      pmod
... nov.    nnp     9       vmod
... 29      cd      16      nmod
... .       .       9       vmod
... """""")",FALSE,98
Class for measuring labelled and unlabelled attachment score for dependency parsing. Note that the evaluation ignores punctuation.,">>> parsed_sent = dependencygraph(""""""
... pierre  nnp     8       nmod
... vinken  nnp     1       sub
... ,       ,       3       p
... 61      cd      6       nmod
... years   nns     6       amod
... old     jj      2       nmod
... ,       ,       3       amod
... will    md      0       root
... join    vb      8       vc
... the     dt      11      amod
... board   nn      9       object
... as      in      9       nmod
... a       dt      15      nmod
... nonexecutive    jj      15      nmod
... director        nn      12      pmod
... nov.    nnp     9       vmod
... 29      cd      16      nmod
... .       .       9       vmod
... """""")",">>> de = dependencyevaluator([parsed_sent],[gold_sent])
>>> las, uas = de.eval()
>>> las
0.6...
>>> uas
0.8...
>>> abs(uas - 0.8) < 0.00001
true",FALSE,26
Class for measuring labelled and unlabelled attachment score for dependency parsing. Note that the evaluation ignores punctuation.,">>> de = dependencyevaluator([parsed_sent],[gold_sent])
>>> las, uas = de.eval()
>>> las
0.6...
>>> uas
0.8...
>>> abs(uas - 0.8) < 0.00001
true",">>> from nltk.parse import dependencygraph, dependencyevaluator",FALSE,31
Class for measuring labelled and unlabelled attachment score for dependency parsing. Note that the evaluation ignores punctuation.,">>> de = dependencyevaluator([parsed_sent],[gold_sent])
>>> las, uas = de.eval()
>>> las
0.6...
>>> uas
0.8...
>>> abs(uas - 0.8) < 0.00001
true",">>> gold_sent = dependencygraph(""""""
... pierre  nnp     2       nmod
... vinken  nnp     8       sub
... ,       ,       2       p
... 61      cd      5       nmod
... years   nns     6       amod
... old     jj      2       nmod
... ,       ,       2       p
... will    md      0       root
... join    vb      8       vc
... the     dt      11      nmod
... board   nn      9       obj
... as      in      9       vmod
... a       dt      15      nmod
... nonexecutive    jj      15      nmod
... director        nn      12      pmod
... nov.    nnp     9       vmod
... 29      cd      16      nmod
... .       .       9       vmod
... """""")",FALSE,26
Class for measuring labelled and unlabelled attachment score for dependency parsing. Note that the evaluation ignores punctuation.,">>> de = dependencyevaluator([parsed_sent],[gold_sent])
>>> las, uas = de.eval()
>>> las
0.6...
>>> uas
0.8...
>>> abs(uas - 0.8) < 0.00001
true",">>> parsed_sent = dependencygraph(""""""
... pierre  nnp     8       nmod
... vinken  nnp     1       sub
... ,       ,       3       p
... 61      cd      6       nmod
... years   nns     6       amod
... old     jj      2       nmod
... ,       ,       3       amod
... will    md      0       root
... join    vb      8       vc
... the     dt      11      amod
... board   nn      9       object
... as      in      9       nmod
... a       dt      15      nmod
... nonexecutive    jj      15      nmod
... director        nn      12      pmod
... nov.    nnp     9       vmod
... 29      cd      16      nmod
... .       .       9       vmod
... """""")",FALSE,26
Class for measuring labelled and unlabelled attachment score for dependency parsing. Note that the evaluation ignores punctuation.,">>> de = dependencyevaluator([parsed_sent],[gold_sent])
>>> las, uas = de.eval()
>>> las
0.6...
>>> uas
0.8...
>>> abs(uas - 0.8) < 0.00001
true",">>> de = dependencyevaluator([parsed_sent],[gold_sent])
>>> las, uas = de.eval()
>>> las
0.6...
>>> uas
0.8...
>>> abs(uas - 0.8) < 0.00001
true",FALSE,98
A class for dependency parsing with MaltParser. The input is the paths to: - a maltparser directory - (optionally) the path to a pre-trained MaltParser .mco model file - (optionally) the tagger to use for POS tagging before parsing - (optionally) additional Java arguments,">>> from nltk.parse import malt
>>> # with malt_parser and malt_model environment set.
>>> mp = malt.maltparser('maltparser-1.7.2', 'engmalt.linear-1.7.mco') 
>>> mp.parse_one('i shot an elephant in my pajamas .'.split()).tree() 
(shot i (elephant an) (in (pajamas my)) .)
>>> # without malt_parser and malt_model environment.
>>> mp = malt.maltparser('/home/user/maltparser-1.7.2/', '/home/user/engmalt.linear-1.7.mco') 
>>> mp.parse_one('i shot an elephant in my pajamas .'.split()).tree() 
(shot i (elephant an) (in (pajamas my)) .)",">>> from nltk.parse import malt
>>> # with malt_parser and malt_model environment set.
>>> mp = malt.maltparser('maltparser-1.7.2', 'engmalt.linear-1.7.mco') 
>>> mp.parse_one('i shot an elephant in my pajamas .'.split()).tree() 
(shot i (elephant an) (in (pajamas my)) .)
>>> # without malt_parser and malt_model environment.
>>> mp = malt.maltparser('/home/user/maltparser-1.7.2/', '/home/user/engmalt.linear-1.7.mco') 
>>> mp.parse_one('i shot an elephant in my pajamas .'.split()).tree() 
(shot i (elephant an) (in (pajamas my)) .)",FALSE,99
"A dependency scorer built around a MaxEnt classifier. In this particular class that classifier is a NaiveBayesClassifier. It uses head-word, head-tag, child-word, and child-tag features for classification.",">>> from nltk.parse.dependencygraph import dependencygraph, conll_data2",">>> from nltk.parse.dependencygraph import dependencygraph, conll_data2",FALSE,100
"A dependency scorer built around a MaxEnt classifier. In this particular class that classifier is a NaiveBayesClassifier. It uses head-word, head-tag, child-word, and child-tag features for classification.",">>> from nltk.parse.dependencygraph import dependencygraph, conll_data2",">>> graphs = [dependencygraph(entry) for entry in conll_data2.split('\n\n') if entry]
>>> npp = probabilisticnonprojectiveparser()
>>> npp.train(graphs, naivebayesdependencyscorer())
>>> parses = npp.parse(['cathy', 'zag', 'hen', 'zwaaien', '.'], ['n', 'v', 'pron', 'adj', 'n', 'punc'])
>>> len(list(parses))
1",FALSE,27
"A dependency scorer built around a MaxEnt classifier. In this particular class that classifier is a NaiveBayesClassifier. It uses head-word, head-tag, child-word, and child-tag features for classification.",">>> graphs = [dependencygraph(entry) for entry in conll_data2.split('\n\n') if entry]
>>> npp = probabilisticnonprojectiveparser()
>>> npp.train(graphs, naivebayesdependencyscorer())
>>> parses = npp.parse(['cathy', 'zag', 'hen', 'zwaaien', '.'], ['n', 'v', 'pron', 'adj', 'n', 'punc'])
>>> len(list(parses))
1",">>> from nltk.parse.dependencygraph import dependencygraph, conll_data2",FALSE,26
"A dependency scorer built around a MaxEnt classifier. In this particular class that classifier is a NaiveBayesClassifier. It uses head-word, head-tag, child-word, and child-tag features for classification.",">>> graphs = [dependencygraph(entry) for entry in conll_data2.split('\n\n') if entry]
>>> npp = probabilisticnonprojectiveparser()
>>> npp.train(graphs, naivebayesdependencyscorer())
>>> parses = npp.parse(['cathy', 'zag', 'hen', 'zwaaien', '.'], ['n', 'v', 'pron', 'adj', 'n', 'punc'])
>>> len(list(parses))
1",">>> graphs = [dependencygraph(entry) for entry in conll_data2.split('\n\n') if entry]
>>> npp = probabilisticnonprojectiveparser()
>>> npp.train(graphs, naivebayesdependencyscorer())
>>> parses = npp.parse(['cathy', 'zag', 'hen', 'zwaaien', '.'], ['n', 'v', 'pron', 'adj', 'n', 'punc'])
>>> len(list(parses))
1",FALSE,99
"ShiftReduceParser maintains a stack, which records the structure of a portion of the text. This stack is a list of strings and Trees that collectively cover a portion of the text. For example, while parsing the sentence “the dog saw the man” with a typical grammar, ShiftReduceParser will produce the following stack, which covers “the dog saw”:","[(np: (det: 'the') (n: 'dog')), (v: 'saw')]","[(np: (det: 'the') (n: 'dog')), (v: 'saw')]",FALSE,100
"A. Check the ARC-STANDARD training >>> import tempfile >>> import os >>> input_file = tempfile.NamedTemporaryFile(prefix=’transition_parse.train’, dir=tempfile.gettempdir(), delete=False)",">>> parser_std = transitionparser('arc-standard')
>>> print(', '.join(parser_std._create_training_examples_arc_std([gold_sent], input_file)))
 number of training examples : 1
 number of valid (projective) examples : 1
shift, leftarc:att, shift, leftarc:sbj, shift, shift, leftarc:att, shift, shift, shift, leftarc:att, rightarc:pc, rightarc:att, rightarc:obj, shift, rightarc:pu, rightarc:root, shift
",">>> parser_std = transitionparser('arc-standard')
>>> print(', '.join(parser_std._create_training_examples_arc_std([gold_sent], input_file)))
 number of training examples : 1
 number of valid (projective) examples : 1
shift, leftarc:att, shift, leftarc:sbj, shift, shift, leftarc:att, shift, shift, shift, leftarc:att, rightarc:pc, rightarc:att, rightarc:obj, shift, rightarc:pu, rightarc:root, shift",FALSE,99
"A. Check the ARC-STANDARD training >>> import tempfile >>> import os >>> input_file = tempfile.NamedTemporaryFile(prefix=’transition_parse.train’, dir=tempfile.gettempdir(), delete=False)",">>> parser_std = transitionparser('arc-standard')
>>> print(', '.join(parser_std._create_training_examples_arc_std([gold_sent], input_file)))
 number of training examples : 1
 number of valid (projective) examples : 1
shift, leftarc:att, shift, leftarc:sbj, shift, shift, leftarc:att, shift, shift, shift, leftarc:att, rightarc:pc, rightarc:att, rightarc:obj, shift, rightarc:pu, rightarc:root, shift
",">>> parser_std.train([gold_sent],'temp.arcstd.model', verbose=false)
 number of training examples : 1
 number of valid (projective) examples : 1
>>> remove(input_file.name)",FALSE,46
"A. Check the ARC-STANDARD training >>> import tempfile >>> import os >>> input_file = tempfile.NamedTemporaryFile(prefix=’transition_parse.train’, dir=tempfile.gettempdir(), delete=False)",">>> parser_std = transitionparser('arc-standard')
>>> print(', '.join(parser_std._create_training_examples_arc_std([gold_sent], input_file)))
 number of training examples : 1
 number of valid (projective) examples : 1
shift, leftarc:att, shift, leftarc:sbj, shift, shift, leftarc:att, shift, shift, shift, leftarc:att, rightarc:pc, rightarc:att, rightarc:obj, shift, rightarc:pu, rightarc:root, shift
",">>> input_file = tempfile.namedtemporaryfile(prefix='transition_parse.train', dir=tempfile.gettempdir(),delete=false)
>>> parser_eager = transitionparser('arc-eager')
>>> print(', '.join(parser_eager._create_training_examples_arc_eager([gold_sent], input_file)))
 number of training examples : 1
 number of valid (projective) examples : 1
shift, leftarc:att, shift, leftarc:sbj, rightarc:root, shift, leftarc:att, rightarc:obj, rightarc:att, shift, leftarc:att, rightarc:pc, reduce, reduce, reduce, rightarc:pu",FALSE,72
"A. Check the ARC-STANDARD training >>> import tempfile >>> import os >>> input_file = tempfile.NamedTemporaryFile(prefix=’transition_parse.train’, dir=tempfile.gettempdir(), delete=False)",">>> parser_std = transitionparser('arc-standard')
>>> print(', '.join(parser_std._create_training_examples_arc_std([gold_sent], input_file)))
 number of training examples : 1
 number of valid (projective) examples : 1
shift, leftarc:att, shift, leftarc:sbj, shift, shift, leftarc:att, shift, shift, shift, leftarc:att, rightarc:pc, rightarc:att, rightarc:obj, shift, rightarc:pu, rightarc:root, shift
",">>> parser_eager.train([gold_sent],'temp.arceager.model', verbose=false)
 number of training examples : 1
 number of valid (projective) examples : 1",FALSE,42
"A. Check the ARC-STANDARD training >>> import tempfile >>> import os >>> input_file = tempfile.NamedTemporaryFile(prefix=’transition_parse.train’, dir=tempfile.gettempdir(), delete=False)",">>> parser_std = transitionparser('arc-standard')
>>> print(', '.join(parser_std._create_training_examples_arc_std([gold_sent], input_file)))
 number of training examples : 1
 number of valid (projective) examples : 1
shift, leftarc:att, shift, leftarc:sbj, shift, shift, leftarc:att, shift, shift, shift, leftarc:att, rightarc:pc, rightarc:att, rightarc:obj, shift, rightarc:pu, rightarc:root, shift
",>>> remove(input_file.name),FALSE,11
"A. Check the ARC-STANDARD training >>> import tempfile >>> import os >>> input_file = tempfile.NamedTemporaryFile(prefix=’transition_parse.train’, dir=tempfile.gettempdir(), delete=False)",">>> parser_std = transitionparser('arc-standard')
>>> print(', '.join(parser_std._create_training_examples_arc_std([gold_sent], input_file)))
 number of training examples : 1
 number of valid (projective) examples : 1
shift, leftarc:att, shift, leftarc:sbj, shift, shift, leftarc:att, shift, shift, shift, leftarc:att, rightarc:pc, rightarc:att, rightarc:obj, shift, rightarc:pu, rightarc:root, shift
",">>> result = parser_std.parse([gold_sent], 'temp.arcstd.model')
>>> de = dependencyevaluator(result, [gold_sent])
>>> de.eval() >= (0, 0)
true",FALSE,29
"A. Check the ARC-STANDARD training >>> import tempfile >>> import os >>> input_file = tempfile.NamedTemporaryFile(prefix=’transition_parse.train’, dir=tempfile.gettempdir(), delete=False)",">>> parser_std.train([gold_sent],'temp.arcstd.model', verbose=false)
 number of training examples : 1
 number of valid (projective) examples : 1
>>> remove(input_file.name)",">>> parser_std = transitionparser('arc-standard')
>>> print(', '.join(parser_std._create_training_examples_arc_std([gold_sent], input_file)))
 number of training examples : 1
 number of valid (projective) examples : 1
shift, leftarc:att, shift, leftarc:sbj, shift, shift, leftarc:att, shift, shift, shift, leftarc:att, rightarc:pc, rightarc:att, rightarc:obj, shift, rightarc:pu, rightarc:root, shift",FALSE,46
"A. Check the ARC-STANDARD training >>> import tempfile >>> import os >>> input_file = tempfile.NamedTemporaryFile(prefix=’transition_parse.train’, dir=tempfile.gettempdir(), delete=False)",">>> parser_std.train([gold_sent],'temp.arcstd.model', verbose=false)
 number of training examples : 1
 number of valid (projective) examples : 1
>>> remove(input_file.name)",">>> parser_std.train([gold_sent],'temp.arcstd.model', verbose=false)
 number of training examples : 1
 number of valid (projective) examples : 1
>>> remove(input_file.name)",FALSE,99
"A. Check the ARC-STANDARD training >>> import tempfile >>> import os >>> input_file = tempfile.NamedTemporaryFile(prefix=’transition_parse.train’, dir=tempfile.gettempdir(), delete=False)",">>> parser_std.train([gold_sent],'temp.arcstd.model', verbose=false)
 number of training examples : 1
 number of valid (projective) examples : 1
>>> remove(input_file.name)",">>> input_file = tempfile.namedtemporaryfile(prefix='transition_parse.train', dir=tempfile.gettempdir(),delete=false)
>>> parser_eager = transitionparser('arc-eager')
>>> print(', '.join(parser_eager._create_training_examples_arc_eager([gold_sent], input_file)))
 number of training examples : 1
 number of valid (projective) examples : 1
shift, leftarc:att, shift, leftarc:sbj, rightarc:root, shift, leftarc:att, rightarc:obj, rightarc:att, shift, leftarc:att, rightarc:pc, reduce, reduce, reduce, rightarc:pu",FALSE,39
"A. Check the ARC-STANDARD training >>> import tempfile >>> import os >>> input_file = tempfile.NamedTemporaryFile(prefix=’transition_parse.train’, dir=tempfile.gettempdir(), delete=False)",">>> parser_std.train([gold_sent],'temp.arcstd.model', verbose=false)
 number of training examples : 1
 number of valid (projective) examples : 1
>>> remove(input_file.name)",">>> parser_eager.train([gold_sent],'temp.arceager.model', verbose=false)
 number of training examples : 1
 number of valid (projective) examples : 1",FALSE,85
"A. Check the ARC-STANDARD training >>> import tempfile >>> import os >>> input_file = tempfile.NamedTemporaryFile(prefix=’transition_parse.train’, dir=tempfile.gettempdir(), delete=False)",">>> parser_std.train([gold_sent],'temp.arcstd.model', verbose=false)
 number of training examples : 1
 number of valid (projective) examples : 1
>>> remove(input_file.name)",>>> remove(input_file.name),FALSE,27
"A. Check the ARC-STANDARD training >>> import tempfile >>> import os >>> input_file = tempfile.NamedTemporaryFile(prefix=’transition_parse.train’, dir=tempfile.gettempdir(), delete=False)",">>> parser_std.train([gold_sent],'temp.arcstd.model', verbose=false)
 number of training examples : 1
 number of valid (projective) examples : 1
>>> remove(input_file.name)",">>> result = parser_std.parse([gold_sent], 'temp.arcstd.model')
>>> de = dependencyevaluator(result, [gold_sent])
>>> de.eval() >= (0, 0)
true",FALSE,50
A module to convert a single POS tagged sentence into CONLL format.,">>> from nltk import word_tokenize, pos_tag
>>> text = ""this is a foobar sentence.""
>>> for line in taggedsent_to_conll(pos_tag(word_tokenize(text))):
...         print(line, end="""")
    1       this    _       dt      dt      _       0       a       _       _
    2       is      _       vbz     vbz     _       0       a       _       _
    3       a       _       dt      dt      _       0       a       _       _
    4       foobar  _       jj      jj      _       0       a       _       _
    5       sentence        _       nn      nn      _       0       a       _       _
    6       .               _       .       .       _       0       a       _       _",">>> from nltk import word_tokenize, pos_tag
>>> text = ""this is a foobar sentence.""
>>> for line in taggedsent_to_conll(pos_tag(word_tokenize(text))):
...         print(line, end="""")
    1       this    _       dt      dt      _       0       a       _       _
    2       is      _       vbz     vbz     _       0       a       _       _
    3       a       _       dt      dt      _       0       a       _       _
    4       foobar  _       jj      jj      _       0       a       _       _
    5       sentence        _       nn      nn      _       0       a       _       _
    6       .               _       .       .       _       0       a       _       _",FALSE,99
"A module to convert the a POS tagged document stream (i.e. list of list of tuples, a list of sentences) and yield lines in CONLL format. This module yields one line per word and two newlines for end of sentence.",">>> from nltk import word_tokenize, sent_tokenize, pos_tag
>>> text = ""this is a foobar sentence. is that right?""
>>> sentences = [pos_tag(word_tokenize(sent)) for sent in sent_tokenize(text)]
>>> for line in taggedsents_to_conll(sentences):
...     if line:
...         print(line, end="""")
1   this    _       dt      dt      _       0       a       _       _
2   is      _       vbz     vbz     _       0       a       _       _
3   a       _       dt      dt      _       0       a       _       _
4   foobar  _       jj      jj      _       0       a       _       _
5   sentence        _       nn      nn      _       0       a       _       _
6   .               _       .       .       _       0       a       _       _


1   is      _       vbz     vbz     _       0       a       _       _
2   that    _       in      in      _       0       a       _       _
3   right   _       nn      nn      _       0       a       _       _
4   ?       _       .       .       _       0       a       _       _
",">>> from nltk import word_tokenize, sent_tokenize, pos_tag
>>> text = ""this is a foobar sentence. is that right?""
>>> sentences = [pos_tag(word_tokenize(sent)) for sent in sent_tokenize(text)]
>>> for line in taggedsents_to_conll(sentences):
...     if line:
...         print(line, end="""")
1   this    _       dt      dt      _       0       a       _       _
2   is      _       vbz     vbz     _       0       a       _       _
3   a       _       dt      dt      _       0       a       _       _
4   foobar  _       jj      jj      _       0       a       _       _
5   sentence        _       nn      nn      _       0       a       _       _
6   .               _       .       .       _       0       a       _       _


1   is      _       vbz     vbz     _       0       a       _       _
2   that    _       in      in      _       0       a       _       _
3   right   _       nn      nn      _       0       a       _       _
4   ?       _       .       .       _       0       a       _       _",FALSE,98
Dependency parser.,>>> dep_parser = corenlpdependencyparser(url='http://localhost:9000'),,FALSE
Dependency parser.,">>> parse, = dep_parser.raw_parse(
...     'the quick brown fox jumps over the lazy dog.'
... )
>>> print(parse.to_conll(4))  
the     dt      4       det
quick   jj      4       amod
brown   jj      4       amod
fox     nn      5       nsubj
jumps   vbz     0       root
over    in      9       case
the     dt      9       det
lazy    jj      9       amod
dog     nn      5       nmod
.       .       5       punct",,FALSE
Dependency parser.,">>> print(parse.tree())  
(jumps (fox the quick brown) (dog over the lazy) .)",,FALSE
Dependency parser.,">>> for governor, dep, dependent in parse.triples():
...     print(governor, dep, dependent)  
    ('jumps', 'vbz') nsubj ('fox', 'nn')
    ('fox', 'nn') det ('the', 'dt')
    ('fox', 'nn') amod ('quick', 'jj')
    ('fox', 'nn') amod ('brown', 'jj')
    ('jumps', 'vbz') nmod ('dog', 'nn')
    ('dog', 'nn') case ('over', 'in')
    ('dog', 'nn') det ('the', 'dt')
    ('dog', 'nn') amod ('lazy', 'jj')
    ('jumps', 'vbz') punct ('.', '.')
",,FALSE
Dependency parser.,">>> (parse_fox, ), (parse_dog, ) = dep_parser.raw_parse_sents(
...     [
...         'the quick brown fox jumps over the lazy dog.',
...         'the quick grey wolf jumps over the lazy fox.',
...     ]
... )
>>> print(parse_fox.to_conll(4))  
the dt      4       det
quick       jj      4       amod
brown       jj      4       amod
fox nn      5       nsubj
jumps       vbz     0       root
over        in      9       case
the dt      9       det
lazy        jj      9       amod
dog nn      5       nmod
.   .       5       punct",,FALSE
Dependency parser.,">>> print(parse_dog.to_conll(4))  
the dt      4       det
quick       jj      4       amod
grey        jj      4       amod
wolf        nn      5       nsubj
jumps       vbz     0       root
over        in      9       case
the dt      9       det
lazy        jj      9       amod
fox nn      5       nmod
.   .       5       punct",,FALSE
Dependency parser.,">>> (parse_dog, ), (parse_friends, ) = dep_parser.parse_sents(
...     [
...         ""i 'm a dog"".split(),
...         ""this is my friends ' cat ( the tabby )"".split(),
...     ]
... )
>>> print(parse_dog.to_conll(4))  
i   prp     4       nsubj
'm  vbp     4       cop
a   dt      4       det
dog nn      0       root",,FALSE
Dependency parser.,">>> print(parse_friends.to_conll(4))  
this        dt      6       nsubj
is  vbz     6       cop
my  prp$    4       nmod:poss
friends     nns     6       nmod:poss
'   pos     4       case
cat nn      0       root
-lrb-       -lrb-   9       punct
the dt      9       det
tabby       nn      6       appos
-rrb-       -rrb-   9       punct",,FALSE
Dependency parser.,">>> parse_john, parse_mary, = dep_parser.parse_text(
...     'john loves mary. mary walks.'
... )
",,FALSE
Dependency parser.,">>> print(parse_john.to_conll(4))  
john        nnp     2       nsubj
loves       vbz     0       root
mary        nnp     2       dobj
.   .       2       punct",,FALSE
Dependency parser.,">>> print(parse_mary.to_conll(4))  
mary        nnp     2       nsubj
walks       vbz     0       root
.   .       2       punct",,FALSE
Non-breaking space inside of a token.,">>> len(
...     next(
...         dep_parser.raw_parse(
...             'anhalt said children typically treat a 20-ounce soda bottle as one '
...             'serving, while it actually contains 2 1/2 servings.'
...         )
...     ).nodes
... )
21",,FALSE
Phone numbers.,">>> len(
...     next(
...         dep_parser.raw_parse('this is not going to crash: 01 111 555.')
...     ).nodes
... )
10",,FALSE
Phone numbers.,">>> print(
...     next(
...         dep_parser.raw_parse('the underscore _ should not simply disappear.')
...     ).to_conll(4)
... )  
the         dt  3   det
underscore  vbp 3   amod
_           nn  7   nsubj
should      md  7   aux
not         rb  7   neg
simply      rb  7   advmod
disappear   vb  0   root
.           .   7   punct",,FALSE
Phone numbers.,">>> print(
...     '\n'.join(
...         next(
...             dep_parser.raw_parse(
...                 'for all of its insights into the dream world of teen life , and its electronic expression through '
...                 'cyber culture , the film gives no quarter to anyone seeking to pull a cohesive story out of its 2 '
...                 '1/2-hour running time .'
...             )
...         ).to_conll(4).split('\n')[-8:]
...     )
... )
its prp$    40      nmod:poss
2 1/2       cd      40      nummod
-   :       40      punct
hour        nn      31      nmod
running     vbg     42      amod
time        nn      40      dep
.   .       24      punct",,FALSE
Bases: nltk.parse.corenlp.GenericCoreNLPParser,>>> parser = corenlpparser(url='http://localhost:9000'),,FALSE
Bases: nltk.parse.corenlp.GenericCoreNLPParser,">>> next(
...     parser.raw_parse('the quick brown fox jumps over the lazy dog.')
... ).pretty_print()  
                     root
                         |
                        s
       _________|____________________________
      |                                                vp                      |
      |                               _________|___                   |
      |                              |                        pp               |
      |                              |         ________|___            |
      np                          |         |                     np        |
  __|__________          |         |       _______|____    |
 dt   jj    jj       nn  vbz   in   dt      jj          nn  .
 |        |      |          |        |         |    |           |            |    |
the quick brown fox jumps over the     lazy     dog  .",,FALSE
Bases: nltk.parse.corenlp.GenericCoreNLPParser,">>> (parse_fox, ), (parse_wolf, ) = parser.raw_parse_sents(
...     [
...         'the quick brown fox jumps over the lazy dog.',
...         'the quick grey wolf jumps over the lazy fox.',
...     ]
... )",,FALSE
Bases: nltk.parse.corenlp.GenericCoreNLPParser,">>> parse_fox.pretty_print()  
                     root
                      |
                      s
       _______________|__________________________
      |                         vp               |
      |                _________|___             |
      |               |             pp           |
      |               |     ________|___         |
      np              |    |            np       |
  ____|__________     |    |     _______|____    |
 dt   jj    jj   nn  vbz   in   dt      jj   nn  .
 |    |     |    |    |    |    |       |    |   |
the quick brown fox jumps over the     lazy dog  .",,FALSE
Bases: nltk.parse.corenlp.GenericCoreNLPParser,">>> parse_wolf.pretty_print()  
                                  root
                                     |
                                    s
       _______________|__________________________
      |                         vp               |
      |                _________|___             |
      |               |             pp           |
      |               |     ________|___         |
      np              |    |            np       |
  ____|_________      |    |     _______|____    |
 dt   jj   jj        nn   vbz   in   dt      jj   nn  .
   |     |        |         |     |    |    |       |    |   |
the quick grey wolf jumps over the     lazy fox  .",,FALSE
Bases: nltk.parse.corenlp.GenericCoreNLPParser,">>> (parse_dog, ), (parse_friends, ) = parser.parse_sents(
...     [
...         ""i 'm a dog"".split(),
...         ""this is my friends ' cat ( the tabby )"".split(),
...     ]
... )",,FALSE
Bases: nltk.parse.corenlp.GenericCoreNLPParser,">>> parse_dog.pretty_print()  
        root
         |
         s
  _______|____
 |            vp
 |    ________|___
 np  |            np
 |   |         ___|___
prp vbp       dt      nn
 |   |        |       |
 i   'm       a      dog",,FALSE
Bases: nltk.parse.corenlp.GenericCoreNLPParser,">>> parse_friends.pretty_print()  
     root
      |
      s
  ____|___________
 |                vp
 |     ___________|_____________
 |    |                         np
 |    |                  _______|_________
 |    |                 np               prn
 |    |            _____|_______      ____|______________
 np   |           np            |    |        np         |
 |    |     ______|_________    |    |     ___|____      |
 dt  vbz  prp$   nns       pos  nn -lrb-  dt       nn  -rrb-
 |    |    |      |         |   |    |    |        |     |
this  is   my  friends      '  cat -lrb- the     tabby -rrb-",,FALSE
Bases: nltk.parse.corenlp.GenericCoreNLPParser,">>> parse_john, parse_mary, = parser.parse_text(
...     'john loves mary. mary walks.'
... )",,FALSE
Bases: nltk.parse.corenlp.GenericCoreNLPParser,">>> parse_john.pretty_print()  
      root
       |
       s
  _____|_____________
 |          vp       |
 |      ____|___     |
 np    |        np   |
 |     |        |    |
nnp   vbz      nnp   .
 |     |        |    |
john loves     mary  .",,FALSE
Bases: nltk.parse.corenlp.GenericCoreNLPParser,">>> parse_mary.pretty_print()  
      root
       |
       s
  _____|____
 np    vp   |
 |     |    |
nnp   vbz   .
 |     |    |
mary walks  .",,FALSE
Bases: nltk.parse.corenlp.GenericCoreNLPParser,">>> next(
...     parser.raw_parse(
...         'nasiriya, iraq—iraqi doctors who treated former prisoner of war '
...         'jessica lynch have angrily dismissed claims made in her biography '
...         'that she was raped by her iraqi captors.'
...     )
... ).height()
20",,FALSE
Bases: nltk.parse.corenlp.GenericCoreNLPParser,">>> next(
...     parser.raw_parse(
...         ""the broader standard & poor's 500 index <.spx> was 0.46 points lower, or ""
...         '0.05 percent, at 997.02.'
...     )
... ).height()
9",,FALSE
Check whether there are cycles.,">>> dg = dependencygraph(treebank_data)
>>> dg.contains_cycle()
false",,FALSE
Check whether there are cycles.,">>> cyclic_dg = dependencygraph()
>>> top = {'word': none, 'deps': [1], 'rel': 'top', 'address': 0}
>>> child1 = {'word': none, 'deps': [2], 'rel': 'ntop', 'address': 1}
>>> child2 = {'word': none, 'deps': [4], 'rel': 'ntop', 'address': 2}
>>> child3 = {'word': none, 'deps': [1], 'rel': 'ntop', 'address': 3}
>>> child4 = {'word': none, 'deps': [3], 'rel': 'ntop', 'address': 4}
>>> cyclic_dg.nodes = {
...     0: top,
...     1: child1,
...     2: child2,
...     3: child3,
...     4: child4,
... }
>>> cyclic_dg.root = top",,FALSE
Check whether there are cycles.,">>> cyclic_dg.contains_cycle()
[3, 1, 2, 4]",,FALSE
A probabilistic non-projective dependency parser.,">>> class scorer(dependencyscoreri):
...     def train(self, graphs):
...         pass
...
...     def score(self, graph):
...         return [
...             [[], [5],  [1],  [1]],
...             [[], [],   [11], [4]],
...             [[], [10], [],   [5]],
...             [[], [8],  [8],  []],
...         ]
",,FALSE
A probabilistic non-projective dependency parser.,">>> npp = probabilisticnonprojectiveparser()
>>> npp.train([], scorer())",,FALSE
A probabilistic non-projective dependency parser.,">>> parses = npp.parse(['v1', 'v2', 'v3'], [none, none, none])
>>> len(list(parses))
1",,FALSE
A probabilistic non-projective dependency parser.,>>> from nltk.grammar import dependencygrammar,,FALSE
A probabilistic non-projective dependency parser.,">>> grammar = dependencygrammar.fromstring('''
... 'taught' -> 'play' | 'man'
... 'man' -> 'the' | 'in'
... 'in' -> 'corner'
... 'corner' -> 'the'
... 'play' -> 'golf' | 'dachshund' | 'to'
... 'dachshund' -> 'his'
... ''')",,FALSE
A probabilistic non-projective dependency parser.,">>> ndp = nonprojectivedependencyparser(grammar)
>>> parses = ndp.parse(['the', 'man', 'in', 'the', 'corner', 'taught', 'his', 'dachshund', 'to', 'play', 'golf'])
>>> len(list(parses))
4",,FALSE
"A probabilistic, projective dependency parser.",>>> from nltk.parse.dependencygraph import conll_data2,,FALSE
"A probabilistic, projective dependency parser.",">>> graphs = [
... dependencygraph(entry) for entry in conll_data2.split('\n\n') if entry
... ]",,FALSE
"A probabilistic, projective dependency parser.",">>> ppdp = probabilisticprojectivedependencyparser()
>>> ppdp.train(graphs)",,FALSE
"A probabilistic, projective dependency parser.",">>> sent = ['cathy', 'zag', 'hen', 'wild', 'zwaaien', '.']
>>> list(ppdp.parse(sent))
[tree('zag', ['cathy', 'hen', tree('zwaaien', ['wild', '.'])])]",,FALSE
###################### Check the Initial Feature ########################,">>> print(', '.join(conf.extract_features()))
stk_0_pos_top, buf_0_form_economic, buf_0_lemma_economic, buf_0_pos_jj, buf_1_form_news, buf_1_pos_nn, buf_2_pos_vbd, buf_3_pos_jj",,FALSE
Do some transition checks for ARC-STANDARD,">>> operation = transition('arc-standard')
>>> operation.shift(conf)
>>> operation.left_arc(conf, ""att"")
>>> operation.shift(conf)
>>> operation.left_arc(conf,""sbj"")
>>> operation.shift(conf)
>>> operation.shift(conf)
>>> operation.left_arc(conf, ""att"")
>>> operation.shift(conf)
>>> operation.shift(conf)
>>> operation.shift(conf)
>>> operation.left_arc(conf, ""att"")",,FALSE
"Middle Configuration and Features Check >>> print(conf) Stack : [0, 3, 5, 6] Buffer : [8, 9] Arcs : [(2, ‘ATT’, 1), (3, ‘SBJ’, 2), (5, ‘ATT’, 4), (8, ‘ATT’, 7)]",">>> print(', '.join(conf.extract_features()))
stk_0_form_on, stk_0_lemma_on, stk_0_pos_in, stk_1_pos_nn, buf_0_form_markets, buf_0_lemma_markets, buf_0_pos_nns, buf_1_form_., buf_1_pos_., buf_0_ldep_att",,FALSE
"Middle Configuration and Features Check >>> print(conf) Stack : [0, 3, 5, 6] Buffer : [8, 9] Arcs : [(2, ‘ATT’, 1), (3, ‘SBJ’, 2), (5, ‘ATT’, 4), (8, ‘ATT’, 7)]",">>> operation.right_arc(conf, ""pc"")
>>> operation.right_arc(conf, ""att"")
>>> operation.right_arc(conf, ""obj"")
>>> operation.shift(conf)
>>> operation.right_arc(conf, ""pu"")
>>> operation.right_arc(conf, ""root"")
>>> operation.shift(conf)",,FALSE
Do some transition checks for ARC-EAGER,">>> conf = configuration(gold_sent)
>>> operation = transition('arc-eager')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'att')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'sbj')
>>> operation.right_arc(conf,'root')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'att')
>>> operation.right_arc(conf,'obj')
>>> operation.right_arc(conf,'att')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'att')
>>> operation.right_arc(conf,'pc')
>>> operation.reduce(conf)
>>> operation.reduce(conf)
>>> operation.reduce(conf)
>>> operation.right_arc(conf,'pu')
>>> print(conf)
stack : [0, 3, 9]  buffer : []   arcs : [(2, 'att', 1), (3, 'sbj', 2), (0, 'root', 3), (5, 'att', 4), (3, 'obj', 5), (5, 'att', 6), (8, 'att', 7), (6, 'pc', 8), (3, 'pu', 9)]
",,FALSE
Check the ARC-EAGER training,">>> input_file = tempfile.namedtemporaryfile(prefix='transition_parse.train', dir=tempfile.gettempdir(),delete=false)
>>> parser_eager = transitionparser('arc-eager')
>>> print(', '.join(parser_eager._create_training_examples_arc_eager([gold_sent], input_file)))
 number of training examples : 1
 number of valid (projective) examples : 1
shift, leftarc:att, shift, leftarc:sbj, rightarc:root, shift, leftarc:att, rightarc:obj, rightarc:att, shift, leftarc:att, rightarc:pc, reduce, reduce, reduce, rightarc:pu
",,FALSE
Check the ARC-EAGER training,">>> parser_eager.train([gold_sent],'temp.arceager.model', verbose=false)
 number of training examples : 1
 number of valid (projective) examples : 1",,FALSE
Check the ARC-EAGER training,>>> remove(input_file.name),,FALSE
Check the ARC-STANDARD parser,">>> result = parser_std.parse([gold_sent], 'temp.arcstd.model')
>>> de = dependencyevaluator(result, [gold_sent])
>>> de.eval() >= (0, 0)
true",,FALSE
Paragraph,,example,Page
"This parser returns the most probable projective parse derived from the
probabilistic dependency grammar derived from the train() method.  The
probabilistic model is an implementation of Eisner’s (1996) Model C, which
conditions on head-word, head-tag, child-word, and child-tag.  The decoding
uses a bottom-up chart-based span concatenation algorithm that’s identical
to the one utilized by the rule-based projective parser.",,>>> from nltk.parse.dependencygraph import conll_data2,https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"This parser returns the most probable projective parse derived from the
probabilistic dependency grammar derived from the train() method.  The
probabilistic model is an implementation of Eisner’s (1996) Model C, which
conditions on head-word, head-tag, child-word, and child-tag.  The decoding
uses a bottom-up chart-based span concatenation algorithm that’s identical
to the one utilized by the rule-based projective parser.",,">>> graphs = [
... dependencygraph(entry) for entry in conll_data2.split('\n\n') if entry
... ]",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"This parser returns the most probable projective parse derived from the
probabilistic dependency grammar derived from the train() method.  The
probabilistic model is an implementation of Eisner’s (1996) Model C, which
conditions on head-word, head-tag, child-word, and child-tag.  The decoding
uses a bottom-up chart-based span concatenation algorithm that’s identical
to the one utilized by the rule-based projective parser.",,">>> ppdp = probabilisticprojectivedependencyparser()
>>> ppdp.train(graphs)",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"This parser returns the most probable projective parse derived from the
probabilistic dependency grammar derived from the train() method.  The
probabilistic model is an implementation of Eisner’s (1996) Model C, which
conditions on head-word, head-tag, child-word, and child-tag.  The decoding
uses a bottom-up chart-based span concatenation algorithm that’s identical
to the one utilized by the rule-based projective parser.",,">>> sent = ['cathy', 'zag', 'hen', 'wild', 'zwaaien', '.']
>>> list(ppdp.parse(sent))
[tree('zag', ['cathy', 'hen', tree('zwaaien', ['wild', '.'])])]",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"###################### Check The Transition #######################
Check the Initialized Configuration
>>> print(conf)
Stack : [0]  Buffer : [1, 2, 3, 4, 5, 6, 7, 8, 9]   Arcs : []",,">>> operation = transition('arc-standard')
>>> operation.shift(conf)
>>> operation.left_arc(conf, ""att"")
>>> operation.shift(conf)
>>> operation.left_arc(conf,""sbj"")
>>> operation.shift(conf)
>>> operation.shift(conf)
>>> operation.left_arc(conf, ""att"")
>>> operation.shift(conf)
>>> operation.shift(conf)
>>> operation.shift(conf)
>>> operation.left_arc(conf, ""att"")",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"###################### Check The Transition #######################
Check the Initialized Configuration
>>> print(conf)
Stack : [0]  Buffer : [1, 2, 3, 4, 5, 6, 7, 8, 9]   Arcs : []",,">>> print(', '.join(conf.extract_features()))
stk_0_form_on, stk_0_lemma_on, stk_0_pos_in, stk_1_pos_nn, buf_0_form_markets, buf_0_lemma_markets, buf_0_pos_nns, buf_1_form_., buf_1_pos_., buf_0_ldep_att",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"###################### Check The Transition #######################
Check the Initialized Configuration
>>> print(conf)
Stack : [0]  Buffer : [1, 2, 3, 4, 5, 6, 7, 8, 9]   Arcs : []",,">>> operation.right_arc(conf, ""pc"")
>>> operation.right_arc(conf, ""att"")
>>> operation.right_arc(conf, ""obj"")
>>> operation.shift(conf)
>>> operation.right_arc(conf, ""pu"")
>>> operation.right_arc(conf, ""root"")
>>> operation.shift(conf)",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"###################### Check The Transition #######################
Check the Initialized Configuration
>>> print(conf)
Stack : [0]  Buffer : [1, 2, 3, 4, 5, 6, 7, 8, 9]   Arcs : []",,">>> conf = configuration(gold_sent)
>>> operation = transition('arc-eager')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'att')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'sbj')
>>> operation.right_arc(conf,'root')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'att')
>>> operation.right_arc(conf,'obj')
>>> operation.right_arc(conf,'att')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'att')
>>> operation.right_arc(conf,'pc')
>>> operation.reduce(conf)
>>> operation.reduce(conf)
>>> operation.reduce(conf)
>>> operation.right_arc(conf,'pu')
>>> print(conf)
stack : [0, 3, 9]  buffer : []   arcs : [(2, 'att', 1), (3, 'sbj', 2), (0, 'root', 3), (5, 'att', 4), (3, 'obj', 5), (5, 'att', 6), (8, 'att', 7), (6, 'pc', 8), (3, 'pu', 9)]",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
