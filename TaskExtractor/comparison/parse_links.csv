Paragraph,Ground Truth link,Program link,Partial ratio
Return a dot representation suitable for using with Graphviz.,">>> dg = dependencygraph(
...     'john n 2\n'
...     'loves v 0\n'
...     'mary n 2'
... )
>>> print(dg.to_dot())
digraph g{
edge [dir=forward]
node [shape=plaintext]

0 [label=""0 (none)""]
0 -> 2 [label=""root""]
1 [label=""1 (john)""]
2 [label=""2 (loves)""]
2 -> 1 [label=""""]
2 -> 3 [label=""""]
3 [label=""3 (mary)""]
}",to_dot,4
A module to convert a single POS tagged sentence into CONLL format.,">>> from nltk import word_tokenize, pos_tag
>>> text = ""this is a foobar sentence.""
>>> for line in taggedsent_to_conll(pos_tag(word_tokenize(text))):
...         print(line, end="""")
    1       this    _       dt      dt      _       0       a       _       _
    2       is      _       vbz     vbz     _       0       a       _       _
    3       a       _       dt      dt      _       0       a       _       _
    4       foobar  _       jj      jj      _       0       a       _       _
    5       sentence        _       nn      nn      _       0       a       _       _
    6       .               _       .       .       _       0       a       _       _",taggedsent_to_conll,11
Dependency parser.,>>> dep_parser = corenlpdependencyparser(url='http://localhost:9000')
Dependency parser.,">>> parse, = dep_parser.raw_parse(
...     'the quick brown fox jumps over the lazy dog.'
... )
>>> print(parse.to_conll(4))  
the     dt      4       det
quick   jj      4       amod
brown   jj      4       amod
fox     nn      5       nsubj
jumps   vbz     0       root
over    in      9       case
the     dt      9       det
lazy    jj      9       amod
dog     nn      5       nmod
.       .       5       punct"
Dependency parser.,">>> print(parse.tree())  
(jumps (fox the quick brown) (dog over the lazy) .)"
Dependency parser.,">>> for governor, dep, dependent in parse.triples():
...     print(governor, dep, dependent)  
    ('jumps', 'vbz') nsubj ('fox', 'nn')
    ('fox', 'nn') det ('the', 'dt')
    ('fox', 'nn') amod ('quick', 'jj')
    ('fox', 'nn') amod ('brown', 'jj')
    ('jumps', 'vbz') nmod ('dog', 'nn')
    ('dog', 'nn') case ('over', 'in')
    ('dog', 'nn') det ('the', 'dt')
    ('dog', 'nn') amod ('lazy', 'jj')
    ('jumps', 'vbz') punct ('.', '.')
"
Dependency parser.,">>> (parse_fox, ), (parse_dog, ) = dep_parser.raw_parse_sents(
...     [
...         'the quick brown fox jumps over the lazy dog.',
...         'the quick grey wolf jumps over the lazy fox.',
...     ]
... )
>>> print(parse_fox.to_conll(4))  
the dt      4       det
quick       jj      4       amod
brown       jj      4       amod
fox nn      5       nsubj
jumps       vbz     0       root
over        in      9       case
the dt      9       det
lazy        jj      9       amod
dog nn      5       nmod
.   .       5       punct"
Dependency parser.,">>> print(parse_dog.to_conll(4))  
the dt      4       det
quick       jj      4       amod
grey        jj      4       amod
wolf        nn      5       nsubj
jumps       vbz     0       root
over        in      9       case
the dt      9       det
lazy        jj      9       amod
fox nn      5       nmod
.   .       5       punct"
Dependency parser.,">>> (parse_dog, ), (parse_friends, ) = dep_parser.parse_sents(
...     [
...         ""i 'm a dog"".split(),
...         ""this is my friends ' cat ( the tabby )"".split(),
...     ]
... )
>>> print(parse_dog.to_conll(4))  
i   prp     4       nsubj
'm  vbp     4       cop
a   dt      4       det
dog nn      0       root"
Dependency parser.,">>> print(parse_friends.to_conll(4))  
this        dt      6       nsubj
is  vbz     6       cop
my  prp$    4       nmod:poss
friends     nns     6       nmod:poss
'   pos     4       case
cat nn      0       root
-lrb-       -lrb-   9       punct
the dt      9       det
tabby       nn      6       appos
-rrb-       -rrb-   9       punct"
Dependency parser.,">>> parse_john, parse_mary, = dep_parser.parse_text(
...     'john loves mary. mary walks.'
... )
"
Dependency parser.,">>> print(parse_john.to_conll(4))  
john        nnp     2       nsubj
loves       vbz     0       root
mary        nnp     2       dobj
.   .       2       punct"
Dependency parser.,">>> print(parse_mary.to_conll(4))  
mary        nnp     2       nsubj
walks       vbz     0       root
.   .       2       punct"
Non-breaking space inside of a token.,">>> len(
...     next(
...         dep_parser.raw_parse(
...             'anhalt said children typically treat a 20-ounce soda bottle as one '
...             'serving, while it actually contains 2 1/2 servings.'
...         )
...     ).nodes
... )
21"
Phone numbers.,">>> len(
...     next(
...         dep_parser.raw_parse('this is not going to crash: 01 111 555.')
...     ).nodes
... )
10"
Phone numbers.,">>> print(
...     next(
...         dep_parser.raw_parse('the underscore _ should not simply disappear.')
...     ).to_conll(4)
... )  
the         dt  3   det
underscore  vbp 3   amod
_           nn  7   nsubj
should      md  7   aux
not         rb  7   neg
simply      rb  7   advmod
disappear   vb  0   root
.           .   7   punct"
Phone numbers.,">>> print(
...     '\n'.join(
...         next(
...             dep_parser.raw_parse(
...                 'for all of its insights into the dream world of teen life , and its electronic expression through '
...                 'cyber culture , the film gives no quarter to anyone seeking to pull a cohesive story out of its 2 '
...                 '1/2-hour running time .'
...             )
...         ).to_conll(4).split('\n')[-8:]
...     )
... )
its prp$    40      nmod:poss
2 1/2       cd      40      nummod
-   :       40      punct
hour        nn      31      nmod
running     vbg     42      amod
time        nn      40      dep
.   .       24      punct"
Bases: nltk.parse.corenlp.GenericCoreNLPParser,>>> parser = corenlpparser(url='http://localhost:9000')
Bases: nltk.parse.corenlp.GenericCoreNLPParser,">>> next(
...     parser.raw_parse('the quick brown fox jumps over the lazy dog.')
... ).pretty_print()  
                     root
                         |
                        s
       _________|____________________________
      |                                                vp                      |
      |                               _________|___                   |
      |                              |                        pp               |
      |                              |         ________|___            |
      np                          |         |                     np        |
  __|__________          |         |       _______|____    |
 dt   jj    jj       nn  vbz   in   dt      jj          nn  .
 |        |      |          |        |         |    |           |            |    |
the quick brown fox jumps over the     lazy     dog  ."
Bases: nltk.parse.corenlp.GenericCoreNLPParser,">>> (parse_fox, ), (parse_wolf, ) = parser.raw_parse_sents(
...     [
...         'the quick brown fox jumps over the lazy dog.',
...         'the quick grey wolf jumps over the lazy fox.',
...     ]
... )"
Bases: nltk.parse.corenlp.GenericCoreNLPParser,">>> parse_fox.pretty_print()  
                     root
                      |
                      s
       _______________|__________________________
      |                         vp               |
      |                _________|___             |
      |               |             pp           |
      |               |     ________|___         |
      np              |    |            np       |
  ____|__________     |    |     _______|____    |
 dt   jj    jj   nn  vbz   in   dt      jj   nn  .
 |    |     |    |    |    |    |       |    |   |
the quick brown fox jumps over the     lazy dog  ."
Bases: nltk.parse.corenlp.GenericCoreNLPParser,">>> parse_wolf.pretty_print()  
                                  root
                                     |
                                    s
       _______________|__________________________
      |                         vp               |
      |                _________|___             |
      |               |             pp           |
      |               |     ________|___         |
      np              |    |            np       |
  ____|_________      |    |     _______|____    |
 dt   jj   jj        nn   vbz   in   dt      jj   nn  .
   |     |        |         |     |    |    |       |    |   |
the quick grey wolf jumps over the     lazy fox  ."
Bases: nltk.parse.corenlp.GenericCoreNLPParser,">>> (parse_dog, ), (parse_friends, ) = parser.parse_sents(
...     [
...         ""i 'm a dog"".split(),
...         ""this is my friends ' cat ( the tabby )"".split(),
...     ]
... )"
Bases: nltk.parse.corenlp.GenericCoreNLPParser,">>> parse_dog.pretty_print()  
        root
         |
         s
  _______|____
 |            vp
 |    ________|___
 np  |            np
 |   |         ___|___
prp vbp       dt      nn
 |   |        |       |
 i   'm       a      dog"
Bases: nltk.parse.corenlp.GenericCoreNLPParser,">>> parse_friends.pretty_print()  
     root
      |
      s
  ____|___________
 |                vp
 |     ___________|_____________
 |    |                         np
 |    |                  _______|_________
 |    |                 np               prn
 |    |            _____|_______      ____|______________
 np   |           np            |    |        np         |
 |    |     ______|_________    |    |     ___|____      |
 dt  vbz  prp$   nns       pos  nn -lrb-  dt       nn  -rrb-
 |    |    |      |         |   |    |    |        |     |
this  is   my  friends      '  cat -lrb- the     tabby -rrb-"
Bases: nltk.parse.corenlp.GenericCoreNLPParser,">>> parse_john, parse_mary, = parser.parse_text(
...     'john loves mary. mary walks.'
... )"
Bases: nltk.parse.corenlp.GenericCoreNLPParser,">>> parse_john.pretty_print()  
      root
       |
       s
  _____|_____________
 |          vp       |
 |      ____|___     |
 np    |        np   |
 |     |        |    |
nnp   vbz      nnp   .
 |     |        |    |
john loves     mary  ."
Bases: nltk.parse.corenlp.GenericCoreNLPParser,">>> parse_mary.pretty_print()  
      root
       |
       s
  _____|____
 np    vp   |
 |     |    |
nnp   vbz   .
 |     |    |
mary walks  ."
Bases: nltk.parse.corenlp.GenericCoreNLPParser,">>> next(
...     parser.raw_parse(
...         'nasiriya, iraq—iraqi doctors who treated former prisoner of war '
...         'jessica lynch have angrily dismissed claims made in her biography '
...         'that she was raped by her iraqi captors.'
...     )
... ).height()
20"
Bases: nltk.parse.corenlp.GenericCoreNLPParser,">>> next(
...     parser.raw_parse(
...         ""the broader standard & poor's 500 index <.spx> was 0.46 points lower, or ""
...         '0.05 percent, at 997.02.'
...     )
... ).height()
9"
Tag a list of tokens.,">>> parser = corenlpparser(url='http://localhost:9000', tagtype='ner')
>>> tokens = 'rami eid is studying at stony brook university in ny'.split()
>>> parser.tag(tokens)
[('rami', 'person'), ('eid', 'person'), ('is', 'o'), ('studying', 'o'), ('at', 'o'), ('stony', 'organization'),
('brook', 'organization'), ('university', 'organization'), ('in', 'o'), ('ny', 'o')]"
Tag a list of tokens.,">>> parser = corenlpparser(url='http://localhost:9000', tagtype='pos')
>>> tokens = ""what is the airspeed of an unladen swallow ?"".split()
>>> parser.tag(tokens)
[('what', 'wp'), ('is', 'vbz'), ('the', 'dt'),
('airspeed', 'nn'), ('of', 'in'), ('an', 'dt'),
('unladen', 'jj'), ('swallow', 'vb'), ('?', '.')]"
Tokenize a string of text.,>>> parser = corenlpparser(url='http://localhost:9000')
Tokenize a string of text.,">>> text = 'good muffins cost $3.88\nin new york.  please buy me\ntwo of them.\nthanks.'
>>> list(parser.tokenize(text))
['good', 'muffins', 'cost', '$', '3.88', 'in', 'new', 'york', '.', 'please', 'buy', 'me', 'two', 'of', 'them', '.', 'thanks', '.']"
Tokenize a string of text.,">>> s = ""the colour of the wall is blue.""
>>> list(
...     parser.tokenize(
...         'the colour of the wall is blue.',
...             properties={'tokenize.options': 'americanize=true'},
...     )
... )
['the', 'color', 'of', 'the', 'wall', 'is', 'blue', '.']"
Check whether there are cycles.,">>> dg = dependencygraph(treebank_data)
>>> dg.contains_cycle()
false"
Check whether there are cycles.,">>> cyclic_dg = dependencygraph()
>>> top = {'word': none, 'deps': [1], 'rel': 'top', 'address': 0}
>>> child1 = {'word': none, 'deps': [2], 'rel': 'ntop', 'address': 1}
>>> child2 = {'word': none, 'deps': [4], 'rel': 'ntop', 'address': 2}
>>> child3 = {'word': none, 'deps': [1], 'rel': 'ntop', 'address': 3}
>>> child4 = {'word': none, 'deps': [3], 'rel': 'ntop', 'address': 4}
>>> cyclic_dg.nodes = {
...     0: top,
...     1: child1,
...     2: child2,
...     3: child3,
...     4: child4,
... }
>>> cyclic_dg.root = top"
Check whether there are cycles.,">>> cyclic_dg.contains_cycle()
[3, 1, 2, 4]"
Class for measuring labelled and unlabelled attachment score for dependency parsing. Note that the evaluation ignores punctuation.,">>> from nltk.parse import dependencygraph, dependencyevaluator"
Class for measuring labelled and unlabelled attachment score for dependency parsing. Note that the evaluation ignores punctuation.,">>> gold_sent = dependencygraph(""""""
... pierre  nnp     2       nmod
... vinken  nnp     8       sub
... ,       ,       2       p
... 61      cd      5       nmod
... years   nns     6       amod
... old     jj      2       nmod
... ,       ,       2       p
... will    md      0       root
... join    vb      8       vc
... the     dt      11      nmod
... board   nn      9       obj
... as      in      9       vmod
... a       dt      15      nmod
... nonexecutive    jj      15      nmod
... director        nn      12      pmod
... nov.    nnp     9       vmod
... 29      cd      16      nmod
... .       .       9       vmod
... """""")"
Class for measuring labelled and unlabelled attachment score for dependency parsing. Note that the evaluation ignores punctuation.,">>> parsed_sent = dependencygraph(""""""
... pierre  nnp     8       nmod
... vinken  nnp     1       sub
... ,       ,       3       p
... 61      cd      6       nmod
... years   nns     6       amod
... old     jj      2       nmod
... ,       ,       3       amod
... will    md      0       root
... join    vb      8       vc
... the     dt      11      amod
... board   nn      9       object
... as      in      9       nmod
... a       dt      15      nmod
... nonexecutive    jj      15      nmod
... director        nn      12      pmod
... nov.    nnp     9       vmod
... 29      cd      16      nmod
... .       .       9       vmod
... """""")"
Class for measuring labelled and unlabelled attachment score for dependency parsing. Note that the evaluation ignores punctuation.,">>> de = dependencyevaluator([parsed_sent],[gold_sent])
>>> las, uas = de.eval()
>>> las
0.6...
>>> uas
0.8...
>>> abs(uas - 0.8) < 0.00001
true"
A class for dependency parsing with MaltParser. The input is the paths to: - a maltparser directory - (optionally) the path to a pre-trained MaltParser .mco model file - (optionally) the tagger to use for POS tagging before parsing - (optionally) additional Java arguments,">>> from nltk.parse import malt
>>> # with malt_parser and malt_model environment set.
>>> mp = malt.maltparser('maltparser-1.7.2', 'engmalt.linear-1.7.mco') 
>>> mp.parse_one('i shot an elephant in my pajamas .'.split()).tree() 
(shot i (elephant an) (in (pajamas my)) .)
>>> # without malt_parser and malt_model environment.
>>> mp = malt.maltparser('/home/user/maltparser-1.7.2/', '/home/user/engmalt.linear-1.7.mco') 
>>> mp.parse_one('i shot an elephant in my pajamas .'.split()).tree() 
(shot i (elephant an) (in (pajamas my)) .)"
"A dependency scorer built around a MaxEnt classifier. In this particular class that classifier is a NaiveBayesClassifier. It uses head-word, head-tag, child-word, and child-tag features for classification.",">>> from nltk.parse.dependencygraph import dependencygraph, conll_data2"
"A dependency scorer built around a MaxEnt classifier. In this particular class that classifier is a NaiveBayesClassifier. It uses head-word, head-tag, child-word, and child-tag features for classification.",">>> graphs = [dependencygraph(entry) for entry in conll_data2.split('\n\n') if entry]
>>> npp = probabilisticnonprojectiveparser()
>>> npp.train(graphs, naivebayesdependencyscorer())
>>> parses = npp.parse(['cathy', 'zag', 'hen', 'zwaaien', '.'], ['n', 'v', 'pron', 'adj', 'n', 'punc'])
>>> len(list(parses))
1"
A probabilistic non-projective dependency parser.,">>> class scorer(dependencyscoreri):
...     def train(self, graphs):
...         pass
...
...     def score(self, graph):
...         return [
...             [[], [5],  [1],  [1]],
...             [[], [],   [11], [4]],
...             [[], [10], [],   [5]],
...             [[], [8],  [8],  []],
...         ]
"
A probabilistic non-projective dependency parser.,">>> npp = probabilisticnonprojectiveparser()
>>> npp.train([], scorer())"
A probabilistic non-projective dependency parser.,">>> parses = npp.parse(['v1', 'v2', 'v3'], [none, none, none])
>>> len(list(parses))
1"
A probabilistic non-projective dependency parser.,>>> from nltk.grammar import dependencygrammar
A probabilistic non-projective dependency parser.,">>> grammar = dependencygrammar.fromstring('''
... 'taught' -> 'play' | 'man'
... 'man' -> 'the' | 'in'
... 'in' -> 'corner'
... 'corner' -> 'the'
... 'play' -> 'golf' | 'dachshund' | 'to'
... 'dachshund' -> 'his'
... ''')"
A probabilistic non-projective dependency parser.,">>> ndp = nonprojectivedependencyparser(grammar)
>>> parses = ndp.parse(['the', 'man', 'in', 'the', 'corner', 'taught', 'his', 'dachshund', 'to', 'play', 'golf'])
>>> len(list(parses))
4"
"A probabilistic, projective dependency parser.",>>> from nltk.parse.dependencygraph import conll_data2
"A probabilistic, projective dependency parser.",">>> graphs = [
... dependencygraph(entry) for entry in conll_data2.split('\n\n') if entry
... ]"
"A probabilistic, projective dependency parser.",">>> ppdp = probabilisticprojectivedependencyparser()
>>> ppdp.train(graphs)"
"A probabilistic, projective dependency parser.",">>> sent = ['cathy', 'zag', 'hen', 'wild', 'zwaaien', '.']
>>> list(ppdp.parse(sent))
[tree('zag', ['cathy', 'hen', tree('zwaaien', ['wild', '.'])])]"
"ShiftReduceParser maintains a stack, which records the structure of a portion of the text. This stack is a list of strings and Trees that collectively cover a portion of the text. For example, while parsing the sentence “the dog saw the man” with a typical grammar, ShiftReduceParser will produce the following stack, which covers “the dog saw”:","[(np: (det: 'the') (n: 'dog')), (v: 'saw')]"
,">>> dep_parser=stanforddependencyparser(
...     model_path=""edu/stanford/nlp/models/lexparser/englishpcfg.ser.gz""
... )"
,">>> [parse.tree() for parse in dep_parser.raw_parse(""the quick brown fox jumps over the lazy dog."")] 
[tree('jumps', [tree('fox', ['the', 'quick', 'brown']), tree('dog', ['over', 'the', 'lazy'])])]"
,">>> [list(parse.triples()) for parse in dep_parser.raw_parse(""the quick brown fox jumps over the lazy dog."")] 
[[((u'jumps', u'vbz'), u'nsubj', (u'fox', u'nn')), ((u'fox', u'nn'), u'det', (u'the', u'dt')),
((u'fox', u'nn'), u'amod', (u'quick', u'jj')), ((u'fox', u'nn'), u'amod', (u'brown', u'jj')),
((u'jumps', u'vbz'), u'nmod', (u'dog', u'nn')), ((u'dog', u'nn'), u'case', (u'over', u'in')),
((u'dog', u'nn'), u'det', (u'the', u'dt')), ((u'dog', u'nn'), u'amod', (u'lazy', u'jj'))]]"
,">>> sum([[parse.tree() for parse in dep_graphs] for dep_graphs in dep_parser.raw_parse_sents((
...     ""the quick brown fox jumps over the lazy dog."",
...     ""the quick grey wolf jumps over the lazy fox.""
... ))], []) 
[tree('jumps', [tree('fox', ['the', 'quick', 'brown']), tree('dog', ['over', 'the', 'lazy'])]),
tree('jumps', [tree('wolf', ['the', 'quick', 'grey']), tree('fox', ['over', 'the', 'lazy'])])]"
,">>> sum([[parse.tree() for parse in dep_graphs] for dep_graphs in dep_parser.parse_sents((
...     ""i 'm a dog"".split(),
...     ""this is my friends ' cat ( the tabby )"".split(),
... ))], []) 
[tree('dog', ['i', ""'m"", 'a']), tree('cat', ['this', 'is', tree('friends', ['my', ""'""]), tree('tabby', ['the'])])]"
,">>> sum([[list(parse.triples()) for parse in dep_graphs] for dep_graphs in dep_parser.tagged_parse_sents((
...     (
...         (""the"", ""dt""),
...         (""quick"", ""jj""),
...         (""brown"", ""jj""),
...         (""fox"", ""nn""),
...         (""jumped"", ""vbd""),
...         (""over"", ""in""),
...         (""the"", ""dt""),
...         (""lazy"", ""jj""),
...         (""dog"", ""nn""),
...         (""."", "".""),
...     ),
... ))],[]) 
[[((u'jumped', u'vbd'), u'nsubj', (u'fox', u'nn')), ((u'fox', u'nn'), u'det', (u'the', u'dt')),
((u'fox', u'nn'), u'amod', (u'quick', u'jj')), ((u'fox', u'nn'), u'amod', (u'brown', u'jj')),
((u'jumped', u'vbd'), u'nmod', (u'dog', u'nn')), ((u'dog', u'nn'), u'case', (u'over', u'in')),
((u'dog', u'nn'), u'det', (u'the', u'dt')), ((u'dog', u'nn'), u'amod', (u'lazy', u'jj'))]]"
,">>> from nltk.parse.stanford import stanfordneuraldependencyparser
>>> dep_parser=stanfordneuraldependencyparser(java_options='-mx4g')"
,">>> [parse.tree() for parse in dep_parser.raw_parse(""the quick brown fox jumps over the lazy dog."")] 
[tree('jumps', [tree('fox', ['the', 'quick', 'brown']), tree('dog', ['over', 'the', 'lazy']), '.'])]"
,">>> [list(parse.triples()) for parse in dep_parser.raw_parse(""the quick brown fox jumps over the lazy dog."")] 
[[((u'jumps', u'vbz'), u'nsubj', (u'fox', u'nn')), ((u'fox', u'nn'), u'det',
(u'the', u'dt')), ((u'fox', u'nn'), u'amod', (u'quick', u'jj')), ((u'fox', u'nn'),
u'amod', (u'brown', u'jj')), ((u'jumps', u'vbz'), u'nmod', (u'dog', u'nn')),
((u'dog', u'nn'), u'case', (u'over', u'in')), ((u'dog', u'nn'), u'det',
(u'the', u'dt')), ((u'dog', u'nn'), u'amod', (u'lazy', u'jj')), ((u'jumps', u'vbz'),
u'punct', (u'.', u'.'))]]"
,">>> sum([[parse.tree() for parse in dep_graphs] for dep_graphs in dep_parser.raw_parse_sents((
...     ""the quick brown fox jumps over the lazy dog."",
...     ""the quick grey wolf jumps over the lazy fox.""
... ))], []) 
[tree('jumps', [tree('fox', ['the', 'quick', 'brown']), tree('dog', ['over',
'the', 'lazy']), '.']), tree('jumps', [tree('wolf', ['the', 'quick', 'grey']),
tree('fox', ['over', 'the', 'lazy']), '.'])]"
,">>> sum([[parse.tree() for parse in dep_graphs] for dep_graphs in dep_parser.parse_sents((
...     ""i 'm a dog"".split(),
...     ""this is my friends ' cat ( the tabby )"".split(),
... ))], []) 
[tree('dog', ['i', ""'m"", 'a']), tree('cat', ['this', 'is', tree('friends',
['my', ""'""]), tree('tabby', ['-lrb-', 'the', '-rrb-'])])]"
,">>> parser=stanfordparser(
...     model_path=""edu/stanford/nlp/models/lexparser/englishpcfg.ser.gz""
... )"
,">>> list(parser.raw_parse(""the quick brown fox jumps over the lazy dog"")) 
[tree('root', [tree('np', [tree('np', [tree('dt', ['the']), tree('jj', ['quick']), tree('jj', ['brown']),
tree('nn', ['fox'])]), tree('np', [tree('np', [tree('nns', ['jumps'])]), tree('pp', [tree('in', ['over']),
tree('np', [tree('dt', ['the']), tree('jj', ['lazy']), tree('nn', ['dog'])])])])])])]"
,">>> sum([list(dep_graphs) for dep_graphs in parser.raw_parse_sents((
...     ""the quick brown fox jumps over the lazy dog"",
...     ""the quick grey wolf jumps over the lazy fox""
... ))], []) 
[tree('root', [tree('np', [tree('np', [tree('dt', ['the']), tree('jj', ['quick']), tree('jj', ['brown']),
tree('nn', ['fox'])]), tree('np', [tree('np', [tree('nns', ['jumps'])]), tree('pp', [tree('in', ['over']),
tree('np', [tree('dt', ['the']), tree('jj', ['lazy']), tree('nn', ['dog'])])])])])]), tree('root', [tree('np',
[tree('np', [tree('dt', ['the']), tree('jj', ['quick']), tree('jj', ['grey']), tree('nn', ['wolf'])]), tree('np',
[tree('np', [tree('nns', ['jumps'])]), tree('pp', [tree('in', ['over']), tree('np', [tree('dt', ['the']),
tree('jj', ['lazy']), tree('nn', ['fox'])])])])])])]"
,">>> sum([list(dep_graphs) for dep_graphs in parser.parse_sents((
...     ""i 'm a dog"".split(),
...     ""this is my friends ' cat ( the tabby )"".split(),
... ))], []) 
[tree('root', [tree('s', [tree('np', [tree('prp', ['i'])]), tree('vp', [tree('vbp', [""'m""]),
tree('np', [tree('dt', ['a']), tree('nn', ['dog'])])])])]), tree('root', [tree('s', [tree('np',
[tree('dt', ['this'])]), tree('vp', [tree('vbz', ['is']), tree('np', [tree('np', [tree('np', [tree('prp$', ['my']),
tree('nns', ['friends']), tree('pos', [""'""])]), tree('nn', ['cat'])]), tree('prn', [tree('-lrb-', [tree('', []),
tree('np', [tree('dt', ['the']), tree('nn', ['tabby'])]), tree('-rrb-', [])])])])])])])]"
,">>> sum([list(dep_graphs) for dep_graphs in parser.tagged_parse_sents((
...     (
...         (""the"", ""dt""),
...         (""quick"", ""jj""),
...         (""brown"", ""jj""),
...         (""fox"", ""nn""),
...         (""jumped"", ""vbd""),
...         (""over"", ""in""),
...         (""the"", ""dt""),
...         (""lazy"", ""jj""),
...         (""dog"", ""nn""),
...         (""."", "".""),
...     ),
... ))],[]) 
[tree('root', [tree('s', [tree('np', [tree('dt', ['the']), tree('jj', ['quick']), tree('jj', ['brown']),
tree('nn', ['fox'])]), tree('vp', [tree('vbd', ['jumped']), tree('pp', [tree('in', ['over']), tree('np',
[tree('dt', ['the']), tree('jj', ['lazy']), tree('nn', ['dog'])])])]), tree('.', ['.'])])])]"
,">>> from nltk.parse import dependencygraph, dependencyevaluator
>>> from nltk.parse.transitionparser import transitionparser, configuration, transition
>>> gold_sent = dependencygraph(""""""
... economic  jj     2      att
... news  nn     3       sbj
... has       vbd       0       root
... little      jj      5       att
... effect   nn     3       obj
... on     in      5       att
... financial       jj       8       att
... markets    nns      6       pc
... .    .      3       pu
... """""")"
,>>> conf = configuration(gold_sent)
###################### Check the Initial Feature ########################,">>> print(', '.join(conf.extract_features()))
stk_0_pos_top, buf_0_form_economic, buf_0_lemma_economic, buf_0_pos_jj, buf_1_form_news, buf_1_pos_nn, buf_2_pos_vbd, buf_3_pos_jj"
"Do some transition checks for ARC-STANDARD
",">>> operation = transition('arc-standard')
>>> operation.shift(conf)
>>> operation.left_arc(conf, ""att"")
>>> operation.shift(conf)
>>> operation.left_arc(conf,""sbj"")
>>> operation.shift(conf)
>>> operation.shift(conf)
>>> operation.left_arc(conf, ""att"")
>>> operation.shift(conf)
>>> operation.shift(conf)
>>> operation.shift(conf)
>>> operation.left_arc(conf, ""att"")"
"Middle Configuration and Features Check >>> print(conf) Stack : [0, 3, 5, 6] Buffer : [8, 9] Arcs : [(2, ‘ATT’, 1), (3, ‘SBJ’, 2), (5, ‘ATT’, 4), (8, ‘ATT’, 7)]",">>> print(', '.join(conf.extract_features()))
stk_0_form_on, stk_0_lemma_on, stk_0_pos_in, stk_1_pos_nn, buf_0_form_markets, buf_0_lemma_markets, buf_0_pos_nns, buf_1_form_., buf_1_pos_., buf_0_ldep_att"
"Middle Configuration and Features Check >>> print(conf) Stack : [0, 3, 5, 6] Buffer : [8, 9] Arcs : [(2, ‘ATT’, 1), (3, ‘SBJ’, 2), (5, ‘ATT’, 4), (8, ‘ATT’, 7)]",">>> operation.right_arc(conf, ""pc"")
>>> operation.right_arc(conf, ""att"")
>>> operation.right_arc(conf, ""obj"")
>>> operation.shift(conf)
>>> operation.right_arc(conf, ""pu"")
>>> operation.right_arc(conf, ""root"")
>>> operation.shift(conf)"
"

    Do some transition checks for ARC-EAGER
",">>> conf = configuration(gold_sent)
>>> operation = transition('arc-eager')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'att')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'sbj')
>>> operation.right_arc(conf,'root')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'att')
>>> operation.right_arc(conf,'obj')
>>> operation.right_arc(conf,'att')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'att')
>>> operation.right_arc(conf,'pc')
>>> operation.reduce(conf)
>>> operation.reduce(conf)
>>> operation.reduce(conf)
>>> operation.right_arc(conf,'pu')
>>> print(conf)
stack : [0, 3, 9]  buffer : []   arcs : [(2, 'att', 1), (3, 'sbj', 2), (0, 'root', 3), (5, 'att', 4), (3, 'obj', 5), (5, 'att', 6), (8, 'att', 7), (6, 'pc', 8), (3, 'pu', 9)]
"
"A. Check the ARC-STANDARD training >>> import tempfile >>> import os >>> input_file = tempfile.NamedTemporaryFile(prefix=’transition_parse.train’, dir=tempfile.gettempdir(), delete=False)",">>> parser_std = transitionparser('arc-standard')
>>> print(', '.join(parser_std._create_training_examples_arc_std([gold_sent], input_file)))
 number of training examples : 1
 number of valid (projective) examples : 1
shift, leftarc:att, shift, leftarc:sbj, shift, shift, leftarc:att, shift, shift, shift, leftarc:att, rightarc:pc, rightarc:att, rightarc:obj, shift, rightarc:pu, rightarc:root, shift
"
"A. Check the ARC-STANDARD training >>> import tempfile >>> import os >>> input_file = tempfile.NamedTemporaryFile(prefix=’transition_parse.train’, dir=tempfile.gettempdir(), delete=False)",">>> parser_std.train([gold_sent],'temp.arcstd.model', verbose=false)
 number of training examples : 1
 number of valid (projective) examples : 1
>>> remove(input_file.name)"
Check the ARC-EAGER training,">>> input_file = tempfile.namedtemporaryfile(prefix='transition_parse.train', dir=tempfile.gettempdir(),delete=false)
>>> parser_eager = transitionparser('arc-eager')
>>> print(', '.join(parser_eager._create_training_examples_arc_eager([gold_sent], input_file)))
 number of training examples : 1
 number of valid (projective) examples : 1
shift, leftarc:att, shift, leftarc:sbj, rightarc:root, shift, leftarc:att, rightarc:obj, rightarc:att, shift, leftarc:att, rightarc:pc, reduce, reduce, reduce, rightarc:pu
"
Check the ARC-EAGER training,">>> parser_eager.train([gold_sent],'temp.arceager.model', verbose=false)
 number of training examples : 1
 number of valid (projective) examples : 1"
Check the ARC-EAGER training,>>> remove(input_file.name)
Check the ARC-STANDARD parser,">>> result = parser_std.parse([gold_sent], 'temp.arcstd.model')
>>> de = dependencyevaluator(result, [gold_sent])
>>> de.eval() >= (0, 0)
true"
"A module to convert the a POS tagged document stream (i.e. list of list of tuples, a list of sentences) and yield lines in CONLL format. This module yields one line per word and two newlines for end of sentence.",">>> from nltk import word_tokenize, sent_tokenize, pos_tag
>>> text = ""this is a foobar sentence. is that right?""
>>> sentences = [pos_tag(word_tokenize(sent)) for sent in sent_tokenize(text)]
>>> for line in taggedsents_to_conll(sentences):
...     if line:
...         print(line, end="""")
1   this    _       dt      dt      _       0       a       _       _
2   is      _       vbz     vbz     _       0       a       _       _
3   a       _       dt      dt      _       0       a       _       _
4   foobar  _       jj      jj      _       0       a       _       _
5   sentence        _       nn      nn      _       0       a       _       _
6   .               _       .       .       _       0       a       _       _


1   is      _       vbz     vbz     _       0       a       _       _
2   that    _       in      in      _       0       a       _       _
3   right   _       nn      nn      _       0       a       _       _
4   ?       _       .       .       _       0       a       _       _
"
When possible this list is sorted from most likely to least likely.,,nltk.parse.api.
When possible this list is sorted from most likely to least likely.,,parseri
The grammar used by this parser.,,grammar
When possible this list is sorted from most likely to least likely.,,parse
When possible this list is sorted from most likely to least likely.,,parse_all
When possible this list is sorted from most likely to least likely.,,parse_one
When possible this list is sorted from most likely to least likely.,,parse_sents
An iterator that generates parse trees for the sentence.,,nltk.parse.bllip.
An iterator that generates parse trees for the sentence.,,bllipparser
A BllipParser object using the parser and reranker,,from_unified_model_dir
An iterator that generates parse trees for the sentence.,,tagged_parse
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,nltk.parse.chart.
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,abstractchartrule
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,apply
"Return a generator that will add all edges licensed by
this rule, given the edges that are currently in the
chart, one at a time.  Each time the generator is resumed,
it will either add a new edge and yield that edge; or return.",,apply_everywhere
"A ChartParser using a bottom-up parsing strategy.
See ChartParser for more information.",,bottomupchartparser
"A ChartParser using a bottom-up left-corner parsing strategy.
This strategy is often more efficient than standard bottom-up.
See ChartParser for more information.",,bottomupleftcornerchartparser
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,bottomuppredictcombinerule
"This is like BottomUpPredictRule, but it also applies
the FundamentalRule to the resulting edge.",,num_edges
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,bottomuppredictrule
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,cachedtopdownpredictrule
"child_pointer_lists (sequence of tuple(EdgeI)) – A sequence of lists of the edges that
were used to form this edge.  This list is used to reconstruct
the trees (or partial trees) that are associated with edge.",,chart
"Return the set of child pointer lists for the given edge.
Each child pointer list is a list of edges that have
been used to form this edge.",,child_pointer_lists
"Return the set of child pointer lists for the given edge.
Each child pointer list is a list of edges that have
been used to form this edge.",,dot_digraph
"Return a list of all edges in this chart.  New edges
that are added to the chart after the call to edges()
will not be contained in this list.",,edges
"Return a list of all edges in this chart.  New edges
that are added to the chart after the call to edges()
will not be contained in this list.",,initialize
"child_pointer_lists (sequence of tuple(EdgeI)) – A sequence of lists of the edges that
were used to form this edge.  This list is used to reconstruct
the trees (or partial trees) that are associated with edge.",,insert
"Add a new edge to the chart, using a pointer to the previous edge.",,insert_with_backpointer
"Return an iterator over the edges in this chart.  It is
not guaranteed that new edges which are added to the
chart before the iterator is exhausted will also be generated.",,iteredges
Return the leaf value of the word at the given index.,,leaf
Return the leaf value of the word at the given index.,,leaves
Return the number of edges contained in this chart.,,num_edges
Return the number of edges contained in this chart.,,num_leaves
Return the number of edges contained in this chart.,,parses
Return the number of edges contained in this chart.,,pretty_format
"Return a pretty-printed string representation of a given edge
in this chart.",,pretty_format_edge
"Return a pretty-printed string representation of this
chart’s leaves.  This string can be used as a header
for calls to pretty_format_edge.",,pretty_format_leaves
"Return an iterator over the edges in this chart.  Any
new edges that are added to the chart before the iterator
is exahusted will also be generated.  restrictions
can be used to restrict the set of edges that will be
generated.",,select
"If two trees share a common subtree, then the same
Tree may be used to encode that subtree in
both trees.  If you need to eliminate this subtree
sharing, then create a deep copy of each tree.",,trees
When possible this list is sorted from most likely to least likely.,,chartparser
"Return the final parse Chart from which all possible
parse trees can be extracted.",,chart_parse
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,chartrulei
"Return a tuple (s, e), where tokens[s:e] is the
portion of the sentence that is consistent with this
edge’s structure.",,edgei
"Return this edge’s dot position, which indicates how much of
the hypothesized structure is consistent with the
sentence.  In particular, self.rhs[:dot] is consistent
with tokens[self.start():self.end()].",,dot
"Return this edge’s dot position, which indicates how much of
the hypothesized structure is consistent with the
sentence.  In particular, self.rhs[:dot] is consistent
with tokens[self.start():self.end()].",,end
"Return this edge’s dot position, which indicates how much of
the hypothesized structure is consistent with the
sentence.  In particular, self.rhs[:dot] is consistent
with tokens[self.start():self.end()].",,is_complete
"Return this edge’s dot position, which indicates how much of
the hypothesized structure is consistent with the
sentence.  In particular, self.rhs[:dot] is consistent
with tokens[self.start():self.end()].",,is_incomplete
"Return this edge’s dot position, which indicates how much of
the hypothesized structure is consistent with the
sentence.  In particular, self.rhs[:dot] is consistent
with tokens[self.start():self.end()].",,length
"Return this edge’s left-hand side, which specifies what kind
of structure is hypothesized by this edge.",,lhs
"Return the element of this edge’s right-hand side that
immediately follows its dot.",,nextsym
"Return this edge’s right-hand side, which specifies
the content of the structure hypothesized by this edge.",,rhs
"Return a tuple (s, e), where tokens[s:e] is the
portion of the sentence that is consistent with this
edge’s structure.",,span
"Return a tuple (s, e), where tokens[s:e] is the
portion of the sentence that is consistent with this
edge’s structure.",,start
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,emptypredictrule
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,filteredbottomuppredictcombinerule
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,filteredsingleedgefundamentalrule
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,fundamentalrule
"Return a tuple (s, e), where tokens[s:e] is the
portion of the sentence that is consistent with this
edge’s structure.",,leafedge
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,leafinitrule
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,leftcornerchartparser
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,singleedgefundamentalrule
When possible this list is sorted from most likely to least likely.,,steppingchartparser
"_restart – Records whether the parser’s strategy, grammar,
or chart has been changed.  If so, then step must restart
the parsing algorithm.",,chart
Return the chart rule used to generate the most recent edge.,,current_chartrule
Load a given chart into the chart parser.,,set_chart
Change the grammar used by the parser.,,set_grammar
"strategy (list(ChartRuleI)) – A list of rules that should be used to decide
what edges to add to the chart.",,set_strategy
"Note that this generator never terminates, since the grammar
or strategy might be changed to values that would add new
edges.  Instead, it yields None when no more edges can be
added with the current strategy and grammar.",,step
"Note that this generator never terminates, since the grammar
or strategy might be changed to values that would add new
edges.  Instead, it yields None when no more edges can be
added with the current strategy and grammar.",,strategy
"A ChartParser using a top-down parsing strategy.
See ChartParser for more information.",,topdownchartparser
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,topdowninitrule
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,topdownpredictrule
"Return a tuple (s, e), where tokens[s:e] is the
portion of the sentence that is consistent with this
edge’s structure.",,treeedge
"Return this edge’s dot position, which indicates how much of
the hypothesized structure is consistent with the
sentence.  In particular, self.rhs[:dot] is consistent
with tokens[self.start():self.end()].",,from_production
"Return a new TreeEdge formed from this edge.
The new edge’s dot position is increased by 1,
and its end index will be replaced by new_end.",,move_dot_forward
"Return a tuple (s, e), where tokens[s:e] is the
portion of the sentence that is consistent with this
edge’s structure.",,demo
"Return a tuple (s, e), where tokens[s:e] is the
portion of the sentence that is consistent with this
edge’s structure.",,demo_grammar
"stderr (stdout,) – Specifies where CoreNLP output is redirected. Valid values are ‘devnull’, ‘stdout’, ‘pipe’",,nltk.parse.corenlp.
"stderr (stdout,) – Specifies where CoreNLP output is redirected. Valid values are ‘devnull’, ‘stdout’, ‘pipe’",,corenlpdependencyparser
"stderr (stdout,) – Specifies where CoreNLP output is redirected. Valid values are ‘devnull’, ‘stdout’, ‘pipe’",,corenlpparser
"stderr (stdout,) – Specifies where CoreNLP output is redirected. Valid values are ‘devnull’, ‘stdout’, ‘pipe’",,corenlpserver
"stderr (stdout,) – Specifies where CoreNLP output is redirected. Valid values are ‘devnull’, ‘stdout’, ‘pipe’",,stop
"stderr (stdout,) – Specifies where CoreNLP output is redirected. Valid values are ‘devnull’, ‘stdout’, ‘pipe’",,corenlpservererror
"stderr (stdout,) – Specifies where CoreNLP output is redirected. Valid values are ‘devnull’, ‘stdout’, ‘pipe’",,genericcorenlpparser
The text might contain several sentences which will be split by CoreNLP.,,parse_text
The text might contain several sentences which will be split by CoreNLP.,,raw_parse
The text might contain several sentences which will be split by CoreNLP.,,raw_parse_sents
The text might contain several sentences which will be split by CoreNLP.,,raw_tag_sents
The text might contain several sentences which will be split by CoreNLP.,,tag
The text might contain several sentences which will be split by CoreNLP.,,tag_sents
The text might contain several sentences which will be split by CoreNLP.,,tokenize
"stderr (stdout,) – Specifies where CoreNLP output is redirected. Valid values are ‘devnull’, ‘stdout’, ‘pipe’",,setup_module
"stderr (stdout,) – Specifies where CoreNLP output is redirected. Valid values are ‘devnull’, ‘stdout’, ‘pipe’",,teardown_module
"stderr (stdout,) – Specifies where CoreNLP output is redirected. Valid values are ‘devnull’, ‘stdout’, ‘pipe’",,transform
"stderr (stdout,) – Specifies where CoreNLP output is redirected. Valid values are ‘devnull’, ‘stdout’, ‘pipe’",,try_port
"Starting with the root node, build a dependency tree using the NLTK
Tree constructor. Dependency labels are omitted.",,nltk.parse.dependencygraph.
"Starting with the root node, build a dependency tree using the NLTK
Tree constructor. Dependency labels are omitted.",,dependencygraph
"Adds an arc from the node specified by head_address to the
node specified by the mod address.",,add_arc
"Adds an arc from the node specified by head_address to the
node specified by the mod address.",,add_node
"Fully connects all non-root nodes.  All nodes are set to be dependents
of the root node.",,connect_graph
"Returns true if the graph contains a node with the given node
address, false otherwise.",,contains_address
"Returns true if the graph contains a node with the given node
address, false otherwise.",,contains_cycle
Return the node with the given address.,,get_by_address
Return the node with the given address.,,get_cycle_path
"Returns the number of right children under the node specified
by the given address.",,left_children
"rather than 1 (as produced by, e.g., zpar)
:param str cell_separator: the cell separator. If not provided, cells
are split by whitespace.
:param str top_relation_label: the label by which the top relation is
identified, for examlple, ROOT, null or TOP.",,load
Convert the data in a nodelist into a networkx labeled directed graph.,,nx_graph
Convert the data in a nodelist into a networkx labeled directed graph.,,redirect_arcs
"Removes the node with the given address.  References
to this node in others will still exist.",,remove_by_address
"Returns the number of right children under the node specified
by the given address.",,right_children
"style (int) – the style to use for the format (3, 4, 10 columns)",,to_conll
"Starting with the root node, build a dependency tree using the NLTK
Tree constructor. Dependency labels are omitted.",,tree
"Starting with the root node, build a dependency tree using the NLTK
Tree constructor. Dependency labels are omitted.",,triples
"Starting with the root node, build a dependency tree using the NLTK
Tree constructor. Dependency labels are omitted.",,dependencygrapherror
"A demonstration of how to read a string representation of
a CoNLL format dependency tree.",,conll_demo
"A demonstration of how to read a string representation of
a CoNLL format dependency tree.",,conll_file_demo
"A demonstration of how to read a string representation of
a CoNLL format dependency tree.",,cycle_finding_demo
"A demonstration of the result of reading a dependency
version of the first sentence of the Penn Treebank.",,malt_demo
"Data classes and parser implementations for incremental chart
parsers, which use dynamic programming to efficiently parse a text.
A “chart parser” derives parse trees for a text by iteratively adding
“edges” to a “chart”.  Each “edge” represents a hypothesis about the tree
structure for a subsequence of the text.  The “chart” is a
“blackboard” for composing and combining these hypotheses.",,nltk.parse.earleychart.
"Data classes and parser implementations for incremental chart
parsers, which use dynamic programming to efficiently parse a text.
A “chart parser” derives parse trees for a text by iteratively adding
“edges” to a “chart”.  Each “edge” represents a hypothesis about the tree
structure for a subsequence of the text.  The “chart” is a
“blackboard” for composing and combining these hypotheses.",,completefundamentalrule
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,completerrule
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,earleychartparser
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,featurecompletefundamentalrule
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,featurecompleterrule
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,featureearleychartparser
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,featureincrementalbottomupchartparser
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,featureincrementalbottomupleftcornerchartparser
"Returns an iterator over the edges in this chart.
See Chart.select for more information about the
restrictions on the edges.",,featureincrementalchart
"Returns an iterator over the edges in this chart.
See Chart.select for more information about the
restrictions on the edges.",,featureincrementalchartparser
"Returns an iterator over the edges in this chart.
See Chart.select for more information about the
restrictions on the edges.",,featureincrementaltopdownchartparser
"Returns an iterator over the edges in this chart.
See Chart.select for more information about the
restrictions on the edges.",,featurepredictorrule
"Returns an iterator over the edges in this chart.
See Chart.select for more information about the
restrictions on the edges.",,featurescannerrule
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,filteredcompletefundamentalrule
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,incrementalbottomupchartparser
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,incrementalbottomupleftcornerchartparser
"Return an iterator over the edges in this chart.  Any
new edges that are added to the chart before the iterator
is exahusted will also be generated.  restrictions
can be used to restrict the set of edges that will be
generated.",,incrementalchart
"Return the final parse Chart from which all possible
parse trees can be extracted.",,incrementalchartparser
"Return the final parse Chart from which all possible
parse trees can be extracted.",,incrementalleftcornerchartparser
"Return the final parse Chart from which all possible
parse trees can be extracted.",,incrementaltopdownchartparser
"Return the final parse Chart from which all possible
parse trees can be extracted.",,predictorrule
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,scannerrule
"Class for measuring labelled and unlabelled attachment score for
dependency parsing. Note that the evaluation ignores punctuation.",,nltk.parse.evaluate.
"Class for measuring labelled and unlabelled attachment score for
dependency parsing. Note that the evaluation ignores punctuation.",,dependencyevaluator
"Class for measuring labelled and unlabelled attachment score for
dependency parsing. Note that the evaluation ignores punctuation.",,eval
"Extension of chart parsing implementation to handle grammars with
feature structures as nodes.",,nltk.parse.featurechart.
"Extension of chart parsing implementation to handle grammars with
feature structures as nodes.",,featurebottomupchartparser
"Extension of chart parsing implementation to handle grammars with
feature structures as nodes.",,featurebottomupleftcornerchartparser
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,featurebottomuppredictcombinerule
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,featurebottomuppredictrule
"Returns an iterator over the edges in this chart.
See Chart.select for more information about the
restrictions on the edges.",,featurechart
"Returns an iterator over the edges in this chart.
See Chart.select for more information about the
restrictions on the edges.",,featurechartparser
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,featureemptypredictrule
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,featurefundamentalrule
"A specialized version of the completer / single edge fundamental rule
that operates on nonterminals whose symbols are ``FeatStructNonterminal``s.
Rather than simply comparing the nonterminals for equality, they are
unified.",,featuresingleedgefundamentalrule
"A specialized version of the completer / single edge fundamental rule
that operates on nonterminals whose symbols are ``FeatStructNonterminal``s.
Rather than simply comparing the nonterminals for equality, they are
unified.",,featuretopdownchartparser
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,featuretopdowninitrule
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,featuretopdownpredictrule
"A new FeatureTreeEdge formed from this edge.
The new edge’s dot position is increased by 1,
and its end index will be replaced by new_end.",,featuretreeedge
"Each FeatureTreeEdge contains a set of bindings, i.e., a
dictionary mapping from variables to values.  If the edge is not
complete, then these bindings are simply stored.  However, if the
edge is complete, then the constructor applies these bindings to
every nonterminal in the edge whose symbol implements the
interface SubstituteBindingsI.",,bindings
"A new FeatureTreeEdge formed from this edge.
The new edge’s dot position is increased by 1,
and its end index will be replaced by new_end.",,next_with_bindings
The set of variables used by this edge.,,variables
"If the edge is a FeatureTreeEdge, and it is complete,
then instantiate all variables whose names start with ‘@’,
by replacing them with unique new variables.",,instantiatevarschart
"child_pointer_lists (sequence of tuple(EdgeI)) – A sequence of lists of the edges that
were used to form this edge.  This list is used to reconstruct
the trees (or partial trees) that are associated with edge.",,inst_vars
"If the edge is a FeatureTreeEdge, and it is complete,
then instantiate all variables whose names start with ‘@’,
by replacing them with unique new variables.",,instantiate_edge
"If the edge is a FeatureTreeEdge, and it is complete,
then instantiate all variables whose names start with ‘@’,
by replacing them with unique new variables.",,run_profile
start – The Nonterminal from which to start generate sentences.,,nltk.parse.generate.
start – The Nonterminal from which to start generate sentences.,,generate
A module to find MaltParser .jar file and its dependencies.,,nltk.parse.malt.
A module to find MaltParser .jar file and its dependencies.,,maltparser
This function generates the maltparser command use at the terminal.,,generate_malt_command
"Use MaltParser to parse multiple POS tagged sentences. Takes multiple
sentences where each sentence is a list of (word, tag) tuples.
The sentences must have already been tokenized and tagged.",,parse_tagged_sents
"Use MaltParser to parse multiple POS tagged sentences. Takes multiple
sentences where each sentence is a list of (word, tag) tuples.
The sentences must have already been tokenized and tagged.",,train
"Use MaltParser to parse multiple POS tagged sentences. Takes multiple
sentences where each sentence is a list of (word, tag) tuples.
The sentences must have already been tokenized and tagged.",,train_from_file
A module to find MaltParser .jar file and its dependencies.,,find_malt_model
A module to find MaltParser .jar file and its dependencies.,,find_maltparser
A module to find MaltParser .jar file and its dependencies.,,malt_regex_tagger
"Parses a list of tokens in accordance to the MST parsing algorithm
for non-projective dependency parses.  Assumes that the tokens to
be parsed have already been tagged and those tags are provided.  Various
scoring methods can be used by implementing the DependencyScorerI
interface and passing it to the training algorithm.",,nltk.parse.nonprojectivedependencyparser.
"Parses a list of tokens in accordance to the MST parsing algorithm
for non-projective dependency parses.  Assumes that the tokens to
be parsed have already been tagged and those tags are provided.  Various
scoring methods can be used by implementing the DependencyScorerI
interface and passing it to the training algorithm.",,demoscorer
"When used in conjunction with a MaxEntClassifier, each score would
correspond to the confidence of a particular edge being classified
with the positive training examples.",,score
"Parses a list of tokens in accordance to the MST parsing algorithm
for non-projective dependency parses.  Assumes that the tokens to
be parsed have already been tagged and those tags are provided.  Various
scoring methods can be used by implementing the DependencyScorerI
interface and passing it to the training algorithm.",,dependencyscoreri
"Parses a list of tokens in accordance to the MST parsing algorithm
for non-projective dependency parses.  Assumes that the tokens to
be parsed have already been tagged and those tags are provided.  Various
scoring methods can be used by implementing the DependencyScorerI
interface and passing it to the training algorithm.",,naivebayesdependencyscorer
"Parses a list of tokens in accordance to the MST parsing algorithm
for non-projective dependency parses.  Assumes that the tokens to
be parsed have already been tagged and those tags are provided.  Various
scoring methods can be used by implementing the DependencyScorerI
interface and passing it to the training algorithm.",,nonprojectivedependencyparser
"Parses a list of tokens in accordance to the MST parsing algorithm
for non-projective dependency parses.  Assumes that the tokens to
be parsed have already been tagged and those tags are provided.  Various
scoring methods can be used by implementing the DependencyScorerI
interface and passing it to the training algorithm.",,probabilisticnonprojectiveparser
"Returns the source of the best incoming arc to the
node with address: node_index",,best_incoming_arc
"Takes a list of nodes that have been identified to belong to a cycle,
and collapses them into on larger node.  The arcs of all nodes in
the graph must be updated to account for this.",,collapse_nodes
"When updating scores the score of the highest-weighted incoming
arc is subtracted upon collapse.  This returns the correct
amount to subtract from that edge.",,compute_max_subtract_score
"As nodes are collapsed into others, they are replaced
by the new node in the graph, but it’s still necessary
to keep track of what these original nodes were.  This
takes a list of node addresses and replaces any collapsed
node addresses with their original addresses.",,compute_original_indexes
graph (DependencyGraph) – A dependency graph to assign scores to.,,initialize_edge_scores
graph (DependencyGraph) – A dependency graph to assign scores to.,,original_best_arc
"dependency_scorer (DependencyScorerI) – A scorer which implements the
DependencyScorerI interface.",,update_edge_scores
"Parses a list of tokens in accordance to the MST parsing algorithm
for non-projective dependency parses.  Assumes that the tokens to
be parsed have already been tagged and those tags are provided.  Various
scoring methods can be used by implementing the DependencyScorerI
interface and passing it to the training algorithm.",,hall_demo
"Parses a list of tokens in accordance to the MST parsing algorithm
for non-projective dependency parses.  Assumes that the tokens to
be parsed have already been tagged and those tags are provided.  Various
scoring methods can be used by implementing the DependencyScorerI
interface and passing it to the training algorithm.",,nonprojective_conll_parse_demo
"Parses a list of tokens in accordance to the MST parsing algorithm
for non-projective dependency parses.  Assumes that the tokens to
be parsed have already been tagged and those tags are provided.  Various
scoring methods can be used by implementing the DependencyScorerI
interface and passing it to the training algorithm.",,rule_based_demo
"trace (int) – The trace level.  A trace level of 0 will
generate no tracing output; and higher trace levels will
produce more verbose tracing output.",,nltk.parse.pchart.
"trace (int) – The trace level.  A trace level of 0 will
generate no tracing output; and higher trace levels will
produce more verbose tracing output.",,bottomupprobabilisticchartparser
"chart (Chart) – The chart being used to parse the text.  This
chart can be used to provide extra information for sorting
the queue.",,sort_queue
"trace (int) – The trace level.  A trace level of 0 will
generate no tracing output; and higher trace levels will
produce more verbose tracing output.",,trace
"chart (Chart) – The chart being used to parse the text.  This
chart can be used to provide extra information for sorting
the queue.",,insidechartparser
"chart (Chart) – The chart being used to parse the text.  This
chart can be used to provide extra information for sorting
the queue.",,longestchartparser
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,probabilisticbottomupinitrule
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,probabilisticbottomuppredictrule
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,probabilisticfundamentalrule
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,probabilisticleafedge
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,probabilistictreeedge
"chart (Chart) – The chart being used to parse the text.  This
chart can be used to provide extra information for sorting
the queue.",,randomchartparser
"edges (list(EdgeI)) – A set of existing edges.  The number of edges
that should be passed to apply() is specified by the
NUM_EDGES class variable.",,singleedgeprobabilisticfundamentalrule
"chart (Chart) – The chart being used to parse the text.  This
chart can be used to provide extra information for sorting
the queue.",,unsortedchartparser
"A demonstration showing the creation and use of a
DependencyGrammar to perform a projective dependency
parse.",,nltk.parse.projectivedependencyparser.
"A demonstration showing the creation and use of a
DependencyGrammar to perform a projective dependency
parse.",,chartcell
"Appends the given span to the list of spans
representing the chart cell’s entries.",,add
"A demonstration showing the creation and use of a
DependencyGrammar to perform a projective dependency
parse.",,dependencyspan
"A demonstration showing the creation and use of a
DependencyGrammar to perform a projective dependency
parse.",,probabilisticprojectivedependencyparser
"Computes the probability of a dependency graph based
on the parser’s probability model (defined by the parser’s
statistical dependency grammar).",,compute_prob
"Concatenates the two spans in whichever way possible.  This
includes rightward concatenation (from the leftmost word of the
leftmost span to the rightmost word of the rightmost span) and
leftward concatenation (vice-versa) between adjacent spans.  Unlike
Eisner’s presentation of span concatenation, these spans do not
share or pivot on a particular word/word-index.",,concatenate
"A demonstration showing the creation and use of a
DependencyGrammar to perform a projective dependency
parse.",,projectivedependencyparser
"A demonstration showing the creation and use of a
DependencyGrammar to perform a projective dependency
parse.",,arity_parse_demo
"A demonstration showing the creation and use of a
DependencyGrammar to perform a projective dependency
parse.",,projective_prob_parse_demo
"A demonstration showing the creation and use of a
DependencyGrammar to perform a projective dependency
parse.",,projective_rule_parse_demo
"trace (int) – The trace level.  A trace level of 0 will
generate no tracing output; and higher trace levels will
produce more verbose tracing output.",,nltk.parse.recursivedescent.
"trace (int) – The trace level.  A trace level of 0 will
generate no tracing output; and higher trace levels will
produce more verbose tracing output.",,recursivedescentparser
"trace (int) – The trace level.  A trace level of 0 will
generate no tracing output; and higher trace levels will
produce more verbose tracing output.",,steppingrecursivedescentparser
"Return the parser to its state before the most recent
match or expand operation.  Calling undo repeatedly return
the parser to successively earlier states.  If no match or
expand operations have been performed, undo will make no
changes.",,backtrack
"Return the parser to its state before the most recent
match or expand operation.  Calling undo repeatedly return
the parser to successively earlier states.  If no match or
expand operations have been performed, undo will make no
changes.",,currently_complete
"The production used to expand the frontier, if an
expansion was performed.  If no expansion was performed,
return None.",,expand
"The production used to expand the frontier, if an
expansion was performed.  If no expansion was performed,
return None.",,expandable_productions
"The production used to expand the frontier, if an
expansion was performed.  If no expansion was performed,
return None.",,frontier
"The token matched, if a match operation was
performed.  If no match was performed, return None",,match
"An iterator of the parses that have been found by this
parser so far.",,remaining_text
"A partial structure for the text that is
currently being parsed.  The elements specified by the
frontier have not yet been expanded or matched.",,untried_expandable_productions
"A partial structure for the text that is
currently being parsed.  The elements specified by the
frontier have not yet been expanded or matched.",,untried_match
"trace (int) – The trace level.  A trace level of 0 will
generate no tracing output; and higher trace levels will
produce more verbose tracing output.",,nltk.parse.shiftreduce.
"trace (int) – The trace level.  A trace level of 0 will
generate no tracing output; and higher trace levels will
produce more verbose tracing output.",,shiftreduceparser
"trace (int) – The trace level.  A trace level of 0 will
generate no tracing output; and higher trace levels will
produce more verbose tracing output.",,steppingshiftreduceparser
"The production used to reduce the stack, if a
reduction was performed.  If no reduction was performed,
return None.",,reduce
"The production used to reduce the stack, if a
reduction was performed.  If no reduction was performed,
return None.",,reducible_productions
"Move a token from the beginning of the remaining text to the
end of the stack.  If there are no more tokens in the
remaining text, then do nothing.",,shift
"Move a token from the beginning of the remaining text to the
end of the stack.  If there are no more tokens in the
remaining text, then do nothing.",,stack
"Return the parser to its state before the most recent
shift or reduce operation.  Calling undo repeatedly return
the parser to successively earlier states.  If no shift or
reduce operations have been performed, undo will make no
changes.",,undo
"Currently unimplemented because the neural dependency parser (and
the StanfordCoreNLP pipeline class) doesn’t support passing in pre-
tagged tokens.",,nltk.parse.stanford.
"Currently unimplemented because the neural dependency parser (and
the StanfordCoreNLP pipeline class) doesn’t support passing in pre-
tagged tokens.",,genericstanfordparser
"Use StanfordParser to parse multiple sentences. Takes multiple sentences
where each sentence is a list of (word, tag) tuples.
The sentences must have already been tokenized and tagged.",,tagged_parse_sents
"Currently unimplemented because the neural dependency parser (and
the StanfordCoreNLP pipeline class) doesn’t support passing in pre-
tagged tokens.",,stanforddependencyparser
"Currently unimplemented because the neural dependency parser (and
the StanfordCoreNLP pipeline class) doesn’t support passing in pre-
tagged tokens.",,stanfordneuraldependencyparser
"Currently unimplemented because the neural dependency parser (and
the StanfordCoreNLP pipeline class) doesn’t support passing in pre-
tagged tokens.",,stanfordparser
"A. Check the ARC-STANDARD training
>>> import tempfile
>>> import os
>>> input_file = tempfile.NamedTemporaryFile(prefix=’transition_parse.train’, dir=tempfile.gettempdir(), delete=False)",,nltk.parse.transitionparser.
"A. Check the ARC-STANDARD training
>>> import tempfile
>>> import os
>>> input_file = tempfile.NamedTemporaryFile(prefix=’transition_parse.train’, dir=tempfile.gettempdir(), delete=False)",,configuration
"Extract the set of features for the current configuration. Implement standard features as describe in
Table 3.2 (page 31) in Dependency Parsing book by Sandra Kubler, Ryan McDonal, Joakim Nivre.
Please note that these features are very basic.
:return: list(str)",,extract_features
"A. Check the ARC-STANDARD training
>>> import tempfile
>>> import os
>>> input_file = tempfile.NamedTemporaryFile(prefix=’transition_parse.train’, dir=tempfile.gettempdir(), delete=False)",,transition
"This class defines a set of transition which is applied to a configuration to get another configuration
Note that for different parsing algorithm, the transition is different.",,left_arc
"This class defines a set of transition which is applied to a configuration to get another configuration
Note that for different parsing algorithm, the transition is different.",,reduce
"This class defines a set of transition which is applied to a configuration to get another configuration
Note that for different parsing algorithm, the transition is different.",,right_arc
"This class defines a set of transition which is applied to a configuration to get another configuration
Note that for different parsing algorithm, the transition is different.",,shift
"This class defines a set of transition which is applied to a configuration to get another configuration
Note that for different parsing algorithm, the transition is different.",,left_arc
"This class defines a set of transition which is applied to a configuration to get another configuration
Note that for different parsing algorithm, the transition is different.",,right_arc
"A. Check the ARC-STANDARD training
>>> import tempfile
>>> import os
>>> input_file = tempfile.NamedTemporaryFile(prefix=’transition_parse.train’, dir=tempfile.gettempdir(), delete=False)",,transitionparser
Class for transition based parser. Implement 2 algorithms which are “arc-standard” and “arc-eager”,,arc_eager
Class for transition based parser. Implement 2 algorithms which are “arc-standard” and “arc-eager”,,arc_standard
Unit tests for  CFG.,,nltk.parse.util.
Unit tests for  CFG.,,testgrammar
Unit tests for  CFG.,,run
"The result information is followed by a colon, and then the sentence.
Empty lines and lines beginning with a comment char are ignored.",,extract_test_sentences
"load_args – Keyword parameters used when loading the grammar.
See data.load for more information.",,load_parser
"A module to convert the a POS tagged document stream
(i.e. list of list of tuples, a list of sentences) and yield lines
in CONLL format. This module yields one line per word and two newlines
for end of sentence.",,taggedsents_to_conll
"A demonstration of the probabilistic parsers.  The user is
prompted to select which demo to run, and how many parses should
be found; and then each parser is run on the same demo, and a
summary of the results are displayed.",,nltk.parse.viterbi.
"A demonstration of the probabilistic parsers.  The user is
prompted to select which demo to run, and how many parses should
be found; and then each parser is run on the same demo, and a
summary of the results are displayed.",,viterbiparser
