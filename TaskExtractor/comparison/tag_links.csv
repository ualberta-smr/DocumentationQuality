Paragraph,Ground Truth link,Program link,Partial ratio
A module for POS tagging using CRFSuite https://pypi.python.org/pypi/python-crfsuite,">>> from nltk.tag import crftagger
>>> ct = crftagger()",set_model_file,17
A module for POS tagging using CRFSuite https://pypi.python.org/pypi/python-crfsuite,">>> train_data = [[('university','noun'), ('is','verb'), ('a','det'), ('good','adj'), ('place','noun')],
... [('dog','noun'),('eat','verb'),('meat','noun')]]",set_model_file,8
A module for POS tagging using CRFSuite https://pypi.python.org/pypi/python-crfsuite,">>> ct.train(train_data,'model.crf.tagger')
>>> ct.tag_sents([['dog','is','good'], ['cat','eat','meat']])
[[('dog', 'noun'), ('is', 'verb'), ('good', 'adj')], [('cat', 'noun'), ('eat', 'verb'), ('meat', 'noun')]]",set_model_file,9
A module for POS tagging using CRFSuite https://pypi.python.org/pypi/python-crfsuite,">>> gold_sentences = [[('dog','noun'),('is','verb'),('good','adj')] , [('cat','noun'),('eat','verb'), ('meat','noun')]]
>>> ct.evaluate(gold_sentences)
1.0",set_model_file,11
Retrieve the mapping dictionary between tagsets.,">>> tagset_mapping('ru-rnc', 'universal') == {'!': '.', 'a': 'adj', 'c': 'conj', 'ad': 'adv',            'nn': 'noun', 'vg': 'verb', 'comp': 'conj', 'nc': 'num', 'vp': 'verb', 'p': 'adp',            'ij': 'x', 'v': 'verb', 'z': 'x', 'vi': 'verb', 'yes_no_sent': 'x', 'ptcl': 'prt'}
true",tagset_mapping,10
Use the pretrain model (the default constructor),>>> pretrain = perceptrontagger(),end,11
Use the pretrain model (the default constructor),>>> pretrain = perceptrontagger(),start,21
Use the pretrain model (the default constructor),">>> pretrain.tag('the quick brown fox jumps over the lazy dog'.split())
[('the', 'dt'), ('quick', 'jj'), ('brown', 'nn'), ('fox', 'nn'), ('jumps', 'vbz'), ('over', 'in'), ('the', 'dt'), ('lazy', 'jj'), ('dog', 'nn')]",end,3
Use the pretrain model (the default constructor),">>> pretrain.tag('the quick brown fox jumps over the lazy dog'.split())
[('the', 'dt'), ('quick', 'jj'), ('brown', 'nn'), ('fox', 'nn'), ('jumps', 'vbz'), ('over', 'in'), ('the', 'dt'), ('lazy', 'jj'), ('dog', 'nn')]",start,5
Use the pretrain model (the default constructor),">>> pretrain.tag(""the red cat"".split())
[('the', 'dt'), ('red', 'jj'), ('cat', 'nn')]",end,7
Use the pretrain model (the default constructor),">>> pretrain.tag(""the red cat"".split())
[('the', 'dt'), ('red', 'jj'), ('cat', 'nn')]",start,9
,">>> backoff = regexptagger([
... (r'^-?[0-9]+(.[0-9]+)?$', 'cd'),   # cardinal numbers
... (r'(the|the|a|a|an|an)$', 'at'),   # articles
... (r'.*able$', 'jj'),                # adjectives
... (r'.*ness$', 'nn'),                # nouns formed from adjectives
... (r'.*ly$', 'rb'),                  # adverbs
... (r'.*s$', 'nns'),                  # plural nouns
... (r'.*ing$', 'vbg'),                # gerunds
... (r'.*ed$', 'vbd'),                 # past tense verbs
... (r'.*', 'nn')                      # nouns (default)
... ])"
"# NOTE1: (!!FIXME) A far better baseline uses nltk.tag.UnigramTagger, # with a RegexpTagger only as backoff. For instance, # >>> baseline = UnigramTagger(baseline_data, backoff=backoff) # However, as of Nov 2013, nltk.tag.UnigramTagger does not yield consistent results # between python versions. The simplistic backoff above is a workaround to make doctests # get consistent input.",>>> baseline = backoff #see note1
,">>> baseline.evaluate(gold_data) 
0.2450142..."
,">>> tagger1 = tt.train(training_data, max_rules=10)
tbl train (fast) (seqs: 100; tokens: 2417; tpls: 2; min score: 2; min acc: none)
finding initial useful rules...
    found 845 useful rules.

           b      |
   s   f   r   o  |        score = fixed - broken
   c   i   o   t  |  r     fixed = num tags changed incorrect -> correct
   o   x   k   h  |  u     broken = num tags changed correct -> incorrect
   r   e   e   e  |  l     other = num tags changed incorrect -> incorrect
   e   d   n   r  |  e
------------------+-------------------------------------------------------
 132 132   0   0  | at->dt if pos:nn@[-1]
  85  85   0   0  | nn->, if pos:nn@[-1] & word:,@[0]
  69  69   0   0  | nn->. if pos:nn@[-1] & word:.@[0]
  51  51   0   0  | nn->in if pos:nn@[-1] & word:of@[0]
  47  63  16 161  | nn->in if pos:nns@[-1]
  33  33   0   0  | nn->to if pos:nn@[-1] & word:to@[0]
  26  26   0   0  | in->. if pos:nns@[-1] & word:.@[0]
  24  24   0   0  | in->, if pos:nns@[-1] & word:,@[0]
  22  27   5  24  | nn->-none- if pos:vbd@[-1]
  17  17   0   0  | nn->cc if pos:nn@[-1] & word:and@[0]"
,">>> tagger1.rules()[1:3]
(rule('001', 'nn', ',', [(pos([-1]),'nn'), (word([0]),',')]), rule('001', 'nn', '.', [(pos([-1]),'nn'), (word([0]),'.')]))"
,">>> train_stats = tagger1.train_stats()
>>> [train_stats[stat] for stat in ['initialerrors', 'finalerrors', 'rulescores']]
[1775, 1269, [132, 85, 69, 51, 47, 33, 26, 24, 22, 17]]"
,">>> tagger1.print_template_statistics(printunused=false)
template statistics (train)  2 templates, 10 rules)
train (   2417 tokens) initial  1775 0.2656 final:  1269 0.4750
#id | score (train) |  #rules     | template
--------------------------------------------
001 |   305   0.603 |   7   0.700 | template(pos([-1]),word([0]))
000 |   201   0.397 |   3   0.300 | template(pos([-1]))"
,">>> tagger1.evaluate(gold_data) 
0.43996..."
,">>> tagged, test_stats = tagger1.batch_tag_incremental(testing_data, gold_data)"
,">>> tagged[33][12:] == [('foreign', 'in'), ('debt', 'nn'), ('of', 'in'), ('$', 'nn'), ('64', 'cd'),
... ('billion', 'nn'), ('*u*', 'nn'), ('--', 'nn'), ('the', 'dt'), ('third-highest', 'nn'), ('in', 'nn'),
... ('the', 'dt'), ('developing', 'vbg'), ('world', 'nn'), ('.', '.')]
true"
,">>> [test_stats[stat] for stat in ['initialerrors', 'finalerrors', 'rulescores']]
[1855, 1376, [100, 85, 67, 58, 27, 36, 27, 16, 31, 32]]"
,">>> tagger2.evaluate(gold_data)  
0.44159544...
>>> tagger2.rules()[2:4]
(rule('001', 'nn', '.', [(pos([-1]),'nn'), (word([0]),'.')]), rule('001', 'nn', 'in', [(pos([-1]),'nn'), (word([0]),'of')])"
Returns the entropy over labellings of the given sequence. This is given by:,h(o) = - sum_s pr(s | o) log pr(s | o)
"where the summation ranges over all state sequences, S. Let Z = Pr(O) = sum_S Pr(S, O)} where the summation ranges over all state sequences and O is the observation sequence. As such the entropy can be re-expressed as:","h = - sum_s pr(s | o) log [ pr(s, o) / z ]
= log z - sum_s pr(s | o) log pr(s, 0)
= log z - sum_s pr(s | o) [ log pr(s_0) + sum_t pr(s_t | s_{t-1}) + sum_t pr(o_t | s_t) ]"
"The order of summation for the log terms can be flipped, allowing dynamic programming to be used to calculate the entropy. Specifically, we use the forward and backward probabilities (alpha, beta) giving:","h = log z - sum_s0 alpha_0(s0) beta_0(s0) / z * log pr(s0)
+ sum_t,si,sj alpha_t(si) pr(sj | si) pr(o_t+1 | sj) beta_t(sj) / z * log pr(sj | si)
+ sum_t,st alpha_t(st) beta_t(st) / z * log pr(o_t | st)"
"
A class for pos tagging with HunPos. The input is the paths to:",">>> from nltk.tag import hunpostagger
>>> ht = hunpostagger('en_wsj.model')
>>> ht.tag('what is the airspeed of an unladen swallow ?'.split())
[('what', 'wp'), ('is', 'vbz'), ('the', 'dt'), ('airspeed', 'nn'), ('of', 'in'), ('an', 'dt'), ('unladen', 'nn'), ('swallow', 'vb'), ('?', '.')]
>>> ht.close()"
"This class communicates with the hunpos-tag binary via pipes. When the tagger object is no longer needed, the close() method should be called to free system resources. The class supports the context manager interface; if used in a with statement, the close() method is invoked automatically:",">>> with hunpostagger('en_wsj.model') as ht:
...     ht.tag('what is the airspeed of an unladen swallow ?'.split())
...
[('what', 'wp'), ('is', 'vbz'), ('the', 'dt'), ('airspeed', 'nn'), ('of', 'in'), ('an', 'dt'), ('unladen', 'nn'), ('swallow', 'vb'), ('?', '.')]"
Maps the tag from the source tagset to the target tagset.,">>> map_tag('en-ptb', 'universal', 'vbz')
'verb'
>>> map_tag('en-ptb', 'universal', 'vbp')
'verb'
>>> map_tag('en-ptb', 'universal', '``')
'.'"
"Greedy Averaged Perceptron tagger, as implemented by Matthew Honnibal. See more implementation details here:",>>> from nltk.tag.perceptron import perceptrontagger
Train the model,>>> tagger = perceptrontagger(load=false)
Train the model,">>> tagger.train([[('today','nn'),('is','vbz'),('good','jj'),('day','nn')],
... [('yes','nns'),('it','prp'),('beautiful','jj')]])"
Train the model,">>> tagger.tag(['today','is','a','beautiful','day'])
[('today', 'nn'), ('is', 'prp'), ('a', 'prp'), ('beautiful', 'jj'), ('day', 'nn')]"
SennaTagger will automatically search for executable file specified in SENNA environment variable,">>> from nltk.tag import sennatagger
>>> tagger = sennatagger('/usr/share/senna-v3.0')
>>> tagger.tag('what is the airspeed of an unladen swallow ?'.split()) 
[('what', 'wp'), ('is', 'vbz'), ('the', 'dt'), ('airspeed', 'nn'),
('of', 'in'), ('an', 'dt'), ('unladen', 'nn'), ('swallow', 'nn'), ('?', '.')]"
SennaTagger will automatically search for executable file specified in SENNA environment variable,">>> from nltk.tag import sennachunktagger
>>> chktagger = sennachunktagger('/usr/share/senna-v3.0')
>>> chktagger.tag('what is the airspeed of an unladen swallow ?'.split()) 
[('what', 'b-np'), ('is', 'b-vp'), ('the', 'b-np'), ('airspeed', 'i-np'),
('of', 'b-pp'), ('an', 'b-np'), ('unladen', 'i-np'), ('swallow', 'i-np'),
('?', 'o')]"
SennaTagger will automatically search for executable file specified in SENNA environment variable,">>> from nltk.tag import sennanertagger
>>> nertagger = sennanertagger('/usr/share/senna-v3.0')
>>> nertagger.tag('shakespeare theatre was in london .'.split()) 
[('shakespeare', 'b-per'), ('theatre', 'o'), ('was', 'o'), ('in', 'o'),
('london', 'b-loc'), ('.', 'o')]
>>> nertagger.tag('un headquarters are in ny , usa .'.split()) 
[('un', 'b-org'), ('headquarters', 'o'), ('are', 'o'), ('in', 'o'),
('ny', 'b-loc'), (',', 'o'), ('usa', 'b-loc'), ('.', 'o')]"
Extracts the chunks in a BIO chunk-tagged sentence.,">>> from nltk.tag import sennachunktagger
>>> chktagger = sennachunktagger('/usr/share/senna-v3.0')
>>> sent = 'what is the airspeed of an unladen swallow ?'.split()
>>> tagged_sent = chktagger.tag(sent) 
>>> tagged_sent 
[('what', 'b-np'), ('is', 'b-vp'), ('the', 'b-np'), ('airspeed', 'i-np'),
('of', 'b-pp'), ('an', 'b-np'), ('unladen', 'i-np'), ('swallow', 'i-np'),
('?', 'o')]
>>> list(chktagger.bio_to_chunks(tagged_sent, chunk_type='np')) 
[('what', '0'), ('the airspeed', '2-3'), ('an unladen swallow', '5-6-7')]"
Return the feature detector that this tagger uses to generate featuresets for its classifier. The feature detector is a function with the signature:,"feature_detector(tokens, index, history) -> featureset"
A sequential tagger that uses a classifier to choose the tag for each token in a sentence. The featureset input for the classifier is generated by a feature detector function:,"feature_detector(tokens, index, history) -> featureset"
Return the feature detector that this tagger uses to generate featuresets for its classifier. The feature detector is a function with the signature:,"feature_detector(tokens, index, history) -> featureset"
A tagger that assigns the same tag to every token.,">>> from nltk.tag import defaulttagger
>>> default_tagger = defaulttagger('nn')
>>> list(default_tagger.tag('this is a test'.split()))
[('this', 'nn'), ('is', 'nn'), ('a', 'nn'), ('test', 'nn')]"
The RegexpTagger assigns tags to tokens by comparing their word strings to a series of regular expressions. The following tagger uses word suffixes to make guesses about the correct Brown Corpus part of speech tag:,">>> from nltk.corpus import brown
>>> from nltk.tag import regexptagger
>>> test_sent = brown.sents(categories='news')[0]
>>> regexp_tagger = regexptagger(
...     [(r'^-?[0-9]+(.[0-9]+)?$', 'cd'),   # cardinal numbers
...      (r'(the|the|a|a|an|an)$', 'at'),   # articles
...      (r'.*able$', 'jj'),                # adjectives
...      (r'.*ness$', 'nn'),                # nouns formed from adjectives
...      (r'.*ly$', 'rb'),                  # adverbs
...      (r'.*s$', 'nns'),                  # plural nouns
...      (r'.*ing$', 'vbg'),                # gerunds
...      (r'.*ed$', 'vbd'),                 # past tense verbs
...      (r'.*', 'nn')                      # nouns (default)
... ])
>>> regexp_tagger
<regexp tagger: size=9>
>>> regexp_tagger.tag(test_sent)
[('the', 'at'), ('fulton', 'nn'), ('county', 'nn'), ('grand', 'nn'), ('jury', 'nn'),
('said', 'nn'), ('friday', 'nn'), ('an', 'at'), ('investigation', 'nn'), ('of', 'nn'),
(""atlanta's"", 'nns'), ('recent', 'nn'), ('primary', 'nn'), ('election', 'nn'),
('produced', 'vbd'), ('``', 'nn'), ('no', 'nn'), ('evidence', 'nn'), (""''"", 'nn'),
('that', 'nn'), ('any', 'nn'), ('irregularities', 'nns'), ('took', 'nn'),
('place', 'nn'), ('.', 'nn')]"
"The UnigramTagger finds the most likely tag for each word in a training corpus, and then uses that information to assign tags to new tokens.",">>> from nltk.corpus import brown
>>> from nltk.tag import unigramtagger
>>> test_sent = brown.sents(categories='news')[0]
>>> unigram_tagger = unigramtagger(brown.tagged_sents(categories='news')[:500])
>>> for tok, tag in unigram_tagger.tag(test_sent):
...     print(""({}, {}), "".format(tok, tag))
(the, at), (fulton, np-tl), (county, nn-tl), (grand, jj-tl),
(jury, nn-tl), (said, vbd), (friday, nr), (an, at),
(investigation, nn), (of, in), (atlanta's, np$), (recent, jj),
(primary, nn), (election, nn), (produced, vbd), (``, ``),
(no, at), (evidence, nn), ('', ''), (that, cs), (any, dti),
(irregularities, nns), (took, vbd), (place, nn), (., .),"
A class for Named-Entity Tagging with Stanford Tagger. The input is the paths to:,">>> from nltk.tag import stanfordnertagger
>>> st = stanfordnertagger('english.all.3class.distsim.crf.ser.gz') 
>>> st.tag('rami eid is studying at stony brook university in ny'.split()) 
[('rami', 'person'), ('eid', 'person'), ('is', 'o'), ('studying', 'o'),
 ('at', 'o'), ('stony', 'organization'), ('brook', 'organization'),
 ('university', 'organization'), ('in', 'o'), ('ny', 'location')]"
A class for pos tagging with Stanford Tagger. The input is the paths to:,">>> from nltk.tag import stanfordpostagger
>>> st = stanfordpostagger('english-bidirectional-distsim.tagger')
>>> st.tag('what is the airspeed of an unladen swallow ?'.split())
[('what', 'wp'), ('is', 'vbz'), ('the', 'dt'), ('airspeed', 'nn'), ('of', 'in'), ('an', 'dt'), ('unladen', 'jj'), ('swallow', 'vb'), ('?', '.')]"
"Given the string representation of a tagged token, return the corresponding tuple representation. The rightmost occurrence of sep in s will be used to divide s into a word string and a tag string. If sep does not occur in s, return (s, None).",">>> from nltk.tag.util import str2tuple
>>> str2tuple('fly/nn')
('fly', 'nn')"
"Given the tuple representation of a tagged token, return the corresponding string representation. This representation is formed by concatenating the token’s word string, followed by the separator, followed by the token’s tag. (If the tag is None, then just return the bare word string.)",">>> from nltk.tag.util import tuple2str
>>> tagged_token = ('fly', 'nn')
>>> tuple2str(tagged_token)
'fly/nn'"
"Given a tagged sentence, return an untagged version of that sentence. I.e., return a list containing the first element of each tuple in tagged_sentence.",">>> from nltk.tag.util import untag
>>> untag([('john', 'nnp'), ('saw', 'vbd'), ('mary', 'nnp')])
['john', 'saw', 'mary']"
"A “tag” is a case-sensitive string that specifies some property of a token, such as its part of speech. Tagged tokens are encoded as tuples (tag, token). For example, the following tagged token combines the word 'fly' with a noun part of speech tag ('NN'):",">>> tagged_tok = ('fly', 'nn')"
An off-the-shelf tagger is available for English. It uses the Penn Treebank tagset:,">>> from nltk import pos_tag, word_tokenize
>>> pos_tag(word_tokenize(""john's big idea isn't all that bad.""))
[('john', 'nnp'), (""'s"", 'pos'), ('big', 'jj'), ('idea', 'nn'), ('is', 'vbz'),
(""n't"", 'rb'), ('all', 'pdt'), ('that', 'dt'), ('bad', 'jj'), ('.', '.')]"
A Russian tagger is also available if you specify lang=”rus”. It uses the Russian National Corpus tagset:,">>> pos_tag(word_tokenize(""илья оторопел и дважды перечитал бумажку.""), lang='rus')    
[('илья', 's'), ('оторопел', 'v'), ('и', 'conj'), ('дважды', 'adv'), ('перечитал', 'v'),
('бумажку', 's'), ('.', 'nonlex')]"
"This package defines several taggers, which take a list of tokens, assign a tag to each one, and return the resulting list of tagged tokens. Most of the taggers are built automatically based on a training corpus. For example, the unigram tagger tags each word w by checking what the most frequent tag for w was in a training corpus:",">>> from nltk.corpus import brown
>>> from nltk.tag import unigramtagger
>>> tagger = unigramtagger(brown.tagged_sents(categories='news')[:500])
>>> sent = ['mitchell', 'decried', 'the', 'high', 'rate', 'of', 'unemployment']
>>> for word, tag in tagger.tag(sent):
...     print(word, '->', tag)
mitchell -> np
decried -> none
the -> at
high -> jj
rate -> nn
of -> in
unemployment -> none"
We evaluate a tagger on data that was not seen during training:,">>> tagger.evaluate(brown.tagged_sents(categories='news')[500:600])
0.7..."
Use NLTK’s currently recommended part of speech tagger to tag the given list of tokens.,">>> from nltk.tag import pos_tag
>>> from nltk.tokenize import word_tokenize
>>> pos_tag(word_tokenize(""john's big idea isn't all that bad.""))
[('john', 'nnp'), (""'s"", 'pos'), ('big', 'jj'), ('idea', 'nn'), ('is', 'vbz'),
(""n't"", 'rb'), ('all', 'pdt'), ('that', 'dt'), ('bad', 'jj'), ('.', '.')]
>>> pos_tag(word_tokenize(""john's big idea isn't all that bad.""), tagset='universal')
[('john', 'noun'), (""'s"", 'prt'), ('big', 'adj'), ('idea', 'noun'), ('is', 'verb'),
(""n't"", 'adv'), ('all', 'det'), ('that', 'det'), ('bad', 'adj'), ('.', '.')]"
return [self.tag(sent) for sent in sentences],,nltk.tag.api.
return [self.tag(sent) for sent in sentences],,taggeri
"Score the accuracy of the tagger against the gold standard.
Strip the tags from the gold standard text, retag it using
the tagger, then compute the accuracy score.",,evaluate
"Determine the most appropriate tag sequence for the given
token sequence, and return a corresponding list of tagged
tokens.  A tagged token is encoded as a tuple (token, tag).",,tag
return [self.tag(sent) for sent in sentences],,tag_sents
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",,nltk.tag.brill.
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",,brilltagger
"NOTE: This is inefficient (does not build any index, so will traverse the entire
corpus N times for N rules) – usually you would not care about statistics for
individual rules and thus use batch_tag() instead",,batch_tag_incremental
"NOTE: This is inefficient (does not build any index, so will traverse the entire
corpus N times for N rules) – usually you would not care about statistics for
individual rules and thus use batch_tag() instead",,decode_json_obj
"NOTE: This is inefficient (does not build any index, so will traverse the entire
corpus N times for N rules) – usually you would not care about statistics for
individual rules and thus use batch_tag() instead",,encode_json_obj
"NOTE: This is inefficient (does not build any index, so will traverse the entire
corpus N times for N rules) – usually you would not care about statistics for
individual rules and thus use batch_tag() instead",,json_tag
"printunused (bool) – if True, print a list of all unused templates",,print_template_statistics
the ordered list of transformation rules that correct the initial tagging,,rules
"Return a named statistic collected during training, or a dictionary of all
available statistics if no name given",,train_stats
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",,pos
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",,word
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",,brill24
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",,describe_template_sets
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",,fntbl37
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",,nltkdemo18
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",,nltkdemo18plus
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",,nltk.tag.brill_trainer.
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",,brilltaggertrainer
min_score (int) – stop training when no rules better than min_score can be found,,train
"Train the CRF tagger using CRFSuite
:params train_data : is the list of annotated sentences.
:type train_data : list (list(tuple(str,str)))
:params model_file : the model will be saved to this file.",,nltk.tag.crf.
"Train the CRF tagger using CRFSuite
:params train_data : is the list of annotated sentences.
:type train_data : list (list(tuple(str,str)))
:params model_file : the model will be saved to this file.",,crftagger
"Train a new HiddenMarkovModelTagger using the given labeled and
unlabeled training instances. Testing will be performed if test
instances are provided.",,nltk.tag.hmm.
"Train a new HiddenMarkovModelTagger using the given labeled and
unlabeled training instances. Testing will be performed if test
instances are provided.",,hiddenmarkovmodeltagger
"Returns the state sequence of the optimal (most probable) path through
the HMM. Uses the Viterbi algorithm to calculate this part by dynamic
programming.",,best_path
"Returns the state sequence of the optimal (most probable) path through
the HMM. Uses the Viterbi algorithm to calculate this part by dynamic
programming.  This uses a simple, direct method, and is included for
teaching purposes.",,best_path_simple
"This simply uses alpha and beta to find the probabilities of partial
sequences, constrained to include the given state(s) at some point in
time.",,entropy
"Returns the log-probability of the given symbol sequence. If the
sequence is labelled, then returns the joint log-probability of the
symbol, state sequence. Otherwise, uses the forward algorithm to find
the log-probability over all label sequences.",,log_probability
"Returns the pointwise entropy over the possible states at each
position in the chain, given the observation sequence.",,point_entropy
"Returns the probability of the given symbol sequence. If the sequence
is labelled, then returns the joint probability of the symbol, state
sequence. Otherwise, uses the forward algorithm to find the
probability over all label sequences.",,probability
"Randomly sample the HMM to generate a sentence of a given length. This
samples the prior distribution then the observation distribution and
transition distribution for each subsequent observation and state.
This will mostly generate unintelligible garbage, but can provide some
amusement.",,random_sample
"Randomly sample the HMM to generate a sentence of a given length. This
samples the prior distribution then the observation distribution and
transition distribution for each subsequent observation and state.
This will mostly generate unintelligible garbage, but can provide some
amusement.",,reset_cache
"verbose (bool) – boolean flag indicating whether training should be
verbose or include printed output",,test
kwargs may include following parameters:,,hiddenmarkovmodeltrainer
"estimator – a function taking
a FreqDist and a number of bins and returning a CProbDistI;
otherwise a MLE estimate is used",,train_supervised
kwargs may include following parameters:,,train_unsupervised
kwargs may include following parameters:,,demo
kwargs may include following parameters:,,demo_bw
kwargs may include following parameters:,,demo_pos
kwargs may include following parameters:,,demo_pos_bw
kwargs may include following parameters:,,load_pos
kwargs may include following parameters:,,logsumexp2
"This class communicates with the hunpos-tag binary via pipes. When the
tagger object is no longer needed, the close() method should be called to
free system resources. The class supports the context manager interface; if
used in a with statement, the close() method is invoked automatically:",,nltk.tag.hunpos.
"This class communicates with the hunpos-tag binary via pipes. When the
tagger object is no longer needed, the close() method should be called to
free system resources. The class supports the context manager interface; if
used in a with statement, the close() method is invoked automatically:",,hunpostagger
"This class communicates with the hunpos-tag binary via pipes. When the
tagger object is no longer needed, the close() method should be called to
free system resources. The class supports the context manager interface; if
used in a with statement, the close() method is invoked automatically:",,close
"Interface for converting POS tags from various treebanks
to the universal tagset of Petrov, Das, & McDonald.",,nltk.tag.mapping.
"Interface for converting POS tags from various treebanks
to the universal tagset of Petrov, Das, & McDonald.",,map_tag
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",,nltk.tag.perceptron.
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",,averagedperceptron
"An averaged perceptron, as implemented by Matthew Honnibal.",,average_weights
"An averaged perceptron, as implemented by Matthew Honnibal.",,load
Dot-product the features and current weights and return the best label.,,predict
Save the pickled model weights.,,save
Update the feature weights.,,update
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",,perceptrontagger
"Normalization used in pre-processing.
- All words are lower cased
- Groups of digits of length 4 are represented as !YEAR;
- Other digits are represented as !DIGITS",,normalize
"Applies the tag method over a list of sentences. This method will return
for each sentence a list of tuples of (word, tag).",,nltk.tag.senna.
"Applies the tag method over a list of sentences. This method will return
for each sentence a list of tuples of (word, tag).",,sennachunktagger
"Applies the tag method over a list of sentences. This method will return
for each sentence a list of tuples of (word, tag).",,sennanertagger
"Applies the tag method over a list of sentences. This method will return
for each sentence a list of tuples of (word, tag).",,sennatagger
"min_stem_length – Any words whose length is less than
min_stem_length+abs(affix_length) will be assigned a
tag of None by this tagger.",,nltk.tag.sequential.
"min_stem_length – Any words whose length is less than
min_stem_length+abs(affix_length) will be assigned a
tag of None by this tagger.",,affixtagger
"min_stem_length – Any words whose length is less than
min_stem_length+abs(affix_length) will be assigned a
tag of None by this tagger.",,context
"A tagger that chooses a token’s tag based its word string and on
the preceding two words’ tags.  In particular, a tuple consisting
of the previous two tags and the word is looked up in a table, and
the corresponding tag is returned.",,bigramtagger
"Return the feature detector that this tagger uses to generate
featuresets for its classifier.  The feature detector is a
function with the signature:",,classifierbasedpostagger
"Return the feature detector that this tagger uses to generate
featuresets for its classifier.  The feature detector is a
function with the signature:",,feature_detector
"index (int) – The index of the word whose tag should be
returned.",,classifierbasedtagger
"index (int) – The index of the word whose tag should be
returned.",,choose_tag
"Return the classifier that this tagger uses to choose a tag
for each word in a sentence.  The input for this classifier is
generated using this tagger’s feature detector.
See feature_detector()",,classifier
"index (int) – The index of the word whose tag should be
returned.",,contexttagger
"The number of entries in the table used by this
tagger to map from contexts to tags.",,size
"index (int) – The index of the word whose tag should be
returned.",,defaulttagger
"cutoff – If the most likely tag for a context occurs
fewer than cutoff times, then exclude it from the
context-to-tag table for the new tagger.",,ngramtagger
"index (int) – The index of the word whose tag should be
returned.",,regexptagger
"Determine the most appropriate tag sequence for the given
token sequence, and return a corresponding list of tagged
tokens.  A tagged token is encoded as a tuple (token, tag).",,sequentialbackofftagger
"An abstract base class for taggers that tags words sequentially,
left to right.  Tagging of individual words is performed by the
choose_tag() method, which should be defined by subclasses.  If
a tagger is unable to determine a tag for the specified token,
then its backoff tagger is consulted.",,backoff
"index (int) – The index of the word whose tag should be
returned.",,tag_one
"A tagger that chooses a token’s tag based its word string and on
the preceding two words’ tags.  In particular, a tuple consisting
of the previous two tags and the word is looked up in a table, and
the corresponding tag is returned.",,trigramtagger
"The UnigramTagger finds the most likely tag for each word in a training
corpus, and then uses that information to assign tags to new tokens.",,unigramtagger
"(optionally) the path to the stanford tagger jar file. If not specified here,
then this jar file must be specified in the CLASSPATH envinroment variable.",,nltk.tag.stanford.
"(optionally) the path to the stanford tagger jar file. If not specified here,
then this jar file must be specified in the CLASSPATH envinroment variable.",,stanfordnertagger
"(optionally) the path to the stanford tagger jar file. If not specified here,
then this jar file must be specified in the CLASSPATH envinroment variable.",,parse_output
"(optionally) the path to the stanford tagger jar file. If not specified here,
then this jar file must be specified in the CLASSPATH envinroment variable.",,stanfordpostagger
return [self.tag(sent) for sent in sentences],,stanfordtagger
"Uses a set of tagged data to train the tagger.
If an unknown word tagger is specified,
it is trained on the same data.",,nltk.tag.tnt.
"Uses a set of tagged data to train the tagger.
If an unknown word tagger is specified,
it is trained on the same data.",,tnt
"Invokes tag(sent) function for each sentence
compiles the results into a list of tagged sentences
each tagged sentence is a list of (word, tag) tuples",,tagdata
"Function takes a list of tokens and separates the tokens into lists
where each list represents a sentence fragment
This function can separate both tagged and raw sequences into
basic sentences.",,basic_sent_chop
"Function takes a list of tokens and separates the tokens into lists
where each list represents a sentence fragment
This function can separate both tagged and raw sequences into
basic sentences.",,demo2
"Function takes a list of tokens and separates the tokens into lists
where each list represents a sentence fragment
This function can separate both tagged and raw sequences into
basic sentences.",,demo3
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",,nltk.tag.util.
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",,str2tuple
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",,tuple2str
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",,untag
NB. Use pos_tag_sents() for efficient tagging of more than one sentence.,,nltk.tag.
NB. Use pos_tag_sents() for efficient tagging of more than one sentence.,,pos_tag
NB. Use pos_tag_sents() for efficient tagging of more than one sentence.,,pos_tag_sents
"S   F   r   O  |        Score = Fixed - Broken
c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct
o   x   k   h  |  u     Broken = num tags changed correct -> incorrect
r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect
e   d   n   r  |  e",,">>> tagger1 = tt.train(training_data, max_rules=10)
tbl train (fast) (seqs: 100; tokens: 2417; tpls: 2; min score: 2; min acc: none)
finding initial useful rules...
    found 845 useful rules.

           b      |
   s   f   r   o  |        score = fixed - broken
   c   i   o   t  |  r     fixed = num tags changed incorrect -> correct
   o   x   k   h  |  u     broken = num tags changed correct -> incorrect
   r   e   e   e  |  l     other = num tags changed incorrect -> incorrect
   e   d   n   r  |  e
------------------+-------------------------------------------------------
 132 132   0   0  | at->dt if pos:nn@[-1]
  85  85   0   0  | nn->, if pos:nn@[-1] & word:,@[0]
  69  69   0   0  | nn->. if pos:nn@[-1] & word:.@[0]
  51  51   0   0  | nn->in if pos:nn@[-1] & word:of@[0]
  47  63  16 161  | nn->in if pos:nns@[-1]
  33  33   0   0  | nn->to if pos:nn@[-1] & word:to@[0]
  26  26   0   0  | in->. if pos:nns@[-1] & word:.@[0]
  24  24   0   0  | in->, if pos:nns@[-1] & word:,@[0]
  22  27   5  24  | nn->-none- if pos:vbd@[-1]
  17  17   0   0  | nn->cc if pos:nn@[-1] & word:and@[0]"
"feature_detector – A function used to generate the
featureset input for the classifier::
feature_detector(tokens, index, history) -> featureset",,"feature_detector(tokens, index, history) -> featureset"
