return [self.tag(sent) for sent in sentences],nltk.tag.api.
return [self.tag(sent) for sent in sentences],TaggerI
"Score the accuracy of the tagger against the gold standard.
Strip the tags from the gold standard text, retag it using
the tagger, then compute the accuracy score.",evaluate
"Determine the most appropriate tag sequence for the given
token sequence, and return a corresponding list of tagged
tokens.  A tagged token is encoded as a tuple (token, tag).",tag
return [self.tag(sent) for sent in sentences],tag_sents
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",nltk.tag.brill.
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",BrillTagger
"NOTE: This is inefficient (does not build any index, so will traverse the entire
corpus N times for N rules) – usually you would not care about statistics for
individual rules and thus use batch_tag() instead",batch_tag_incremental
"NOTE: This is inefficient (does not build any index, so will traverse the entire
corpus N times for N rules) – usually you would not care about statistics for
individual rules and thus use batch_tag() instead",decode_json_obj
"NOTE: This is inefficient (does not build any index, so will traverse the entire
corpus N times for N rules) – usually you would not care about statistics for
individual rules and thus use batch_tag() instead",encode_json_obj
"NOTE: This is inefficient (does not build any index, so will traverse the entire
corpus N times for N rules) – usually you would not care about statistics for
individual rules and thus use batch_tag() instead",json_tag
"printunused (bool) – if True, print a list of all unused templates",print_template_statistics
the ordered list of transformation rules that correct the initial tagging,rules
"Return a named statistic collected during training, or a dictionary of all
available statistics if no name given",train_stats
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",Pos
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",Word
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",brill24
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",describe_template_sets
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",fntbl37
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",nltkdemo18
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",nltkdemo18plus
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",nltk.tag.brill_trainer.
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",BrillTaggerTrainer
min_score (int) – stop training when no rules better than min_score can be found,train
"Train the CRF tagger using CRFSuite
:params train_data : is the list of annotated sentences.
:type train_data : list (list(tuple(str,str)))
:params model_file : the model will be saved to this file.",nltk.tag.crf.
"Train the CRF tagger using CRFSuite
:params train_data : is the list of annotated sentences.
:type train_data : list (list(tuple(str,str)))
:params model_file : the model will be saved to this file.",CRFTagger
A module for POS tagging using CRFSuite https://pypi.python.org/pypi/python-crfsuite,set_model_file
"Train a new HiddenMarkovModelTagger using the given labeled and
unlabeled training instances. Testing will be performed if test
instances are provided.",nltk.tag.hmm.
"Train a new HiddenMarkovModelTagger using the given labeled and
unlabeled training instances. Testing will be performed if test
instances are provided.",HiddenMarkovModelTagger
"Returns the state sequence of the optimal (most probable) path through
the HMM. Uses the Viterbi algorithm to calculate this part by dynamic
programming.",best_path
"Returns the state sequence of the optimal (most probable) path through
the HMM. Uses the Viterbi algorithm to calculate this part by dynamic
programming.  This uses a simple, direct method, and is included for
teaching purposes.",best_path_simple
"This simply uses alpha and beta to find the probabilities of partial
sequences, constrained to include the given state(s) at some point in
time.",entropy
"Returns the log-probability of the given symbol sequence. If the
sequence is labelled, then returns the joint log-probability of the
symbol, state sequence. Otherwise, uses the forward algorithm to find
the log-probability over all label sequences.",log_probability
"Returns the pointwise entropy over the possible states at each
position in the chain, given the observation sequence.",point_entropy
"Returns the probability of the given symbol sequence. If the sequence
is labelled, then returns the joint probability of the symbol, state
sequence. Otherwise, uses the forward algorithm to find the
probability over all label sequences.",probability
"Randomly sample the HMM to generate a sentence of a given length. This
samples the prior distribution then the observation distribution and
transition distribution for each subsequent observation and state.
This will mostly generate unintelligible garbage, but can provide some
amusement.",random_sample
"Randomly sample the HMM to generate a sentence of a given length. This
samples the prior distribution then the observation distribution and
transition distribution for each subsequent observation and state.
This will mostly generate unintelligible garbage, but can provide some
amusement.",reset_cache
"verbose (bool) – boolean flag indicating whether training should be
verbose or include printed output",test
kwargs may include following parameters:,HiddenMarkovModelTrainer
"estimator – a function taking
a FreqDist and a number of bins and returning a CProbDistI;
otherwise a MLE estimate is used",train_supervised
kwargs may include following parameters:,train_unsupervised
kwargs may include following parameters:,demo
kwargs may include following parameters:,demo_bw
kwargs may include following parameters:,demo_pos
kwargs may include following parameters:,demo_pos_bw
kwargs may include following parameters:,load_pos
kwargs may include following parameters:,logsumexp2
"This class communicates with the hunpos-tag binary via pipes. When the
tagger object is no longer needed, the close() method should be called to
free system resources. The class supports the context manager interface; if
used in a with statement, the close() method is invoked automatically:",nltk.tag.hunpos.
"This class communicates with the hunpos-tag binary via pipes. When the
tagger object is no longer needed, the close() method should be called to
free system resources. The class supports the context manager interface; if
used in a with statement, the close() method is invoked automatically:",HunposTagger
"This class communicates with the hunpos-tag binary via pipes. When the
tagger object is no longer needed, the close() method should be called to
free system resources. The class supports the context manager interface; if
used in a with statement, the close() method is invoked automatically:",close
"Interface for converting POS tags from various treebanks
to the universal tagset of Petrov, Das, & McDonald.",nltk.tag.mapping.
"Interface for converting POS tags from various treebanks
to the universal tagset of Petrov, Das, & McDonald.",map_tag
Retrieve the mapping dictionary between tagsets.,tagset_mapping
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",nltk.tag.perceptron.
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",AveragedPerceptron
"An averaged perceptron, as implemented by Matthew Honnibal.",average_weights
"An averaged perceptron, as implemented by Matthew Honnibal.",load
Dot-product the features and current weights and return the best label.,predict
Save the pickled model weights.,save
Update the feature weights.,update
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",PerceptronTagger
Use the pretrain model (the default constructor),END
Use the pretrain model (the default constructor),START
"Normalization used in pre-processing.
- All words are lower cased
- Groups of digits of length 4 are represented as !YEAR;
- Other digits are represented as !DIGITS",normalize
"Applies the tag method over a list of sentences. This method will return
for each sentence a list of tuples of (word, tag).",nltk.tag.senna.
"Applies the tag method over a list of sentences. This method will return
for each sentence a list of tuples of (word, tag).",SennaChunkTagger
"Applies the tag method over a list of sentences. This method will return
for each sentence a list of tuples of (word, tag).",SennaNERTagger
"Applies the tag method over a list of sentences. This method will return
for each sentence a list of tuples of (word, tag).",SennaTagger
"min_stem_length – Any words whose length is less than
min_stem_length+abs(affix_length) will be assigned a
tag of None by this tagger.",nltk.tag.sequential.
"min_stem_length – Any words whose length is less than
min_stem_length+abs(affix_length) will be assigned a
tag of None by this tagger.",AffixTagger
"min_stem_length – Any words whose length is less than
min_stem_length+abs(affix_length) will be assigned a
tag of None by this tagger.",context
"A tagger that chooses a token’s tag based its word string and on
the preceding two words’ tags.  In particular, a tuple consisting
of the previous two tags and the word is looked up in a table, and
the corresponding tag is returned.",BigramTagger
"Return the feature detector that this tagger uses to generate
featuresets for its classifier.  The feature detector is a
function with the signature:",ClassifierBasedPOSTagger
"Return the feature detector that this tagger uses to generate
featuresets for its classifier.  The feature detector is a
function with the signature:",feature_detector
"index (int) – The index of the word whose tag should be
returned.",ClassifierBasedTagger
"index (int) – The index of the word whose tag should be
returned.",choose_tag
"Return the classifier that this tagger uses to choose a tag
for each word in a sentence.  The input for this classifier is
generated using this tagger’s feature detector.
See feature_detector()",classifier
"index (int) – The index of the word whose tag should be
returned.",ContextTagger
"The number of entries in the table used by this
tagger to map from contexts to tags.",size
"index (int) – The index of the word whose tag should be
returned.",DefaultTagger
"cutoff – If the most likely tag for a context occurs
fewer than cutoff times, then exclude it from the
context-to-tag table for the new tagger.",NgramTagger
"index (int) – The index of the word whose tag should be
returned.",RegexpTagger
"Determine the most appropriate tag sequence for the given
token sequence, and return a corresponding list of tagged
tokens.  A tagged token is encoded as a tuple (token, tag).",SequentialBackoffTagger
"An abstract base class for taggers that tags words sequentially,
left to right.  Tagging of individual words is performed by the
choose_tag() method, which should be defined by subclasses.  If
a tagger is unable to determine a tag for the specified token,
then its backoff tagger is consulted.",backoff
"index (int) – The index of the word whose tag should be
returned.",tag_one
"A tagger that chooses a token’s tag based its word string and on
the preceding two words’ tags.  In particular, a tuple consisting
of the previous two tags and the word is looked up in a table, and
the corresponding tag is returned.",TrigramTagger
"The UnigramTagger finds the most likely tag for each word in a training
corpus, and then uses that information to assign tags to new tokens.",UnigramTagger
"(optionally) the path to the stanford tagger jar file. If not specified here,
then this jar file must be specified in the CLASSPATH envinroment variable.",nltk.tag.stanford.
"(optionally) the path to the stanford tagger jar file. If not specified here,
then this jar file must be specified in the CLASSPATH envinroment variable.",StanfordNERTagger
"(optionally) the path to the stanford tagger jar file. If not specified here,
then this jar file must be specified in the CLASSPATH envinroment variable.",parse_output
"(optionally) the path to the stanford tagger jar file. If not specified here,
then this jar file must be specified in the CLASSPATH envinroment variable.",StanfordPOSTagger
return [self.tag(sent) for sent in sentences],StanfordTagger
"Uses a set of tagged data to train the tagger.
If an unknown word tagger is specified,
it is trained on the same data.",nltk.tag.tnt.
"Uses a set of tagged data to train the tagger.
If an unknown word tagger is specified,
it is trained on the same data.",TnT
"Invokes tag(sent) function for each sentence
compiles the results into a list of tagged sentences
each tagged sentence is a list of (word, tag) tuples",tagdata
"Function takes a list of tokens and separates the tokens into lists
where each list represents a sentence fragment
This function can separate both tagged and raw sequences into
basic sentences.",basic_sent_chop
"Function takes a list of tokens and separates the tokens into lists
where each list represents a sentence fragment
This function can separate both tagged and raw sequences into
basic sentences.",demo2
"Function takes a list of tokens and separates the tokens into lists
where each list represents a sentence fragment
This function can separate both tagged and raw sequences into
basic sentences.",demo3
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",nltk.tag.util.
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",str2tuple
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",tuple2str
"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",untag
NB. Use pos_tag_sents() for efficient tagging of more than one sentence.,nltk.tag.
NB. Use pos_tag_sents() for efficient tagging of more than one sentence.,pos_tag
NB. Use pos_tag_sents() for efficient tagging of more than one sentence.,pos_tag_sents
"S   F   r   O  |        Score = Fixed - Broken
c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct
o   x   k   h  |  u     Broken = num tags changed correct -> incorrect
r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect
e   d   n   r  |  e",">>> tagger1 = tt.train(training_data, max_rules=10)
TBL train (fast) (seqs: 100; tokens: 2417; tpls: 2; min score: 2; min acc: None)
Finding initial useful rules...
    Found 845 useful rules.

           B      |
   S   F   r   O  |        Score = Fixed - Broken
   c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct
   o   x   k   h  |  u     Broken = num tags changed correct -> incorrect
   r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect
   e   d   n   r  |  e
------------------+-------------------------------------------------------
 132 132   0   0  | AT->DT if Pos:NN@[-1]
  85  85   0   0  | NN->, if Pos:NN@[-1] & Word:,@[0]
  69  69   0   0  | NN->. if Pos:NN@[-1] & Word:.@[0]
  51  51   0   0  | NN->IN if Pos:NN@[-1] & Word:of@[0]
  47  63  16 161  | NN->IN if Pos:NNS@[-1]
  33  33   0   0  | NN->TO if Pos:NN@[-1] & Word:to@[0]
  26  26   0   0  | IN->. if Pos:NNS@[-1] & Word:.@[0]
  24  24   0   0  | IN->, if Pos:NNS@[-1] & Word:,@[0]
  22  27   5  24  | NN->-NONE- if Pos:VBD@[-1]
  17  17   0   0  | NN->CC if Pos:NN@[-1] & Word:and@[0]"
"feature_detector – A function used to generate the
featureset input for the classifier::
feature_detector(tokens, index, history) -> featureset","feature_detector(tokens, index, history) -> featureset"
