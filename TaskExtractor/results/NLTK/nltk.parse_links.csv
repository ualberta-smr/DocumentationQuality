Paragraph,Example,Page
Tag a list of tokens.,">>> parser = CoreNLPParser(url='http://localhost:9000', tagtype='ner')
>>> tokens = 'Rami Eid is studying at Stony Brook University in NY'.split()
>>> parser.tag(tokens)
[('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'),
('Brook', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'O')]",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
Tag a list of tokens.,">>> parser = CoreNLPParser(url='http://localhost:9000', tagtype='pos')
>>> tokens = ""What is the airspeed of an unladen swallow ?"".split()
>>> parser.tag(tokens)
[('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'),
('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'),
('unladen', 'JJ'), ('swallow', 'VB'), ('?', '.')]",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
Tokenize a string of text.,>>> parser = CoreNLPParser(url='http://localhost:9000'),https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
Tokenize a string of text.,">>> text = 'Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.\nThanks.'
>>> list(parser.tokenize(text))
['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
Tokenize a string of text.,">>> s = ""The colour of the wall is blue.""
>>> list(
...     parser.tokenize(
...         'The colour of the wall is blue.',
...             properties={'tokenize.options': 'americanize=true'},
...     )
... )
['The', 'color', 'of', 'the', 'wall', 'is', 'blue', '.']",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
Return a dot representation suitable for using with Graphviz.,">>> dg = DependencyGraph(
...     'John N 2\n'
...     'loves V 0\n'
...     'Mary N 2'
... )
>>> print(dg.to_dot())
digraph G{
edge [dir=forward]
node [shape=plaintext]

0 [label=""0 (None)""]
0 -> 2 [label=""ROOT""]
1 [label=""1 (John)""]
2 [label=""2 (loves)""]
2 -> 1 [label=""""]
2 -> 3 [label=""""]
3 [label=""3 (Mary)""]
}",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"Class for measuring labelled and unlabelled attachment score for
dependency parsing. Note that the evaluation ignores punctuation.",">>> from nltk.parse import DependencyGraph, DependencyEvaluator",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"Class for measuring labelled and unlabelled attachment score for
dependency parsing. Note that the evaluation ignores punctuation.",">>> gold_sent = DependencyGraph(""""""
... Pierre  NNP     2       NMOD
... Vinken  NNP     8       SUB
... ,       ,       2       P
... 61      CD      5       NMOD
... years   NNS     6       AMOD
... old     JJ      2       NMOD
... ,       ,       2       P
... will    MD      0       ROOT
... join    VB      8       VC
... the     DT      11      NMOD
... board   NN      9       OBJ
... as      IN      9       VMOD
... a       DT      15      NMOD
... nonexecutive    JJ      15      NMOD
... director        NN      12      PMOD
... Nov.    NNP     9       VMOD
... 29      CD      16      NMOD
... .       .       9       VMOD
... """""")",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"Class for measuring labelled and unlabelled attachment score for
dependency parsing. Note that the evaluation ignores punctuation.",">>> parsed_sent = DependencyGraph(""""""
... Pierre  NNP     8       NMOD
... Vinken  NNP     1       SUB
... ,       ,       3       P
... 61      CD      6       NMOD
... years   NNS     6       AMOD
... old     JJ      2       NMOD
... ,       ,       3       AMOD
... will    MD      0       ROOT
... join    VB      8       VC
... the     DT      11      AMOD
... board   NN      9       OBJECT
... as      IN      9       NMOD
... a       DT      15      NMOD
... nonexecutive    JJ      15      NMOD
... director        NN      12      PMOD
... Nov.    NNP     9       VMOD
... 29      CD      16      NMOD
... .       .       9       VMOD
... """""")",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"Class for measuring labelled and unlabelled attachment score for
dependency parsing. Note that the evaluation ignores punctuation.",">>> de = DependencyEvaluator([parsed_sent],[gold_sent])
>>> las, uas = de.eval()
>>> las
0.6...
>>> uas
0.8...
>>> abs(uas - 0.8) < 0.00001
True",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"A class for dependency parsing with MaltParser. The input is the paths to:
- a maltparser directory
- (optionally) the path to a pre-trained MaltParser .mco model file
- (optionally) the tagger to use for POS tagging before parsing
- (optionally) additional Java arguments",">>> from nltk.parse import malt
>>> # With MALT_PARSER and MALT_MODEL environment set.
>>> mp = malt.MaltParser('maltparser-1.7.2', 'engmalt.linear-1.7.mco') 
>>> mp.parse_one('I shot an elephant in my pajamas .'.split()).tree() 
(shot I (elephant an) (in (pajamas my)) .)
>>> # Without MALT_PARSER and MALT_MODEL environment.
>>> mp = malt.MaltParser('/home/user/maltparser-1.7.2/', '/home/user/engmalt.linear-1.7.mco') 
>>> mp.parse_one('I shot an elephant in my pajamas .'.split()).tree() 
(shot I (elephant an) (in (pajamas my)) .)",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"A dependency scorer built around a MaxEnt classifier.  In this
particular class that classifier is a NaiveBayesClassifier.
It uses head-word, head-tag, child-word, and child-tag features
for classification.",">>> from nltk.parse.dependencygraph import DependencyGraph, conll_data2",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"A dependency scorer built around a MaxEnt classifier.  In this
particular class that classifier is a NaiveBayesClassifier.
It uses head-word, head-tag, child-word, and child-tag features
for classification.",">>> graphs = [DependencyGraph(entry) for entry in conll_data2.split('\n\n') if entry]
>>> npp = ProbabilisticNonprojectiveParser()
>>> npp.train(graphs, NaiveBayesDependencyScorer())
>>> parses = npp.parse(['Cathy', 'zag', 'hen', 'zwaaien', '.'], ['N', 'V', 'Pron', 'Adj', 'N', 'Punc'])
>>> len(list(parses))
1",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"This parser returns the most probable projective parse derived from the
probabilistic dependency grammar derived from the train() method.  The
probabilistic model is an implementation of Eisner’s (1996) Model C, which
conditions on head-word, head-tag, child-word, and child-tag.  The decoding
uses a bottom-up chart-based span concatenation algorithm that’s identical
to the one utilized by the rule-based projective parser.",>>> from nltk.parse.dependencygraph import conll_data2,https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"This parser returns the most probable projective parse derived from the
probabilistic dependency grammar derived from the train() method.  The
probabilistic model is an implementation of Eisner’s (1996) Model C, which
conditions on head-word, head-tag, child-word, and child-tag.  The decoding
uses a bottom-up chart-based span concatenation algorithm that’s identical
to the one utilized by the rule-based projective parser.",">>> graphs = [
... DependencyGraph(entry) for entry in conll_data2.split('\n\n') if entry
... ]",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"This parser returns the most probable projective parse derived from the
probabilistic dependency grammar derived from the train() method.  The
probabilistic model is an implementation of Eisner’s (1996) Model C, which
conditions on head-word, head-tag, child-word, and child-tag.  The decoding
uses a bottom-up chart-based span concatenation algorithm that’s identical
to the one utilized by the rule-based projective parser.",">>> ppdp = ProbabilisticProjectiveDependencyParser()
>>> ppdp.train(graphs)",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"This parser returns the most probable projective parse derived from the
probabilistic dependency grammar derived from the train() method.  The
probabilistic model is an implementation of Eisner’s (1996) Model C, which
conditions on head-word, head-tag, child-word, and child-tag.  The decoding
uses a bottom-up chart-based span concatenation algorithm that’s identical
to the one utilized by the rule-based projective parser.",">>> sent = ['Cathy', 'zag', 'hen', 'wild', 'zwaaien', '.']
>>> list(ppdp.parse(sent))
[Tree('zag', ['Cathy', 'hen', Tree('zwaaien', ['wild', '.'])])]",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"ShiftReduceParser maintains a stack, which records the
structure of a portion of the text.  This stack is a list of
strings and Trees that collectively cover a portion of
the text.  For example, while parsing the sentence “the dog saw
the man” with a typical grammar, ShiftReduceParser will produce
the following stack, which covers “the dog saw”:","[(NP: (Det: 'the') (N: 'dog')), (V: 'saw')]",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"###################### Check The Transition #######################
Check the Initialized Configuration
>>> print(conf)
Stack : [0]  Buffer : [1, 2, 3, 4, 5, 6, 7, 8, 9]   Arcs : []",">>> operation = Transition('arc-standard')
>>> operation.shift(conf)
>>> operation.left_arc(conf, ""ATT"")
>>> operation.shift(conf)
>>> operation.left_arc(conf,""SBJ"")
>>> operation.shift(conf)
>>> operation.shift(conf)
>>> operation.left_arc(conf, ""ATT"")
>>> operation.shift(conf)
>>> operation.shift(conf)
>>> operation.shift(conf)
>>> operation.left_arc(conf, ""ATT"")",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"###################### Check The Transition #######################
Check the Initialized Configuration
>>> print(conf)
Stack : [0]  Buffer : [1, 2, 3, 4, 5, 6, 7, 8, 9]   Arcs : []",">>> print(', '.join(conf.extract_features()))
STK_0_FORM_on, STK_0_LEMMA_on, STK_0_POS_IN, STK_1_POS_NN, BUF_0_FORM_markets, BUF_0_LEMMA_markets, BUF_0_POS_NNS, BUF_1_FORM_., BUF_1_POS_., BUF_0_LDEP_ATT",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"###################### Check The Transition #######################
Check the Initialized Configuration
>>> print(conf)
Stack : [0]  Buffer : [1, 2, 3, 4, 5, 6, 7, 8, 9]   Arcs : []",">>> operation.right_arc(conf, ""PC"")
>>> operation.right_arc(conf, ""ATT"")
>>> operation.right_arc(conf, ""OBJ"")
>>> operation.shift(conf)
>>> operation.right_arc(conf, ""PU"")
>>> operation.right_arc(conf, ""ROOT"")
>>> operation.shift(conf)",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"###################### Check The Transition #######################
Check the Initialized Configuration
>>> print(conf)
Stack : [0]  Buffer : [1, 2, 3, 4, 5, 6, 7, 8, 9]   Arcs : []",">>> conf = Configuration(gold_sent)
>>> operation = Transition('arc-eager')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'ATT')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'SBJ')
>>> operation.right_arc(conf,'ROOT')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'ATT')
>>> operation.right_arc(conf,'OBJ')
>>> operation.right_arc(conf,'ATT')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'ATT')
>>> operation.right_arc(conf,'PC')
>>> operation.reduce(conf)
>>> operation.reduce(conf)
>>> operation.reduce(conf)
>>> operation.right_arc(conf,'PU')
>>> print(conf)
Stack : [0, 3, 9]  Buffer : []   Arcs : [(2, 'ATT', 1), (3, 'SBJ', 2), (0, 'ROOT', 3), (5, 'ATT', 4), (3, 'OBJ', 5), (5, 'ATT', 6), (8, 'ATT', 7), (6, 'PC', 8), (3, 'PU', 9)]",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"A. Check the ARC-STANDARD training
>>> import tempfile
>>> import os
>>> input_file = tempfile.NamedTemporaryFile(prefix=’transition_parse.train’, dir=tempfile.gettempdir(), delete=False)",">>> parser_std = TransitionParser('arc-standard')
>>> print(', '.join(parser_std._create_training_examples_arc_std([gold_sent], input_file)))
 Number of training examples : 1
 Number of valid (projective) examples : 1
SHIFT, LEFTARC:ATT, SHIFT, LEFTARC:SBJ, SHIFT, SHIFT, LEFTARC:ATT, SHIFT, SHIFT, SHIFT, LEFTARC:ATT, RIGHTARC:PC, RIGHTARC:ATT, RIGHTARC:OBJ, SHIFT, RIGHTARC:PU, RIGHTARC:ROOT, SHIFT",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"A. Check the ARC-STANDARD training
>>> import tempfile
>>> import os
>>> input_file = tempfile.NamedTemporaryFile(prefix=’transition_parse.train’, dir=tempfile.gettempdir(), delete=False)",">>> parser_std.train([gold_sent],'temp.arcstd.model', verbose=False)
 Number of training examples : 1
 Number of valid (projective) examples : 1
>>> remove(input_file.name)",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"A. Check the ARC-STANDARD training
>>> import tempfile
>>> import os
>>> input_file = tempfile.NamedTemporaryFile(prefix=’transition_parse.train’, dir=tempfile.gettempdir(), delete=False)",">>> input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(),delete=False)
>>> parser_eager = TransitionParser('arc-eager')
>>> print(', '.join(parser_eager._create_training_examples_arc_eager([gold_sent], input_file)))
 Number of training examples : 1
 Number of valid (projective) examples : 1
SHIFT, LEFTARC:ATT, SHIFT, LEFTARC:SBJ, RIGHTARC:ROOT, SHIFT, LEFTARC:ATT, RIGHTARC:OBJ, RIGHTARC:ATT, SHIFT, LEFTARC:ATT, RIGHTARC:PC, REDUCE, REDUCE, REDUCE, RIGHTARC:PU",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"A. Check the ARC-STANDARD training
>>> import tempfile
>>> import os
>>> input_file = tempfile.NamedTemporaryFile(prefix=’transition_parse.train’, dir=tempfile.gettempdir(), delete=False)",">>> parser_eager.train([gold_sent],'temp.arceager.model', verbose=False)
 Number of training examples : 1
 Number of valid (projective) examples : 1",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"A. Check the ARC-STANDARD training
>>> import tempfile
>>> import os
>>> input_file = tempfile.NamedTemporaryFile(prefix=’transition_parse.train’, dir=tempfile.gettempdir(), delete=False)",>>> remove(input_file.name),https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"A. Check the ARC-STANDARD training
>>> import tempfile
>>> import os
>>> input_file = tempfile.NamedTemporaryFile(prefix=’transition_parse.train’, dir=tempfile.gettempdir(), delete=False)",">>> result = parser_std.parse([gold_sent], 'temp.arcstd.model')
>>> de = DependencyEvaluator(result, [gold_sent])
>>> de.eval() >= (0, 0)
True",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
A module to convert a single POS tagged sentence into CONLL format.,">>> from nltk import word_tokenize, pos_tag
>>> text = ""This is a foobar sentence.""
>>> for line in taggedsent_to_conll(pos_tag(word_tokenize(text))):
...         print(line, end="""")
    1       This    _       DT      DT      _       0       a       _       _
    2       is      _       VBZ     VBZ     _       0       a       _       _
    3       a       _       DT      DT      _       0       a       _       _
    4       foobar  _       JJ      JJ      _       0       a       _       _
    5       sentence        _       NN      NN      _       0       a       _       _
    6       .               _       .       .       _       0       a       _       _",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
"A module to convert the a POS tagged document stream
(i.e. list of list of tuples, a list of sentences) and yield lines
in CONLL format. This module yields one line per word and two newlines
for end of sentence.",">>> from nltk import word_tokenize, sent_tokenize, pos_tag
>>> text = ""This is a foobar sentence. Is that right?""
>>> sentences = [pos_tag(word_tokenize(sent)) for sent in sent_tokenize(text)]
>>> for line in taggedsents_to_conll(sentences):
...     if line:
...         print(line, end="""")
1   This    _       DT      DT      _       0       a       _       _
2   is      _       VBZ     VBZ     _       0       a       _       _
3   a       _       DT      DT      _       0       a       _       _
4   foobar  _       JJ      JJ      _       0       a       _       _
5   sentence        _       NN      NN      _       0       a       _       _
6   .               _       .       .       _       0       a       _       _


1   Is      _       VBZ     VBZ     _       0       a       _       _
2   that    _       IN      IN      _       0       a       _       _
3   right   _       NN      NN      _       0       a       _       _
4   ?       _       .       .       _       0       a       _       _",https://web.archive.org/web/20210417122335/https://www.nltk.org/api/nltk.parse.html
