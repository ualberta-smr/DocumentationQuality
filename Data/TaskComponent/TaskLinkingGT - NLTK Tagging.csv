Examples,Henry's paragraphs,Sarah's paragraphs,Conflict?,Resolved paragraphs,Notes,Program,Has paragraphs,Updated Program,Paragraph count,Program count,Program correct,Updated program count,Updated program correct
">>> backoff = RegexpTagger([
... (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers
... (r'(The|the|A|a|An|an)$', 'AT'),   # articles
... (r'.*able$', 'JJ'),                # adjectives
... (r'.*ness$', 'NN'),                # nouns formed from adjectives
... (r'.*ly$', 'RB'),                  # adverbs
... (r'.*s$', 'NNS'),                  # plural nouns
... (r'.*ing$', 'VBG'),                # gerunds
... (r'.*ed$', 'VBD'),                 # past tense verbs
... (r'.*', 'NN')                      # nouns (default)
... ])",,,0,,hmm I don't think there's a clear description for this one,"Trains the Brill tagger on the corpus train_sents,
producing at most max_rules transformations, each of which
reduces the net number of errors in the corpus by at least
min_score, and each of which has accuracy not lower than
min_acc.",FALSE,,0,1,0,0,
>>> baseline = backoff #see NOTE1,"# NOTE1: (!!FIXME) A far better baseline uses nltk.tag.UnigramTagger, # with a RegexpTagger only as backoff. For instance, # >>> baseline = UnigramTagger(baseline_data, backoff=backoff) # However, as of Nov 2013, nltk.tag.UnigramTagger does not yield consistent results # between python versions. The simplistic backoff above is a workaround to make doctests # get consistent input.",,1,,hmm I don't think there's a clear description for this one,"Trains the Brill tagger on the corpus train_sents,
producing at most max_rules transformations, each of which
reduces the net number of errors in the corpus by at least
min_score, and each of which has accuracy not lower than
min_acc.",,,0,1,0,0,
">>> baseline.evaluate(gold_data) 
0.2450142...",,,0,,hmm I don't think there's a clear description for this one,"Trains the Brill tagger on the corpus train_sents,
producing at most max_rules transformations, each of which
reduces the net number of errors in the corpus by at least
min_score, and each of which has accuracy not lower than
min_acc.",,,0,1,0,0,
">>> tagger1 = tt.train(training_data, max_rules=10)
TBL train (fast) (seqs: 100; tokens: 2417; tpls: 2; min score: 2; min acc: None)
Finding initial useful rules...
    Found 845 useful rules.

           B      |
   S   F   r   O  |        Score = Fixed - Broken
   c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct
   o   x   k   h  |  u     Broken = num tags changed correct -> incorrect
   r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect
   e   d   n   r  |  e
------------------+-------------------------------------------------------
 132 132   0   0  | AT->DT if Pos:NN@[-1]
  85  85   0   0  | NN->, if Pos:NN@[-1] & Word:,@[0]
  69  69   0   0  | NN->. if Pos:NN@[-1] & Word:.@[0]
  51  51   0   0  | NN->IN if Pos:NN@[-1] & Word:of@[0]
  47  63  16 161  | NN->IN if Pos:NNS@[-1]
  33  33   0   0  | NN->TO if Pos:NN@[-1] & Word:to@[0]
  26  26   0   0  | IN->. if Pos:NNS@[-1] & Word:.@[0]
  24  24   0   0  | IN->, if Pos:NNS@[-1] & Word:,@[0]
  22  27   5  24  | NN->-NONE- if Pos:VBD@[-1]
  17  17   0   0  | NN->CC if Pos:NN@[-1] & Word:and@[0]",,,0,,hmm I don't think there's a clear description for this one,"#templates
>>> Template._cleartemplates() #clear any templates created in earlier tests
>>> templates = [Template(Pos([-1])), Template(Pos([-1]), Word([0]))]",,,0,1,0,0,
">>> tagger1.rules()[1:3]
(Rule('001', 'NN', ',', [(Pos([-1]),'NN'), (Word([0]),',')]), Rule('001', 'NN', '.', [(Pos([-1]),'NN'), (Word([0]),'.')]))",,,0,,hmm I don't think there's a clear description for this one,"#templates
>>> Template._cleartemplates() #clear any templates created in earlier tests
>>> templates = [Template(Pos([-1])), Template(Pos([-1]), Word([0]))]",,,0,1,0,0,
">>> train_stats = tagger1.train_stats()
>>> [train_stats[stat] for stat in ['initialerrors', 'finalerrors', 'rulescores']]
[1775, 1269, [132, 85, 69, 51, 47, 33, 26, 24, 22, 17]]",,,0,,hmm I don't think there's a clear description for this one,"#templates
>>> Template._cleartemplates() #clear any templates created in earlier tests
>>> templates = [Template(Pos([-1])), Template(Pos([-1]), Word([0]))]",,,0,1,0,0,
">>> tagger1.print_template_statistics(printunused=False)
TEMPLATE STATISTICS (TRAIN)  2 templates, 10 rules)
TRAIN (   2417 tokens) initial  1775 0.2656 final:  1269 0.4750
#ID | Score (train) |  #Rules     | Template
--------------------------------------------
001 |   305   0.603 |   7   0.700 | Template(Pos([-1]),Word([0]))
000 |   201   0.397 |   3   0.300 | Template(Pos([-1]))",,,0,,hmm I don't think there's a clear description for this one,"#templates
>>> Template._cleartemplates() #clear any templates created in earlier tests
>>> templates = [Template(Pos([-1])), Template(Pos([-1]), Word([0]))]",,,0,1,0,0,
">>> tagger1.evaluate(gold_data) 
0.43996...",,,0,,hmm I don't think there's a clear description for this one,"#templates
>>> Template._cleartemplates() #clear any templates created in earlier tests
>>> templates = [Template(Pos([-1])), Template(Pos([-1]), Word([0]))]",,,0,1,0,0,
">>> tagged, test_stats = tagger1.batch_tag_incremental(testing_data, gold_data)",,,0,,hmm I don't think there's a clear description for this one,"#templates
>>> Template._cleartemplates() #clear any templates created in earlier tests
>>> templates = [Template(Pos([-1])), Template(Pos([-1]), Word([0]))]",,,0,1,0,0,
">>> tagged[33][12:] == [('foreign', 'IN'), ('debt', 'NN'), ('of', 'IN'), ('$', 'NN'), ('64', 'CD'),
... ('billion', 'NN'), ('*U*', 'NN'), ('--', 'NN'), ('the', 'DT'), ('third-highest', 'NN'), ('in', 'NN'),
... ('the', 'DT'), ('developing', 'VBG'), ('world', 'NN'), ('.', '.')]
True",,,0,,hmm I don't think there's a clear description for this one,"#templates
>>> Template._cleartemplates() #clear any templates created in earlier tests
>>> templates = [Template(Pos([-1])), Template(Pos([-1]), Word([0]))]",,,0,1,0,0,
">>> [test_stats[stat] for stat in ['initialerrors', 'finalerrors', 'rulescores']]
[1855, 1376, [100, 85, 67, 58, 27, 36, 27, 16, 31, 32]]",,,0,,hmm I don't think there's a clear description for this one,"#templates
>>> Template._cleartemplates() #clear any templates created in earlier tests
>>> templates = [Template(Pos([-1])), Template(Pos([-1]), Word([0]))]",,,0,1,0,0,
">>> tagger2.evaluate(gold_data)  
0.44159544...
>>> tagger2.rules()[2:4]
(Rule('001', 'NN', '.', [(Pos([-1]),'NN'), (Word([0]),'.')]), Rule('001', 'NN', 'IN', [(Pos([-1]),'NN'), (Word([0]),'of')])",,,0,,hmm I don't think there's a clear description for this one,"# a high-accuracy tagger
>>> tagger2 = tt.train(training_data, max_rules=10, min_acc=0.99)
TBL train (fast) (seqs: 100; tokens: 2417; tpls: 2; min score: 2; min acc: 0.99)
Finding initial useful rules…",,,0,1,0,0,
">>> from nltk.tag import CRFTagger
>>> ct = CRFTagger()",A module for POS tagging using CRFSuite https://pypi.python.org/pypi/python-crfsuite,A module for POS tagging using CRFSuite https://pypi.python.org/pypi/python-crfsuite,0,A module for POS tagging using CRFSuite https://pypi.python.org/pypi/python-crfsuite,,A module for POS tagging using CRFSuite https://pypi.python.org/pypi/python-crfsuite,,,1,1,1,0,
">>> train_data = [[('University','Noun'), ('is','Verb'), ('a','Det'), ('good','Adj'), ('place','Noun')],
... [('dog','Noun'),('eat','Verb'),('meat','Noun')]]",A module for POS tagging using CRFSuite https://pypi.python.org/pypi/python-crfsuite,A module for POS tagging using CRFSuite https://pypi.python.org/pypi/python-crfsuite,0,A module for POS tagging using CRFSuite https://pypi.python.org/pypi/python-crfsuite,,A module for POS tagging using CRFSuite https://pypi.python.org/pypi/python-crfsuite,,,1,1,1,0,
">>> ct.train(train_data,'model.crf.tagger')
>>> ct.tag_sents([['dog','is','good'], ['Cat','eat','meat']])
[[('dog', 'Noun'), ('is', 'Verb'), ('good', 'Adj')], [('Cat', 'Noun'), ('eat', 'Verb'), ('meat', 'Noun')]]",A module for POS tagging using CRFSuite https://pypi.python.org/pypi/python-crfsuite,A module for POS tagging using CRFSuite https://pypi.python.org/pypi/python-crfsuite,0,A module for POS tagging using CRFSuite https://pypi.python.org/pypi/python-crfsuite,,A module for POS tagging using CRFSuite https://pypi.python.org/pypi/python-crfsuite,,,1,1,1,0,
">>> gold_sentences = [[('dog','Noun'),('is','Verb'),('good','Adj')] , [('Cat','Noun'),('eat','Verb'), ('meat','Noun')]]
>>> ct.evaluate(gold_sentences)
1.0",A module for POS tagging using CRFSuite https://pypi.python.org/pypi/python-crfsuite,A module for POS tagging using CRFSuite https://pypi.python.org/pypi/python-crfsuite,0,A module for POS tagging using CRFSuite https://pypi.python.org/pypi/python-crfsuite,,A module for POS tagging using CRFSuite https://pypi.python.org/pypi/python-crfsuite,,,1,1,1,0,
H(O) = - sum_S Pr(S | O) log Pr(S | O),Returns the entropy over labellings of the given sequence. This is given by:,Returns the entropy over labellings of the given sequence. This is given by:,0,Returns the entropy over labellings of the given sequence. This is given by:,,"Returns the entropy over labellings of the given sequence. This is
given by:",,,1,1,1,0,
"H = - sum_S Pr(S | O) log [ Pr(S, O) / Z ]
= log Z - sum_S Pr(S | O) log Pr(S, 0)
= log Z - sum_S Pr(S | O) [ log Pr(S_0) + sum_t Pr(S_t | S_{t-1}) + sum_t Pr(O_t | S_t) ]","where the summation ranges over all state sequences, S. Let Z = Pr(O) = sum_S Pr(S, O)} where the summation ranges over all state sequences and O is the observation sequence. As such the entropy can be re-expressed as:","where the summation ranges over all state sequences, S. Let Z = Pr(O) = sum_S Pr(S, O)} where the summation ranges over all state sequences and O is the observation sequence. As such the entropy can be re-expressed as:",0,"where the summation ranges over all state sequences, S. Let Z = Pr(O) = sum_S Pr(S, O)} where the summation ranges over all state sequences and O is the observation sequence. As such the entropy can be re-expressed as:",,"Returns the entropy over labellings of the given sequence. This is
given by:",,,1,1,0,0,
"H = log Z - sum_s0 alpha_0(s0) beta_0(s0) / Z * log Pr(s0)
+ sum_t,si,sj alpha_t(si) Pr(sj | si) Pr(O_t+1 | sj) beta_t(sj) / Z * log Pr(sj | si)
+ sum_t,st alpha_t(st) beta_t(st) / Z * log Pr(O_t | st)","The order of summation for the log terms can be flipped, allowing dynamic programming to be used to calculate the entropy. Specifically, we use the forward and backward probabilities (alpha, beta) giving:","This simply uses alpha and beta to find the probabilities of partial sequences, constrained to include the given state(s) at some point in time.",1,,,"The order of summation for the log terms can be flipped, allowing
dynamic programming to be used to calculate the entropy. Specifically,
we use the forward and backward probabilities (alpha, beta) giving:",,,0,1,0,0,
">>> from nltk.tag import HunposTagger
>>> ht = HunposTagger('en_wsj.model')
>>> ht.tag('What is the airspeed of an unladen swallow ?'.split())
[('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'), ('unladen', 'NN'), ('swallow', 'VB'), ('?', '.')]
>>> ht.close()","
A class for pos tagging with HunPos. The input is the paths to:","This class communicates with the hunpos-tag binary via pipes. When the tagger object is no longer needed, the close() method should be called to free system resources. The class supports the context manager interface; if used in a with statement, the close() method is invoked automatically:",1,,,,,,0,1,1,0,
">>> with HunposTagger('en_wsj.model') as ht:
...     ht.tag('What is the airspeed of an unladen swallow ?'.split())
...
[('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'), ('unladen', 'NN'), ('swallow', 'VB'), ('?', '.')]","This class communicates with the hunpos-tag binary via pipes. When the tagger object is no longer needed, the close() method should be called to free system resources. The class supports the context manager interface; if used in a with statement, the close() method is invoked automatically:","This class communicates with the hunpos-tag binary via pipes. When the tagger object is no longer needed, the close() method should be called to free system resources. The class supports the context manager interface; if used in a with statement, the close() method is invoked automatically:",0,"This class communicates with the hunpos-tag binary via pipes. When the tagger object is no longer needed, the close() method should be called to free system resources. The class supports the context manager interface; if used in a with statement, the close() method is invoked automatically:",,"This class communicates with the hunpos-tag binary via pipes. When the
tagger object is no longer needed, the close() method should be called to
free system resources. The class supports the context manager interface; if
used in a with statement, the close() method is invoked automatically:",,,1,1,1,0,
">>> map_tag('en-ptb', 'universal', 'VBZ')
'VERB'
>>> map_tag('en-ptb', 'universal', 'VBP')
'VERB'
>>> map_tag('en-ptb', 'universal', '``')
'.'",Maps the tag from the source tagset to the target tagset.,Maps the tag from the source tagset to the target tagset.,0,Maps the tag from the source tagset to the target tagset.,,,,,1,1,0,0,
">>> tagset_mapping('ru-rnc', 'universal') == {'!': '.', 'A': 'ADJ', 'C': 'CONJ', 'AD': 'ADV',            'NN': 'NOUN', 'VG': 'VERB', 'COMP': 'CONJ', 'NC': 'NUM', 'VP': 'VERB', 'P': 'ADP',            'IJ': 'X', 'V': 'VERB', 'Z': 'X', 'VI': 'VERB', 'YES_NO_SENT': 'X', 'PTCL': 'PRT'}
True",Retrieve the mapping dictionary between tagsets.,Retrieve the mapping dictionary between tagsets.,0,Retrieve the mapping dictionary between tagsets.,,Retrieve the mapping dictionary between tagsets.,,,1,1,1,0,
>>> from nltk.tag.perceptron import PerceptronTagger,"Greedy Averaged Perceptron tagger, as implemented by Matthew Honnibal. See more implementation details here:","Greedy Averaged Perceptron tagger, as implemented by Matthew Honnibal. See more implementation details here:",0,"Greedy Averaged Perceptron tagger, as implemented by Matthew Honnibal. See more implementation details here:",,"Greedy Averaged Perceptron tagger, as implemented by Matthew Honnibal.
See more implementation details here:",,,1,1,1,0,
>>> tagger = PerceptronTagger(load=False),Train the model,Train the model,0,Train the model,,"Greedy Averaged Perceptron tagger, as implemented by Matthew Honnibal.
See more implementation details here:",,,1,1,0,0,
">>> tagger.train([[('today','NN'),('is','VBZ'),('good','JJ'),('day','NN')],
... [('yes','NNS'),('it','PRP'),('beautiful','JJ')]])",Train the model,Train the model,0,Train the model,,"Greedy Averaged Perceptron tagger, as implemented by Matthew Honnibal.
See more implementation details here:",,,1,1,0,0,
">>> tagger.tag(['today','is','a','beautiful','day'])
[('today', 'NN'), ('is', 'PRP'), ('a', 'PRP'), ('beautiful', 'JJ'), ('day', 'NN')]",Train the model,Train the model,0,Train the model,,"Greedy Averaged Perceptron tagger, as implemented by Matthew Honnibal.
See more implementation details here:",,,1,1,0,0,
>>> pretrain = PerceptronTagger(),Use the pretrain model (the default constructor),Use the pretrain model (the default constructor),0,Use the pretrain model (the default constructor),,Use the pretrain model (the default constructor),,,1,1,1,0,
">>> pretrain.tag('The quick brown fox jumps over the lazy dog'.split())
[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]",Use the pretrain model (the default constructor),Use the pretrain model (the default constructor),0,Use the pretrain model (the default constructor),,Use the pretrain model (the default constructor),,,1,1,1,0,
">>> pretrain.tag(""The red cat"".split())
[('The', 'DT'), ('red', 'JJ'), ('cat', 'NN')]",Use the pretrain model (the default constructor),Use the pretrain model (the default constructor),0,Use the pretrain model (the default constructor),,Use the pretrain model (the default constructor),,,1,1,1,0,
">>> from nltk.tag import SennaTagger
>>> tagger = SennaTagger('/usr/share/senna-v3.0')
>>> tagger.tag('What is the airspeed of an unladen swallow ?'.split()) 
[('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('airspeed', 'NN'),
('of', 'IN'), ('an', 'DT'), ('unladen', 'NN'), ('swallow', 'NN'), ('?', '.')]",SennaTagger will automatically search for executable file specified in SENNA environment variable,SennaTagger will automatically search for executable file specified in SENNA environment variable,0,SennaTagger will automatically search for executable file specified in SENNA environment variable,,Note: Unit tests for this module can be found in test/unit/test_senna.py,,,1,1,0,0,
">>> from nltk.tag import SennaChunkTagger
>>> chktagger = SennaChunkTagger('/usr/share/senna-v3.0')
>>> chktagger.tag('What is the airspeed of an unladen swallow ?'.split()) 
[('What', 'B-NP'), ('is', 'B-VP'), ('the', 'B-NP'), ('airspeed', 'I-NP'),
('of', 'B-PP'), ('an', 'B-NP'), ('unladen', 'I-NP'), ('swallow', 'I-NP'),
('?', 'O')]",SennaTagger will automatically search for executable file specified in SENNA environment variable,SennaTagger will automatically search for executable file specified in SENNA environment variable,0,SennaTagger will automatically search for executable file specified in SENNA environment variable,,Note: Unit tests for this module can be found in test/unit/test_senna.py,,,1,1,0,0,
">>> from nltk.tag import SennaNERTagger
>>> nertagger = SennaNERTagger('/usr/share/senna-v3.0')
>>> nertagger.tag('Shakespeare theatre was in London .'.split()) 
[('Shakespeare', 'B-PER'), ('theatre', 'O'), ('was', 'O'), ('in', 'O'),
('London', 'B-LOC'), ('.', 'O')]
>>> nertagger.tag('UN headquarters are in NY , USA .'.split()) 
[('UN', 'B-ORG'), ('headquarters', 'O'), ('are', 'O'), ('in', 'O'),
('NY', 'B-LOC'), (',', 'O'), ('USA', 'B-LOC'), ('.', 'O')]",SennaTagger will automatically search for executable file specified in SENNA environment variable,SennaTagger will automatically search for executable file specified in SENNA environment variable,0,SennaTagger will automatically search for executable file specified in SENNA environment variable,,Note: Unit tests for this module can be found in test/unit/test_senna.py,,,1,1,0,0,
">>> from nltk.tag import SennaChunkTagger
>>> chktagger = SennaChunkTagger('/usr/share/senna-v3.0')
>>> sent = 'What is the airspeed of an unladen swallow ?'.split()
>>> tagged_sent = chktagger.tag(sent) 
>>> tagged_sent 
[('What', 'B-NP'), ('is', 'B-VP'), ('the', 'B-NP'), ('airspeed', 'I-NP'),
('of', 'B-PP'), ('an', 'B-NP'), ('unladen', 'I-NP'), ('swallow', 'I-NP'),
('?', 'O')]
>>> list(chktagger.bio_to_chunks(tagged_sent, chunk_type='NP')) 
[('What', '0'), ('the airspeed', '2-3'), ('an unladen swallow', '5-6-7')]",Extracts the chunks in a BIO chunk-tagged sentence.,Extracts the chunks in a BIO chunk-tagged sentence.,0,Extracts the chunks in a BIO chunk-tagged sentence.,,,,,1,1,0,0,
"feature_detector(tokens, index, history) -> featureset",Return the feature detector that this tagger uses to generate featuresets for its classifier. The feature detector is a function with the signature:,Return the feature detector that this tagger uses to generate featuresets for its classifier. The feature detector is a function with the signature:,0,Return the feature detector that this tagger uses to generate featuresets for its classifier. The feature detector is a function with the signature:,,"Return the feature detector that this tagger uses to generate
featuresets for its classifier.  The feature detector is a
function with the signature:",,,1,1,1,0,
"feature_detector(tokens, index, history) -> featureset",A sequential tagger that uses a classifier to choose the tag for each token in a sentence. The featureset input for the classifier is generated by a feature detector function:,A sequential tagger that uses a classifier to choose the tag for each token in a sentence. The featureset input for the classifier is generated by a feature detector function:,0,A sequential tagger that uses a classifier to choose the tag for each token in a sentence. The featureset input for the classifier is generated by a feature detector function:,,"Return the feature detector that this tagger uses to generate
featuresets for its classifier.  The feature detector is a
function with the signature:",,,1,1,0,0,
"feature_detector(tokens, index, history) -> featureset",Return the feature detector that this tagger uses to generate featuresets for its classifier. The feature detector is a function with the signature:,Return the feature detector that this tagger uses to generate featuresets for its classifier. The feature detector is a function with the signature:,0,Return the feature detector that this tagger uses to generate featuresets for its classifier. The feature detector is a function with the signature:,,"Return the feature detector that this tagger uses to generate
featuresets for its classifier.  The feature detector is a
function with the signature:",,,1,1,1,0,
">>> from nltk.tag import DefaultTagger
>>> default_tagger = DefaultTagger('NN')
>>> list(default_tagger.tag('This is a test'.split()))
[('This', 'NN'), ('is', 'NN'), ('a', 'NN'), ('test', 'NN')]",A tagger that assigns the same tag to every token.,A tagger that assigns the same tag to every token.,0,A tagger that assigns the same tag to every token.,,A tagger that assigns the same tag to every token.,,,1,1,1,0,
">>> from nltk.corpus import brown
>>> from nltk.tag import RegexpTagger
>>> test_sent = brown.sents(categories='news')[0]
>>> regexp_tagger = RegexpTagger(
...     [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers
...      (r'(The|the|A|a|An|an)$', 'AT'),   # articles
...      (r'.*able$', 'JJ'),                # adjectives
...      (r'.*ness$', 'NN'),                # nouns formed from adjectives
...      (r'.*ly$', 'RB'),                  # adverbs
...      (r'.*s$', 'NNS'),                  # plural nouns
...      (r'.*ing$', 'VBG'),                # gerunds
...      (r'.*ed$', 'VBD'),                 # past tense verbs
...      (r'.*', 'NN')                      # nouns (default)
... ])
>>> regexp_tagger
<Regexp Tagger: size=9>
>>> regexp_tagger.tag(test_sent)
[('The', 'AT'), ('Fulton', 'NN'), ('County', 'NN'), ('Grand', 'NN'), ('Jury', 'NN'),
('said', 'NN'), ('Friday', 'NN'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'NN'),
(""Atlanta's"", 'NNS'), ('recent', 'NN'), ('primary', 'NN'), ('election', 'NN'),
('produced', 'VBD'), ('``', 'NN'), ('no', 'NN'), ('evidence', 'NN'), (""''"", 'NN'),
('that', 'NN'), ('any', 'NN'), ('irregularities', 'NNS'), ('took', 'NN'),
('place', 'NN'), ('.', 'NN')]",The RegexpTagger assigns tags to tokens by comparing their word strings to a series of regular expressions. The following tagger uses word suffixes to make guesses about the correct Brown Corpus part of speech tag:,The RegexpTagger assigns tags to tokens by comparing their word strings to a series of regular expressions. The following tagger uses word suffixes to make guesses about the correct Brown Corpus part of speech tag:,0,The RegexpTagger assigns tags to tokens by comparing their word strings to a series of regular expressions. The following tagger uses word suffixes to make guesses about the correct Brown Corpus part of speech tag:,,,,,1,1,0,0,
">>> from nltk.corpus import brown
>>> from nltk.tag import UnigramTagger
>>> test_sent = brown.sents(categories='news')[0]
>>> unigram_tagger = UnigramTagger(brown.tagged_sents(categories='news')[:500])
>>> for tok, tag in unigram_tagger.tag(test_sent):
...     print(""({}, {}), "".format(tok, tag))
(The, AT), (Fulton, NP-TL), (County, NN-TL), (Grand, JJ-TL),
(Jury, NN-TL), (said, VBD), (Friday, NR), (an, AT),
(investigation, NN), (of, IN), (Atlanta's, NP$), (recent, JJ),
(primary, NN), (election, NN), (produced, VBD), (``, ``),
(no, AT), (evidence, NN), ('', ''), (that, CS), (any, DTI),
(irregularities, NNS), (took, VBD), (place, NN), (., .),","The UnigramTagger finds the most likely tag for each word in a training corpus, and then uses that information to assign tags to new tokens.","The UnigramTagger finds the most likely tag for each word in a training corpus, and then uses that information to assign tags to new tokens.",0,"The UnigramTagger finds the most likely tag for each word in a training corpus, and then uses that information to assign tags to new tokens.",,"The UnigramTagger finds the most likely tag for each word in a training
corpus, and then uses that information to assign tags to new tokens.",,,1,1,1,0,
">>> from nltk.tag import StanfordNERTagger
>>> st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') 
>>> st.tag('Rami Eid is studying at Stony Brook University in NY'.split()) 
[('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'),
 ('at', 'O'), ('Stony', 'ORGANIZATION'), ('Brook', 'ORGANIZATION'),
 ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'LOCATION')]",A class for Named-Entity Tagging with Stanford Tagger. The input is the paths to:,"A class for Named-Entity Tagging with Stanford Tagger. The input is the paths to:

a model trained on training data

(optionally) the path to the stanford tagger jar file. If not specified here, then this jar file must be specified in the CLASSPATH envinroment variable.

(optionally) the encoding of the training data (default: UTF-8)
",1,,,,,,0,1,1,0,
">>> from nltk.tag import StanfordPOSTagger
>>> st = StanfordPOSTagger('english-bidirectional-distsim.tagger')
>>> st.tag('What is the airspeed of an unladen swallow ?'.split())
[('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'), ('unladen', 'JJ'), ('swallow', 'VB'), ('?', '.')]",A class for pos tagging with Stanford Tagger. The input is the paths to:,"A class for pos tagging with Stanford Tagger. The input is the paths to:
a model trained on training data

(optionally) the path to the stanford tagger jar file. If not specified here, then this jar file must be specified in the CLASSPATH envinroment variable.

(optionally) the encoding of the training data (default: UTF-8)",1,,,,,,0,1,1,0,
">>> from nltk.tag.util import str2tuple
>>> str2tuple('fly/NN')
('fly', 'NN')","Given the string representation of a tagged token, return the corresponding tuple representation. The rightmost occurrence of sep in s will be used to divide s into a word string and a tag string. If sep does not occur in s, return (s, None).","Given the string representation of a tagged token, return the corresponding tuple representation. The rightmost occurrence of sep in s will be used to divide s into a word string and a tag string. If sep does not occur in s, return (s, None).",0,"Given the string representation of a tagged token, return the corresponding tuple representation. The rightmost occurrence of sep in s will be used to divide s into a word string and a tag string. If sep does not occur in s, return (s, None).",,"Given the string representation of a tagged token, return the
corresponding tuple representation.  The rightmost occurrence of
sep in s will be used to divide s into a word string and
a tag string.  If sep does not occur in s, return (s, None).",,,1,1,1,0,
">>> from nltk.tag.util import tuple2str
>>> tagged_token = ('fly', 'NN')
>>> tuple2str(tagged_token)
'fly/NN'","Given the tuple representation of a tagged token, return the corresponding string representation. This representation is formed by concatenating the token’s word string, followed by the separator, followed by the token’s tag. (If the tag is None, then just return the bare word string.)","Given the tuple representation of a tagged token, return the corresponding string representation. This representation is formed by concatenating the token’s word string, followed by the separator, followed by the token’s tag. (If the tag is None, then just return the bare word string.)",0,"Given the tuple representation of a tagged token, return the corresponding string representation. This representation is formed by concatenating the token’s word string, followed by the separator, followed by the token’s tag. (If the tag is None, then just return the bare word string.)",,"Given the tuple representation of a tagged token, return the
corresponding string representation.  This representation is
formed by concatenating the token’s word string, followed by the
separator, followed by the token’s tag.  (If the tag is None,
then just return the bare word string.)",,,1,1,1,0,
">>> from nltk.tag.util import untag
>>> untag([('John', 'NNP'), ('saw', 'VBD'), ('Mary', 'NNP')])
['John', 'saw', 'Mary']","Given a tagged sentence, return an untagged version of that sentence. I.e., return a list containing the first element of each tuple in tagged_sentence.","Given a tagged sentence, return an untagged version of that sentence. I.e., return a list containing the first element of each tuple in tagged_sentence.",0,"Given a tagged sentence, return an untagged version of that sentence. I.e., return a list containing the first element of each tuple in tagged_sentence.",,"Given a tagged sentence, return an untagged version of that
sentence.  I.e., return a list containing the first element
of each tuple in tagged_sentence.",,,1,1,1,0,
">>> tagged_tok = ('fly', 'NN')","A “tag” is a case-sensitive string that specifies some property of a token, such as its part of speech. Tagged tokens are encoded as tuples (tag, token). For example, the following tagged token combines the word 'fly' with a noun part of speech tag ('NN'):","A “tag” is a case-sensitive string that specifies some property of a token, such as its part of speech. Tagged tokens are encoded as tuples (tag, token). For example, the following tagged token combines the word 'fly' with a noun part of speech tag ('NN'):",0,"A “tag” is a case-sensitive string that specifies some property of a token, such as its part of speech. Tagged tokens are encoded as tuples (tag, token). For example, the following tagged token combines the word 'fly' with a noun part of speech tag ('NN'):",,"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",,"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",1,1,1,1,1
">>> from nltk import pos_tag, word_tokenize
>>> pos_tag(word_tokenize(""John's big idea isn't all that bad.""))
[('John', 'NNP'), (""'s"", 'POS'), ('big', 'JJ'), ('idea', 'NN'), ('is', 'VBZ'),
(""n't"", 'RB'), ('all', 'PDT'), ('that', 'DT'), ('bad', 'JJ'), ('.', '.')]",An off-the-shelf tagger is available for English. It uses the Penn Treebank tagset:,An off-the-shelf tagger is available for English. It uses the Penn Treebank tagset:,0,An off-the-shelf tagger is available for English. It uses the Penn Treebank tagset:,,An off-the-shelf tagger is available for English. It uses the Penn Treebank tagset:,,,1,1,1,0,
">>> pos_tag(word_tokenize(""Илья оторопел и дважды перечитал бумажку.""), lang='rus')    
[('Илья', 'S'), ('оторопел', 'V'), ('и', 'CONJ'), ('дважды', 'ADV'), ('перечитал', 'V'),
('бумажку', 'S'), ('.', 'NONLEX')]",A Russian tagger is also available if you specify lang=”rus”. It uses the Russian National Corpus tagset:,A Russian tagger is also available if you specify lang=”rus”. It uses the Russian National Corpus tagset:,0,A Russian tagger is also available if you specify lang=”rus”. It uses the Russian National Corpus tagset:,,"A Russian tagger is also available if you specify lang=”rus”. It uses
the Russian National Corpus tagset:",,"A “tag” is a case-sensitive string that specifies some property of a token,
such as its part of speech.  Tagged tokens are encoded as tuples
(tag, token).  For example, the following tagged token combines
the word 'fly' with a noun part of speech tag ('NN'):",1,1,1,1,0
">>> from nltk.corpus import brown
>>> from nltk.tag import UnigramTagger
>>> tagger = UnigramTagger(brown.tagged_sents(categories='news')[:500])
>>> sent = ['Mitchell', 'decried', 'the', 'high', 'rate', 'of', 'unemployment']
>>> for word, tag in tagger.tag(sent):
...     print(word, '->', tag)
Mitchell -> NP
decried -> None
the -> AT
high -> JJ
rate -> NN
of -> IN
unemployment -> None","This package defines several taggers, which take a list of tokens, assign a tag to each one, and return the resulting list of tagged tokens. Most of the taggers are built automatically based on a training corpus. For example, the unigram tagger tags each word w by checking what the most frequent tag for w was in a training corpus:","This package defines several taggers, which take a list of tokens, assign a tag to each one, and return the resulting list of tagged tokens. Most of the taggers are built automatically based on a training corpus. For example, the unigram tagger tags each word w by checking what the most frequent tag for w was in a training corpus:",0,"This package defines several taggers, which take a list of tokens, assign a tag to each one, and return the resulting list of tagged tokens. Most of the taggers are built automatically based on a training corpus. For example, the unigram tagger tags each word w by checking what the most frequent tag for w was in a training corpus:",,"This package defines several taggers, which take a list of tokens,
assign a tag to each one, and return the resulting list of tagged tokens.
Most of the taggers are built automatically based on a training corpus.
For example, the unigram tagger tags each word w by checking what
the most frequent tag for w was in a training corpus:",,"This package defines several taggers, which take a list of tokens,
assign a tag to each one, and return the resulting list of tagged tokens.
Most of the taggers are built automatically based on a training corpus.
For example, the unigram tagger tags each word w by checking what
the most frequent tag for w was in a training corpus:",1,1,1,1,1
">>> tagger.evaluate(brown.tagged_sents(categories='news')[500:600])
0.7...",We evaluate a tagger on data that was not seen during training:,We evaluate a tagger on data that was not seen during training:,0,We evaluate a tagger on data that was not seen during training:,,"Note that words that the tagger has not seen during training receive a tag
of None.",,,1,1,0,0,
">>> from nltk.tag import pos_tag
>>> from nltk.tokenize import word_tokenize
>>> pos_tag(word_tokenize(""John's big idea isn't all that bad.""))
[('John', 'NNP'), (""'s"", 'POS'), ('big', 'JJ'), ('idea', 'NN'), ('is', 'VBZ'),
(""n't"", 'RB'), ('all', 'PDT'), ('that', 'DT'), ('bad', 'JJ'), ('.', '.')]
>>> pos_tag(word_tokenize(""John's big idea isn't all that bad.""), tagset='universal')
[('John', 'NOUN'), (""'s"", 'PRT'), ('big', 'ADJ'), ('idea', 'NOUN'), ('is', 'VERB'),
(""n't"", 'ADV'), ('all', 'DET'), ('that', 'DET'), ('bad', 'ADJ'), ('.', '.')]",Use NLTK’s currently recommended part of speech tagger to tag the given list of tokens.,Use NLTK’s currently recommended part of speech tagger to tag the given list of tokens.,0,Use NLTK’s currently recommended part of speech tagger to tag the given list of tokens.,,,,,1,1,0,0,
,,,,,,,,,35,51,25,3,2
,,,,,,Program,,,,,,,
,,,,Precision ,,49.02%,,66.67%,,,,,
,,,,Recall,,71.43%,,5.71%,,,,,