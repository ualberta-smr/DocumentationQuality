Paragraph,Tasks (Old),Henry's mechanical tasks,Henry's library tasks,Henry's library tasks (updated),Sarah's mechanical tasks,Sarah's library tasks,Sarah's library tasks (updated),Conflict,Resolved mechanical tasks,Resolved library tasks,Program,Updated program,,,,,
"Interface for tagging each token in a sentence with supplementary information, such as its part of speech.",Tag each token in a sentence ,Tag token,,,Tag each token in a sentence with parts of speech,Tag each token in a sentence with parts of speech,Tag each token in a sentence with parts of speech,1,,,,,0,1,0,1,0
Bases: nltk.tag.api.TaggerI,,,,,,,,,,,,,,,,,
A tagger that requires tokens to be featuresets. A featureset is a dictionary that maps from feature names to feature values. See nltk.classify for more information about features and featuresets.,,,,,Require tokens to be featuresets,,,1,,,,,,,,,
Bases: object,,,,,,,,,,,,,,,,,
"A processing interface for assigning a tag to each token in a list. Tags are case sensitive strings that identify some property of each token, such as its part of speech or its sense.",Assign a tag for each token in a list,"Assign tag
Identify property such as part of speech",,,Assign a tag to each token in a list,Assign a tag to each token in a list,Assign a tag to each token in a list,1,,,"assign tag
identify property such_as part
identify case sensitive strings such_as part","assign tag
identify property such_as part
identify case sensitive strings such_as part",,,,,
"Some taggers require specific types for their tokens. This is generally indicated by the use of a sub-interface to TaggerI. For example, featureset taggers, which are subclassed from FeaturesetTagger, require that each token be a featureset.",,,,,,,,1,,,,,,,,,
either tag() or tag_sents() (or both),,,,,,,,,,,,,,,,,
"Score the accuracy of the tagger against the gold standard. Strip the tags from the gold standard text, retag it using the tagger, then compute the accuracy score.",Score the accuracy of the tagger against the gold standard,Compute accuracy score,Compute accuracy score,Compute accuracy score,"Score the accuracy of the tagger against the gold standard
Strip tags from gold standard text ","Score the accuracy of the tagger against the gold standard
Strip tags from gold standard text ","Score the accuracy of the tagger against the gold standard
Strip tags from gold standard text ",1,,,"use tagger
compute accuracy score",compute accuracy score,,,,,
"gold (list(list(tuple(str, str)))) – The list of tagged sentences to score the tagger on.",,,,,,,,,,,,,,,,,
float,,,,,,,,,,,,,,,,,
"Determine the most appropriate tag sequence for the given token sequence, and return a corresponding list of tagged tokens. A tagged token is encoded as a tuple (token, tag).",Determine most appropriate tagging sequence for token sequence,"Determine appropriate tag sequence for given token sequence
Return corresponding list of tagged tokens
Encode tagged token as tuple",,,"Determine most appropriate tag sequence for given token sequence
Return corresponding list of tagged tokens",Determine most appropriate tag sequence for given token sequence,"Determine most appropriate tag sequence for given token sequence
Return corresponding list of tagged tokens",1,,,,,,,,,
"list(tuple(str, str))",,,,,,,,,,,,,,,,,
Apply self.tag() to each element of sentences. I.e.:,Apply tag to each element of sentences,Apply self.tag to each element of sentences,,,Apply self.tag() to each element of sentences,,,0,,,,,,,,,
return [self.tag(sent) for sent in sentences],,,,,,,,,,,"return [ self.tag(sent)
send  in sentences ]","return [ self.tag(sent)
send  in sentences ]",,,,,
Bases: nltk.tag.api.TaggerI,,,,,,,,,,,,,,,,,
Brill’s transformational rule-based tagger. Brill taggers use an initial tagger (such as tag.DefaultTagger) to assign an initial tag sequence to a text; and then apply an ordered list of transformational rules to correct the tags of individual tokens. These transformation rules are specified by the TagRule interface.,,"Assign initial sequence
Apply ordered list of rules",,,"Assign an initial tag sequest to text
Apply an ordered list of transformational rules to correct the tags of individual tokens",Apply an ordered list of transformational rules to correct the tags of individual tokens,Apply an ordered list of transformational rules to correct the tags of individual tokens,1,,,,,,,,,
"Brill taggers can be created directly, from an initial tagger and a list of transformational rules; but more often, Brill taggers are created by learning rules from a training corpus, using one of the TaggerTrainers available.",How to use Brill Tagger,Create Brill tagger,Create Brill tagger,Create Brill tagger,Learn rules from a training corpus,,Learn rules from a training corpus,1,,,"learn rules from training corpus
create brill taggers from initial tagger
create brill taggers from list
create brill taggers","learn rules from training corpus
create brill taggers from initial tagger
create brill taggers from list
create brill taggers",,,,,
Tags by applying each rule to the entire corpus (rather than all rules to a single sequence). The point is to collect statistics on the test set for individual rules.,,,,,Collect statistics on the test set for individual rules,,Collect statistics on the test set for individual rules,1,,,"apply rule to entire corpus
set  for individual rules","apply rule to entire corpus
set  for individual rules",,,,,
"NOTE: This is inefficient (does not build any index, so will traverse the entire corpus N times for N rules) – usually you would not care about statistics for individual rules and thus use batch_tag() instead",,,,,,,,,,,use batch_tag(),,,,,,
"sequences (list of list of strings) – lists of token sequences (sentences, in some applications) to be tagged",Get list of token sequences,Tag list of token sequences,Tag list of token sequences,Tag list of token sequences,,,,1,,,,,,,,,
gold (list of list of strings) – the gold standard,get gold standard,,,,,,,,,,,,,,,,
"tuple of (tagged_sequences, ordered list of rule scores (one for each rule))",,,,,,,,,,,,,,,,,
"Print a list of all templates, ranked according to efficiency.",Print templates ranked by efficiency,Print list of templates,Print list of templates,Print list of templates,"Print a list of all templates, ranked according to efficiency.","Print a list of all templates, ranked according to efficiency.","Print a list of all templates, ranked according to efficiency.",0,,,print list of templates,print list of templates,,,,,
"If test_stats is available, the templates are ranked according to their relative contribution (summed for all rules created from a given template, weighted by score) to the performance on the test set. If no test_stats, then statistics collected during training are used instead. There is also an unweighted measure (just counting the rules). This is less informative, though, as many low-score rules will appear towards end of training.",Rank templates,Use statistics ,,,Rank templates according to their relative contribution,,Rank templates according to their relative contribution,1,,,use statistics,,,,,,
test_stats (dict of str -> any (but usually numbers)) – dictionary of statistics collected during testing,,,,,,,,,,,,,,,,,
"printunused (bool) – if True, print a list of all unused templates",Print unused templates,Print unused templates,Print unused templates,Print unused templates,Print a list of all unused templates,,,0,,,print list of unused templates,print list of unused templates,,,,,
None,,,,,,,,,,,,,,,,,
None,,,,,,,,,,,,,,,,,
Return the ordered list of transformation rules that this tagger has learnt,Return ordered list of transformation rules,Order list of transformation rules,Return ordered list of transformation rules,,Return the ordered list of transformation rules that this tagger has learnt,,,1,,,,,,,,,
the ordered list of transformation rules that correct the initial tagging,,,,,,,,,,,order list of transformation rules,order list of transformation rules,,,,,
list of Rules,,,,,,,,,,,,,,,,,
"Determine the most appropriate tag sequence for the given token sequence, and return a corresponding list of tagged tokens. A tagged token is encoded as a tuple (token, tag).",Determine most appropriate tag sequence,"Determine appropriate tag sequence for given token sequence
Return corresponding list of tagged tokens
Encode tagged token as tuple",,,Determine the most appropriate tag sequence for the given token sequence,Determine the most appropriate tag sequence for the given token sequence,Determine the most appropriate tag sequence for the given token sequence,1,,,,,,,,,
"list(tuple(str, str))",,,,,,,,,,,,,,,,,
"Return a named statistic collected during training, or a dictionary of all available statistics if no name given",Return a named statistic,"Return named statistic of available statistics, return dictionary of available statistics","Return named statistic of available statistics
Return dictionary of available statistics",,Return a named statistic collected during training,,,1,,,"return named statistic of available statistics
return dictionary of available statistics","return named statistic of available statistics
return dictionary of available statistics",,,,,
statistic (str) – name of statistic,,,,,,,,,,,,,,,,,
some statistic collected during training of this tagger,,,,,,,,,,,,,,,,,
any (but usually a number),,,,,,,,,,,,,,,,,
Bases: nltk.tbl.feature.Feature,,,,,,,,,,,,,,,,,
Feature which examines the tags of nearby tokens.,,,,,,,,,,,,,,,,,
@return: The given token’s tag.,,,,,,,,,,,,,,,,,
Bases: nltk.tbl.feature.Feature,,,,,,,,,,,,,,,,,
Feature which examines the text (word) of nearby tokens.,,,,,,,,,,,,,,,,,
@return: The given token’s text.,,,,,,,,,,,,,,,,,
"Return 24 templates of the seminal TBL paper, Brill (1995)",Return the 24 templates from TBL paper,,,,,,,,,,,,,,,,
"Print the available template sets in this demo, with a short description”",Print available template sets,Print available templates,Print available templates,Print available templates,Print the available template sets ,,Print the available template sets ,0,Print the available template sets ,Print the available template sets ,"set  with short description
set  in demo","set  with short description
set  in demo",,,,,
Return 37 templates taken from the postagging task of the fntbl distribution http://www.cs.jhu.edu/~rflorian/fntbl/ (37 is after excluding a handful which do not condition on Pos[0]; fntbl can do that but the current nltk implementation cannot.),Return the 37 templates from postagging of fntbl distribution,Return templates,,,Return 37 templates taken from the postagging task,,,0,,,,,,,,,
"Return 18 templates, from the original nltk demo, in multi-feature syntax",Return 18 templates from nltk demo,Return templates,,,Return 18 templates,,,0,,,,,,,,,
"Return 18 templates, from the original nltk demo, and additionally a few multi-feature ones (the motivation is easy comparison with nltkdemo18)",Return 18 templates from nltk demo,Return templates,,,Return 18 templates,,,0,,,,,,,,,
Bases: object,,,,,,,,,,,,,,,,,
A trainer for tbl taggers.,,,,,,,,,,,,,,,,,
"Trains the Brill tagger on the corpus train_sents, producing at most max_rules transformations, each of which reduces the net number of errors in the corpus by at least min_score, and each of which has accuracy not lower than min_acc.",Train Brill tagger,Train Brill tagger,Train Brill tagger,Train Brill tagger,Train the Brill tagger on the corpus,,Train the Brill tagger on the corpus,0,,,produce  at most max_rules transformations,produce  at most max_rules transformations,,,,,
"#imports >>> from nltk.tbl.template import Template >>> from nltk.tag.brill import Pos, Word >>> from nltk.tag import untag, RegexpTagger, BrillTaggerTrainer",,,,,,,,,,,,,,,,,
#some data >>> from nltk.corpus import treebank >>> training_data = treebank.tagged_sents()[:100] >>> baseline_data = treebank.tagged_sents()[100:200] >>> gold_data = treebank.tagged_sents()[200:300] >>> testing_data = [untag(s) for s in gold_data],,,,,,,,,,,,,,,,,
"#templates >>> Template._cleartemplates() #clear any templates created in earlier tests >>> templates = [Template(Pos([-1])), Template(Pos([-1]), Word([0]))]",,,,,,,,,,,create  in tests &gt; &gt; &gt; templates,create  in tests &gt; &gt; &gt; templates,,,,,
"#construct a BrillTaggerTrainer >>> tt = BrillTaggerTrainer(baseline, templates, trace=3)",,,,,,,,,,,,,,,,,
"# a high-accuracy tagger >>> tagger2 = tt.train(training_data, max_rules=10, min_acc=0.99) TBL train (fast) (seqs: 100; tokens: 2417; tpls: 2; min score: 2; min acc: 0.99) Finding initial useful rules…",,,,,,,,,,,find initial useful rules,find initial useful rules,,,,,
Found 845 useful rules.,,,,,,,,,,,find useful rules,find useful rules,,,,,
B |,,,,,,,,,,,,,,,,,
S F r O | Score = Fixed - Broken c i o t | R Fixed = num tags changed incorrect -> correct o x k h | u Broken = num tags changed correct -> incorrect r e e e | l Other = num tags changed incorrect -> incorrect e d n r | e,,,,,,,,,,,,,,,,,
"85 85 0 0 | NN->, if Pos:NN@[-1] & Word:,@[0] 69 69 0 0 | NN->. if Pos:NN@[-1] & Word:.@[0] 51 51 0 0 | NN->IN if Pos:NN@[-1] & Word:of@[0] 36 36 0 0 | NN->TO if Pos:NN@[-1] & Word:to@[0] 26 26 0 0 | NN->. if Pos:NNS@[-1] & Word:.@[0] 24 24 0 0 | NN->, if Pos:NNS@[-1] & Word:,@[0] 19 19 0 6 | NN->VB if Pos:TO@[-1] 18 18 0 0 | CD->-NONE- if Pos:NN@[-1] & Word:0@[0] 18 18 0 0 | NN->CC if Pos:NN@[-1] & Word:and@[0]",,,,,,,,,,,,,,,,,
"# NOTE1: (!!FIXME) A far better baseline uses nltk.tag.UnigramTagger, # with a RegexpTagger only as backoff. For instance, # >>> baseline = UnigramTagger(baseline_data, backoff=backoff) # However, as of Nov 2013, nltk.tag.UnigramTagger does not yield consistent results # between python versions. The simplistic backoff above is a workaround to make doctests # get consistent input.",,,,,,,,,,,,,,,,,
train_sents (list(list(tuple))) – training data,Get training data,,,,,,,,,,,,,,,,
max_rules (int) – output at most max_rules rules,Print max_rules rules,,,,Output at most max_rules rules,,,1,,,,,,,,,
min_score (int) – stop training when no rules better than min_score can be found,Specify min score for all rules,Find rules,,,Stop training,,,1,,,find rules,find rules,,,,,
min_acc (float or None) – discard any rule with lower accuracy than min_acc,Specify min accuracy ,,,,Discard any rule with lower accuracy than min_acc,,Discard any rule with lower accuracy than min_acc,1,,,,,,,,,
the learned tagger,,learn tagger,,,,,,1,,,learn tagger,learn tagger,,,,,
BrillTagger,,,,,,,,,,,,,,,,,
A module for POS tagging using CRFSuite,,,,,,,,,,,use CRFSuite,,,,,,
Bases: nltk.tag.api.TaggerI,,,,,,,,,,,,,,,,,
A module for POS tagging using CRFSuite https://pypi.python.org/pypi/python-crfsuite,,,,,,,,,,,use CRFSuite https://pypi.python.org/pypi/python-crfsuite,,,,,,
Setting learned model file >>> ct = CRFTagger() >>> ct.set_model_file(‘model.crf.tagger’) >>> ct.evaluate(gold_sentences) 1.0,,,,,,,,,,,,,,,,,
Train a new model using ``train’’ function,Train new model,"Train a new model
Use train",Train a new model,Train a new model,Train a new model using ``train’’ function,Train a new model using ``train’’ function,Train a new model using ``train’’ function,1,,,use train,,,,,,
Use the pre-trained model which is set via ``set_model_file’’ function,Use pre-trained model,"Use pre-trained model
Set pre-trained model via set_model_file function",Use pre-trained model,,,,,1,,,"use pre-trained model
set pre-trained model via set_model_file function",set pre-trained model via set_model_file function,,,,,
":params tokens : list of tokens needed to tag. :type tokens : list(str) :return : list of tagged tokens. :rtype : list (tuple(str,str))",,,,,,,,,,,,,,,,,
Train a new model using ``train’’ function,Train new model,"Train a new model
Use train",Train a new model,Train a new model,Train a new model using ``train’’ function,Train a new model using ``train’’ function,Train a new model using ``train’’ function,1,,,use train,,,,,,
Use the pre-trained model which is set via ``set_model_file’’ function,User pre-trained model,"Use pre-trained model, Set pre-trained model via set_model_file function",Use pre-trained model,,Use pre-trained model,Use pre-trained model,,1,,,"use pre-trained model
set pre-trained model via set_model_file function",set pre-trained model via set_model_file function,,,,,
":params sentences : list of sentences needed to tag. :type sentences : list(list(str)) :return : list of tagged sentences. :rtype : list (list (tuple(str,str)))",,,,,,,,,,,,,,,,,
"Train the CRF tagger using CRFSuite :params train_data : is the list of annotated sentences. :type train_data : list (list(tuple(str,str))) :params model_file : the model will be saved to this file.",Train CRF tagger using CRFSuite,"Use CRFSuite
Save model to file",Train CRF tagger using CRFSuite,,Train the CRF tagger,,Train the CRF tagger,1,,,"use CRFSuite
save model to file",save model to file,,,,,
"Hidden Markov Models (HMMs) largely used to assign the correct label sequence to sequential data or assess the probability of a given label and data sequence. These models are finite state machines characterised by a number of states, transitions between these states, and output symbols emitted while in each state. The HMM is an extension to the Markov chain, where each state corresponds deterministically to a given event. In the HMM the observation is a probabilistic function of the state. HMMs share the Markov chain’s assumption, being that the probability of transition from one state to another only depends on the current state - i.e. the series of states that led to the current state are not used. They are also time invariant.",,"Assign label sequence
Assess probability of sequence",,,"Assign the correct label sequence to sequential data
Assess the probabiliy of a given label and data sequence","Assign the correct label sequence to sequential data
Assess the probabiliy of a given label and data sequence",Assess the probabiliy of a given label and data sequence,1,,,"assign correct label sequence to sequential data
share assumption","assign correct label sequence to sequential data
share assumption",,,,,
"The HMM is a directed graph, with probability weighted edges (representing the probability of a transition between the source and sink states) where each vertex emits an output symbol when entered. The symbol (or observation) is non-deterministically generated. For this reason, knowing that a sequence of output observations was generated by a given HMM does not mean that the corresponding sequence of states (and what the current state is) is known. This is the ‘hidden’ in the hidden markov model.",,,,,,,,,,,"generate symbol
generate sequence of output observations","generate symbol
generate sequence of output observations",,,,,
"Formally, a HMM can be characterised by:",,,,,,,,,,,,,,,,,
the output observation alphabet. This is the set of symbols which may be observed as output of the system.,,,,,,,,,,,,,,,,,
the set of states.,,,,,,,,,,,,,,,,,
the transition probabilities a_{ij} = P(s_t = j | s_{t-1} = i). These represent the probability of transition to each state from a given state.,Get transition probabilities,,,,,,,,,,,,,,,,
the output probability matrix b_i(k) = P(X_t = o_k | s_t = i). These represent the probability of observing each symbol in a given state.,Get probability matrix,,,,,,,,,,,,,,,,
the initial state distribution. This gives the probability of starting in each state.,Get initial state distribution,,,,,,,,,,,,,,,,
"To ground this discussion, take a common NLP application, part-of-speech (POS) tagging. An HMM is desirable for this task as the highest probability tag sequence can be calculated for a given sequence of word forms. This differs from other tagging techniques which often tag each word individually, seeking to optimise each individual tagging greedily without regard to the optimal combination of tags for a larger unit, such as a sentence. The HMM does this with the Viterbi algorithm, which efficiently computes the optimal path through the graph given the sequence of words forms.",,,,,Ground this discussion,,,1,,,"calculate highest probability for given sequence
compute optimal path through graph
compute Viterbi algorithm through graph","calculate highest probability for given sequence
compute optimal path through graph
compute Viterbi algorithm through graph",,,,,
"In POS tagging the states usually have a 1:1 correspondence with the tag alphabet - i.e. each state represents a single tag. The output observation alphabet is the set of word forms (the lexicon), and the remaining three parameters are derived by a training regime. With this information the probability of a given sentence can be easily derived, by simply summing the probability of each distinct path through the model. Similarly, the highest probability tagging sequence can be derived with the Viterbi algorithm, yielding a state sequence which can be mapped into a tag sequence.",,,,,Derive probability of a given sentence,Derive probability of a given sentence,Derive probability of a given sentence,1,,,,,,,,,
"This discussion assumes that the HMM has been trained. This is probably the most difficult task with the model, and requires either MLE estimates of the parameters or unsupervised learning using the Baum-Welch algorithm, a variant of EM.",,,,,,,,,,,use baum-welch algorithm,,,,,,
"For more information, please consult the source code for this module, which includes extensive demonstration code.",,,,,,,,,,,"include extensive demonstration code
include module","include extensive demonstration code
include module",,,,,
Bases: nltk.tag.api.TaggerI,,,,,,,,,,,,,,,,,
"Hidden Markov model class, a generative model for labelling sequence data. These models define the joint probability of a sequence of symbols and their labels (state transitions) as the product of the starting state probability, the probability of each state transition, and the probability of each observation being generated from each state. This is described in more detail in the module documentation.",,Label sequence data,Label sequence data,,Label sequence data,Label sequence data,Label sequence data,,,,"define probability as product
define probability of observation
define probability of sequence
define probability of state transition
define joint probability as product
define joint probability of observation
define joint probability of sequence
define joint probability of state transition
define probability as product
define probability of observation
define probability of sequence
define probability of state transition
define labels as product
define labels of observation
define labels of sequence
define labels of state transition
generate  from state
describe  in more detail",generate  from state,,,,,
"This implementation is based on the HMM description in Chapter 8, Huang, Acero and Hon, Spoken Language Processing and includes an extension for training shallow HMM parsers or specialized HMMs as in Molina et. al, 2002. A specialized HMM modifies training data by applying a specialization function to create a new training set that is more appropriate for sequential tagging with an HMM. A typical use case is chunking.",,Modify training data,,,Modify training data by applying a specialization function,,,0,,,,,,,,,
symbols (seq of any) – the set of output symbols (alphabet),Get set of output symbols,,,,,,,,,,,,,,,,
states (seq of any) – a set of states representing state space,Get state space,,,,,,,,,,,,,,,,
transitions (ConditionalProbDistI) – transition probabilities; Pr(s_i | s_j) is the probability of transition from state i given the model is in state_j,Get transition probabilities ,,,,,,,,,,,,,,,,
outputs (ConditionalProbDistI) – output probabilities; Pr(o_k | s_i) is the probability of emitting symbol k when entering state i,Get output probabilities,,,,,,,,,,enter state i.,enter state i.,,,,,
priors (ProbDistI) – initial state distribution; Pr(s_i) is the probability of starting in state i,,,,,,,,,,,,,,,,,
"transform (callable) – an optional function for transforming training instances, defaults to the identity function.",Transform training instances,Transform training instances,Transform training instances,Transform training instances,Transform training instances,,Transform training instances,,,,,,,,,,
Returns the state sequence of the optimal (most probable) path through the HMM. Uses the Viterbi algorithm to calculate this part by dynamic programming.,Return state sequence of optimal path through HMM,Return state sequence of optimal path,Return state sequence of optimal path,,Return the state sequent of the optimal path,,,1,,,"return state sequence through HMM
return state sequence of optimal path
calculate part by dynamic programming","return state sequence through HMM
return state sequence of optimal path
calculate part by dynamic programming",,,,,
the state sequence,,,,,,,,,,,,,,,,,
sequence of any,,,,,,,,,,,,,,,,,
unlabeled_sequence (list) – the sequence of unlabeled symbols,,,,,,,,,,,,,,,,,
"Returns the state sequence of the optimal (most probable) path through the HMM. Uses the Viterbi algorithm to calculate this part by dynamic programming. This uses a simple, direct method, and is included for teaching purposes.",Get state sequence of optimal path through HMM,,,,Return the state sequence of the optimal path through the HMM,Return the state sequence of the optimal path through the HMM,,1,,,,,,,,,
the state sequence,,,,,,,,,,,,,,,,,
sequence of any,,,,,,,,,,,,,,,,,
unlabeled_sequence (list) – the sequence of unlabeled symbols,,,,,,,,,,,,,,,,,
Returns the entropy over labellings of the given sequence. This is given by:,Returns entropy over labellings of the given sequence,Return entropy over labellings of given sequence,Return entropy over labellings of given sequence,,Return the entropy over labellings of the given sequence,Return the entropy over labellings of the given sequence,Return the entropy over labellings of the given sequence,0,,,return entropy over labellings,return entropy over labellings,,,,,
"where the summation ranges over all state sequences, S. Let Z = Pr(O) = sum_S Pr(S, O)} where the summation ranges over all state sequences and O is the observation sequence. As such the entropy can be re-expressed as:",,,,,,,,,,,,,,,,,
"The order of summation for the log terms can be flipped, allowing dynamic programming to be used to calculate the entropy. Specifically, we use the forward and backward probabilities (alpha, beta) giving:",Calculate the entropy,Calculate entropy,Calculate entropy,Calculate entropy,Flip order of summation for log terms,,Flip order of summation for log terms,1,,,calculate entropy,calculate entropy,,,,,
"This simply uses alpha and beta to find the probabilities of partial sequences, constrained to include the given state(s) at some point in time.",Find probabilities of partial sequences,Find probabilities of partial sequences,Find probabilities of partial sequences,,Find the probabilities of partial sequences,Find the probabilities of partial sequences,,0,,,"find probabilities of partial sequences
include given state at point","find probabilities of partial sequences
include given state at point",,,,,
"Returns the log-probability of the given symbol sequence. If the sequence is labelled, then returns the joint log-probability of the symbol, state sequence. Otherwise, uses the forward algorithm to find the log-probability over all label sequences.",Get log-probability of given symbol sequence,"Return log-probability of given symbol sequence
Return joint log-probability of symbol
Use forward algorithm
Find log-probability over label sequences","Return log-probability of given symbol sequence
Return joint log-probability of symbol
Find log-probability over label sequences",,Return the log-probability of the given symbol sequence,Return the log-probability of the given symbol sequence,Return the log-probability of the given symbol sequence,1,,,"return log-probability of given symbol sequence
return joint log-probability of symbol
use forward algorithm
find log-probability over label sequences","return log-probability of given symbol sequence
return joint log-probability of symbol
find log-probability over label sequences",,,,,
the log-probability of the sequence,,,,,,,,,,,,,,,,,
float,,,,,,,,,,,,,,,,,
"sequence (Token) – the sequence of symbols which must contain the TEXT property, and optionally the TAG property",,,,,,,,,,,,,,,,,
"Returns the pointwise entropy over the possible states at each position in the chain, given the observation sequence.",Get pointwise entropy,Return pointwise entropy over possible states,Return pointwise entropy over possible states,,Return the pointwise entropy over the possible states at each position in the chain,Return the pointwise entropy over the possible states at each position in the chain,Return the pointwise entropy over the possible states at each position in the chain,0,,,return pointwise entropy over possible states,return pointwise entropy over possible states,,,,,
"Returns the probability of the given symbol sequence. If the sequence is labelled, then returns the joint probability of the symbol, state sequence. Otherwise, uses the forward algorithm to find the probability over all label sequences.",Get probability of given symbol sequence,"Return probability of given symbol sequence
Return joint probability of symbol
Find probability over label sequence","Return probability of given symbol sequence
Return joint probability of symbol
Find probability over label sequence",,Return the probability of the given symbol sequence,Return the probability of the given symbol sequence,Return the probability of the given symbol sequence,1,,,"return probability of given symbol sequence
return joint probability of symbol
use forward algorithm
find probability over label sequences","return probability of given symbol sequence
return joint probability of symbol
find probability over label sequences",,,,,
the probability of the sequence,,,,,,,,,,,,,,,,,
float,,,,,,,,,,,,,,,,,
"sequence (Token) – the sequence of symbols which must contain the TEXT property, and optionally the TAG property",,,,,,,,,,,,,,,,,
"Randomly sample the HMM to generate a sentence of a given length. This samples the prior distribution then the observation distribution and transition distribution for each subsequent observation and state. This will mostly generate unintelligible garbage, but can provide some amusement.",Randomly sample HMM to generate a sentence,"Generate sentence of given length
Generate unintelligible garbage",,Generate sentence of given length,Randomly sample the HMM to generate a sentence of a given length,,Randomly sample the HMM to generate a sentence of a given length,1,,,"generate sentence of given length
generate unintelligible garbage
provide amusement","generate sentence of given length
generate unintelligible garbage
provide amusement",,,,,
"the randomly created state/observation sequence, generated according to the HMM’s probability distributions. The SUBTOKENS have TEXT and TAG properties containing the observation and state respectively.",,,,,,,,,,,,,,,,,
list,,,,,,,,,,,,,,,,,
rng (Random (or any object with a random() method)) – random number generator,,,,,,,,,,,,,,,,,
length (int) – desired output length,,,,,,,,,,,,,,,,,
Tags the sequence with the highest probability state sequence. This uses the best_path method to find the Viterbi path.,Tag sequence with highest probability state sequence,"Tag highest probability state sequence
Find Viterbi path",,,Tag the sequence with the highest probability state,Tag the sequence with the highest probability state,Tag the sequence with the highest probability state,1,,,find Viterbi path,find Viterbi path,,,,,
a labelled sequence of symbols,Get labelled sequence of symbols ,,,,,,,,,,,,,,,,
list,,,,,,,,,,,,,,,,,
unlabeled_sequence (list) – the sequence of unlabeled symbols,Get unlabeled symbols,,,,,,,,,,,,,,,,
Tests the HiddenMarkovModelTagger instance.,Test HiddenMarkovModelTagger,,,,,,,,,,,,,,,,
test_sequence (list(list)) – a sequence of labeled test instances,,,,,,,,,,,,,,,,,
verbose (bool) – boolean flag indicating whether training should be verbose or include printed output,Set verbose,Include printed output,,,Indicate whether training should be verbose or include printed output,,Indicate whether training should be verbose or include printed output,1,,,include printed output,include printed output,,,,,
Train a new HiddenMarkovModelTagger using the given labeled and unlabeled training instances. Testing will be performed if test instances are provided.,Train HidenMarkovModelTagger,"Train new HiddenMarkovModelTagger
Use given labeled
Use unlabeled training instances
Perform testing
Provide test instances",Train new HiddenMarkovModelTagger,,Train a new HiddenMarkoveModelTagger,Train a new HiddenMarkoveModelTagger,Train a new HiddenMarkoveModelTagger,1,,,"use given labeled
use unlabeled training instances
perform testing
provide test instances","perform testing
provide test instances",,,,,
a hidden markov model tagger,,,,,,,,,,,,,,,,,
HiddenMarkovModelTagger,,,,,,,,,,,,,,,,,
"labeled_sequence (list(list)) – a sequence of labeled training instances, i.e. a list of sentences represented as tuples",Get labeled training instances,,,,,,,,,,,,,,,,
test_sequence (list(list)) – a sequence of labeled test instances,Get sequence of labeled test instances,,,,,,,,,,,,,,,,
"unlabeled_sequence (list(list)) – a sequence of unlabeled training instances, i.e. a list of sentences represented as words",Get unlabeled training instances,,,,,,,,,,,,,,,,
"transform (function) – an optional function for transforming training instances, defaults to the identity function, see transform()",Transform training instances,transform training instances,,,Transform training instances,Transform training instances,Transform training instances,1,,,,,,,,,
"estimator (class or function) – an optional function or class that maps a condition’s frequency distribution to its probability distribution, defaults to a Lidstone distribution with gamma = 0.1",Map condition frequency distribution to probability distribution,Map condition frequency distribution ,,,Map a condition's frequencey distribution to its probability distribution,Map a condition's frequencey distribution to its probability distribution,,1,,,,,,,,,
verbose (bool) – boolean flag indicating whether training should be verbose or include printed output,Make verbose,Include printed output,,,Indicate whether training should be verbose or include printed output,,Indicate whether training should be verbose or include printed output,1,,,include printed output,include printed output,,,,,
max_iterations (int) – number of Baum-Welch interations to perform,Set max iterations,,,,,,,,,,,,,,,,
Bases: object,,,,,,,,,,,,,,,,,
Algorithms for learning HMM parameters from training data. These include both supervised learning (MLE) and unsupervised learning (Baum-Welch).,,,,,,,,,,,"learn HMM parameters from training data
include supervised learning","learn HMM parameters from training data
include supervised learning",,,,,
"Creates an HMM trainer to induce an HMM with the given states and output symbol alphabet. A supervised and unsupervised training method may be used. If either of the states or symbols are not given, these may be derived from supervised training.",Create HMM trainer,"Create HMM trainer
Use supervised unsupervised training method ",,,Create an HMM trainer to induce an HMM with the given states and output symbol alphabet,Create an HMM trainer to induce an HMM with the given states and output symbol alphabet,,1,,,use supervised unsupervised training method,,,,,,
states (sequence of any) – the set of state labels,Get state labels,,,,,,,,,,,,,,,,
symbols (sequence of any) – the set of observation symbols,Get observation symbols,,,,,,,,,,,,,,,,
Trains the HMM using both (or either of) supervised and unsupervised techniques.,Train HMM ,"Train HMM
Use supervised unsupervised techniques",,,Train the HMM using both supervised and unsupervised techniques,Train the HMM using both supervised and unsupervised techniques,Train the HMM using both supervised and unsupervised techniques,1,,,use supervised unsupervised techniques,,,,,,
the trained model,,,,,,,,,,,,,,,,,
HiddenMarkovModelTagger,,,,,,,,,,,,,,,,,
"labelled_sequences (list) – the supervised training data, a set of labelled sequences of observations ex: [ (word_1, tag_1),…,(word_n,tag_n) ]",Get the supervised training data,,,,,,,,,,,,,,,,
"unlabeled_sequences (list) – the unsupervised training data, a set of sequences of observations ex: [ word_1, …, word_n ]",Get unsupervised training data,,,,,,,,,,,,,,,,
kwargs – additional arguments to pass to the training methods,,,,,,,,,,,pass  to training methods,pass  to training methods,,,,,
"Supervised training maximising the joint probability of the symbol and state sequences. This is done via collecting frequencies of transitions between states, symbol observations while within each state and which states start a sentence. These frequency distributions are then normalised into probability estimates, which can be smoothed if desired.",,,,,,Train supervised,,1,,,,,,,,,
the trained model,,,,,,,,,,,,,,,,,
HiddenMarkovModelTagger,,,,,,,,,,,,,,,,,
"labelled_sequences (list) – the training data, a set of labelled sequences of observations",Get labelled sequence of observations,,,,,,,,,,,,,,,,
estimator – a function taking a FreqDist and a number of bins and returning a CProbDistI; otherwise a MLE estimate is used,Estimate the CProbDistl,"Return CProbDistl
Use MLE estimate",,,,,,1,,,"return cprobdisti
use MLE estimate",return cprobdisti,,,,,
"Trains the HMM using the Baum-Welch algorithm to maximise the probability of the data sequence. This is a variant of the EM algorithm, and is unsupervised in that it doesn’t need the state sequences for the symbols. The code is based on ‘A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition’, Lawrence Rabiner, IEEE, 1989.",Train HMM using Baum-Welch algorithm,Use Baum-Welch algorithm,Use Baum-Welch algorithm,Use Baum-Welch algorithm,"Train HMM using Baum-Welch algorithm
Maximize the probability of the data sequence","Train HMM using Baum-Welch algorithm
Maximize the probabiliyof the data sequence",Train HMM using Baum-Welch algorithm,1,,,use baum-welch algorithm,,,,,,
the trained model,,,,,,,,,,,,,,,,,
HiddenMarkovModelTagger,,,,,,,,,,,,,,,,,
"unlabeled_sequences (list) – the training data, a set of sequences of observations",Get training data of set of sequences of observations,,,,,,,,,,,,,,,,
kwargs may include following parameters:,,,,,,,,,,,include following parameters,include following parameters,,,,,
model – a HiddenMarkovModelTagger instance used to begin the Baum-Welch algorithm,,,,,Begin the Baum-Welch algorithm,,Begin the Baum-Welch algorithm,1,,,,,,,,,
max_iterations – the maximum number of EM iterations,,,,,,,,,,,,,,,,,
convergence_logprob – the maximum change in log probability to allow convergence,,,,,,,,,,,,,,,,,
A module for interfacing with the HunPos open-source POS-tagger.,,,,,,,,,,,,,,,,,
Bases: nltk.tag.api.TaggerI,,,,,,,,,,,,,,,,,
a model trained on training data,,,,,Train model on training data,,Train model on training data,1,,,,,,,,,
(optionally) the path to the hunpos-tag binary,,,,,,,,,,,,,,,,,
(optionally) the encoding of the training data (default: ISO-8859-1),,,,,,,,,,,,,,,,,
Example:,,,,,,,,,,,,,,,,,
"This class communicates with the hunpos-tag binary via pipes. When the tagger object is no longer needed, the close() method should be called to free system resources. The class supports the context manager interface; if used in a with statement, the close() method is invoked automatically:",,,,,Communicate with the hunpos-tag binary via pipes,,,,,,"call close() method to free system resources
support context manager interface","call close() method to free system resources
support context manager interface",,,,,
Closes the pipe to the hunpos executable.,Close pipe to hunpos executable,Close pipe ,,,Close the pipe to the hunpos executable,,,1,,,,,,,,,
Tags a single sentence: a list of words. The tokens should not contain any newline characters.,Tag a single sentence,Tag sentence,Tag sentence,Tag sentence,Tag a single sentence,Tag a single sentence,Tag a single sentence,1,,,,,,,,,
"Interface for converting POS tags from various treebanks to the universal tagset of Petrov, Das, & McDonald.",,,,,,,,,,,"convert POS tags from various treebanks
convert POS tags to universal tagset","convert POS tags from various treebanks
convert POS tags to universal tagset",,,,,
The tagset consists of the following 12 coarse tags:,,,,,,,,,,,,,,,,,
"VERB - verbs (all tenses and modes) NOUN - nouns (common and proper) PRON - pronouns ADJ - adjectives ADV - adverbs ADP - adpositions (prepositions and postpositions) CONJ - conjunctions DET - determiners NUM - cardinal numbers PRT - particles or other function words X - other: foreign words, typos, abbreviations . - punctuation",,,,,,,,,,,,,,,,,
@see: http://arxiv.org/abs/1104.2086 and http://code.google.com/p/universal-pos-tags/,,,,,,,,,,,,,,,,,
Maps the tag from the source tagset to the target tagset.,Map tag from source to target,,,,Map the tag from the source tagset to the target tagset,Map the tag from the source tagset to the target tagset,,1,,,,,,,,,
Retrieve the mapping dictionary between tagsets.,Get mapping dictionary between tagsets,Retrieve mapping dictionary between tagsets,,,Retrieve the mapping dictionary between tagsets,Retrieve the mapping dictionary between tagsets,,1,,,retrieve mapping dictionary between tagsets,retrieve mapping dictionary between tagsets,,,,,
Bases: object,,,,,,,,,,,,,,,,,
"An averaged perceptron, as implemented by Matthew Honnibal.",,,,,,,,,,,implement  by Matthew honnibal,implement  by Matthew honnibal,,,,,
https://explosion.ai/blog/part-of-speech-pos-tagger-in-python,,,,,,,,,,,,,,,,,
Average weights from all iterations.,,,,,,,,,,,,,,,,,
Load the pickled model weights.,Load pickled model weights,Pickle model weights,,,Load the pickled model weights,,Load the pickled model weights,1,,,,,,,,,
Dot-product the features and current weights and return the best label.,Dot-product the features and current weights,Return best label,,,Dot-product the features and current weights,,,1,,,return best label,return best label,,,,,
Save the pickled model weights.,Save pickled model weights,Save pickled model weights,Save pickled model weights,Save pickled model weights,Save the picked model weights,,,1,,,save pickled model weights,save pickled model weights,,,,,
Update the feature weights.,,,,,Update the feature weights,,Update the feature weights,1,,,update feature weights,update feature weights,,,,,
Bases: nltk.tag.api.TaggerI,,,,,,,,,,,,,,,,,
"Greedy Averaged Perceptron tagger, as implemented by Matthew Honnibal. See more implementation details here:",,,,,,,,,,,implement  by Matthew honnibal,implement  by Matthew honnibal,,,,,
https://explosion.ai/blog/part-of-speech-pos-tagger-in-python,,,,,,,,,,,,,,,,,
Train the model,Train the model,Train model,Train model,Train model,Train the model,,Train the model,1,,,,,,,,,
Use the pretrain model (the default constructor),Use pretrained model,Use pretrain model,Use pretrain model,Use pretrain model,,,,1,,,use pretrain model,,,,,,
loc (str) – Load a pickled model at location.,Load a pickled model,Pickle model,,,Load a pickled model at location,,,1,,,,,,,,,
Normalization used in pre-processing. - All words are lower cased - Groups of digits of length 4 are represented as !YEAR; - Other digits are represented as !DIGITS,,,,,,,,,,,use  in pre-processing,,,,,,
str,,,,,,,,,,,,,,,,,
Tag tokenized sentences. :params tokens: list of word :type tokens: list(str),,,,,Tag tokenized sentences,Tag tokenized sentences,Tag tokenized sentences,1,,,,,,,,,
"Train a model from sentences, and save it at save_loc. nr_iter controls the number of Perceptron training iterations.",Train a model and save it,Save at save_loc,,,Train a model from sentences,Train a model from sentences,Train a model from sentences,1,,,save  at save_loc,save  at save_loc,,,,,
"sentences – A list or iterator of sentences, where each sentence is a list of (words, tags) tuples.",,,,,,,,,,,,,,,,,
"save_loc – If not None, saves a pickled model in this location.",,,,,Save pickled model in this location,,,1,,,save pickled model in location,save pickled model in location,,,,,
nr_iter – Number of training iterations.,,,,,,,,,,,,,,,,,
"Senna POS tagger, NER Tagger, Chunk Tagger",,,,,,,,,,,,,,,,,
"The input is: - path to the directory that contains SENNA executables. If the path is incorrect,",,,,,,,,,,,,,,,,,
SennaTagger will automatically search for executable file specified in SENNA environment variable,,,,,Search for executable file specified in SENNA environment variable,,,1,,,"search  for executable file
specify  in SENNA environment variable",specify  in SENNA environment variable,,,,,
(optionally) the encoding of the input data (default:utf-8),,,,,,,,,,,,,,,,,
Note: Unit tests for this module can be found in test/unit/test_senna.py,,,,,,,,,,,"find unit tests in test/unit/test_senna.py
find unit tests for module","find unit tests in test/unit/test_senna.py
find unit tests for module",,,,,
Bases: nltk.classify.senna.Senna,,,,,,,,,,,,,,,,,
Extracts the chunks in a BIO chunk-tagged sentence.,Extract chunks in BIO chunk-tagged sentence,,,,Extract the chunks in the BIO chunk-tagged sentence,Extract the chunks in the BIO chunk-tagged sentence,,1,,,,,,,,,
tagged_sent (str) – A list of tuples of word and BIO chunk tag.,Get list of tuples of words and BIO chunk tag,,,,,,,,,,,,,,,,
"tagged_sent – The chunk tag that users want to extract, e.g. ‘NP’ or ‘VP’",The chunk tag to extract,Extract chunk tag,,,,,,1,,,,,,,,,
An iterable of tuples of chunks that users want to extract and their corresponding indices.,Get an iterable of tuples of chunks,"Extract chunk tuples
Extract corresponding indices",,,,,,1,,,,,,,,,
iter(tuple(str)),,,,,,,,,,,,,,,,,
"Applies the tag method over a list of sentences. This method will return for each sentence a list of tuples of (word, tag).",Apply tag method over list of sentences,"Return list of tuples
Return list for sentence",,,Apply the tag method over a list of sentences,,Apply the tag method over a list of sentences,1,,,"return list of tuples
return list for sentence","return list of tuples
return list for sentence",,,,,
Bases: nltk.classify.senna.Senna,,,,,,,,,,,,,,,,,
"Applies the tag method over a list of sentences. This method will return for each sentence a list of tuples of (word, tag).",Apply tag method over list of sentences,"Return list of types,
Return list for sentence",,,Apply the tag method over a list of sentences,,Apply the tag method over a list of sentences,1,,,"return list of tuples
return list for sentence","return list of tuples
return list for sentence",,,,,
Bases: nltk.classify.senna.Senna,,,,,,,,,,,,,,,,,
"Applies the tag method over a list of sentences. This method will return for each sentence a list of tuples of (word, tag).",Apply tag method over list of sentences,"Return list of types
Return list for sentence",,,Apply the tag method over a list of sentences,,,1,,,"return list of tuples
return list for sentence","return list of tuples
return list for sentence",,,,,
"Classes for tagging sentences sequentially, left to right. The abstract base class SequentialBackoffTagger serves as the base class for all the taggers in this module. Tagging of individual words is performed by the method choose_tag(), which is defined by subclasses of SequentialBackoffTagger. If a tagger is unable to determine a tag for the specified token, then its backoff tagger is consulted instead. Any SequentialBackoffTagger may serve as a backoff tagger for any other SequentialBackoffTagger.",,,,,"Tag sentences sequentially, left to right","Tag sentences sequentially, left to right","Tag sentences sequentially, left to right",1,,,,,,,,,
Bases: nltk.tag.sequential.ContextTagger,,,,,,,,,,,,,,,,,
"A tagger that chooses a token’s tag based on a leading or trailing substring of its word string. (It is important to note that these substrings are not necessarily “true” morphological affixes). In particular, a fixed-length substring of the word is looked up in a table, and the corresponding tag is returned. Affix taggers are typically constructed by training them on a tagged corpus.",,,,,Choose a token's tag,,,,,,,,,,,,
Construct a new affix tagger.,Make new affix tagger,Construct affix tagger,Construct affix tagger,Construct affix tagger,Construct a new affix tagger.,,,1,,,,,,,,,
affix_length – The length of the affixes that should be considered during training and tagging. Use negative numbers for suffixes.,Get length of affixes considered during training and tagging,Use negative numbers for suffixes,,,Consider length of the affixes during training and tagging,,,1,,,,,,,,,
min_stem_length – Any words whose length is less than min_stem_length+abs(affix_length) will be assigned a tag of None by this tagger.,Define minimum stem length,"Assign tag of None, Assign min_stem_length+abs(affix_length) of None",,,,,,1,,,"assign tag of None
assign min_stem_length + abs(affix_length) of None","assign tag of None
assign min_stem_length + abs(affix_length) of None",,,,,
the context that should be used to look up the tag for the specified token; or None if the specified token should not be handled by this tagger.,Define context to use when looking at tag for specified token,,,,,,,,,,,,,,,,
(hashable),,,,,,,,,,,,,,,,,
Bases: nltk.tag.sequential.NgramTagger,,,,,,,,,,,,,,,,,
"A tagger that chooses a token’s tag based its word string and on the preceding words’ tag. In particular, a tuple consisting of the previous tag and the word is looked up in a table, and the corresponding tag is returned.",,,,,Choose a token's tag,,,1,,,,,,,,,
"train (list(list(tuple(str, str)))) – The corpus of training data, a list of tagged sentences",Train on training data,,,,,,,,,,,,,,,,
model (dict) – The tagger model,Get the tagger model,,,,,,,,,,,,,,,,
backoff (TaggerI) – Another tagger which this tagger will consult when it is unable to tag a word,Specify a backoff tagger,,,,,,,,,,,,,,,,
cutoff (int) – The number of instances of training data the tagger must see in order not to use the backoff tagger,Specify number of instances the tagger must see to not use backoff tagger,Use backoff tagger,,,,,,1,,,,,,,,,
Bases: nltk.tag.sequential.ClassifierBasedTagger,,,,,,,,,,,,,,,,,
A classifier based part of speech tagger.,,,,,,,,,,,,,,,,,
Return the feature detector that this tagger uses to generate featuresets for its classifier. The feature detector is a function with the signature:,Get the feature detector for this tagger,"Return feature detector
Generate futuresets for classifier ",,,Return the feature detector that this tagger uses,,,1,,,,,,,,,
See classifier(),,,,,,,,,,,,,,,,,
"Bases: nltk.tag.sequential.SequentialBackoffTagger, nltk.tag.api.FeaturesetTaggerI",,,,,,,,,,,,,,,,,
A sequential tagger that uses a classifier to choose the tag for each token in a sentence. The featureset input for the classifier is generated by a feature detector function:,,,,,Choose the tag for each token in a sentence,,Choose the tag for each token in a sentence,1,,,,,,,,,
Where tokens is the list of unlabeled tokens in the sentence; index is the index of the token for which feature detection should be performed; and history is list of the tags for all tokens before index.,,,,,,,,,,,"perform detection of tags
perform index of tags
perform list of tags","perform detection of tags
perform index of tags
perform list of tags",,,,,
Construct a new classifier-based sequential tagger.,,,,,Construct a new classifier-based sequential tagger,Construct a new classifier-based sequential tagger,Construct a new classifier-based sequential tagger,1,,,,,,,,,
"feature_detector – A function used to generate the featureset input for the classifier:: feature_detector(tokens, index, history) -> featureset",Generate featureset input for classifier,Generate featureset input for classifier,,,Generate the featureset input for the classified,,,1,,,generate featureset input for classifier,generate featureset input for classifier,,,,,
"train – A tagged corpus consisting of a list of tagged sentences, where each sentence is a list of (word, tag) tuples.",Train tagger on tagged sentences,,,,,,,,,,,,,,,,
"backoff – A backoff tagger, to be used by the new tagger if it encounters an unknown context.",Specify backoff tagger,,,,,,,,,,,,,,,,
"classifier_builder – A function used to train a new classifier based on the data in train. It should take one argument, a list of labeled featuresets (i.e., (featureset, label) tuples).",Function to train a new classifier based on old training data,Train classifier,Train classifier,Train classifier,Train a new classifier based on the data in train,Train a new classifier based on the data in train,Train a new classifier based on the data in train,0,,,,,,,,,
"classifier – The classifier that should be used by the tagger. This is only useful if you want to manually construct the classifier; normally, you would use train instead.",Specify classifier that should be used by the tagger,"Use classifier
Use train",,,Manually construct the classiefier,,Manually construct the classiefier,1,,,,,,,,,
"backoff – A backoff tagger, used if this tagger is unable to determine a tag for a given token.",Specify a backoff tagger,Determine tag for given token,,,,,,1,,,determine tag for given token,determine tag for given token,,,,,
"cutoff_prob – If specified, then this tagger will fall back on its backoff tagger if the probability of the most likely tag is less than cutoff_prob.",Specify a cutoff probability where the backoff tagger will be used,,,,,,,,,,,,,,,,
"Decide which tag should be used for the specified token, and return that tag. If this tagger is unable to determine a tag for the specified token, return None – do not consult the backoff tagger. This method should be overridden by subclasses of SequentialBackoffTagger.",,,,,Decide which tag should be used for the specified token,,,1,,,,,,,,,
str,,,,,,,,,,,,,,,,,
tokens (list) – The list of words that are being tagged.,Get list of tagged words,,,,,,,,,,,,,,,,
index (int) – The index of the word whose tag should be returned.,Get index of word whose tag should be returned,"Return index
Return word",,,,,,1,,,"return tag
return word","return tag
return word",,,,,
history (list(str)) – A list of the tags for all words before index.,list of tags for all words before index,,,,,,,,,,,,,,,,
Return the classifier that this tagger uses to choose a tag for each word in a sentence. The input for this classifier is generated using this tagger’s feature detector. See feature_detector(),Get classifier for the tagger,"Return classifier
Choose tag for word
Use feature detector
Generate input for classifier",,,Return the classifier that this tagger uses,,,1,,,,,,,,,
Return the feature detector that this tagger uses to generate featuresets for its classifier. The feature detector is a function with the signature:,Get feature detector that the tagger uses,"Return classifier
Generate featuresets for classifier",,,Return the classifier that this tagger uses,,,1,,,,,,,,,
See classifier(),,,,,,,,,,,,,,,,,
Bases: nltk.tag.sequential.SequentialBackoffTagger,,,,,,,,,,,,,,,,,
An abstract base class for sequential backoff taggers that choose a tag for a token based on the value of its “context”. Different subclasses are used to define different contexts.,,,,,Choose a tag for a token based on its value,,,1,,,,,,,,,
"A ContextTagger chooses the tag for a token by calculating the token’s context, and looking up the corresponding tag in a table. This table can be constructed manually; or it can be automatically constructed based on a training corpus, using the _train() factory method.",,,,,"Choose the tag for a token by calculating the token's context, and looking up the corresponding tag in a table",,,1,,,"choose tag by looking
choose tag by calculating
calculate context
use _ train() factory method","choose tag by looking
choose tag by calculating
calculate context",,,,,
_context_to_tag – Dictionary mapping contexts to tags.,,,,,,,,,,,,,,,,,
"Decide which tag should be used for the specified token, and return that tag. If this tagger is unable to determine a tag for the specified token, return None – do not consult the backoff tagger. This method should be overridden by subclasses of SequentialBackoffTagger.",,,,,Decide which tag should be used for the specified token,,,,,,,,,,,,
str,,,,,,,,,,,,,,,,,
tokens (list) – The list of words that are being tagged.,Get list of tagged words,,,,,,,,,,,,,,,,
index (int) – The index of the word whose tag should be returned.,Get index of word whose tag should be returned,"Return index
Return word",,,,,,1,,,"return tag
return word","return tag
return word",,,,,
history (list(str)) – A list of the tags for all words before index.,,,,,,,,,,,,,,,,,
the context that should be used to look up the tag for the specified token; or None if the specified token should not be handled by this tagger.,,,,,,,,,,,,,,,,,
(hashable),,,,,,,,,,,,,,,,,
The number of entries in the table used by this tagger to map from contexts to tags.,,,,,,,,,,,"use  by tagger
use  from contexts",,,,,,
Bases: nltk.tag.sequential.SequentialBackoffTagger,,,,,,,,,,,,,,,,,
A tagger that assigns the same tag to every token.,,,,,,,,,,,assign same tag,assign same tag,,,,,
"This tagger is recommended as a backoff tagger, in cases where a more powerful tagger is unable to assign a tag to the word (e.g. because the word was not seen during training).",,,,,,,,,,,assign tag to word,assign tag to word,,,,,
tag (str) – The tag to assign to each token,The tag to give each token,Assign tag to token,Assign tag to token,Assign tag to token,,,,1,,,,,,,,,
"Decide which tag should be used for the specified token, and return that tag. If this tagger is unable to determine a tag for the specified token, return None – do not consult the backoff tagger. This method should be overridden by subclasses of SequentialBackoffTagger.",,,,,"Decide which tag should be used for the specified token
Return that tag",,,1,,,,,,,,,
str,,,,,,,,,,,,,,,,,
tokens (list) – The list of words that are being tagged.,Get list of tagged words,,,,,,,,,,,,,,,,
index (int) – The index of the word whose tag should be returned.,Get index of word whose tag should be returned,,,,,,,,,,"return tag
return word","return tag
return word",,,,,
history (list(str)) – A list of the tags for all words before index.,list of tags for all words before index,,,,,,,,,,,,,,,,
Bases: nltk.tag.sequential.ContextTagger,,,,,,,,,,,,,,,,,
"A tagger that chooses a token’s tag based on its word string and on the preceding n word’s tags. In particular, a tuple (tags[i-n:i-1], words[i]) is looked up in a table, and the corresponding tag is returned. N-gram taggers are typically trained on a tagged corpus.",,,,,Choose a token's tag based on its word string,,,1,,,,,,,,,
"Train a new NgramTagger using the given training data or the supplied model. In particular, construct a new tagger whose table maps from each context (tag[i-n:i-1], word[i]) to the most frequent tag for that context. But exclude any contexts that are already tagged perfectly by the backoff tagger.",,,,,Train a new NgramTagger using the given training data or the supplied model,Train a new NgramTagger using the given training data or the supplied model,Train a new NgramTagger using the given training data or the supplied model,1,,,,,,,,,
"train – A tagged corpus consisting of a list of tagged sentences, where each sentence is a list of (word, tag) tuples.",Train on tagged sentences,,,,,,,,,,,,,,,,
"backoff – A backoff tagger, to be used by the new tagger if it encounters an unknown context.",Specify a backoff tagger,,,,,,,,,,,,,,,,
"cutoff – If the most likely tag for a context occurs fewer than cutoff times, then exclude it from the context-to-tag table for the new tagger.",Define a cutoff to use the backoff,Exclude from context-to-tag table,,,,,,1,,,exclude  from context-to-tag table,exclude  from context-to-tag table,,,,,
the context that should be used to look up the tag for the specified token; or None if the specified token should not be handled by this tagger.,,,,,,,,,,,,,,,,,
(hashable),,,,,,,,,,,,,,,,,
Bases: nltk.tag.sequential.SequentialBackoffTagger,,,,,,,,,,,,,,,,,
Regular Expression Tagger,,,,,,,,,,,,,,,,,
The RegexpTagger assigns tags to tokens by comparing their word strings to a series of regular expressions. The following tagger uses word suffixes to make guesses about the correct Brown Corpus part of speech tag:,,,,,Assign tags to tokens by comparing their word strings,,,1,,,,,,,,,
"regexps (list(tuple(str, str))) – A list of (regexp, tag) pairs, each of which indicates that a word matching regexp should be tagged with tag. The pairs will be evalutated in order. If none of the regexps match a word, then the optional backoff tagger is invoked, else it is assigned the tag None.",,,,,Evaluate pairs in order,,,1,,,,,,,,,
"Decide which tag should be used for the specified token, and return that tag. If this tagger is unable to determine a tag for the specified token, return None – do not consult the backoff tagger. This method should be overridden by subclasses of SequentialBackoffTagger.",,,,,"Decide which tag should be used for the specified token
Return that tag
Do not consult the backoff tagger",,,1,,,,,,,,,
str,,,,,,,,,,,,,,,,,
tokens (list) – The list of words that are being tagged.,Get list of tagged words,,,,,,,,,,,,,,,,
index (int) – The index of the word whose tag should be returned.,Get index of word whose tag should be returned,"Return index
Return word",,,,,,1,,,"return tag
return word","return tag
return word",,,,,
history (list(str)) – A list of the tags for all words before index.,list of tags for all words before index,,,,,,,,,,,,,,,,
Bases: nltk.tag.api.TaggerI,,,,,,,,,,,,,,,,,
"An abstract base class for taggers that tags words sequentially, left to right. Tagging of individual words is performed by the choose_tag() method, which should be defined by subclasses. If a tagger is unable to determine a tag for the specified token, then its backoff tagger is consulted.",,,,,"Tag words sequentually, left to right","Tag words sequentually, left to right",,1,,,,,,,,,
"_taggers – A list of all the taggers that should be tried to tag a token (i.e., self and its backoff taggers).",,,,,,,,,,,,,,,,,
The backoff tagger for this tagger.,,,,,,,,,,,,,,,,,
"Decide which tag should be used for the specified token, and return that tag. If this tagger is unable to determine a tag for the specified token, return None – do not consult the backoff tagger. This method should be overridden by subclasses of SequentialBackoffTagger.",,,,,"Decide which tag should be used for the specified token
Return that tag",,,1,,,,,,,,,
str,,,,,,,,,,,,,,,,,
tokens (list) – The list of words that are being tagged.,Get list of tagged words,,,,,,,,,,,,,,,,
index (int) – The index of the word whose tag should be returned.,Get index of word whose tag should be returned,"Return index
Return word",,,,,,1,,,"return tag
return word","return tag
return word",,,,,
history (list(str)) – A list of the tags for all words before index.,list of tags for all words before index,,,,,,,,,,,,,,,,
"Determine the most appropriate tag sequence for the given token sequence, and return a corresponding list of tagged tokens. A tagged token is encoded as a tuple (token, tag).",,,,,Determine the most appropriate tag sequence for the given token sequence,,,1,,,,,,,,,
"list(tuple(str, str))",,,,,,,,,,,,,,,,,
"Determine an appropriate tag for the specified token, and return that tag. If this tagger is unable to determine a tag for the specified token, then its backoff tagger is consulted.",,,,,Determine an appropriate tag for the specified token,,,1,,,,,,,,,
str,,,,,,,,,,,,,,,,,
tokens (list) – The list of words that are being tagged.,Get list of tagged words,,,,,,,,,,,,,,,,
index (int) – The index of the word whose tag should be returned.,Get index of word whose tag should be returned,"Return index
Return word",,,,,,1,,,"return tag
return word","return tag
return word",,,,,
history (list(str)) – A list of the tags for all words before index.,list of tags for all words before index,,,,,,,,,,,,,,,,
Bases: nltk.tag.sequential.NgramTagger,,,,,,,,,,,,,,,,,
"A tagger that chooses a token’s tag based its word string and on the preceding two words’ tags. In particular, a tuple consisting of the previous two tags and the word is looked up in a table, and the corresponding tag is returned.",,,,,Chosoe a token's tag based on its word string,,,1,,,,,,,,,
"train (list(list(tuple(str, str)))) – The corpus of training data, a list of tagged sentences",Train on tagged sentences,,,,,,,,,,,,,,,,
model (dict) – The tagger model,Get the tagger model,,,,,,,,,,,,,,,,
backoff (TaggerI) – Another tagger which this tagger will consult when it is unable to tag a word,Specify a backoff tagger,,,,Consult tagger,,,1,,,,,,,,,
cutoff (int) – The number of instances of training data the tagger must see in order not to use the backoff tagger,Define a cutoff to use the backoff,,,,,,,,,,,,,,,,
Bases: nltk.tag.sequential.NgramTagger,,,,,,,,,,,,,,,,,
Unigram Tagger,,,,,,,,,,,,,,,,,
"The UnigramTagger finds the most likely tag for each word in a training corpus, and then uses that information to assign tags to new tokens.",,Find most likely tag,,,Find most likely tag for each word in the training corpus,,,1,,,"find likely tag for word
use information
assign tags to new tokens","find likely tag for word
assign tags to new tokens",,,,,
"train (list(list(tuple(str, str)))) – The corpus of training data, a list of tagged sentences",Train on tagged sentences,,,,,,,,,,,,,,,,
model (dict) – The tagger model,Get the tagger model,,,,,,,,,,,,,,,,
backoff (TaggerI) – Another tagger which this tagger will consult when it is unable to tag a word,Specify a backoff tagger,,,,Consult tagger,,,1,,,,,,,,,
cutoff (int) – The number of instances of training data the tagger must see in order not to use the backoff tagger,Define a cutoff to use the backoff,,,,,,,,,,,,,,,,
the context that should be used to look up the tag for the specified token; or None if the specified token should not be handled by this tagger.,,,,,,,,,,,,,,,,,
(hashable),,,,,,,,,,,,,,,,,
A module for interfacing with the Stanford taggers.,,,,,Interface with the Stanford taggers,,Interface with the Stanford taggers,,,,,,,,,,
Tagger models need to be downloaded from https://nlp.stanford.edu/software and the STANFORD_MODELS environment variable set (a colon-separated list of paths).,,,,,Download tagger models,,Download tagger models,,,,"download  from https://nlp.stanford.edu/software
download  from STANFORD_MODELS environment variable set","download  from https://nlp.stanford.edu/software
download  from STANFORD_MODELS environment variable set",,,,,
For more details see the documentation for StanfordPOSTagger and StanfordNERTagger.,,,,,See the documentation,,,1,,,,,,,,,
Bases: nltk.tag.stanford.StanfordTagger,,,,,,,,,,,,,,,,,
A class for Named-Entity Tagging with Stanford Tagger. The input is the paths to:,,,,,,,,,,,,,,,,,
a model trained on training data,,,,,,,,,,,,,,,,,
"(optionally) the path to the stanford tagger jar file. If not specified here, then this jar file must be specified in the CLASSPATH envinroment variable.",,,,,,,,,,,specify jar file in CLASSPATH envinroment variable,specify jar file in CLASSPATH envinroment variable,,,,,
(optionally) the encoding of the training data (default: UTF-8),,,,,,,,,,,,,,,,,
Example:,,,,,,,,,,,,,,,,,
Bases: nltk.tag.stanford.StanfordTagger,,,,,,,,,,,,,,,,,
a model trained on training data,,,,,,,,,,,,,,,,,
"(optionally) the path to the stanford tagger jar file. If not specified here, then this jar file must be specified in the CLASSPATH envinroment variable.",,,,,Specify jar file in the CLASSPATH environment variable,,,,,,specify jar file in CLASSPATH envinroment variable,specify jar file in CLASSPATH envinroment variable,,,,,
(optionally) the encoding of the training data (default: UTF-8),,,,,,,,,,,,,,,,,
Example:,,,,,,,,,,,,,,,,,
Bases: nltk.tag.api.TaggerI,,,,,,,,,,,,,,,,,
An interface to Stanford taggers. Subclasses must define:,,,,,,,,,,,,,,,,,
_cmd property: A property that returns the command that will be executed.,,,,,,,,,,,"return command
return property
execute command","return command
return property
execute command",,,,,
_SEPARATOR: Class constant that represents that character that is used to separate the tokens from their tags.,,,,,Represent that character that is used to separate the tokens,,,1,,,"separate tokens from tags
use character",separate tokens from tags,,,,,
_JAR file: Class constant that represents the jar file name.,,,,,,,,,,,,,,,,,
"Determine the most appropriate tag sequence for the given token sequence, and return a corresponding list of tagged tokens. A tagged token is encoded as a tuple (token, tag).",,Determine tag sequence,,,Determine the most appropriate tag sequence for the given token sequence,,,1,,,,,,,,,
"list(tuple(str, str))",,,,,,,,,,,,,,,,,
Apply self.tag() to each element of sentences. I.e.:,,,,,,,,,,,,,,,,,
return [self.tag(sent) for sent in sentences],,,,,,,,,,,"return [ self.tag(sent)
send  in sentences ]","return [ self.tag(sent)
send  in sentences ]",,,,,
Implementation of ‘TnT - A Statisical Part of Speech Tagger’ by Thorsten Brants,,,,,,,,,,,,,,,,,
http://acl.ldc.upenn.edu/A/A00/A00-1031.pdf,,,,,,,,,,,,,,,,,
Bases: nltk.tag.api.TaggerI,,,,,,,,,,,,,,,,,
TnT - Statistical POS tagger,,,,,,,,,,,,,,,,,
IMPORTANT NOTES:,,,,,,,,,,,,,,,,,
DOES NOT AUTOMATICALLY DEAL WITH UNSEEN WORDS,,,,,Deal with unseen words,,Deal with unseen words,1,,,,,,,,,
"It is possible to provide an untrained POS tagger to create tags for unknown words, see __init__ function",,Create tags for unknown words,Create tags for unknown words,,Provide an untrained POS tagger to create tags for unknown words,Provide an untrained POS tagger to create tags for unknown words,,1,,,"provide untrained POS tagger
create tags for unknown words","provide untrained POS tagger
create tags for unknown words",,,,,
SHOULD BE USED WITH SENTENCE-DELIMITED INPUT,,,,,,,,,,,,,,,,,
"Due to the nature of this tagger, it works best when trained over sentence delimited input.",,,,,Train over sentence delimited input,,Train over sentence delimited input,,,,,,,,,,
"However it still produces good results if the training data and testing data are separated on all punctuation eg: [,.?!]",,,,,,,,,,,"produce good results
separate training data on punctuation eg
separate testing data on punctuation eg","produce good results
separate training data on punctuation eg
separate testing data on punctuation eg",,,,,
"Input for training is expected to be a list of sentences where each sentence is a list of (word, tag) tuples",,,,,,,,,,,,,,,,,
Input for tag function is a single sentence Input for tagdata function is a list of sentences Output is of a similar form,,,,,,,,,,,,,,,,,
Function provided to process text that is unsegmented,,,,,,,,,,,process text,process text,,,,,
Please see basic_sent_chop(),,,,,,,,,,,,,,,,,
"TnT uses a second order Markov model to produce tags for a sequence of input, specifically:",,,,,Use a second order Markov model to product tags for a sequence of input,,Use a second order Markov model to product tags for a sequence of input,,,,produce tags for sequence,produce tags for sequence,,,,,
"argmax [Proj(P(t_i|t_i-1,t_i-2)P(w_i|t_i))] P(t_T+1 | t_T)",,,,,,,,,,,,,,,,,
IE: the maximum projection of a set of probabilities,,,,,,,,,,,,,,,,,
The set of possible tags for a given word is derived from the training data. It is the set of all tags that exact word has been assigned.,,,,,Dervice set of all possible tags for a given word,,Dervice set of all possible tags for a given word,,,,assign exact word,assign exact word,,,,,
"To speed up and get more precision, we can use log addition to instead multiplication, specifically:",,,,,Use log addition to instead multiplication,,,1,,,"get more precision
use log addition",get more precision,,,,,
log(P(t_T+1|t_T)),,,,,,,,,,,,,,,,,
"The probability of a tag for a given word is the linear interpolation of 3 markov models; a zero-order, first-order, and a second order model.",,,,,,,,,,,,,,,,,
"l3*P(t_i| t_i-1, t_i-2)",,,,,,,,,,,,,,,,,
A beam search is used to limit the memory usage of the algorithm. The degree of the beam can be changed using N in the initialization. N represents the maximum number of possible solutions to maintain while tagging.,,,,,Limit the memory usage of the algorithm,,,1,,,"limit memory usage of algorithm
use beam search
use n in initialization
change degree of beam",limit memory usage of algorithm,,,,,
It is possible to differentiate the tags which are assigned to capitalized words. However this does not result in a significant gain in the accuracy of the results.,,,,,Differentiate the tags which are assigned to capitilized words,Differentiate the tags which are assigned to capitilized words,Differentiate the tags which are assigned to capitilized words,1,,,"differentiate tags
assign tags to capitalized words","differentiate tags
assign tags to capitalized words",,,,,
Tags a single sentence,,,,,Tag a single sentence,,Tag a single sentence,,,,,,,,,,
"data ([string,]) – list of words",,,,,,,,,,,,,,,,,
"[(word, tag),]",,,,,,,,,,,,,,,,,
Calls recursive function ‘_tagword’ to produce a list of tags,,,,,Call recursive function,,,,,,produce list of tags,produce list of tags,,,,,
Associates the sequence of returned tags with the correct words in the input sequence,,,,,Associate the sequence of returned tags with the correct words in the input sequence,,,1,,,"return tags with correct words
return tags in input sequence","return tags with correct words
return tags in input sequence",,,,,
"returns a list of (word, tag) tuples",,,,,return a list of tuples,,,,,,return list of tuples,return list of tuples,,,,,
Tags each sentence in a list of sentences,Tag each sentence in list of sentences,,,,Tag each sentence in a list of sentences,,Tag each sentence in a list of sentences,,,,,,,,,,
":param data:list of list of words :type data: [[string,],] :return: list of list of (word, tag) tuples",,,,,,,,,,,,,,,,,
"Invokes tag(sent) function for each sentence compiles the results into a list of tagged sentences each tagged sentence is a list of (word, tag) tuples",,,,,Compile the results into a list of tagged sentences,,,,,,compile results into list,compile results into list,,,,,
"Uses a set of tagged data to train the tagger. If an unknown word tagger is specified, it is trained on the same data.",,,,,Use a set of tagged data to train the tagger,Use a set of tagged data to train the tagger,Use a set of tagged data to train the tagger,1,,,"use set of tagged data
specify unknown word tagger",specify unknown word tagger,,,,,
"data (tuple(str)) – List of lists of (word, tag) tuples",,,,,,,,,,,,,,,,,
Basic method for tokenizing input into sentences for this tagger:,,,,,Tokenize input into sentences for this tagger,,Tokenize input into sentences for this tagger,,,,,,,,,,
"data (str or tuple(str, str)) – list of tokens (words or (word, tag) tuples)",,,,,,,,,,,,,,,,,
raw (bool) – boolean flag marking the input data as a list of words or a list of tagged words,,,,,,,,,,,"mark input data as list
mark input data as list","mark input data as list
mark input data as list",,,,,
list of sentences sentences are a list of tokens tokens are the same as the input,,,,,,,,,,,,,,,,,
Function takes a list of tokens and separates the tokens into lists where each list represents a sentence fragment This function can separate both tagged and raw sequences into basic sentences.,,Seperate tokens into lists,,,"Separate tokens into list
Separate both tagged and raw sequences into basic sentences",Separate both tagged and raw sequences into basic sentences,,1,,,"separate tokens into lists
separate tagged raw sequences into basic sentences","separate tokens into lists
separate tagged raw sequences into basic sentences",,,,,
"Sentence markers are the set of [,.!?]",,,,,,,,,,,,,,,,,
This is a simple method which enhances the performance of the TnT tagger. Better sentence tokenization will further enhance the results.,,,,,Enhance the performance of the TnT tagger,Enhance the performance of the TnT tagger,,1,,,,,,,,,
"Given the string representation of a tagged token, return the corresponding tuple representation. The rightmost occurrence of sep in s will be used to divide s into a word string and a tag string. If sep does not occur in s, return (s, None).",,Return tuple representation,,,Return the corresponding tuple representation,,,1,,,,,,,,,
s (str) – The string representation of a tagged token.,,,,,,,,,,,,,,,,,
sep (str) – The separator string used to separate word strings from tags.,,,,,,,,,,,"use  from tags
use  to separate word strings",,,,,,
"Given the tuple representation of a tagged token, return the corresponding string representation. This representation is formed by concatenating the token’s word string, followed by the separator, followed by the token’s tag. (If the tag is None, then just return the bare word string.)",,"Return string representation
Return bare word string",,,"Return the corresponding string representation
Return the bare word string",,,0,,,,,,,,,
"tagged_token (tuple(str, str)) – The tuple representation of a tagged token.",,,,,,,,,,,,,,,,,
sep (str) – The separator string used to separate word strings from tags.,,Seperate word strings from tags,,,Separate word strings from tags,,,0,,,"use  from tags
use  to separate word strings",,,,,,
"Given a tagged sentence, return an untagged version of that sentence. I.e., return a list containing the first element of each tuple in tagged_sentence.",,Return untagged version of sentence,,,Return an untagged version of that sentence,,,0,,,,,,,,,
NLTK Taggers,,,,,,,,,,,,,,,,,
"This package contains classes and interfaces for part-of-speech tagging, or simply “tagging”.",,,,,,,,,,,,,,,,,
"A “tag” is a case-sensitive string that specifies some property of a token, such as its part of speech. Tagged tokens are encoded as tuples (tag, token). For example, the following tagged token combines the word 'fly' with a noun part of speech tag ('NN'):",,,,,Specify some property of a token,,,1,,,,,,,,,
An off-the-shelf tagger is available for English. It uses the Penn Treebank tagset:,,,,,,,,,,,use Penn treebank tagset,,,,,,
A Russian tagger is also available if you specify lang=”rus”. It uses the Russian National Corpus tagset:,,,,,,,,,,,use Russian national Corpus tagset,,,,,,
"This package defines several taggers, which take a list of tokens, assign a tag to each one, and return the resulting list of tagged tokens. Most of the taggers are built automatically based on a training corpus. For example, the unigram tagger tags each word w by checking what the most frequent tag for w was in a training corpus:",,,,,,,,,,,"define several taggers
assign tag
assign several taggers
return resulting list of tagged tokens
return several taggers of tagged tokens","assign tag
assign several taggers
return resulting list of tagged tokens
return several taggers of tagged tokens",,,,,
Note that words that the tagger has not seen during training receive a tag of None.,,,,,,,,,,,receive tag of none,receive tag of none,,,,,
We evaluate a tagger on data that was not seen during training:,,,,,,,,,,,,,,,,,
"For more information, please consult chapter 5 of the NLTK Book.",,,,,,,,,,,,,,,,,
Use NLTK’s currently recommended part of speech tagger to tag the given list of tokens.,,,,,Use NLTK’s currently recommended part of speech tagger to tag the given list of tokens.,,,1,,,,,,,,,
NB. Use pos_tag_sents() for efficient tagging of more than one sentence.,,,,,Use pos_tag_sents() for efficient tagging of more than one sentence,,,1,,,use pos_tag_sents() for efficient tagging,,,,,,
tokens (list(str)) – Sequence of tokens to be tagged,,,,,,,,,,,,,,,,,
"tagset (str) – the tagset to be used, e.g. universal, wsj, brown",,,,,,,,,,,,,,,,,
"lang (str) – the ISO 639 code of the language, e.g. ‘eng’ for English, ‘rus’ for Russian",,,,,,,,,,,,,,,,,
The tagged tokens,,,,,,,,,,,,,,,,,
"list(tuple(str, str))",,,,,,,,,,,,,,,,,
"Use NLTK’s currently recommended part of speech tagger to tag the given list of sentences, each consisting of a list of tokens.",,,,,,,,,,,,,,,,,
sentences (list(list(str))) – List of sentences to be tagged,,,,,,,,,,,,,,,,,
"tagset (str) – the tagset to be used, e.g. universal, wsj, brown",,,,,,,,,,,,,,,,,
"lang (str) – the ISO 639 code of the language, e.g. ‘eng’ for English, ‘rus’ for Russian",,,,,,,,,,,,,,,,,
The list of tagged sentences,,,,,,,,,,,,,,,,,
"list(list(tuple(str, str)))",,,,,,,,,,,,,,,,,